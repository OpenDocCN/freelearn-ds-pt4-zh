<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch008.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="the-geometric-structure-of-vector-spaces" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">2</span></span><br/>
<span id="x1-390003"></span><span class="cmss-10x-x-109">The Geometric Structure of Vector Spaces</span></h1>
<p><span class="cmss-10x-x-109">Let’s revisit the Iris dataset introduced in the previous chapter! I want to test your intuition. I plotted the petal widths against the petal lengths while hiding the class labels in </span><span class="cmssi-10x-x-109">Figure 2.1:</span></p>
<div id="tcolobox-55" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris 
 
# Load the iris dataset 
iris = load_iris() 
data = iris.data 
 
# Extract petal length (3rd column) and petal width (4th column) 
petal_length = data[:, 2] 
petal_width = data[:, 3] 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    # Create the scatter plot 
    plt.figure(figsize=(7, 7)) 
    plt.scatter(petal_length, petal_width, color=’indigo’, alpha=0.8, edgecolor=’none’, s=70, marker=’o’) 
    plt.xlabel(’petal length (cm)’) 
    plt.ylabel(’petal width (cm)’) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file69.png" width="284" height="284" alt="PIC"/> <span id="x1-39019r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.1: The “petal width” and “petal length” features of the Iris dataset</span> </span>
</div>
<p><span class="cmss-10x-x-109">Even without knowing any labels, we can intuitively point out that there are probably at least two classes. Can you summarize your reasoning in a single sentence?</span></p>
<p><span class="cmss-10x-x-109">There are many valid arguments, but the most prevalent one is that the two clusters are </span><span class="cmssi-10x-x-109">far </span><span class="cmss-10x-x-109">away from each other. As this example illustrates, the concept of distance plays an essential role in machine learning. In this chapter, we will translate the notion of distance into the language of mathematics and put it into the context of vector spaces.</span></p>
<section id="norms-and-distances" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_26"><span class="titlemark"><span class="cmss-10x-x-109">2.1 </span></span> <span id="x1-400003.1"></span><span class="cmss-10x-x-109">Norms and distances</span></h2>
<p><span class="cmss-10x-x-109">Previously, we</span><span id="dx1-40001"></span> <span class="cmss-10x-x-109">saw that vectors are essentially arrows, starting from the null vector. In addition</span><span id="dx1-40002"></span> <span class="cmss-10x-x-109">to their direction, vectors also have </span><span class="cmssi-10x-x-109">magnitude</span><span class="cmss-10x-x-109">. For example, as we have learned in high school mathematics, the magnitude in the Euclidean plane is defined by</span></p>
<div class="math-display">
<img src="../media/file70.png" class="math-display" alt=" ∘ ------- 2 2 ∥x ∥ = x1 + x 2, x = (x1,x2), "/>
</div>
<p><span class="cmss-10x-x-109">while we can calculate the distance between </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">as</span></p>
<div class="math-display">
<img src="../media/file71.png" class="math-display" alt=" ∘ --------2-----------2 d(x,y) = (x1 − y1) + (x2 − y2) . "/>
</div>
<p><span class="cmss-10x-x-109">(The function ∥⋅∥ </span> <span class="cmss-10x-x-109">simply denotes the magnitude of a vector.)</span></p>
<div class="minipage">
<p><img src="../media/file73.png" width="284" height="284" alt="PIC"/> <span id="x1-40003r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.2: Magnitude in the Euclidean plane</span> </span>
</div>
<p><span class="cmss-10x-x-109">The magnitude formula</span> <img src="../media/file74.png" class="sqrt" width="50" alt="∘ ------- x21 + x22"/> <span class="cmss-10x-x-109">can be simply generalized to higher dimensions by</span></p>
<div class="math-display">
<img src="../media/file75.png" class="math-display" alt=" ∘ ------------ 2 2 n ∥x ∥ = x1 + ⋅⋅⋅+ xn, x = (x1,...,xn) ∈ ℝ . "/>
</div>
<p><span class="cmss-10x-x-109">However, just from looking at this formula, it is not clear why it is defined this way. What does the square root of a sum of squares have to do with distance and magnitude? Behind the scenes, it is just the Pythagorean theorem.</span></p>
<p><span class="cmss-10x-x-109">Recall that the</span> <span id="dx1-40004"></span><span class="cmss-10x-x-109">Pythagorean theorem states that in right triangles, the</span><span id="dx1-40005"></span> <span class="cmss-10x-x-109">squared length of the</span> <span id="dx1-40006"></span><span class="cmss-10x-x-109">hypotenuse equals the sum of the squared lengths of the other sides, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">2.3</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file76.png" width="284" height="284" alt="PIC"/> <span id="x1-40007r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.3: The Pythagorean theorem</span> </span>
</div>
<p><span class="cmss-10x-x-109">To put this into an algebraic form, it states that </span><span class="cmmi-10x-x-109">a</span><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">b</span><sup><span class="cmr-8">2</span></sup> = <span class="cmmi-10x-x-109">c</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, when </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">is the hypotenuse of the right triangle, and </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">b </span><span class="cmss-10x-x-109">are its two other sides. If we apply this to a two-dimensional vector </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>)<span class="cmss-10x-x-109">, we can see that the Pythagorean theorem gives its magnitude </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥ </span>= <img src="../media/file77.png" width="50" class="sqrt" alt="∘ -2----2 x1 + x2"/> <span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">This can be generalized to higher dimensions. To see what is happening, we are going to check the three-dimensional case, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">1.4</span></a><span class="cmss-10x-x-109">. Here, we can apply the Pythagorean theorem twice to obtain the magnitude!</span></p>
<div class="minipage">
<p><img src="../media/file78.png" width="284" height="284" alt="PIC"/> <span id="x1-40008r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.4: The Pythagorean theorem in three dimensions</span> </span>
</div>
<p><span class="cmss-10x-x-109">For each vector </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">3</span></sub>)<span class="cmss-10x-x-109">, we can take a look at the triangle determined by</span> (0<span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">,</span>0)<span class="cmmi-10x-x-109">,</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">,</span>0)<span class="cmss-10x-x-109">, and </span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>0) <span class="cmss-10x-x-109">first. The length of the hypotenuse can be calculated by</span> <img src="../media/file79.png" class="sqrt" alt="∘ ------- x21 + x22" width="50"/><span class="cmss-10x-x-109">. However, the points </span>(0<span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">,</span>0)<span class="cmss-10x-x-109">, </span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>0)<span class="cmss-10x-x-109">, and </span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">3</span></sub>) <span class="cmss-10x-x-109">form a right triangle. Applying the Pythagorean</span><span id="dx1-40009"></span> <span class="cmss-10x-x-109">theorem once again, we obtain</span></p>
<div class="math-display">
<img src="../media/file80.png" class="math-display" alt=" ∘ ------------ ∥x∥ = x2 + x2+ x2, 1 2 3 "/>
</div>
<p><span class="cmss-10x-x-109">which is called the </span><span class="cmssi-10x-x-109">Euclidean norm</span><span class="cmss-10x-x-109">. This is exactly what is going</span><span id="dx1-40010"></span> <span class="cmss-10x-x-109">on in the general </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional case.</span></p>
<p><span class="cmss-10x-x-109">The notions of </span><span class="cmssi-10x-x-109">magnitude </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">distance </span><span class="cmss-10x-x-109">are critical in machine learning, as we can use them to determine the similarity between data points, measure and control the complexity of neural networks, and much more.</span></p>
<p><span class="cmss-10x-x-109">Is the Pythagorean theorem the only viable way to measure magnitude and distance? Certainly not.</span></p>
<p><span class="cmss-10x-x-109">Because Manhattan’s street layout is essentially a rectangular grid, its residents are famed for measuring distances in blocks. If something is two blocks to the north and three blocks east, it means that you have to</span><span id="dx1-40011"></span> <span class="cmss-10x-x-109">travel two intersections to the north and three to the east to find it. This gives rise to a mathematically perfectly valid notion of measurement called </span><span class="cmssi-10x-x-109">Manhattan distance</span><span class="cmss-10x-x-109">, defined by</span></p>
<div class="math-display">
<img src="../media/file81.png" class="math-display" alt="d(x,y ) = |x1 − y1| + |x2 − y2|. "/>
</div>
<p><span class="cmss-10x-x-109">When using the Manhattan distance, the shortest path between two points is</span> <span id="dx1-40012"></span><span class="cmss-10x-x-109">not unique.</span></p>
<div class="minipage">
<p><img src="../media/file82.png" width="284" height="284" alt="PIC"/> <span id="x1-40013r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.5: For the Manhattan distance, the shortest path between two points is not unique</span> </span>
</div>
<p><span class="cmss-10x-x-109">Besides the Euclidean and Manhattan distances, there are several other metrics. Once again, we are going to step away from the concrete examples to take an abstract viewpoint.</span></p>
<p><span class="cmss-10x-x-109">If we talk about measurements and metrics in general, what are the properties that we expect from all of them? What makes a measurement </span><span class="cmssi-10x-x-109">distance</span><span class="cmss-10x-x-109">? Essentially, there are three such traits:</span></p>
<ul>
<li><span class="cmss-10x-x-109">the distance should be nonnegative,</span></li>
<li><span class="cmss-10x-x-109">it should preserve scaling (that is, </span><img src="../media/file83.png" width="151" class="math" alt="d(cx,cy) = cd(x,y ) "/> <span class="cmss-10x-x-109">for all scalars</span> <img src="../media/file84.png" width="11" class="math" alt="c "/><span class="cmss-10x-x-109">),</span></li>
<li><span class="cmss-10x-x-109">the distance straight from point </span><img src="../media/file85.png" class="math" width="11" alt="x "/> <span class="cmss-10x-x-109">to </span><img src="../media/file86.png" width="11" class="math" alt="y "/> <span class="cmss-10x-x-109">is always equal to or smaller than touching any other point </span><img src="../media/file87.png" class="math" width="11" alt="z "/><span class="cmss-10x-x-109">.</span></li>
</ul>
<p><span class="cmss-10x-x-109">These are formalized by</span> <span id="dx1-40014"></span><span class="cmss-10x-x-109">the notion of </span><span class="cmssi-10x-x-109">norms</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-40015r7"></span> <span class="cmbx-10x-x-109">Definition 7.</span> </span> <span class="cmbx-10x-x-109">(Norms)</span></p>
<p>Let <img src="../media/file88.png" class="math" width="11" alt="V "/> be a vector space. A function <img src="../media/file89.png" class="math" width="101" alt="∥⋅∥ : V → [0,∞ ) "/> is said to be a <span class="cmbx-10x-x-109">norm </span>if for all <img src="../media/file90.png" class="math" width="11" alt="x, y ∈ V "/>, the following properties hold:</p>
<ol>
<li><span id="x1-40017x1"><span class="cmbx-10x-x-109">Positive definiteness</span>: <img src="../media/file91.png" class="math" width="51" alt="∥x ∥ ≥ 0 "/> and <img src="../media/file92.png" class="math" width="51" alt="∥x∥ = 0 "/> if and only if <img src="../media/file93.png" class="math" width="51" alt="x = 0 "/>.</span></li>
<li><span id="x1-40019x2"><span class="cmbx-10x-x-109">Positive homogeneity</span>: <img src="../media/file94.png" class="math" width="101" alt="∥cx∥ = |c|∥x ∥ "/> for all <img src="../media/file95.png" class="math" width="51" alt="c ∈ ℝ "/>.</span></li>
<li><span id="x1-40021x3"><span class="cmbx-10x-x-109">Triangle inequality</span>: <img src="../media/file96.png" class="math" width="151" alt="∥x + y ∥ ≤ ∥x ∥+ ∥y ∥ "/> for all <img src="../media/file97.png" class="math" width="51" alt="x,y ∈ V "/>.</span></li>
</ol>
<p>A vector space equipped with a norm is <span id="dx1-40022"></span>called a <span class="cmbx-10x-x-109">normed space</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Let’s see some</span> <span id="dx1-40023"></span><span class="cmss-10x-x-109">examples!</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">p </span><span class="cmsy-10x-x-109">∈</span> [1<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmss-10x-x-109">and define</span></p>
<div class="math-display">
<img src="../media/file98.png" class="math-display" alt=" ∑n p 1∕p ∥x∥p = ( |xi|) , x = (x1,...,xn ) i=1 "/>
</div>
<p><span class="cmss-10x-x-109">on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. The function </span><span class="cmsy-10x-x-109">∥⋅∥</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">is called the </span><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">-norm. Showing that </span><span class="cmsy-10x-x-109">∥⋅∥</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">is indeed</span><span id="dx1-40024"></span> <span class="cmss-10x-x-109">a norm is a bit technical. Thus, we won’t go into the details. (The triangle inequality requires some work, but the other two properties are easy to see.)</span></p>
<p><span class="cmss-10x-x-109">We have already seen</span> <span id="dx1-40025"></span><span class="cmss-10x-x-109">two special cases: the Euclidean norm (</span><span class="cmmi-10x-x-109">p </span>= 2<span class="cmss-10x-x-109">) and the</span> <span id="dx1-40026"></span><span class="cmss-10x-x-109">Manhattan norm (</span><span class="cmmi-10x-x-109">p </span>= 1<span class="cmss-10x-x-109">). Both of them frequently appear in machine learning. For instance, the familiar mean squared error is just the scaled Euclidean distance between prediction and ground truth:</span></p>
<div class="math-display">
<img src="../media/file99.png" class="math-display" alt=" n MSE (y,yˆ) =-1∥y − ˆy∥2 = 1-∑ (y − ˆy )2 n 2 n i i i=1 "/>
</div>
<p><span class="cmss-10x-x-109">As mentioned before, the </span>2<span class="cmss-10x-x-109">-norm, along with the </span>1<span class="cmss-10x-x-109">-norm, is commonly used to control the complexity of models during training. To give a concrete example, suppose that we are fitting a polynomial </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=0</span></sub><sup><span class="cmmi-8">m</span></sup><span class="cmmi-10x-x-109">q</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">x</span><sup><span class="cmmi-8">i</span></sup> <span class="cmss-10x-x-109">to the data </span><span class="cmsy-10x-x-109">{</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,y</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub><span class="cmmi-10x-x-109">,y</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">. To obtain a model that generalizes well to new data, we prefer our models to be as simple as possible. Thus, instead of using the plain mean squared error, we might consider minimizing the loss:</span></p>

<img src="../media/file100.png" width="450" class="math-display" alt="Loss(y,ˆy,q) = MSE (y, ˆy)+ λ∥q ∥p, q = (q0,q1,...,qm), λ ∈ [0,∞ ) "/>

<p><span class="cmss-10x-x-109">where the term </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">q</span><span class="cmsy-10x-x-109">∥</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">is responsible for keeping the coefficients of the polynomial </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">small, while </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">controls the strength of regularization. Usually, </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">is either </span>1 <span class="cmss-10x-x-109">or </span>2<span class="cmss-10x-x-109">, but other values from</span> [1<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmss-10x-x-109">are also valid.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">Let’s stay in </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">for a bit longer! The so-called </span><span class="cmsy-10x-x-109">∞</span><span class="cmss-10x-x-109">-norm is</span> <span id="dx1-40027"></span><span class="cmss-10x-x-109">defined by</span></p>
<div class="math-display">
<img src="../media/file101.png" class="math-display" alt="∥x ∥ = max {|x |,...,|x |}. ∞ 1 n "/>
</div>
<p><span class="cmss-10x-x-109">Showing that </span><span class="cmsy-10x-x-109">∥⋅∥</span><sub><span class="cmsy-8">∞</span></sub> <span class="cmss-10x-x-109">is indeed a norm is a simple task and left to you for practice. (This is perhaps one of the most notorious sentences written in mathematical textbooks, but trust me, this is truly easy. Give it a shot! If you don’t see it, try the special case </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">This</span> <span id="dx1-40028"></span><span class="cmss-10x-x-109">is</span> <span id="dx1-40029"></span><span class="cmss-10x-x-109">called the </span><span class="cmsy-10x-x-109">∞</span><span class="cmss-10x-x-109">-norm, and is strongly related to the </span><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">-norm that we have just seen. In fact, if we let the value </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">grow infinitely, </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">will be very close to </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sub><span class="cmsy-8">∞</span></sub> <span class="cmss-10x-x-109">, ultimately reaching it at the limit.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-40030r2"></span> <span class="cmbx-10x-x-109">Remark 2.</span> </span><span class="cmbx-10x-x-109">(The </span><span class="cmsy-10x-x-109">∞</span><span class="cmbx-10x-x-109">-norm as the limit of </span><span class="cmmi-10x-x-109">p</span><span class="cmbx-10x-x-109">-norm)</span></p>
<p>If you are already familiar with convergent sequences and limits, you can see that this is called the <span class="cmsy-10x-x-109">∞</span>-norm because</p>
<div class="math-display">
<img src="../media/file102.png" class="math-display" alt="lim ∥x∥ = ∥x∥ . p→ ∞ p ∞ "/>
</div>
<p>To show this, consider that</p>

<img src="../media/file103.png" width="450" class="math-display" alt=" ∑n p 1∕p ∑n -|xi|-p 1∕p pli→m∞ ∥x ∥p = pl→im∞ ( |xi|) = pli→m∞ ∥x∥∞ ( (∥x ∥∞ )) . i=1 i=1 "/>

<p>Since <img src="../media/file104.png" class="frac" width="51" data-align="middle" alt="-|xi|- ∥x∥∞"/> <span class="cmsy-10x-x-109">≤ </span>1 by definition,</p>
<div class="math-display">
<img src="../media/file105.png" class="math-display" alt=" ∑n |xi| 1 ≤ ( (------)p)1∕p ≤ n1 ∕p i=1 ∥x ∥∞ "/>
</div>
<p>holds. Because</p>
<div class="math-display">
<img src="../media/file106.png" class="math-display" alt="lim n1∕p = 1, p→∞ "/>
</div>
<p>we can conclude that</p>
<div class="math-display">
<img src="../media/file107.png" class="math-display" alt="lim ∥x∥p = ∥x∥∞. p→ ∞ "/>
</div>
<p>This is the reason why the <span class="cmsy-10x-x-109">∞</span>-norm is considered a <span class="cmmi-10x-x-109">p</span>-norm with <span class="cmmi-10x-x-109">p </span>= <span class="cmsy-10x-x-109">∞</span>.</p>
<p>If you are not familiar with taking limits of sequences, don’t worry. We’ll cover everything in detail when studying single-variable calculus.</p>
</div>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmsy-10x-x-109">∞</span><span class="cmss-10x-x-109">-norms can be generalized for function spaces. Remember </span><span class="cmmi-10x-x-109">C</span>([0<span class="cmmi-10x-x-109">,</span>1])<span class="cmss-10x-x-109">, the vector space of functions continuous on</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">? We introduced this when talking about examples of vector spaces in </span><span class="cmssi-10x-x-109">Section </span><a href="ch007.xhtml#examples-of-vector-spaces"><span class="cmssi-10x-x-109">1.1.1</span></a><span class="cmss-10x-x-109">. There, </span><span class="cmsy-10x-x-109">∥⋅∥</span><sub><span class="cmsy-8">∞</span></sub> <span class="cmss-10x-x-109">can be defined as</span></p>
<div class="math-display">
<img src="../media/file108.png" class="math-display" alt="∥f∥∞ = sup |f (x )|. x∈[0,1] "/>
</div>
<p><span class="cmss-10x-x-109">This norm can be defined on other function spaces, like </span><span class="cmmi-10x-x-109">C</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmss-10x-x-109">, the space of continuous real functions. Since the maximum is not guaranteed to exist (as for the sigmoid function in </span><span class="cmmi-10x-x-109">C</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmss-10x-x-109">), the maximum is replaced with supremum. Hence, the </span><span class="cmsy-10x-x-109">∞</span><span class="cmss-10x-x-109">-norm is often called the </span><span class="cmssi-10x-x-109">supremum norm</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">If you imagine the</span> <span id="dx1-40031"></span><span class="cmss-10x-x-109">function as a landscape, the supremum norm is the height of the highest peak or the depth of the deepest trench (whichever is larger in absolute value).</span></p>
<div class="minipage">
<p><img src="../media/file109.png" width="241" height="241" alt="PIC"/> <span id="x1-40032r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.6: The supremum norm</span> </span>
</div>
<p><span class="cmss-10x-x-109">When</span> <span id="dx1-40033"></span><span class="cmss-10x-x-109">encountering this norm for the first time, it might seem challenging to understand what this has to do with any notion of magnitude. However, </span><span class="cmsy-10x-x-109">∥</span><span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">g</span><span class="cmsy-10x-x-109">∥</span><sub><span class="cmsy-8">∞</span></sub> <span class="cmss-10x-x-109">is a natural way to measure the distance between two functions </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">g</span><span class="cmss-10x-x-109">, and in general, magnitude is just the distance from </span>0<span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file110.png" width="241" height="241" alt="PIC"/> <span id="x1-40034r7"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.7: The distance between two functions, given by the supremum norm</span></span>
</div>
<section id="defining-distances-from-norms" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_27"><span class="titlemark"><span class="cmss-10x-x-109">2.1.1 </span></span> <span id="x1-410003.1.1"></span><span class="cmss-10x-x-109">Defining distances from norms</span></h3>
<p><span class="cmss-10x-x-109">Besides measuring</span> <span id="dx1-41001"></span><span class="cmss-10x-x-109">the magnitude of vectors, we are also interested in measuring the distance between them. If you are at the location </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">in some normed space, how far is </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">? In normed vector spaces, we can define the distance between any </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">by</span></p>
<div class="math-display">
<img src="../media/file111.png" class="math-display" alt="d(x,y) = ∥x− y∥. "/>
</div>
<p><span class="cmss-10x-x-109">This is called the</span><span id="dx1-41002"></span> <span class="cmss-10x-x-109">norm-induced metric. Thus, norms measure the distance from the zero vector, and the metric </span><span class="cmmi-10x-x-109">d </span><span class="cmss-10x-x-109">measures the norm of the difference.</span></p>
<p><span class="cmss-10x-x-109">In general, we say that a function </span><span class="cmmi-10x-x-109">d </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span> [0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmss-10x-x-109">is a </span><span class="cmssi-10x-x-109">metric </span><span class="cmss-10x-x-109">if the following hold.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-41003r8"></span> <span class="cmbx-10x-x-109">Definition 8.</span> </span><span class="cmbx-10x-x-109">(Metrics)</span></p>
<p>Let <img src="../media/file112.png" class="math" width="11" alt="V "/> be a vector space and <span class="cmmi-10x-x-109">d </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span> [0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>) be a function. d is a <span class="cmti-10x-x-109">metric </span>if the following conditions hold for all x,y,z ∈ V:</p>
<ol>
<li><span id="x1-41005x1">Whenever d(x,y ) = 0 , we have x = y (<span class="cmti-10x-x-109">positive definiteness</span>).</span></li>
<li><span id="x1-41007x2">d (x, y) = d (y, x) (<span class="cmti-10x-x-109">symmetry</span>).</span></li>
<li><span id="x1-41009x3">d (x, z) ≤ d(x,y )+ d(y,z)(<span class="cmti-10x-x-109">triangle inequality</span>).</span></li>
</ol>
</div>
<p><span class="cmss-10x-x-109">One of the immediate consequences of the definition is that if </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">≠</span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">d</span>(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span>)<span class="cmmi-10x-x-109">/span&gt;0<span class="cmss-10x-x-109">. (As the positive definiteness gives that </span><span class="cmmi-10x-x-109">d</span>(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span>) = 1 <span class="cmss-10x-x-109">implies </span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">.)</span> </span></p>
<p><span class="cmss-10x-x-109">Given the properties of norms, we can quickly check that </span><span class="cmmi-10x-x-109">d</span>(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span>) = <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥ </span><span class="cmss-10x-x-109">is indeed a metric. Due to the linear structure of vector spaces, the norm-generated metric is invariant to translation. That is, for any </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">z </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file120.png" class="math-display" alt="d(x,y) = d(x + z,y + z). "/>
</div>
<p><span class="cmss-10x-x-109">In</span> <span id="dx1-41010"></span><span class="cmss-10x-x-109">other words, it doesn’t matter where you start: the distance only depends on your displacement. This is not true for any metric. Thus, norm-induced metrics are special. In our studies, we only deal with these special cases. Because of this, we won’t even talk about metrics, just norms.</span></p>
<p><span class="cmss-10x-x-109">In itself, a vector space is just a skeleton that provides a way to represent data. On top of this, norms define a geometric structure that reveals properties such as magnitude and distance. Both of these are essential in machine learning. For instance, some unsupervised learning algorithms separate data points into clusters based on their mutual distances from each other.</span></p>
<p><span class="cmss-10x-x-109">There is yet another way to enhance the geometric structure of vector spaces: </span><span class="cmssi-10x-x-109">inner products</span><span class="cmss-10x-x-109">, also called dot products. We are going to put this concept under our magnifying glass in the next section.</span></p>
</section>
</section>
<section id="inner-products-angles-and-lots-of-reasons-to-care-about-them" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_28"><span class="titlemark"><span class="cmss-10x-x-109">2.2 </span></span> <span id="x1-420003.2"></span><span class="cmss-10x-x-109">Inner products, angles, and lots of reasons to care about them</span></h2>
<p><span class="cmss-10x-x-109">In the previous section, we imbued our vector spaces with </span><span class="cmssi-10x-x-109">norms</span><span class="cmss-10x-x-109">, measuring the magnitude of vectors and the distance between points. In machine learning, these concepts can be used, for instance, to identify clusters in unlabeled datasets. However, without context, distance is often not enough. Following our geometric intuition, we can aspire to measure the </span><span class="cmssi-10x-x-109">similarity </span><span class="cmss-10x-x-109">of data points. This is done by the </span><span class="cmssi-10x-x-109">inner product </span><span class="cmss-10x-x-109">(also known as the dot product).</span></p>
<p><span class="cmss-10x-x-109">You can recall the</span><span id="dx1-42001"></span> <span class="cmss-10x-x-109">inner product as a quantity that we used to measure the angle between two vectors in high school geometry classes. Given two vectors </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span>= (<span class="cmmi-10x-x-109">y</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,y</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">from the plane, we defined their inner product by</span></p>
<div class="math-display">
<img src="../media/file121.png" class="math-display" alt="⟨x, y⟩ = x y + x y , 1 1 2 2 "/>
</div>
<p><span class="cmss-10x-x-109">for which it can be shown that</span></p>
<div class="display-math">
  <span class="eq-body">
    <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>
    = <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span>
    cos<span class="cmmi-10x-x-109">α</span>
  </span>
  <span class="eq-number">(2.1)</span>
</div>

<p><span class="cmss-10x-x-109">holds, where </span><span class="cmmi-10x-x-109">α </span><span class="cmss-10x-x-109">is the angle between </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">. (In fact, there are two such angles, but their cosine is equal.) Thus, the</span> <span id="dx1-42002"></span><span class="cmss-10x-x-109">angle itself can be extracted by</span></p>
<div class="math-display">
<img src="../media/file122.png" class="math-display" alt=" -⟨x,y-⟩ α = arccos∥x ∥∥y∥, "/>
</div>
<p><span class="cmss-10x-x-109">where</span> arccos<span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">is the inverse of the cosine function. We can use the inner products to determine whether two vectors are </span><span class="cmssi-10x-x-109">orthogonal</span><span class="cmss-10x-x-109">, as</span><span id="dx1-42003"></span> <span class="cmss-10x-x-109">this happens if and only if </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= 0 <span class="cmss-10x-x-109">holds. During our earlier encounters with mathematics, geometric intuition (such as orthogonality) came first, on which we built tools such as the inner product. However, if we zoom out and take an abstract viewpoint, things are exactly the opposite. As we’ll see soon, inner products emerge quite naturally, giving rise to the general concept of orthogonality.</span></p>
<p><span class="cmss-10x-x-109">In general, this is the</span> <span id="dx1-42004"></span><span class="cmss-10x-x-109">formal definition of an inner product.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-42005r9"></span> <span class="cmbx-10x-x-109">Definition 9.</span> </span><span class="cmbx-10x-x-109">(Inner products and inner product spaces)</span></p>
<p>Let V be a real vector space. The function ⟨⋅,⋅⟩ : V × V → ℝ is called an <span class="cmti-10x-x-109">inner product </span>if the following holds for all x,y, z ∈ V and a ∈ ℝ:</p>
<ol>
<li><span id="x1-42007x1">⟨ax + y,z⟩ = a⟨x,z⟩ + ⟨y, z⟩ (<span class="cmti-10x-x-109">linearity of the first variable</span>).</span></li>
<li><span id="x1-42009x2">⟨x,y ⟩ = ⟨y,x ⟩ (<span class="cmti-10x-x-109">symmetry</span>).</span></li>
<li><span id="x1-42011x3">⟨x,x ⟩ &gt;0 for all <img src="../media/file130.png" width="40" class="math" alt="x ⁄= 0 "/> (<span class="cmti-10x-x-109">positive definiteness</span>).</span></li>
</ol>
<p>Vector spaces with an inner product are called <span class="cmti-10x-x-109">inner product spaces</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Right off the bat, we can immediately deduce two properties. First,</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file131.png" class="math-display" width="300" alt="⟨0,x ⟩ = ⟨0x,x ⟩ = 0⟨x,x⟩ = 0."/>
    <span id="x1-42011x3.2"></span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.2)
  </span>
</div>

<p><span class="cmss-10x-x-109">As a special case, </span><span class="cmsy-10x-x-109">⟨</span>0<span class="cmmi-10x-x-109">,</span>0<span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. Just like we have seen for norms, a bit more is true: if </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">, then </span><span class="cmbx-10x-x-109">x </span>= 0<span class="cmss-10x-x-109">. This follows from positive definiteness and (</span><a href="ch008.xhtml#x1-42011x3.2"><span class="cmss-10x-x-109">2.2</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">In addition, due to the symmetry and linearity of the first variable, inner products are also linear in the </span><span class="cmssi-10x-x-109">second </span><span class="cmss-10x-x-109">variable. Because of</span><span id="dx1-42012"></span> <span class="cmss-10x-x-109">this, they are called </span><span class="cmssi-10x-x-109">bilinear</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To familiarize</span> <span id="dx1-42013"></span><span class="cmss-10x-x-109">ourselves with the concept, let’s see some examples!</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">As usual, the canonical and most prevalent example of inner product spaces is </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, where the inner product </span><span class="cmsy-10x-x-109">⟨⋅</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">⋅⟩ </span><span class="cmss-10x-x-109">is defined by</span></p>
<img src="../media/file132.png" class="math-display" width="450" alt=" ∑n ⟨x,y⟩ = xiyi, x = (x1,...,xn ), y = (y1,...,yn). i=1 "/>

<p><span class="cmss-10x-x-109">This bilinear function is often called the </span><span class="cmssi-10x-x-109">dot product</span><span class="cmss-10x-x-109">. Equipped with this, </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is called the n-dimensional </span><span class="cmssi-10x-x-109">Euclidean space</span><span class="cmss-10x-x-109">. This is a central concept in machine learning, as data is most frequently represented in Euclidean spaces. Thus, we are going to explore the structure of this space in great detail throughout this book.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">Besides Euclidean spaces, there are other inner product spaces that play a significant role in mathematics and machine learning. If you are</span> <span id="dx1-42014"></span><span class="cmss-10x-x-109">familiar with integration, in certain function spaces, the bilinear function</span></p>
<div class="math-display">
<img src="../media/file133.png" class="math-display" alt=" ∫ ∞ ⟨f,g⟩ = f(x)g(x)dx −∞ "/>
</div>
<p><span class="cmss-10x-x-109">defines an inner product space with a very rich and beautiful structure.</span></p>
<p><span class="cmss-10x-x-109">The symmetry and linearity of </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">f,g</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">is clear. Only the positive definiteness seems to be an issue.</span></p>
<p><span class="cmss-10x-x-109">For instance, if </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is defined by</span></p>
<div class="math-display">
<img src="../media/file134.png" class="math-display" alt=" ( |{1 if x = 0, f(x) = |(0 otherwise, "/>
</div>
<p><span class="cmss-10x-x-109">then </span><span class="cmmi-10x-x-109">f≠</span>0<span class="cmss-10x-x-109">, but </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">f,f</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. This problem can be circumvented by “overloading” the equality operator and letting </span><span class="cmmi-10x-x-109">f </span>= <span class="cmmi-10x-x-109">g </span><span class="cmss-10x-x-109">if and only if </span><span class="cmex-10x-x-109">∫</span> <sub><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>) <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">g</span>(<span class="cmmi-10x-x-109">x</span>)<span class="cmsy-10x-x-109">|</span><sup><span class="cmr-8">2</span></sup> <span class="cmmi-10x-x-109">dx </span>= 0<span class="cmss-10x-x-109">. Even though function spaces such as this play an important role in mathematics and machine learning, their study falls outside of our scope.</span></p>
<section id="the-generated-norm" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_29"><span class="titlemark"><span class="cmss-10x-x-109">2.2.1 </span></span> <span id="x1-430003.2.1"></span><span class="cmss-10x-x-109">The generated norm</span></h3>
<p><span class="cmss-10x-x-109">Recall that the </span>2<span class="cmss-10x-x-109">-norm in </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">was defined by </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sub><span class="cmr-8">2</span></sub> = <span class="bbig">(</span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmr-8">2</span></sup><span class="bbig">)</span><sup><span class="cmr-8">1</span><span class="cmmi-8">∕</span><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">, which, according to our definition of the inner product there, equals</span> <img src="../media/file137.png" class="sqrt" width="45" alt="∘ ------ ⟨x,x⟩"/><span class="cmss-10x-x-109">. This is not a coincidence. Inner products can be used to define norms on vector spaces.</span></p>
<p><span class="cmss-10x-x-109">To</span><span id="dx1-43001"></span> <span class="cmss-10x-x-109">show exactly how, we need a simple tool: the</span> <span id="dx1-43002"></span><span class="cmss-10x-x-109">Cauchy-Schwarz inequality.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-43003r8"></span> <span class="cmbx-10x-x-109">Theorem 8.</span> </span> <span class="cmbxti-10x-x-109">(Cauchy-Schwarz inequality)</span></p>
<p><span class="cmti-10x-x-109">Let </span>V <span class="cmti-10x-x-109">be an inner product space. Then, for any </span>x,y ∈ V<span class="cmti-10x-x-109">, the inequality</span></p>
<div class="math-display">
<img src="../media/file140.png" class="math-display" alt="|⟨x, y⟩|2 ≤ ⟨x,x⟩⟨y,y⟩ "/>
</div>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<div id="tcolobox-56" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">At this</span> <span id="dx1-43004"></span><span class="cmss-10x-x-109">point, we don’t know much about the inner product except its core defining properties. So, we are going to use a little trick. For any </span>λ ∈ ℝ<span class="cmss-10x-x-109">, the positive definiteness implies that</span></p>
<div class="math-display">
<img src="../media/file142.png" class="math-display" alt="⟨x + λy,x + λy ⟩ ≥ 0. "/>
</div>
<p><span class="cmss-10x-x-109">On the other hand, because of bilinearity (that is, linearity in both variables) and symmetry, we have</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file143.png" class="math-display" width="300" alt="2 ⟨x + λy, x+ λy ⟩ = ⟨x,x ⟩+ 2λ⟨x,y ⟩+ λ ⟨y,y⟩,"/>
    <span id="x1-43003r3.2.1"></span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.3)
  </span>
</div>

<p><span class="cmss-10x-x-109">which is a quadratic polynomial in </span>λ<span class="cmss-10x-x-109">. In general, we know that for any quadratic polynomial of the form </span>ax2 + bx + c<span class="cmss-10x-x-109">, the roots are given by the formula</span></p>
<div class="math-display">
<img src="../media/file146.png" class="math-display" alt=" √ -------- − b-±--b2 −-4ac- x1,2 = 2a . "/>
</div>
<p><span class="cmss-10x-x-109">Since</span></p>
<div class="math-display">
<img src="../media/file147.png" class="math-display" alt="⟨x + λy,x + λy ⟩ ≥ 0, "/>
</div>
<p><span class="cmss-10x-x-109">the polynomial defined by (2.3) must have at most one real root. Thus, the discriminant </span><span class="cmmi-10x-x-109">b</span><sup><span class="cmr-8">2</span></sup> <span class="cmsy-10x-x-109">− </span>4<span class="cmmi-10x-x-109">ac </span><span class="cmss-10x-x-109">is non-positive. Plugging in the coefficients of the polynomial (2.3) into the discriminant formula, we obtain</span></p>
<div class="math-display">
<img src="../media/file148.png" class="math-display" alt="|⟨x, y⟩|2 − ⟨x, x⟩⟨y,y⟩ ≤ 0, "/>
</div>
<p><span class="cmss-10x-x-109">which completes the proof.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">The Cauchy-Schwarz inequality</span><span id="dx1-43005"></span> <span class="cmss-10x-x-109">is probably one of the most useful tools in studying inner product spaces. One application we are going to see next is to show how inner products define norms.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-43006r9"></span> <span class="cmbx-10x-x-109">Theorem 9.</span> </span><span class="cmbxti-10x-x-109">(The norm generated by the inner product)</span></p>
<p><span class="cmti-10x-x-109">Let </span>V <span class="cmti-10x-x-109">be an inner product space. Then, the function </span>∥⋅∥ : V → [0,∞ ) <span class="cmti-10x-x-109">defined by</span></p>
<div class="math-display">
<img src="../media/file151.png" class="math-display" alt=" ∘ ------ ∥x∥ = ⟨x,x⟩ "/>
</div>
<p><span class="cmti-10x-x-109">is a norm on </span>V<span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-57" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">According to the definition of norms, we have to show that three properties hold: positive definiteness, homogeneity, and the triangle inequality. The first two follow easily from the properties of inner products. The triangle inequality follows from the Cauchy-Schwarz inequality:</span></p>
<!-- Step 1 -->
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup>
     = <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩</span>
  </span>
  
</div>

<!-- Step 2 -->
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    = <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> + <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> + 2<span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩</span>
  </span>
  
</div>

<!-- Step 3 -->
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    ≤ <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> + <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> + 2<span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span>
  </span>
  
</div>

<!-- Step 4 -->
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    = <span class="big">(</span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥ </span>+ <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><span class="big">)</span><sup><span class="cmr-8">2</span></sup><span class="cmmi-10x-x-109">,</span>
  </span>
  
</div>

<p><span class="cmss-10x-x-109">from which the triangle inequality follows.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Thus, inner</span> <span id="dx1-43007"></span><span class="cmss-10x-x-109">product spaces are normed spaces as well. They have the proper algebraic and geometric structure that we need to represent, manipulate, and transform data.</span></p>
<p><span class="cmss-10x-x-109">Most importantly, </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch008.xhtml#x1-43006r9"><span class="cmssi-10x-x-109">9</span></a> <span class="cmss-10x-x-109">can be reversed! That is, given a norm </span><span class="cmsy-10x-x-109">∥⋅∥</span><span class="cmss-10x-x-109">, we can define a matching inner product.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-43008r10"></span> <span class="cmbx-10x-x-109">Theorem 10.</span> </span><span class="cmbxti-10x-x-109">(The polarization identity)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space, and let </span><span class="cmsy-10x-x-109">∥⋅∥</span><span class="cmti-10x-x-109">be the norm induced by the inner product. Then,</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file155.png" class="math-display" alt="1 ⟨x, y⟩ = -(∥x+ y∥2 − ∥x ∥2 − ∥y ∥2). 2" width="300"/>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.4)
  </span>
</div>

</div>
<p><span class="cmss-10x-x-109">In other words, one can generate an inner product from a norm, not just the other way around.</span></p>
<div id="tcolobox-58" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As the inner product is bilinear, we have</span></p>
<div class="math-display">
<img src="../media/file156.png" class="math-display" alt="⟨x + y,x + y⟩ = ⟨x,x⟩+ 2⟨x, y⟩+ ⟨y,y ⟩, "/>
</div>
<p><span class="cmss-10x-x-109">from which the polarization identity (2.4) follows.</span></p>
</div>
</div>
</section>
<section id="orthogonality" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_30"><span class="titlemark"><span class="cmss-10x-x-109">2.2.2 </span></span> <span id="x1-440003.2.2"></span><span class="cmss-10x-x-109">Orthogonality</span></h3>
<p><span class="cmss-10x-x-109">In vector spaces other than </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, the concept of enclosed angles is not clear at all. For instance, in spaces where vectors are functions, there is no intuitive way to define the angles between two functions. However, as (2.1) suggests, in the special case </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, these can be generalized.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-44001r10"></span> <span class="cmbx-10x-x-109">Definition 10.</span> </span><span class="cmbx-10x-x-109">(Orthogonality of vectors)</span></p>
<p>Let <span class="cmmi-10x-x-109">V </span>be an inner product space, and let <span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span>. We say that <span class="cmbx-10x-x-109">x </span>and <span class="cmbx-10x-x-109">y </span>are <span class="cmti-10x-x-109">orthogonal </span>if</p>

<img src="../media/file157.png" width="150" alt="⟨x,y ⟩ = 0. " class="math-display"/>

<p>Orthogonality is denoted as <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">To illustrate</span> <span id="dx1-44002"></span><span class="cmss-10x-x-109">how inner products and orthogonality define geometry on vector spaces, let’s see how the classic Pythagorean theorem looks in this new form. Recall that the “original” version states that in right triangles, </span><span class="cmmi-10x-x-109">a</span><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">b</span><sup><span class="cmr-8">2</span></sup> = <span class="cmmi-10x-x-109">c</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">is the length of the hypotenuse, while </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">b </span><span class="cmss-10x-x-109">are the lengths of the other two sides.</span></p>
<p><span class="cmss-10x-x-109">In inner product spaces, this generalizes in the following way.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-44003r11"></span> <span class="cmbx-10x-x-109">Theorem 11.</span> </span><span class="cmbxti-10x-x-109">(The Pythagorean theorem)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space, and let </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">. Then, </span><span class="cmbx-10x-x-109">x </span><span class="cmti-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmti-10x-x-109">are orthogonal if and only if</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>+ <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩</span><span class="cmmi-10x-x-109">.</span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.5)
  </span>
</div>

</div>
<div id="tcolobox-59" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Given the definition of inner products and orthogonality, the proof is straightforward. Due to bilinearity, we have</span></p>

<img src="../media/file158.png" class="math-display" alt="⟨x+ y, x+ y ⟩ = ⟨x,x + y⟩ + ⟨y,x + y⟩ = ⟨x,x ⟩+ 2⟨x,y⟩ + ⟨y,y⟩. " width="450"/>
<p><span class="cmss-10x-x-109">Since </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">are orthogonal, we have </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. Thus, the equation simplifies to:</span></p>
<div class="math-display">
<img src="../media/file159.png" class="math-display" alt="⟨x + y, x+ y ⟩ = ⟨x,x ⟩+ ⟨y,y⟩. "/>
</div>
<p><span class="cmss-10x-x-109">This completes the proof.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Why is this</span> <span id="dx1-44004"></span><span class="cmss-10x-x-109">the Pythagorean theorem in another form? Because the norm and the inner product is related by </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup><span class="cmmi-10x-x-109">, </span><span class="cmss-10x-x-109">(</span><a href="ch008.xhtml#x1-44003r11"><span class="cmss-10x-x-109">11</span></a><span class="cmss-10x-x-109">) is equivalent to</span></p>
<div class="math-display">
<img src="../media/file160.png" class="math-display" alt="∥x + y∥2 = ∥x∥2 + ∥y∥2, "/>
</div>
<p><span class="cmss-10x-x-109">which is exactly the famous “</span><img src="../media/file161.png" width="75" alt=" 2 2 2 a + b = c "/><span class="cmss-10x-x-109">”.</span></p>
</section>
<section id="the-geometric-interpretation-of-inner-products" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_31"><span class="titlemark"><span class="cmss-10x-x-109">2.2.3 </span></span> <span id="x1-450003.2.3"></span><span class="cmss-10x-x-109">The geometric interpretation of inner products</span></h3>
<p><span class="cmss-10x-x-109">Looking at</span> <span id="dx1-45001"></span><span class="cmss-10x-x-109">the general definition, it is hard to get an insight into the</span> <span id="dx1-45002"></span><span class="cmss-10x-x-109">inner product. However, by using the concept of orthogonality, we can visualize what </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">represents for any </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Intuitively, any </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be decomposed into the sum of two vectors </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> + <span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub><span class="cmss-10x-x-109">, where </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> <span class="cmss-10x-x-109">is orthogonal to </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">is parallel to it.</span></p>
<div class="minipage">
<p><img src="../media/file162.png" width="284" height="284" alt="PIC"/> <span id="x1-45003r8"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.8: Decomposition of </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">into components parallel and orthogonal to </span><span class="cmmi-10x-x-109">y</span> </span>
</div>
<p><span class="cmss-10x-x-109">Let’s make the intuition precise. How can we find </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub><span class="cmss-10x-x-109">? Since </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">has the same direction as </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, it can be written in the form </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> = <span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">for some scalar </span><span class="cmmi-10x-x-109">c </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">. Because </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> <span class="cmss-10x-x-109">sum up to </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, we also have </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> = <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> = <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Since </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> <span class="cmss-10x-x-109">is orthogonal to </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, the constant </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">can be determined by solving the equation</span></p>
<div class="math-display">
<img src="../media/file163.png" class="math-display" alt="⟨x − cy,y⟩ = 0. "/>
</div>
<p><span class="cmss-10x-x-109">By using</span> <span id="dx1-45004"></span><span class="cmss-10x-x-109">the bilinearity of the inner product, we can express </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">from this</span> <span id="dx1-45005"></span><span class="cmss-10x-x-109">equation. Thus, we have</span></p>

<img src="../media/file164.png" class="math-display" width="150" alt="c = ⟨x,y⟩. ⟨y,y⟩ "/>

<p><span class="cmss-10x-x-109">So,</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file165.png" class="math-display" width="150" alt="xp = ⟨x,y⟩-y, ⟨y,y ⟩ ⟨x,y⟩- xo = x − ⟨y,y⟩y."/>
    <span id="x1-45003r3.2.3"></span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.6)
  </span>
</div>

<p><span class="cmss-10x-x-109">We call </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">the </span><span class="cmssi-10x-x-109">orthogonal projection </span><span class="cmss-10x-x-109">of </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">onto </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">. This is a common transformation, so we are going to introduce the notation</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    proj<sub><span class="cmbx-8">y</span></sub>(<span class="cmbx-10x-x-109">x</span>) = 
    <img src="../media/file166.png" class="frac" data-align="middle" alt="⟨x,y-⟩ ⟨y,y ⟩"/>
    <span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">.</span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.7)
  </span>
</div>

<p><span class="cmss-10x-x-109">From this, we can see that the scaling ratio between </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">and</span> proj<sub><span class="cmbx-8">y</span></sub>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">can be described by inner products.</span></p>
<p><span class="cmss-10x-x-109">So far, we have seen that we can use inner products to define the orthogonality relation between two vectors. Can we use it to measure (and, in some cases, even define) the angle? The answer is yes! In the following, we are going to see how, arriving at the formula (2.1) already familiar from basic geometry.</span></p>
<p><span class="cmss-10x-x-109">To build our intuition, let’s select two arbitrary </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional vectors </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. The inner product of the sum </span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">can be calculated using the bilinearity property.</span></p>
<div class="minipage">
<p><img src="../media/file167.png" width="312" height="313" alt="PIC"/> <span id="x1-45006r9"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.9: The sum of </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y</span> </span>
</div>
<p><span class="cmss-10x-x-109">With this, we obtain</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file168.png" class="math-display" width="300" alt="⟨x + y,x + y⟩ = ∥x+ y∥2 = ∥x∥2 + ∥y∥2 + 2⟨x,y ⟩. "/>
    <span id="x1-45006r3.2.3"></span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.8)
  </span>
</div>

<p><span class="cmss-10x-x-109">On the</span> <span id="dx1-45007"></span><span class="cmss-10x-x-109">other hand, considering that </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, and </span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">form a triangle, we</span> <span id="dx1-45008"></span><span class="cmss-10x-x-109">can use the law of cosines ( </span><a href="https://en.wikipedia.org/wiki/Law_of_cosines" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Law_of_cosines</span></a><span class="cmss-10x-x-109">) to express </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">in a different form.</span></p>
<p><span class="cmss-10x-x-109">Here, the law of cosines implies</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup>
     = <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> + <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> <span class="cmsy-10x-x-109">− </span>2<span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span>
    <img src="../media/file169.png" width="75" alt="c◟os(π◝ −◜-α-)◞"/>
    <sub>
      <span class="cmr-8">=</span><span class="cmsy-8">−</span><span class="cmr-8">cos </span><span class="cmmi-8">α</span>
    </sub><span class="cmmi-10x-x-109">.</span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.9)
  </span>
</div>

<p><span class="cmss-10x-x-109">By combining (2.8) and (2.9), we get</span></p>
<div class="math-display">
<img src="../media/file170.png" class="math-display" alt="⟨x,y⟩ = ∥x∥∥y ∥cosα. "/>
</div>
<p><span class="cmss-10x-x-109">That</span> <span id="dx1-45010"></span><span class="cmss-10x-x-109">is, in </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, the angle enclosed by </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">can be extracted by</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <span class="cmmi-10x-x-109">α </span>= arccos
    <img src="../media/file171.png" class="frac" data-align="middle" alt=" ⟨x,y⟩ ------- ∥x∥∥y ∥"/>
    <span class="cmmi-10x-x-109">.</span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.10)
  </span>
</div>

<div class="minipage">
<p><img src="../media/file172.png" width="512" height="313" alt="PIC"/> <span id="x1-45011r10"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.10: The triangle formed by </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, and </span><span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span> </span>
</div>
<p><span class="cmss-10x-x-109">What about vector spaces where the angle between vectors is not defined? We have seen instances of vector spaces (</span><span class="cmssi-10x-x-109">Section </span><a href="ch007.xhtml#examples-of-vector-spaces"><span class="cmssi-10x-x-109">1.1.1</span></a><span class="cmss-10x-x-109">) where the elements are polynomials, functions, and other mathematical objects. There, (2.10) can be used to </span><span class="cmssi-10x-x-109">define </span><span class="cmss-10x-x-109">the angle!</span></p>
<p><span class="cmss-10x-x-109">Let’s explore this idea further and see how to use inner products to measure </span><span class="cmssi-10x-x-109">similarity</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Given our geometric interpretation of inner products as orthogonal projections, let’s focus on the case when both </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">have unit norms. In this special case, the orthogonal projection equals</span></p>
<div class="math-display">
<img src="../media/file173.png" class="math-display" alt="projy(x) = ⟨x,y ⟩y (∥x∥ = ∥y∥ = 1). "/>
</div>
<p><span class="cmss-10x-x-109">Thus, </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">precisely describes the signed magnitude of the orthogonal projection. (It can be negative when </span><img src="../media/file174.png" class="math" alt="projy(x ) "/> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">have an opposite direction.)</span></p>
<p><span class="cmss-10x-x-109">With this in mind, we</span> <span id="dx1-45012"></span><span class="cmss-10x-x-109">can see that the inner product is equal to the cosine of the angle enclosed by the two vectors. Let’s draw a picture to illustrate! (Recall that in right triangles, the cosine is the ratio of the length</span> <span id="dx1-45013"></span><span class="cmss-10x-x-109">of the adjacent side and the hypotenuse. In this case, the adjacent side has a length of </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩</span><span class="cmss-10x-x-109">, while the hypotenuse is of unit length.)</span></p>
<div class="minipage">
<p><img src="../media/file175.png" width="584" height="284" alt="PIC"/> <span id="x1-45014r11"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 2.11: The inner product of two unit vectors equals the cosine of their angle</span> </span>
</div>
<p><span class="cmss-10x-x-109">In machine learning, this quantity is frequently used to measure the similarity of two vectors.</span></p>
<p><span class="cmss-10x-x-109">Because any vector </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be scaled to unit norm with the transformation </span><span class="cmbx-10x-x-109">x</span>→<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">∕</span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><span class="cmss-10x-x-109">, we define the </span><span class="cmssi-10x-x-109">cosine similarity </span><span class="cmss-10x-x-109">by</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    cos(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span>) = 
    <span class="bigg">
      <img src="../media/file177.png" width="10" data-align="middle" alt="⟨"/>
    </span>
    <img src="../media/file178.png" width="40" data-align="middle" alt="‑x‑‑ ∥x∥"/>
    <span class="cmmi-10x-x-109">,</span>
    <img src="../media/file179.png" width="40" data-align="middle" alt="‑y‑‑ ∥y∥"/>
    <span class="bigg">
      <img src="../media/file180.png" width="10" data-align="middle" alt="⟩"/>
    </span>
    <span class="cmmi-10x-x-109">.</span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.11)
  </span>
</div>

<p><span class="cmss-10x-x-109">If </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">represent the feature vectors of two data samples,</span> cos(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span>) <span class="cmss-10x-x-109">tells us how much the features move together. Note that because of the scaling, two samples with a high cosine similarity can be far from each other. So, this reveals nothing about their relative positions in the feature space.</span></p>
</section>
<section id="orthogonal-and-orthonormal-bases" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_32"><span class="titlemark"><span class="cmss-10x-x-109">2.2.4 </span></span> <span id="x1-460003.2.4"></span><span class="cmss-10x-x-109">Orthogonal and orthonormal bases</span></h3>
<p><span class="cmss-10x-x-109">Through the</span><span id="dx1-46001"></span> <span class="cmss-10x-x-109">lens of similarity, orthogonality means that one vector</span><span id="dx1-46002"></span> <span class="cmss-10x-x-109">does not contain “information” about the other. We will make this notion more precise when learning about correlation, but there are clear implications regarding the structure of inner product spaces. Recall that during the introduction of basis vectors (</span><span class="cmssi-10x-x-109">Section </span><a href="ch007.xhtml#the-basis"><span class="cmssi-10x-x-109">1.2</span></a><span class="cmss-10x-x-109">), our motivation was to find a </span><span class="cmssi-10x-x-109">minimal </span><span class="cmss-10x-x-109">set of vectors that can be used to express any other vector. With the introduction of orthogonality, we can go one step further.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-46003r11"></span> <span class="cmbx-10x-x-109">Definition 11.</span> </span><span class="cmbx-10x-x-109">(Orthogonal and orthonormal bases)</span></p>
<p>Let <img src="../media/file181.png" class="math" alt="V "/> be a vector space and S = {v1,...,vn } its basis. We say that <img src="../media/file183.png" class="math" alt="S "/> is an <span class="cmti-10x-x-109">orthogonal basis </span>if ⟨vi,vj⟩ = 0 whenever <img src="../media/file185.png" width="45" alt="i ⁄= j "/>.</p>
<p>Moreover, <img src="../media/file186.png" class="math" alt="S "/> is called <span class="cmti-10x-x-109">orthonormal </span>if</p>
<div class="math-display">
<img src="../media/file187.png" class="math-display" alt=" ( |{ 1, if i = j, ⟨vi,vj⟩ = |( 0, if i ⁄= j. "/>
</div>
<p>In other words, <img src="../media/file188.png" class="math" alt="S "/> is orthonormal if, in addition to being orthogonal, each vector has unit norm.</p>
</div>
<p><span class="cmss-10x-x-109">Orthogonal and orthonormal bases are extremely convenient to use. If a basis is orthogonal, we can easily obtain an orthonormal basis by simply scaling its vectors to unit norm. Thus, we’ll use orthonormal basis vectors most of the time.</span></p>
<p><span class="cmss-10x-x-109">Why do we love orthonormal bases so much? To see this, let </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">be an arbitrary basis and let </span><img src="../media/file189.png" class="math" alt="x "/> <span class="cmss-10x-x-109">be an arbitrary vector. We know that</span></p>

<img src="../media/file190.png" class="math-display" width="150" alt=" ∑n x = xivi, i=1 "/>

<p><span class="cmss-10x-x-109">but how do we find the coefficients </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">? There is a general method involving linear equations that we will see later in </span><span class="cmssi-10x-x-109">Chapter </span><a href="ch011.xhtml#matrices-and-equations"><span class="cmssi-10x-x-109">6</span></a> <span class="cmss-10x-x-109">, but if </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is orthonormal, the situation is much simpler.</span></p>
<p><span class="cmss-10x-x-109">This is made</span> <span id="dx1-46004"></span><span class="cmss-10x-x-109">more precise in the following theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-46005r12"></span> <span class="cmbx-10x-x-109">Theorem 12.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file191.png" class="math" alt="V "/> <span class="cmti-10x-x-109">be a vector space and </span><img src="../media/file192.png" class="math" alt="S = {v1,...,vn } "/><span class="cmti-10x-x-109">be an orthonormal basis of</span> <img src="../media/file193.png" class="math" alt="V "/><span class="cmti-10x-x-109">. Then, for any </span><img src="../media/file194.png" width="45" alt="x ∈ V "/><span class="cmti-10x-x-109">,</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <span class="cmbx-10x-x-109">x </span>= <span class="cmex-10x-x-109">∑</span>
    <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub>
    <sup><span class="cmmi-8">n</span></sup>
    <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">⟩</span>
    <span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.12)
  </span>
</div>

<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<div id="tcolobox-60" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Since </span><img src="../media/file195.png" class="math" alt="v1,...,vn "/> <span class="cmss-10x-x-109">form a basis, we can express </span><img src="../media/file196.png" class="math" alt="x "/> <span class="cmss-10x-x-109">as</span></p>

<img src="../media/file197.png" class="math-display" width="150" alt=" ∑n x = xivi i=1 "/>
<p><span class="cmss-10x-x-109">for some scalars </span><img src="../media/file198.png" class="math" alt="xi "/><span class="cmss-10x-x-109">. By the</span> <span id="dx1-46006"></span><span class="cmss-10x-x-109">linearity of the inner product, we obtain</span></p>
<div class="math-display">
<img src="../media/file199.png" class="math-display" alt=" ∑n ∑n ⟨x,vj⟩ = ⟨ xivi,vj⟩ = xi⟨vi,vj⟩. i=1 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><img src="../media/file200.png" class="math" alt="v ,...,v 1 n "/> <span class="cmss-10x-x-109">form an orthonormal basis, we have</span></p>
<div class="math-display">
<img src="../media/file201.png" class="math-display" alt=" (| {1, if i = j, ⟨vi,vj⟩ = | (0, if i ⁄= j. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the sum reduces to</span></p>
<img src="../media/file202.png" class="math-display" width="150" alt="⟨x,vj ⟩ = xj. "/>

<p><span class="cmss-10x-x-109">This proves the result.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Thus, the coefficients can be calculated by taking the inner product. In other words, for</span> <span id="dx1-46007"></span><span class="cmss-10x-x-109">orthonormal bases, </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">j</span></sub> <span class="cmss-10x-x-109">depends only on the </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th basis vector.</span></p>
<p><span class="cmss-10x-x-109">As another</span> <span id="dx1-46008"></span><span class="cmss-10x-x-109">consequence of the orthonormality, calculating the norm is also easier, as we can always express it in terms of the coefficients. To be more precise, we have</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file203.png" class="math-display" width="300" alt="2 ∥x∥ = ⟨x,x⟩ ∑n ∑n = ⟨ xivi, xjvj ⟩ i=1 j=1 ∑n ∑n = xixj⟨vi,vj⟩ i=1j=1 n = ∑ x2. i i=1"/>
    <span id="x1-46005r3.2.4"></span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.13)
  </span>
</div>

<p><span class="cmss-10x-x-109">This is</span> <span id="dx1-46009"></span><span class="cmss-10x-x-109">called </span><span class="cmssi-10x-x-109">Parseval’s identity</span><span class="cmss-10x-x-109">. In other words, if </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is given in terms of an orthonormal basis, its norm is easy to find. It is not a coincidence that this formula resembles the Euclidean norm so much! (Note that, here, </span><span class="cmsy-10x-x-109">∥⋅∥ </span><span class="cmss-10x-x-109">is a general norm.) In fact, the squared Euclidean norm</span></p>
<div class="math-display">
<img src="../media/file204.png" class="math-display" alt=" n ∥x ∥2= ∑ x2, x = (x ,...,x ) ∈ ℝn 2 i 1 n i=1 "/>
</div>
<p><span class="cmss-10x-x-109">is just (2.13) using the standard basis.</span></p>
</section>
<section id="the-gramschmidt-orthogonalization-process" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_33"><span class="titlemark"><span class="cmss-10x-x-109">2.2.5 </span></span> <span id="x1-470003.2.5"></span><span class="cmss-10x-x-109">The Gram-Schmidt orthogonalization process</span></h3>
<p><span class="cmss-10x-x-109">Orthogonal bases are awesome and all, but how do we find them?</span></p>
<p><span class="cmss-10x-x-109">There</span><span id="dx1-47001"></span> <span class="cmss-10x-x-109">is a general method called the </span><span class="cmssi-10x-x-109">Gram-Schmidt orthogonalization process </span><span class="cmss-10x-x-109">that solves this problem. The algorithm takes any set of basis vectors </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">and outputs an orthonormal basis </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span></p>
<p><span class="cmss-10x-x-109">such that</span></p>

<img src="../media/file205.png" class="math-display" alt="span(v1,...,vk) = span (e1,...,ek), k = 1,...,n, " width="450"/>

<p><span class="cmss-10x-x-109">that is, the subspaces generated by the first </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">vectors of both sets match.</span></p>
<p><span class="cmss-10x-x-109">How do we do that? The process is straightforward. Let’s focus on finding an </span><span class="cmssi-10x-x-109">orthogonal </span><span class="cmss-10x-x-109">system first, which we can normalize later to achieve orthonormality. We are going to build our set </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">iteratively. It is clear that</span></p>

<img src="../media/file206.png" width="150" class="math-display" alt="e1 := v1 "/>

<p><span class="cmss-10x-x-109">is a good choice. Now, our goal is to find </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">such that </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and, together, they span the same subspace as </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. Remember when we talked about the geometric interpretation of orthogonality in </span><span class="cmssi-10x-x-109">Section </span><a href="ch008.xhtml#the-geometric-interpretation-of-inner-products"><span class="cmssi-10x-x-109">2.2.3</span></a><span class="cmss-10x-x-109">? The orthogonal component of </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">with respect to </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">will be a good choice for </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. Thus, let</span></p>
<div class="math-display">
<img src="../media/file207.png" class="math-display" alt=" ⟨v2,e1⟩ e2 := v2 − proje1(v2) = v2 −-------e1. ⟨e1,e1⟩ "/>
</div>
<p><span class="cmss-10x-x-109">From the definition, it is clear that </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, and it is also clear that </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">spans the same subspace as </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In the next step, we perform the same process. We project </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">3</span></sub> <span class="cmss-10x-x-109">onto the subspace generated by </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, then define </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">3</span></sub> <span class="cmss-10x-x-109">as the difference of </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">3</span></sub> <span class="cmss-10x-x-109">and the projection. That is,</span></p>
<div class="math-display">
<img src="../media/file208.png" class="math-display" alt="e3 := v3 − proje1,e2(v3 ) = v3 − ⟨v3,e1⟩e1 − ⟨v3,e2⟩e2. ⟨e1,e1⟩ ⟨e2,e2⟩ "/>
</div>
<p><span class="cmss-10x-x-109">With this, we</span> <span id="dx1-47002"></span><span class="cmss-10x-x-109">essentially remove the “contributions” of </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">toward </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">3</span></sub><span class="cmss-10x-x-109">, thus obtaining an </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">3</span></sub> <span class="cmss-10x-x-109">that is orthogonal to the previous ones.</span></p>
<p><span class="cmss-10x-x-109">In general, if we have </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">, the vector </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span><span class="cmr-8">+1</span></sub> <span class="cmss-10x-x-109">can be found by</span></p>
<div class="math-display">
<img src="../media/file209.png" class="math-display" alt="ek+1 := vk+1 − proj (vk+1), e1,...,ek "/>
</div>
<p><span class="cmss-10x-x-109">where</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file210.png" class="math-display" width="300" alt="∑k ⟨x, ei⟩ proje1,...,ek(x) = -------ei i=1⟨ei,ei⟩"/>
    <span id="x1-470003.2.5eq"></span>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.14)
  </span>
</div>

<p><span class="cmss-10x-x-109">is the generalized orthogonal projection operator, projecting a vector to the subspace generated by </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To check that </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span><span class="cmr-8">+1</span></sub> <span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">, we have</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file211.png" class="math-display" width="450" alt="∑ k ⟨vk+1,ei⟩ ⟨ek+1,ej⟩ = ⟨vk+1 − ⟨ei,ei⟩ ei,ej⟩ i=1 ∑k ⟨vk+1,ei⟩ = ⟨vk+1,ej⟩− ---------⟨ei,ej⟩ i=1 ⟨ei,ei⟩ = ⟨v ,e ⟩− ⟨v ,e ⟩ k+1 j k+1 j = 0."/>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.15)
  </span>
</div>

<p><span class="cmss-10x-x-109">due to the orthogonality of the </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s and the linearity of the inner product. Since </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">spans the same subspace as </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">k </span>+ 1<span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is a linear combination of </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span><span class="cmr-8">+1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">(where the coefficient of </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span><span class="cmr-8">+1</span></sub> <span class="cmss-10x-x-109">is nonzero),</span></p>
<div class="math-display">
<img src="../media/file212.png" class="math-display" alt="span(v ,...,v ) = span (e ,...,e ) 1 k+1 1 k+1 "/>
</div>
<p><span class="cmss-10x-x-109">also follows. This can be repeated until we run out of vectors and find </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">For the</span> <span id="dx1-47003"></span><span class="cmss-10x-x-109">sake of further reference, mathematical correctness, and a tiny bit of perfectionism, let’s summarize all the above in a single theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-47004r13"></span> <span class="cmbx-10x-x-109">Theorem 13.</span> </span><span class="cmbxti-10x-x-109">(Gram-Schmidt orthogonalization process)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product vector space and </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} ⊆ </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be a set of linearly independent vectors. Then, there exists an orthonormal set </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}⊆</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">such that</span></p>
<div class="math-display">
<img src="../media/file213.png" class="math-display" alt="span(e1,...,ek) = span (v1,...,vk) "/>
</div>
<p><span class="cmti-10x-x-109">holds for any </span><span class="cmmi-10x-x-109">k </span>= 1<span class="cmmi-10x-x-109">,…,n</span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">As a</span> <span id="dx1-47005"></span><span class="cmss-10x-x-109">consequence, we can state that each finite inner product space has an orthonormal basis. We can even construct it explicitly via the Gram-Schmidt process.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-47006r1"></span> <span class="cmbx-10x-x-109">Corollary 1.</span> </span><span class="cmbxti-10x-x-109">(Existence of orthonormal bases)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be a finite-dimensional inner product space. Then, there exists an orthonormal basis in </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">Going one step further, we can view </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch008.xhtml#x1-47004r13"><span class="cmssi-10x-x-109">13</span></a> <span class="cmss-10x-x-109">and its proof as an </span><span class="cmssi-10x-x-109">algorithm</span><span class="cmss-10x-x-109">.</span> <span id="x1-47007r1"></span></p>
<figure class="float">
 <span class="cmbx-10x-x-109">(Gram-Schmidt orthogonalization process) Inputs</span>: A set of linearly independent vectors <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}⊆</span><span class="cmmi-10x-x-109">V </span>. <span class="cmbx-10x-x-109">Output</span>: A set of orthonormal vectors <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span> such that
<div class="math-display">
<img src="../media/file214.png" class="math-display" alt="span(e1,...,ek) = span (v1, ...,vk) "/>
</div>
holds for any <span class="cmmi-10x-x-109">k </span>= 1<span class="cmmi-10x-x-109">,…,n</span>.
</figure>
<div class="newtheorem">
<p><span class="head"> <span id="x1-47008r3"></span> <span class="cmbx-10x-x-109">Remark 3.</span> </span><span class="cmbx-10x-x-109">(Linearly dependent inputs in the Gram-Schmidt process)</span> What happens if we apply the Gram-Schmidt orthogonalization process to a set of linearly *dependent* vectors?</p>
<p>To get a grip on the problem, let’s consider a simple case of two vectors: <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">2</span></sub> = <span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmsy-10x-x-109">}</span>, where <span class="cmmi-10x-x-109">c </span>is an arbitrary scalar. <span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> is chosen to be <span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub>, and <span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> is defined by</p>
<div class="math-display">
<img src="../media/file215.png" class="math-display" alt="e = v − proj (v ). 2 2 e1 2 "/>
</div>
<p>By expanding the projection term, we obtain</p>
<div class="math-display">
<img src="../media/file216.png" class="math-display" alt="e2 = v2 − ⟨v2,e1⟩e1. ⟨e1,e1⟩ "/>
</div>
<p>Since <span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub> = <span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> and <span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">2</span></sub> = <span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub>, we get that</p>
<span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub>
= <span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">−</span><img src="../media/file217.png" class="frac" data-align="middle" alt="c⟨v1,v1-⟩ ⟨v1,v1⟩"/><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub>
= <span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub>= 0


<p>This result generalizes: when the Gram-Schmidt process encounters an input vector that is linearly dependent from the previous ones, a zero vector is produced in the output.</p>
</div>
</section>
<section id="the-orthogonal-complement" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_34"><span class="titlemark"><span class="cmss-10x-x-109">2.2.6 </span></span> <span id="x1-480003.2.6"></span><span class="cmss-10x-x-109">The orthogonal complement</span></h3>
<p><span class="cmss-10x-x-109">Earlier, we</span> <span id="dx1-48001"></span><span class="cmss-10x-x-109">saw that given a fixed vector </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, we can decompose any </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">as </span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> + <span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub><span class="cmss-10x-x-109">, where </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">o</span></sub> <span class="cmss-10x-x-109">is orthogonal to </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, while </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">p</span></sub> <span class="cmss-10x-x-109">is parallel to it. (We used this to provide a geometric motivation for inner products in </span><span class="cmssi-10x-x-109">Section </span><a href="ch008.xhtml#the-geometric-interpretation-of-inner-products"><span class="cmssi-10x-x-109">2.2.3</span></a><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">This is an essential tool, and in this section, we will see that an analogue of this decomposition still holds true when </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">is replaced with an arbitrary subspace </span><span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">⊂</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. To see this, let’s talk about the </span><span class="cmssi-10x-x-109">orthogonality of subspaces</span><span class="cmss-10x-x-109">. (If you want to recall the definition of subspaces, check out </span><span class="cmssi-10x-x-109">Definition </span><a href="ch007.xhtml#x1-29003r5"><span class="cmssi-10x-x-109">5</span></a><span class="cmss-10x-x-109">.)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-48002r12"></span> <span class="cmbx-10x-x-109">Definition 12.</span> </span><span class="cmbx-10x-x-109">(Orthogonal subspaces)</span></p>
<p>Let <span class="cmmi-10x-x-109">V </span>be an arbitrary inner product space. We say that the subspaces <span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,S</span><sub><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">V </span>are orthogonal if, for every pair of vectors <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">1</span></sub> and <span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">2</span></sub>, we have <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= 0. This is denoted as <span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">⊥</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">2</span></sub>.</p>
</div>
<p><span class="cmss-10x-x-109">For example, the </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">-axis and the </span><span class="cmmi-10x-x-109">y</span><span class="cmss-10x-x-109">-axis are orthogonal subspaces in </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">. (Just as the </span><span class="cmmi-10x-x-109">xy </span><span class="cmss-10x-x-109">plane and the </span><span class="cmmi-10x-x-109">z</span><span class="cmss-10x-x-109">-axis in </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">3</span></sup><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">Similarly, we can talk about the orthogonality of a vector and a subspace: </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is orthogonal to the subspace </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, or </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">⊥</span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">in symbols, if </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is orthogonal to all vectors of </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">One of the most</span> <span id="dx1-48003"></span><span class="cmss-10x-x-109">straightforward and essential ways to construct orthogonal subspaces is to take the </span><span class="cmssi-10x-x-109">orthogonal complement</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-48004r13"></span> <span class="cmbx-10x-x-109">Definition 13.</span> </span><span class="cmbx-10x-x-109">(Orthogonal complement)</span></p>
<p>Let <span class="cmmi-10x-x-109">V </span>be an arbitrary inner product space and let <span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">⊆ </span><span class="cmmi-10x-x-109">V </span>be a subspace. The set defined by</p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file218.png" class="math-display" width="200" alt="S⊥ := {x ∈ V | x ⊥ S }"/>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.16)
  </span>
</div>

<p>is called the <span class="cmti-10x-x-109">orthogonal complement </span>of <span class="cmmi-10x-x-109">S</span>.</p>
</div>
<p><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup> <span class="cmss-10x-x-109">is not just any set; it is a subspace, as we are about to see.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-48005r14"></span> <span class="cmbx-10x-x-109">Theorem 14.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file219.png" class="math" alt="V "/> <span class="cmti-10x-x-109">be an arbitrary inner product space and </span><img src="../media/file220.png" class="math" alt="S ⊆ V "/> <span class="cmti-10x-x-109">one of its subspaces. Then</span></p>
<ol>
<li><span id="x1-48007x1"><img src="../media/file221.png" class="math" alt="S ⊥ "/> <span class="cmti-10x-x-109">is orthogonal to </span><img src="../media/file222.png" class="math" alt="S "/><span class="cmti-10x-x-109">,</span></span></li>
<li><span id="x1-48009x2"><img src="../media/file223.png" class="math" alt="S ⊥ "/> <span class="cmti-10x-x-109">is a subspace of </span><img src="../media/file224.png" class="math" alt="V "/><span class="cmti-10x-x-109">,</span></span></li>
<li><span id="x1-48011x3"><span class="cmti-10x-x-109">and </span><img src="../media/file225.png" class="math" alt=" ⊥ S ∩ S = {0} "/><span class="cmti-10x-x-109">.</span></span></li>
</ol>
</div>
<div id="tcolobox-61" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">According to the definition of subspaces, we only have to show that </span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup> <span class="cmss-10x-x-109">is closed with respect to addition and scalar multiplication. As the inner product is bilinear, this is straightforward:</span></p>
<div class="math-display">
<img src="../media/file226.png" class="math-display" alt="⟨ax + by,z⟩ = a⟨x,z⟩+ b⟨y,z⟩ = 0 "/>
</div>
<p><span class="cmss-10x-x-109">holds for any vectors </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">, </span><span class="cmbx-10x-x-109">z </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, and scalars </span><span class="cmmi-10x-x-109">a,b</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To see that </span><span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup> = <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">0</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, let’s take an arbitrary </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">. By the definition of </span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">, we have </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. As the inner product is positive definite per definition, </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">must be the zero vector </span><span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Recall the decomposition of any </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">into a parallel and an orthogonal component with respect to a fixed vector </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">? In terms of subspaces, we can restate this as</span></p>
<div class="math-display">
<img src="../media/file227.png" class="math-display" alt=" ⊥ V = span(y) + span (y) , "/>
</div>
<p><span class="cmss-10x-x-109">that is, </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">can be</span> <span id="dx1-48012"></span><span class="cmss-10x-x-109">written as the direct sum of the vector space spanned by </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, and its orthogonal complement. This is an extremely powerful result, as this allows us to decouple </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">from </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">. For instance, if we think of vectors as a collection of features (just like the sepal and petal width and length measurements in our favorite Iris dataset), </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">can represent a certain trait that we want to exclude from our analysis.</span></p>
<p><span class="cmss-10x-x-109">With the notion of orthogonal complements, we can make this mathematically precise. We can also be more general. In fact, the decomposition</span></p>
<img src="../media/file228.png" class="math-display" width="150" alt="V = S + S⊥ "/>

<p><span class="cmss-10x-x-109">holds for </span><span class="cmssi-10x-x-109">any </span><span class="cmss-10x-x-109">subspace </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">! Let’s see the proof!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-48013r15"></span> <span class="cmbx-10x-x-109">Theorem 15.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an arbitrary finite-dimensional inner product space and </span><span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">⊂</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">its subspace. Then,</span></p>

<img src="../media/file229.png" width="150" class="math-display" alt="V = S + S⊥ "/>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<div id="tcolobox-62" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Let </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">be a basis of </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">. As during the Gram-Schmidt process, we can define the generalized orthogonal projection (</span><a href="ch008.xhtml#the-gramschmidt-orthogonalization-process"><span class="cmss-10x-x-109">3.2.5</span></a><span class="cmss-10x-x-109">), given by</span></p>
<div class="math-display">
<img src="../media/file230.png" class="math-display" alt=" ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Using this, we can decompose any </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">as</span></p>
<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file231.png" class="math-display" width="350" alt="x = (x − proj_{v1,...,vk}(x)) + proj_{v1,...,vk}(x)"/>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.17)
  </span>
</div>

<p><span class="cmss-10x-x-109">Since</span> proj<sub><span class="cmbx-8">v</span><sub><span class="cmr-6">1</span></sub><span class="cmmi-8">,…,</span><span class="cmbx-8">v</span><sub><span class="cmmi-6">k</span></sub></sub>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">is the linear</span> <span id="dx1-48014"></span><span class="cmss-10x-x-109">combination of </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s, it belongs to </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">. On the other hand, the bilinearity of the inner product gives that </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span> proj<sub><span class="cmbx-8">v</span><sub><span class="cmr-6">1</span></sub><span class="cmmi-8">,…,</span><span class="cmbx-8">v</span><sub><span class="cmmi-6">k</span></sub></sub>(<span class="cmbx-10x-x-109">x</span>) <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">. Indeed, as we have</span></p>
<div class="math-display">
<img src="../media/equation.png" class="math-display" alt=" ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 "/>
</div>
<p><span class="cmss-10x-x-109">the vector </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span> proj<sub><span class="cmbx-8">v</span><sub><span class="cmr-6">1</span></sub><span class="cmmi-8">,…,</span><span class="cmbx-8">v</span><sub><span class="cmmi-6">k</span></sub></sub>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">is orthogonal to each </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">j</span></sub><span class="cmss-10x-x-109">. Thus, since </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">is a basis of </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is also orthogonal to </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, hence </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span> proj<sub><span class="cmbx-8">e</span><sub><span class="cmbx-6">1</span></sub><span class="cmmi-8">,…,</span><span class="cmbx-8">v</span><sub><span class="cmmi-6">k</span></sub></sub>(<span class="cmbx-10x-x-109">x</span>) <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">The fact that every </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">can be decomposed as the sum of a vector from </span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">and a vector from </span><span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">, as given by (</span><a href="ch008.xhtml"><span class="cmss-10x-x-109">3.2.6</span></a><span class="cmss-10x-x-109">), means that </span><span class="cmmi-10x-x-109">V </span>= <span class="cmmi-10x-x-109">S </span>+ <span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">, which is what we had to prove.</span></p>
</div>
</div>
</section>
</section>
<section id="summary1" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_35"><span class="titlemark"><span class="cmss-10x-x-109">2.3 </span></span> <span id="x1-490003.3"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">In this chapter, we have learned that, besides the algebraic structure given by addition and scalar multiplication, vectors have a beautiful geometry that rises from the inner product. From the inner product, we have norms; from norms, we have metrics; and from metrics, we have geometry and topology.</span></p>
<p><span class="cmss-10x-x-109">Distance, similarity, angles, and orthogonality all arise from the simple concept of inner products. These are all extremely useful in both theory and practice. For instance, inner products give us a way to quantify the similarity of two vectors via the so-called cosine similarity, but they also provide a means to find optimal bases through the notion of orthogonality.</span></p>
<p><span class="cmss-10x-x-109">To summarize, we’ve learned what norms and distances are, the definition of the inner product, how inner products give angles and norms, and why all of these are useful in machine learning.</span></p>
<p><span class="cmss-10x-x-109">Besides the basic definitions and properties, we’ve encountered our very first algorithm: the Gram-Schmidt process, turning a set of vectors into an orthonormal basis. This is the best kind of basis.</span></p>
<p><span class="cmss-10x-x-109">In the next section, we’ll take all that theory and put it into practice, taking our first steps in computational linear algebra. Let’s go!</span></p>
</section>
<section id="problems1" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_36"><span class="titlemark"><span class="cmss-10x-x-109">2.4 </span></span> <span id="x1-500003.4"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a vector space and define the function </span><span class="cmmi-10x-x-109">d </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span> [0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmss-10x-x-109">by</span></p>
<div class="math-display">
<img src="../media/file234.png" class="math-display" alt=" ( |{ 0 if x = y, d(x,y) = |( 1 otherwise. "/>
</div>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Show that </span><span class="cmmi-10x-x-109">d </span><span class="cmss-10x-x-109">is a metric (see </span><span class="cmssi-10x-x-109">Definition </span><a href="ch008.xhtml#x1-41003r8"><span class="cmssi-10x-x-109">8</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Show that </span><span class="cmmi-10x-x-109">d </span><span class="cmss-10x-x-109">cannot come from a norm.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">be the set of all ASCII strings of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">character length and define the Hamming distance </span><span class="cmmi-10x-x-109">h</span>(<span class="cmmi-10x-x-109">x,y</span>) <span class="cmss-10x-x-109">for any two </span><span class="cmmi-10x-x-109">x,y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">by the number of corresponding positions where </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">y </span><span class="cmss-10x-x-109">are different.</span></p>
<p><span class="cmss-10x-x-109">For instance,</span></p>
<div class="math-display">
<img src="../media/file235.png" class="math-display" alt="h(”001101”,”101110”) = 2, h(”metal”,”petal”) = 1. "/>
</div>
<p><span class="cmss-10x-x-109">Show that </span><span class="cmmi-10x-x-109">h </span><span class="cmss-10x-x-109">satisfies the three defining properties of a metric. (Note that </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is not a vector space so, technically, the Hamming distance is not a metric.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmsy-10x-x-109">∥⋅∥ </span><span class="cmss-10x-x-109">be a norm on the vector space </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, and define the mapping </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file236.png" class="math-display" alt="f : (x1,x2,...,xn ) ↦→ (x1,2x2,...,nxn ). "/>
</div>
<p><span class="cmss-10x-x-109">Show that</span></p>
<div class="math-display">
<img src="../media/file237.png" class="math-display" alt="∥x ∥∗ := ∥f(x)∥ "/>
</div>
<p><span class="cmss-10x-x-109">is a norm on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,a</span><sub><span class="cmmi-8">n</span></sub><span class="cmmi-10x-x-109">/span&gt;0 <span class="cmss-10x-x-109">be arbitrary positive numbers. Show that</span> </span></p>
<div class="math-display">
<img src="../media/file238.png" class="math-display" alt=" n ∑ n ⟨x,y ⟩ := aixiyi, x,y ∈ ℝ . i=1 "/>
</div>
<p><span class="cmss-10x-x-109">is an inner product, where </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span>= (<span class="cmmi-10x-x-109">y</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,y</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 5. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a finite-dimensional inner product space, let </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a basis in </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, and define</span></p>
<div class="math-display">
<img src="../media/file239.png" class="math-display" alt="ai,j := ⟨vi,vj⟩. "/>
</div>
<p><span class="cmss-10x-x-109">Show that for any </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file240.png" class="math-display" alt=" ∑n ⟨x,y ⟩ = ai,jxiyj, i,j=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">x </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">y</span><sub><span class="cmmi-8">i</span></sub><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 6. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a finite-dimensional real inner product space.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Let </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be an arbitrary vector. Show that</span></p>
<div class="math-display">
<img src="../media/file241.png" class="math-display" alt="f : V → ℝ, x ↦→ ⟨x,y⟩ "/>
</div>
<p><span class="cmss-10x-x-109">is a linear function (that is, </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">α</span><span class="cmbx-10x-x-109">u </span>+ <span class="cmmi-10x-x-109">β</span><span class="cmbx-10x-x-109">v</span>) = <span class="cmmi-10x-x-109">αf</span>(<span class="cmbx-10x-x-109">u</span>) + <span class="cmmi-10x-x-109">βf</span>(<span class="cmbx-10x-x-109">v</span>) <span class="cmss-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">u</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">α,β </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">an arbitrary linear function. Show that there exists a </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">such that</span></p>
<img src="../media/file242.png" class="math-display" width="150" alt="f(x) = ⟨x,y⟩. "/>

<p><span class="cmss-10x-x-109">(Note that </span><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">is the reverse of </span><span class="cmssi-10x-x-109">(a)</span><span class="cmss-10x-x-109">, and a much more interesting result.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 7. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a real inner product space and let </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥ </span>= <img src="../media/file243.png" class="sqrt" alt="∘ ------ ⟨x,x ⟩"/> <span class="cmss-10x-x-109">be the generated norm. Show that</span></p>


<div class="display-math" style="text-align: center; position: relative; margin: 1em 0;">
  <span class="eq-body" style="display: inline-block;">
    <img src="../media/file244.png" class="math-display" width="300" alt="Equation 2.16"/>
  </span>
  <span class="eq-number" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%);">
    (2.18)
  </span>
</div>

<p><span class="cmss-10x-x-109">This is called the </span><span class="cmssi-10x-x-109">parallelogram law</span><span class="cmss-10x-x-109">, because if we think of </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">y </span><span class="cmss-10x-x-109">as the two sides determining a parallelogram, (</span><a href="ch008.xhtml#problems1"><span class="cmss-10x-x-109">3.4</span></a><span class="cmss-10x-x-109">) relates the length of its sides to the length of its diagonals.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 8. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a real inner product space and let </span><span class="cmbx-10x-x-109">u</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. Show that if</span></p>
<img src="../media/file245.png" class="math-display" width="150" alt="⟨u, x⟩ = ⟨v, x⟩ "/>

<p><span class="cmss-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, then </span><span class="cmbx-10x-x-109">u </span>= <span class="cmbx-10x-x-109">v</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 9. </span><span class="cmss-10x-x-109">Apply the Gram-Schmidt process to the input vectors</span></p>
<img src="../media/file246.png" class="math-display" alt="v1 = (2,1,1), v2 = (1,1,1), v3 = (1,0,1). " width="150"/>

</section>
<section id="join-our-community-on-discord2" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_37"><span id="x1-51000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" height="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>