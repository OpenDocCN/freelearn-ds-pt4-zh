- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Geometric Structure of Vector Spaces
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s revisit the Iris dataset introduced in the previous chapter! I want to
    test your intuition. I plotted the petal widths against the petal lengths while
    hiding the class labels in Figure 2.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The “petal width” and “petal length” features of the Iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Even without knowing any labels, we can intuitively point out that there are
    probably at least two classes. Can you summarize your reasoning in a single sentence?
  prefs: []
  type: TYPE_NORMAL
- en: There are many valid arguments, but the most prevalent one is that the two clusters
    are far away from each other. As this example illustrates, the concept of distance
    plays an essential role in machine learning. In this chapter, we will translate
    the notion of distance into the language of mathematics and put it into the context
    of vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Norms and distances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we saw that vectors are essentially arrows, starting from the null
    vector. In addition to their direction, vectors also have magnitude. For example,
    as we have learned in high school mathematics, the magnitude in the Euclidean
    plane is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∘ ------- 2 2 ∥x ∥ = x1 + x 2, x = (x1,x2), ](img/file70.png)'
  prefs: []
  type: TYPE_IMG
- en: while we can calculate the distance between x and y as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∘ --------2-----------2 d(x,y) = (x1 − y1) + (x2 − y2) . ](img/file71.png)'
  prefs: []
  type: TYPE_IMG
- en: (The function ∥⋅∥ simply denotes the magnitude of a vector.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Magnitude in the Euclidean plane'
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude formula ![∘ ------- x21 + x22](img/file74.png) can be simply generalized
    to higher dimensions by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∘ ------------ 2 2 n ∥x ∥ = x1 + ⋅⋅⋅+ xn, x = (x1,...,xn) ∈ ℝ . ](img/file75.png)'
  prefs: []
  type: TYPE_IMG
- en: However, just from looking at this formula, it is not clear why it is defined
    this way. What does the square root of a sum of squares have to do with distance
    and magnitude? Behind the scenes, it is just the Pythagorean theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the Pythagorean theorem states that in right triangles, the squared
    length of the hypotenuse equals the sum of the squared lengths of the other sides,
    as illustrated by Figure [2.3](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The Pythagorean theorem'
  prefs: []
  type: TYPE_NORMAL
- en: To put this into an algebraic form, it states that a² + b² = c², when c is the
    hypotenuse of the right triangle, and a and b are its two other sides. If we apply
    this to a two-dimensional vector x = (x[1],x[2]), we can see that the Pythagorean
    theorem gives its magnitude ∥x∥ = ![∘ -2----2 x1 + x2](img/file77.png) .
  prefs: []
  type: TYPE_NORMAL
- en: This can be generalized to higher dimensions. To see what is happening, we are
    going to check the three-dimensional case, as illustrated by Figure [1.4](#).
    Here, we can apply the Pythagorean theorem twice to obtain the magnitude!
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The Pythagorean theorem in three dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: For each vector x = (x[1],x[2],x[3]), we can take a look at the triangle determined
    by (0,0,0),(x[1],0,0), and (x[1],x[2],0) first. The length of the hypotenuse can
    be calculated by ![∘ ------- x21 + x22](img/file79.png). However, the points (0,0,0),
    (x[1],x[2],0), and (x[1],x[2],x[3]) form a right triangle. Applying the Pythagorean
    theorem once again, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∘ ------------ ∥x∥ = x2 + x2+ x2, 1 2 3 ](img/file80.png)'
  prefs: []
  type: TYPE_IMG
- en: which is called the Euclidean norm. This is exactly what is going on in the
    general n-dimensional case.
  prefs: []
  type: TYPE_NORMAL
- en: The notions of magnitude and distance are critical in machine learning, as we
    can use them to determine the similarity between data points, measure and control
    the complexity of neural networks, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Is the Pythagorean theorem the only viable way to measure magnitude and distance?
    Certainly not.
  prefs: []
  type: TYPE_NORMAL
- en: Because Manhattan’s street layout is essentially a rectangular grid, its residents
    are famed for measuring distances in blocks. If something is two blocks to the
    north and three blocks east, it means that you have to travel two intersections
    to the north and three to the east to find it. This gives rise to a mathematically
    perfectly valid notion of measurement called Manhattan distance, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![d(x,y ) = |x1 − y1| + |x2 − y2|. ](img/file81.png)'
  prefs: []
  type: TYPE_IMG
- en: When using the Manhattan distance, the shortest path between two points is not
    unique.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: For the Manhattan distance, the shortest path between two points
    is not unique'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the Euclidean and Manhattan distances, there are several other metrics.
    Once again, we are going to step away from the concrete examples to take an abstract
    viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we talk about measurements and metrics in general, what are the properties
    that we expect from all of them? What makes a measurement distance? Essentially,
    there are three such traits:'
  prefs: []
  type: TYPE_NORMAL
- en: the distance should be nonnegative,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it should preserve scaling (that is, ![d(cx,cy) = cd(x,y ) ](img/file83.png)
    for all scalars ![c ](img/file84.png)),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the distance straight from point ![x ](img/file85.png) to ![y ](img/file86.png)
    is always equal to or smaller than touching any other point ![z ](img/file87.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are formalized by the notion of norms.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 7\. (Norms)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![V ](img/file88.png) be a vector space. A function ![∥⋅∥ : V → [0,∞ )
    ](img/file89.png) is said to be a norm if for all ![x, y ∈ V ](img/file90.png),
    the following properties hold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Positive definiteness: ![∥x ∥ ≥ 0 ](img/file91.png) and ![∥x∥ = 0 ](img/file92.png)
    if and only if ![x = 0 ](img/file93.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Positive homogeneity: ![∥cx∥ = |c|∥x ∥ ](img/file94.png) for all ![c ∈ ℝ ](img/file95.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Triangle inequality: ![∥x + y ∥ ≤ ∥x ∥+ ∥y ∥ ](img/file96.png) for all ![x,y
    ∈ V ](img/file97.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A vector space equipped with a norm is called a normed space.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some examples!
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. Let p ∈ [1,∞) and define
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n p 1∕p ∥x∥p = ( |xi|) , x = (x1,...,xn ) i=1 ](img/file98.png)'
  prefs: []
  type: TYPE_IMG
- en: on ℝ^n. The function ∥⋅∥[p] is called the p-norm. Showing that ∥⋅∥[p] is indeed
    a norm is a bit technical. Thus, we won’t go into the details. (The triangle inequality
    requires some work, but the other two properties are easy to see.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen two special cases: the Euclidean norm (p = 2) and the
    Manhattan norm (p = 1). Both of them frequently appear in machine learning. For
    instance, the familiar mean squared error is just the scaled Euclidean distance
    between prediction and ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n MSE (y,yˆ) =-1∥y − ˆy∥2 = 1-∑ (y − ˆy )2 n 2 n i i i=1 ](img/file99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned before, the 2-norm, along with the 1-norm, is commonly used to
    control the complexity of models during training. To give a concrete example,
    suppose that we are fitting a polynomial f(x) = ∑ [i=0]^mq[i]x^i to the data {(x[1],y[1]),…,(x[n],y[n])}
    . To obtain a model that generalizes well to new data, we prefer our models to
    be as simple as possible. Thus, instead of using the plain mean squared error,
    we might consider minimizing the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss(y,ˆy,q) = MSE (y, ˆy)+ λ∥q ∥p, q = (q0,q1,...,qm), λ ∈ [0,∞ ) ](img/file100.png)'
  prefs: []
  type: TYPE_IMG
- en: where the term ∥q∥[p] is responsible for keeping the coefficients of the polynomial
    f(x) small, while λ controls the strength of regularization. Usually, p is either
    1 or 2, but other values from [1,∞) are also valid.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. Let’s stay in ℝ^n for a bit longer! The so-called ∞-norm is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![∥x ∥ = max {|x |,...,|x |}. ∞ 1 n ](img/file101.png)'
  prefs: []
  type: TYPE_IMG
- en: Showing that ∥⋅∥[∞] is indeed a norm is a simple task and left to you for practice.
    (This is perhaps one of the most notorious sentences written in mathematical textbooks,
    but trust me, this is truly easy. Give it a shot! If you don’t see it, try the
    special case ℝ².)
  prefs: []
  type: TYPE_NORMAL
- en: This is called the ∞-norm, and is strongly related to the p-norm that we have
    just seen. In fact, if we let the value p grow infinitely, ∥x∥[p] will be very
    close to ∥x∥[∞] , ultimately reaching it at the limit.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 2\. (The ∞-norm as the limit of p-norm)
  prefs: []
  type: TYPE_NORMAL
- en: If you are already familiar with convergent sequences and limits, you can see
    that this is called the ∞-norm because
  prefs: []
  type: TYPE_NORMAL
- en: '![lim ∥x∥ = ∥x∥ . p→ ∞ p ∞ ](img/file102.png)'
  prefs: []
  type: TYPE_IMG
- en: To show this, consider that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n p 1∕p ∑n -|xi|-p 1∕p pli→m∞ ∥x ∥p = pl→im∞ ( |xi|) = pli→m∞ ∥x∥∞ ( (∥x
    ∥∞ )) . i=1 i=1 ](img/file103.png)'
  prefs: []
  type: TYPE_IMG
- en: Since ![-|xi|- ∥x∥∞](img/file104.png) ≤ 1 by definition,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n |xi| 1 ≤ ( (------)p)1∕p ≤ n1 ∕p i=1 ∥x ∥∞ ](img/file105.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. Because
  prefs: []
  type: TYPE_NORMAL
- en: '![lim n1∕p = 1, p→∞ ](img/file106.png)'
  prefs: []
  type: TYPE_IMG
- en: we can conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '![lim ∥x∥p = ∥x∥∞. p→ ∞ ](img/file107.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the reason why the ∞-norm is considered a p-norm with p = ∞.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar with taking limits of sequences, don’t worry. We’ll
    cover everything in detail when studying single-variable calculus.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. ∞-norms can be generalized for function spaces. Remember C([0,1]),
    the vector space of functions continuous on [0,1]? We introduced this when talking
    about examples of vector spaces in Section [1.1.1](ch007.xhtml#examples-of-vector-spaces).
    There, ∥⋅∥[∞] can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![∥f∥∞ = sup |f (x )|. x∈[0,1] ](img/file108.png)'
  prefs: []
  type: TYPE_IMG
- en: This norm can be defined on other function spaces, like C(ℝ), the space of continuous
    real functions. Since the maximum is not guaranteed to exist (as for the sigmoid
    function in C(ℝ)), the maximum is replaced with supremum. Hence, the ∞-norm is
    often called the supremum norm.
  prefs: []
  type: TYPE_NORMAL
- en: If you imagine the function as a landscape, the supremum norm is the height
    of the highest peak or the depth of the deepest trench (whichever is larger in
    absolute value).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file109.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: The supremum norm'
  prefs: []
  type: TYPE_NORMAL
- en: When encountering this norm for the first time, it might seem challenging to
    understand what this has to do with any notion of magnitude. However, ∥f −g∥[∞]
    is a natural way to measure the distance between two functions f and g, and in
    general, magnitude is just the distance from 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file110.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: The distance between two functions, given by the supremum norm'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Defining distances from norms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides measuring the magnitude of vectors, we are also interested in measuring
    the distance between them. If you are at the location x in some normed space,
    how far is y? In normed vector spaces, we can define the distance between any
    x and y by
  prefs: []
  type: TYPE_NORMAL
- en: '![d(x,y) = ∥x− y∥. ](img/file111.png)'
  prefs: []
  type: TYPE_IMG
- en: This is called the norm-induced metric. Thus, norms measure the distance from
    the zero vector, and the metric d measures the norm of the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, we say that a function d : V ×V → [0,∞) is a metric if the following
    hold.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 8\. (Metrics)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![V ](img/file112.png) be a vector space and d : V ×V → [0,∞) be a function.
    d is a metric if the following conditions hold for all x,y,z ∈ V:'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever d(x,y ) = 0 , we have x = y (positive definiteness).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: d (x, y) = d (y, x) (symmetry).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: d (x, z) ≤ d(x,y )+ d(y,z)(triangle inequality).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the immediate consequences of the definition is that if x≠y, then d(x,y)/span>0\.
    (As the positive definiteness gives that d(x,y) = 1 implies x = y.)
  prefs: []
  type: TYPE_NORMAL
- en: Given the properties of norms, we can quickly check that d(x,y) = ∥x−y∥ is indeed
    a metric. Due to the linear structure of vector spaces, the norm-generated metric
    is invariant to translation. That is, for any x,y,z ∈V , we have
  prefs: []
  type: TYPE_NORMAL
- en: '![d(x,y) = d(x + z,y + z). ](img/file120.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, it doesn’t matter where you start: the distance only depends
    on your displacement. This is not true for any metric. Thus, norm-induced metrics
    are special. In our studies, we only deal with these special cases. Because of
    this, we won’t even talk about metrics, just norms.'
  prefs: []
  type: TYPE_NORMAL
- en: In itself, a vector space is just a skeleton that provides a way to represent
    data. On top of this, norms define a geometric structure that reveals properties
    such as magnitude and distance. Both of these are essential in machine learning.
    For instance, some unsupervised learning algorithms separate data points into
    clusters based on their mutual distances from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is yet another way to enhance the geometric structure of vector spaces:
    inner products, also called dot products. We are going to put this concept under
    our magnifying glass in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Inner products, angles, and lots of reasons to care about them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we imbued our vector spaces with norms, measuring the
    magnitude of vectors and the distance between points. In machine learning, these
    concepts can be used, for instance, to identify clusters in unlabeled datasets.
    However, without context, distance is often not enough. Following our geometric
    intuition, we can aspire to measure the similarity of data points. This is done
    by the inner product (also known as the dot product).
  prefs: []
  type: TYPE_NORMAL
- en: You can recall the inner product as a quantity that we used to measure the angle
    between two vectors in high school geometry classes. Given two vectors x = (x[1],x[2])
    and y = (y[1],y[2]) from the plane, we defined their inner product by
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x, y⟩ = x y + x y , 1 1 2 2 ](img/file121.png)'
  prefs: []
  type: TYPE_IMG
- en: for which it can be shown that
  prefs: []
  type: TYPE_NORMAL
- en: ⟨x,y⟩ = ∥x∥∥y∥ cosα (2.1)
  prefs: []
  type: TYPE_NORMAL
- en: holds, where α is the angle between x and y. (In fact, there are two such angles,
    but their cosine is equal.) Thus, the angle itself can be extracted by
  prefs: []
  type: TYPE_NORMAL
- en: '![ -⟨x,y-⟩ α = arccos∥x ∥∥y∥, ](img/file122.png)'
  prefs: []
  type: TYPE_IMG
- en: where arccosx is the inverse of the cosine function. We can use the inner products
    to determine whether two vectors are orthogonal, as this happens if and only if
    ⟨x,y⟩ = 0 holds. During our earlier encounters with mathematics, geometric intuition
    (such as orthogonality) came first, on which we built tools such as the inner
    product. However, if we zoom out and take an abstract viewpoint, things are exactly
    the opposite. As we’ll see soon, inner products emerge quite naturally, giving
    rise to the general concept of orthogonality.
  prefs: []
  type: TYPE_NORMAL
- en: In general, this is the formal definition of an inner product.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 9\. (Inner products and inner product spaces)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be a real vector space. The function ⟨⋅,⋅⟩ : V × V → ℝ is called an inner
    product if the following holds for all x,y, z ∈ V and a ∈ ℝ:'
  prefs: []
  type: TYPE_NORMAL
- en: ⟨ax + y,z⟩ = a⟨x,z⟩ + ⟨y, z⟩ (linearity of the first variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ⟨x,y ⟩ = ⟨y,x ⟩ (symmetry).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ⟨x,x ⟩ >0 for all ![x ⁄= 0 ](img/file130.png) (positive definiteness).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vector spaces with an inner product are called inner product spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Right off the bat, we can immediately deduce two properties. First,
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨0,x ⟩ = ⟨0x,x ⟩ = 0⟨x,x⟩ = 0.](img/file131.png) (2.2)'
  prefs: []
  type: TYPE_IMG
- en: 'As a special case, ⟨0,0⟩ = 0\. Just like we have seen for norms, a bit more
    is true: if ⟨x,x⟩ = 0, then x = 0\. This follows from positive definiteness and
    ([2.2](ch008.xhtml#x1-42011x3.2)).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, due to the symmetry and linearity of the first variable, inner
    products are also linear in the second variable. Because of this, they are called
    bilinear.
  prefs: []
  type: TYPE_NORMAL
- en: To familiarize ourselves with the concept, let’s see some examples!
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. As usual, the canonical and most prevalent example of inner product
    spaces is ℝ^n, where the inner product ⟨⋅,⋅⟩ is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ⟨x,y⟩ = xiyi, x = (x1,...,xn ), y = (y1,...,yn). i=1 ](img/file132.png)'
  prefs: []
  type: TYPE_IMG
- en: This bilinear function is often called the dot product. Equipped with this,
    ℝ^n is called the n-dimensional Euclidean space. This is a central concept in
    machine learning, as data is most frequently represented in Euclidean spaces.
    Thus, we are going to explore the structure of this space in great detail throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. Besides Euclidean spaces, there are other inner product spaces that
    play a significant role in mathematics and machine learning. If you are familiar
    with integration, in certain function spaces, the bilinear function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∫ ∞ ⟨f,g⟩ = f(x)g(x)dx −∞ ](img/file133.png)'
  prefs: []
  type: TYPE_IMG
- en: defines an inner product space with a very rich and beautiful structure.
  prefs: []
  type: TYPE_NORMAL
- en: The symmetry and linearity of ⟨f,g⟩ is clear. Only the positive definiteness
    seems to be an issue.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if f is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{1 if x = 0, f(x) = |(0 otherwise, ](img/file134.png)'
  prefs: []
  type: TYPE_IMG
- en: then f≠0, but ⟨f,f⟩ = 0\. This problem can be circumvented by “overloading”
    the equality operator and letting f = g if and only if ∫ [−∞]^∞|f(x) −g(x)|² dx
    = 0\. Even though function spaces such as this play an important role in mathematics
    and machine learning, their study falls outside of our scope.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 The generated norm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that the 2-norm in ℝ^n was defined by ∥x∥[2] = (∑ [i=1]^nx[i]²)^(1∕2)
    , which, according to our definition of the inner product there, equals ![∘ ------
    ⟨x,x⟩](img/file137.png). This is not a coincidence. Inner products can be used
    to define norms on vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show exactly how, we need a simple tool: the Cauchy-Schwarz inequality.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 8\. (Cauchy-Schwarz inequality)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an inner product space. Then, for any x,y ∈ V, the inequality
  prefs: []
  type: TYPE_NORMAL
- en: '![|⟨x, y⟩|2 ≤ ⟨x,x⟩⟨y,y⟩ ](img/file140.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. At this point, we don’t know much about the inner product except its
    core defining properties. So, we are going to use a little trick. For any λ ∈
    ℝ, the positive definiteness implies that
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x + λy,x + λy ⟩ ≥ 0\. ](img/file142.png)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, because of bilinearity (that is, linearity in both variables)
    and symmetry, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![2 ⟨x + λy, x+ λy ⟩ = ⟨x,x ⟩+ 2λ⟨x,y ⟩+ λ ⟨y,y⟩,](img/file143.png) (2.3)'
  prefs: []
  type: TYPE_IMG
- en: which is a quadratic polynomial in λ. In general, we know that for any quadratic
    polynomial of the form ax2 + bx + c, the roots are given by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ √ -------- − b-±--b2 −-4ac- x1,2 = 2a . ](img/file146.png)'
  prefs: []
  type: TYPE_IMG
- en: Since
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x + λy,x + λy ⟩ ≥ 0, ](img/file147.png)'
  prefs: []
  type: TYPE_IMG
- en: the polynomial defined by (2.3) must have at most one real root. Thus, the discriminant
    b² − 4ac is non-positive. Plugging in the coefficients of the polynomial (2.3)
    into the discriminant formula, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![|⟨x, y⟩|2 − ⟨x, x⟩⟨y,y⟩ ≤ 0, ](img/file148.png)'
  prefs: []
  type: TYPE_IMG
- en: which completes the proof.
  prefs: []
  type: TYPE_NORMAL
- en: The Cauchy-Schwarz inequality is probably one of the most useful tools in studying
    inner product spaces. One application we are going to see next is to show how
    inner products define norms.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 9\. (The norm generated by the inner product)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be an inner product space. Then, the function ∥⋅∥ : V → [0,∞ ) defined
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∘ ------ ∥x∥ = ⟨x,x⟩ ](img/file151.png)'
  prefs: []
  type: TYPE_IMG
- en: is a norm on V.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof. According to the definition of norms, we have to show that three properties
    hold: positive definiteness, homogeneity, and the triangle inequality. The first
    two follow easily from the properties of inner products. The triangle inequality
    follows from the Cauchy-Schwarz inequality:'
  prefs: []
  type: TYPE_NORMAL
- en: ∥x + y∥²  = ⟨x + y,x + y⟩
  prefs: []
  type: TYPE_NORMAL
- en: = ∥x∥² + ∥y∥² + 2⟨x,y⟩
  prefs: []
  type: TYPE_NORMAL
- en: ≤ ∥x∥² + ∥y∥² + 2∥x∥∥y∥
  prefs: []
  type: TYPE_NORMAL
- en: = (∥x∥ + ∥y∥)²,
  prefs: []
  type: TYPE_NORMAL
- en: from which the triangle inequality follows.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, inner product spaces are normed spaces as well. They have the proper algebraic
    and geometric structure that we need to represent, manipulate, and transform data.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, Theorem [9](ch008.xhtml#x1-43006r9) can be reversed! That
    is, given a norm ∥⋅∥, we can define a matching inner product.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 10\. (The polarization identity)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an inner product space, and let ∥⋅∥be the norm induced by the inner
    product. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![1 ⟨x, y⟩ = -(∥x+ y∥2 − ∥x ∥2 − ∥y ∥2). 2](img/file155.png) (2.4)'
  prefs: []
  type: TYPE_IMG
- en: In other words, one can generate an inner product from a norm, not just the
    other way around.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As the inner product is bilinear, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x + y,x + y⟩ = ⟨x,x⟩+ 2⟨x, y⟩+ ⟨y,y ⟩, ](img/file156.png)'
  prefs: []
  type: TYPE_IMG
- en: from which the polarization identity (2.4) follows.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Orthogonality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In vector spaces other than ℝ², the concept of enclosed angles is not clear
    at all. For instance, in spaces where vectors are functions, there is no intuitive
    way to define the angles between two functions. However, as (2.1) suggests, in
    the special case ℝ², these can be generalized.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 10\. (Orthogonality of vectors)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an inner product space, and let x,y ∈V . We say that x and y are orthogonal
    if
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x,y ⟩ = 0\. ](img/file157.png)'
  prefs: []
  type: TYPE_IMG
- en: Orthogonality is denoted as x ⊥y.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how inner products and orthogonality define geometry on vector
    spaces, let’s see how the classic Pythagorean theorem looks in this new form.
    Recall that the “original” version states that in right triangles, a² + b² = c²,
    where c is the length of the hypotenuse, while a and b are the lengths of the
    other two sides.
  prefs: []
  type: TYPE_NORMAL
- en: In inner product spaces, this generalizes in the following way.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 11\. (The Pythagorean theorem)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an inner product space, and let x,y ∈V . Then, x and y are orthogonal
    if and only if
  prefs: []
  type: TYPE_NORMAL
- en: ⟨x + y,x + y⟩ = ⟨x,x⟩ + ⟨y,y⟩. (2.5)
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Given the definition of inner products and orthogonality, the proof is
    straightforward. Due to bilinearity, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x+ y, x+ y ⟩ = ⟨x,x + y⟩ + ⟨y,x + y⟩ = ⟨x,x ⟩+ 2⟨x,y⟩ + ⟨y,y⟩. ](img/file158.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since x and y are orthogonal, we have ⟨x,y⟩ = 0\. Thus, the equation simplifies
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x + y, x+ y ⟩ = ⟨x,x ⟩+ ⟨y,y⟩. ](img/file159.png)'
  prefs: []
  type: TYPE_IMG
- en: This completes the proof.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this the Pythagorean theorem in another form? Because the norm and the
    inner product is related by ⟨x,x⟩ = ∥x∥², ([11](ch008.xhtml#x1-44003r11)) is equivalent
    to
  prefs: []
  type: TYPE_NORMAL
- en: '![∥x + y∥2 = ∥x∥2 + ∥y∥2, ](img/file160.png)'
  prefs: []
  type: TYPE_IMG
- en: which is exactly the famous “![ 2 2 2 a + b = c ](img/file161.png)”.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 The geometric interpretation of inner products
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Looking at the general definition, it is hard to get an insight into the inner
    product. However, by using the concept of orthogonality, we can visualize what
    ⟨x,y⟩ represents for any x and y.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, any x can be decomposed into the sum of two vectors x[o] + x[p],
    where x[o] is orthogonal to y and x[p] is parallel to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file162.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Decomposition of x into components parallel and orthogonal to y'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make the intuition precise. How can we find x[p] and x[o]? Since x[p]
    has the same direction as y, it can be written in the form x[p] = cy for some
    scalar c ∈ℝ. Because x[p] and x[o] sum up to x, we also have x[o] = x −x[p] =
    x −cy.
  prefs: []
  type: TYPE_NORMAL
- en: Since x[o] is orthogonal to y, the constant c can be determined by solving the
    equation
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x − cy,y⟩ = 0\. ](img/file163.png)'
  prefs: []
  type: TYPE_IMG
- en: By using the bilinearity of the inner product, we can express c from this equation.
    Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![c = ⟨x,y⟩. ⟨y,y⟩ ](img/file164.png)'
  prefs: []
  type: TYPE_IMG
- en: So,
  prefs: []
  type: TYPE_NORMAL
- en: '![xp = ⟨x,y⟩-y, ⟨y,y ⟩ ⟨x,y⟩- xo = x − ⟨y,y⟩y.](img/file165.png) (2.6)'
  prefs: []
  type: TYPE_IMG
- en: We call x[p] the orthogonal projection of x onto y. This is a common transformation,
    so we are going to introduce the notation
  prefs: []
  type: TYPE_NORMAL
- en: proj[y](x) = ![⟨x,y-⟩ ⟨y,y ⟩](img/file166.png) y. (2.7)
  prefs: []
  type: TYPE_NORMAL
- en: From this, we can see that the scaling ratio between y and proj[y](x) can be
    described by inner products.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen that we can use inner products to define the orthogonality
    relation between two vectors. Can we use it to measure (and, in some cases, even
    define) the angle? The answer is yes! In the following, we are going to see how,
    arriving at the formula (2.1) already familiar from basic geometry.
  prefs: []
  type: TYPE_NORMAL
- en: To build our intuition, let’s select two arbitrary n-dimensional vectors x,y
    ∈ℝ^n. The inner product of the sum x + y can be calculated using the bilinearity
    property.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file167.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The sum of x and y'
  prefs: []
  type: TYPE_NORMAL
- en: With this, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x + y,x + y⟩ = ∥x+ y∥2 = ∥x∥2 + ∥y∥2 + 2⟨x,y ⟩. ](img/file168.png) (2.8)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, considering that x, y, and x + y form a triangle, we can
    use the law of cosines ( [https://en.wikipedia.org/wiki/Law_of_cosines](https://en.wikipedia.org/wiki/Law_of_cosines))
    to express ⟨x + y,x + y⟩ = ∥x + y∥² in a different form.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the law of cosines implies
  prefs: []
  type: TYPE_NORMAL
- en: ∥x + y∥²  = ∥x∥² + ∥y∥² − 2∥x∥∥y∥ ![c◟os(π◝ −◜-α-)◞](img/file169.png) [=−cos α]
    . (2.9)
  prefs: []
  type: TYPE_NORMAL
- en: By combining (2.8) and (2.9), we get
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x,y⟩ = ∥x∥∥y ∥cosα. ](img/file170.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, in ℝ^n, the angle enclosed by x and y can be extracted by
  prefs: []
  type: TYPE_NORMAL
- en: α = arccos ![ ⟨x,y⟩ ------- ∥x∥∥y ∥](img/file171.png) . (2.10)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: The triangle formed by x, y, and x + y'
  prefs: []
  type: TYPE_NORMAL
- en: What about vector spaces where the angle between vectors is not defined? We
    have seen instances of vector spaces (Section [1.1.1](ch007.xhtml#examples-of-vector-spaces))
    where the elements are polynomials, functions, and other mathematical objects.
    There, (2.10) can be used to define the angle!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore this idea further and see how to use inner products to measure
    similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Given our geometric interpretation of inner products as orthogonal projections,
    let’s focus on the case when both x and y have unit norms. In this special case,
    the orthogonal projection equals
  prefs: []
  type: TYPE_NORMAL
- en: '![projy(x) = ⟨x,y ⟩y (∥x∥ = ∥y∥ = 1). ](img/file173.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, ⟨x,y⟩ precisely describes the signed magnitude of the orthogonal projection.
    (It can be negative when ![projy(x ) ](img/file174.png) and y have an opposite
    direction.)
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we can see that the inner product is equal to the cosine
    of the angle enclosed by the two vectors. Let’s draw a picture to illustrate!
    (Recall that in right triangles, the cosine is the ratio of the length of the
    adjacent side and the hypotenuse. In this case, the adjacent side has a length
    of ⟨x,y⟩, while the hypotenuse is of unit length.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file175.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: The inner product of two unit vectors equals the cosine of their
    angle'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, this quantity is frequently used to measure the similarity
    of two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Because any vector x can be scaled to unit norm with the transformation x→x∕∥x∥,
    we define the cosine similarity by
  prefs: []
  type: TYPE_NORMAL
- en: cos(x,y) = ![⟨](img/file177.png) ![‑x‑‑ ∥x∥](img/file178.png) , ![‑y‑‑ ∥y∥](img/file179.png)
    ![⟩](img/file180.png) . (2.11)
  prefs: []
  type: TYPE_NORMAL
- en: If x and y represent the feature vectors of two data samples, cos(x,y) tells
    us how much the features move together. Note that because of the scaling, two
    samples with a high cosine similarity can be far from each other. So, this reveals
    nothing about their relative positions in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Orthogonal and orthonormal bases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Through the lens of similarity, orthogonality means that one vector does not
    contain “information” about the other. We will make this notion more precise when
    learning about correlation, but there are clear implications regarding the structure
    of inner product spaces. Recall that during the introduction of basis vectors
    (Section [1.2](ch007.xhtml#the-basis)), our motivation was to find a minimal set
    of vectors that can be used to express any other vector. With the introduction
    of orthogonality, we can go one step further.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 11\. (Orthogonal and orthonormal bases)
  prefs: []
  type: TYPE_NORMAL
- en: Let ![V ](img/file181.png) be a vector space and S = {v1,...,vn } its basis.
    We say that ![S ](img/file183.png) is an orthogonal basis if ⟨vi,vj⟩ = 0 whenever
    ![i ⁄= j ](img/file185.png).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, ![S ](img/file186.png) is called orthonormal if
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 1, if i = j, ⟨vi,vj⟩ = |( 0, if i ⁄= j. ](img/file187.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, ![S ](img/file188.png) is orthonormal if, in addition to being
    orthogonal, each vector has unit norm.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal and orthonormal bases are extremely convenient to use. If a basis
    is orthogonal, we can easily obtain an orthonormal basis by simply scaling its
    vectors to unit norm. Thus, we’ll use orthonormal basis vectors most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we love orthonormal bases so much? To see this, let {v[1],…,v[n]} be
    an arbitrary basis and let ![x ](img/file189.png) be an arbitrary vector. We know
    that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n x = xivi, i=1 ](img/file190.png)'
  prefs: []
  type: TYPE_IMG
- en: but how do we find the coefficients x[i]? There is a general method involving
    linear equations that we will see later in Chapter [6](ch011.xhtml#matrices-and-equations)
    , but if {v[i]}[i=1]^n is orthonormal, the situation is much simpler.
  prefs: []
  type: TYPE_NORMAL
- en: This is made more precise in the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 12\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![V ](img/file191.png) be a vector space and ![S = {v1,...,vn } ](img/file192.png)be
    an orthonormal basis of ![V ](img/file193.png). Then, for any ![x ∈ V ](img/file194.png),
  prefs: []
  type: TYPE_NORMAL
- en: x = ∑ [i=1] ^n ⟨x,v[i]⟩ v[i] (2.12)
  prefs: []
  type: TYPE_NORMAL
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Since ![v1,...,vn ](img/file195.png) form a basis, we can express ![x
    ](img/file196.png) as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n x = xivi i=1 ](img/file197.png)'
  prefs: []
  type: TYPE_IMG
- en: for some scalars ![xi ](img/file198.png). By the linearity of the inner product,
    we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n ⟨x,vj⟩ = ⟨ xivi,vj⟩ = xi⟨vi,vj⟩. i=1 i=1 ](img/file199.png)'
  prefs: []
  type: TYPE_IMG
- en: Since ![v ,...,v 1 n ](img/file200.png) form an orthonormal basis, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| {1, if i = j, ⟨vi,vj⟩ = | (0, if i ⁄= j. ](img/file201.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the sum reduces to
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x,vj ⟩ = xj. ](img/file202.png)'
  prefs: []
  type: TYPE_IMG
- en: This proves the result.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the coefficients can be calculated by taking the inner product. In other
    words, for orthonormal bases, x[j] depends only on the j-th basis vector.
  prefs: []
  type: TYPE_NORMAL
- en: As another consequence of the orthonormality, calculating the norm is also easier,
    as we can always express it in terms of the coefficients. To be more precise,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![2 ∥x∥ = ⟨x,x⟩ ∑n ∑n = ⟨ xivi, xjvj ⟩ i=1 j=1 ∑n ∑n = xixj⟨vi,vj⟩ i=1j=1 n
    = ∑ x2\. i i=1](img/file203.png) (2.13)'
  prefs: []
  type: TYPE_IMG
- en: This is called Parseval’s identity. In other words, if x is given in terms of
    an orthonormal basis, its norm is easy to find. It is not a coincidence that this
    formula resembles the Euclidean norm so much! (Note that, here, ∥⋅∥ is a general
    norm.) In fact, the squared Euclidean norm
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∥x ∥2= ∑ x2, x = (x ,...,x ) ∈ ℝn 2 i 1 n i=1 ](img/file204.png)'
  prefs: []
  type: TYPE_IMG
- en: is just (2.13) using the standard basis.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 The Gram-Schmidt orthogonalization process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Orthogonal bases are awesome and all, but how do we find them?
  prefs: []
  type: TYPE_NORMAL
- en: There is a general method called the Gram-Schmidt orthogonalization process
    that solves this problem. The algorithm takes any set of basis vectors {v[1],…,v[n]}
    and outputs an orthonormal basis {e[1],…,e[n]}
  prefs: []
  type: TYPE_NORMAL
- en: such that
  prefs: []
  type: TYPE_NORMAL
- en: '![span(v1,...,vk) = span (e1,...,ek), k = 1,...,n, ](img/file205.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, the subspaces generated by the first k vectors of both sets match.
  prefs: []
  type: TYPE_NORMAL
- en: How do we do that? The process is straightforward. Let’s focus on finding an
    orthogonal system first, which we can normalize later to achieve orthonormality.
    We are going to build our set {e[1],…,e[n]} iteratively. It is clear that
  prefs: []
  type: TYPE_NORMAL
- en: '![e1 := v1 ](img/file206.png)'
  prefs: []
  type: TYPE_IMG
- en: is a good choice. Now, our goal is to find e[2] such that e[2] ⊥e[1] and, together,
    they span the same subspace as {v[1],v[2]}. Remember when we talked about the
    geometric interpretation of orthogonality in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products)?
    The orthogonal component of v[2] with respect to e[1] will be a good choice for
    e[2]. Thus, let
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⟨v2,e1⟩ e2 := v2 − proje1(v2) = v2 −-------e1\. ⟨e1,e1⟩ ](img/file207.png)'
  prefs: []
  type: TYPE_IMG
- en: From the definition, it is clear that e[2] ⊥e[1], and it is also clear that
    {e[1],e[2]} spans the same subspace as {v[1],v[2]}.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we perform the same process. We project v[3] onto the subspace
    generated by e[1] and e[2], then define e[3] as the difference of v[3] and the
    projection. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![e3 := v3 − proje1,e2(v3 ) = v3 − ⟨v3,e1⟩e1 − ⟨v3,e2⟩e2\. ⟨e1,e1⟩ ⟨e2,e2⟩
    ](img/file208.png)'
  prefs: []
  type: TYPE_IMG
- en: With this, we essentially remove the “contributions” of e[1] and e[2] toward
    v[3], thus obtaining an e[3] that is orthogonal to the previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: In general, if we have e[1],…,e[k], the vector e[k+1] can be found by
  prefs: []
  type: TYPE_NORMAL
- en: '![ek+1 := vk+1 − proj (vk+1), e1,...,ek ](img/file209.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![∑k ⟨x, ei⟩ proje1,...,ek(x) = -------ei i=1⟨ei,ei⟩](img/file210.png) (2.14)'
  prefs: []
  type: TYPE_IMG
- en: is the generalized orthogonal projection operator, projecting a vector to the
    subspace generated by {e[1],…,e[k]}.
  prefs: []
  type: TYPE_NORMAL
- en: To check that e[k+1] ⊥e[1],…,e[k] , we have
  prefs: []
  type: TYPE_NORMAL
- en: '![∑ k ⟨vk+1,ei⟩ ⟨ek+1,ej⟩ = ⟨vk+1 − ⟨ei,ei⟩ ei,ej⟩ i=1 ∑k ⟨vk+1,ei⟩ = ⟨vk+1,ej⟩−
    ---------⟨ei,ej⟩ i=1 ⟨ei,ei⟩ = ⟨v ,e ⟩− ⟨v ,e ⟩ k+1 j k+1 j = 0.](img/file211.png)
    (2.15)'
  prefs: []
  type: TYPE_IMG
- en: due to the orthogonality of the e[i]-s and the linearity of the inner product.
    Since {e[1],…,e[k]} spans the same subspace as {v[1],…,v[k]} and e{k + 1} is a
    linear combination of v[k+1] and e[1],…,e[k] (where the coefficient of v[k+1]
    is nonzero),
  prefs: []
  type: TYPE_NORMAL
- en: '![span(v ,...,v ) = span (e ,...,e ) 1 k+1 1 k+1 ](img/file212.png)'
  prefs: []
  type: TYPE_IMG
- en: also follows. This can be repeated until we run out of vectors and find {e[1],…,e[n]}.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of further reference, mathematical correctness, and a tiny bit
    of perfectionism, let’s summarize all the above in a single theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 13\. (Gram-Schmidt orthogonalization process)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an inner product vector space and {v[1],…,v[n]} ⊆ V be a set of linearly
    independent vectors. Then, there exists an orthonormal set {e[1],…,e[n]}⊆V such
    that
  prefs: []
  type: TYPE_NORMAL
- en: '![span(e1,...,ek) = span (v1,...,vk) ](img/file213.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for any k = 1,…,n.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, we can state that each finite inner product space has an orthonormal
    basis. We can even construct it explicitly via the Gram-Schmidt process.
  prefs: []
  type: TYPE_NORMAL
- en: Corollary 1\. (Existence of orthonormal bases)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a finite-dimensional inner product space. Then, there exists an orthonormal
    basis in V .
  prefs: []
  type: TYPE_NORMAL
- en: Going one step further, we can view Theorem [13](ch008.xhtml#x1-47004r13) and
    its proof as an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '(Gram-Schmidt orthogonalization process) Inputs: A set of linearly independent
    vectors {v[1],…,v[n]}⊆V . Output: A set of orthonormal vectors {e[1],…,e[n]} such
    that![span(e1,...,ek) = span (v1, ...,vk) ](img/file214.png)holds for any k =
    1,…,n.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3\. (Linearly dependent inputs in the Gram-Schmidt process) What happens
    if we apply the Gram-Schmidt orthogonalization process to a set of linearly *dependent*
    vectors?
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a grip on the problem, let’s consider a simple case of two vectors:
    {v[1],v[2] = cv[1]}, where c is an arbitrary scalar. e[1] is chosen to be v[1],
    and e[2] is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![e = v − proj (v ). 2 2 e1 2 ](img/file215.png)'
  prefs: []
  type: TYPE_IMG
- en: By expanding the projection term, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![e2 = v2 − ⟨v2,e1⟩e1\. ⟨e1,e1⟩ ](img/file216.png)'
  prefs: []
  type: TYPE_IMG
- en: Since v[1] = e[1] and v[2] = cv[1], we get that
  prefs: []
  type: TYPE_NORMAL
- en: e[2] = cv[1] −![c⟨v1,v1-⟩ ⟨v1,v1⟩](img/file217.png)v[1] = cv[1] −cv[1]= 0
  prefs: []
  type: TYPE_NORMAL
- en: 'This result generalizes: when the Gram-Schmidt process encounters an input
    vector that is linearly dependent from the previous ones, a zero vector is produced
    in the output.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.6 The orthogonal complement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, we saw that given a fixed vector y ∈V , we can decompose any x ∈V as
    x = x[o] + x[p], where x[o] is orthogonal to y, while x[p] is parallel to it.
    (We used this to provide a geometric motivation for inner products in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products).)
  prefs: []
  type: TYPE_NORMAL
- en: This is an essential tool, and in this section, we will see that an analogue
    of this decomposition still holds true when y is replaced with an arbitrary subspace
    S ⊂V . To see this, let’s talk about the orthogonality of subspaces. (If you want
    to recall the definition of subspaces, check out Definition [5](ch007.xhtml#x1-29003r5).)
  prefs: []
  type: TYPE_NORMAL
- en: Definition 12\. (Orthogonal subspaces)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an arbitrary inner product space. We say that the subspaces S[1],S[2]
    ⊆V are orthogonal if, for every pair of vectors x ∈S[1] and y ∈S[2], we have ⟨x,y⟩
    = 0\. This is denoted as S[1] ⊥S[2].
  prefs: []
  type: TYPE_NORMAL
- en: For example, the x-axis and the y-axis are orthogonal subspaces in ℝ². (Just
    as the xy plane and the z-axis in ℝ³.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can talk about the orthogonality of a vector and a subspace:
    x is orthogonal to the subspace S, or x ⊥S in symbols, if x is orthogonal to all
    vectors of S.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most straightforward and essential ways to construct orthogonal subspaces
    is to take the orthogonal complement.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 13\. (Orthogonal complement)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an arbitrary inner product space and let S ⊆ V be a subspace. The set
    defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![S⊥ := {x ∈ V | x ⊥ S }](img/file218.png) (2.16)'
  prefs: []
  type: TYPE_IMG
- en: is called the orthogonal complement of S.
  prefs: []
  type: TYPE_NORMAL
- en: S^⊥ is not just any set; it is a subspace, as we are about to see.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 14\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![V ](img/file219.png) be an arbitrary inner product space and ![S ⊆ V ](img/file220.png)
    one of its subspaces. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![S ⊥ ](img/file221.png) is orthogonal to ![S ](img/file222.png),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![S ⊥ ](img/file223.png) is a subspace of ![V ](img/file224.png),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and ![ ⊥ S ∩ S = {0} ](img/file225.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Proof. According to the definition of subspaces, we only have to show that
    S^⊥ is closed with respect to addition and scalar multiplication. As the inner
    product is bilinear, this is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨ax + by,z⟩ = a⟨x,z⟩+ b⟨y,z⟩ = 0 ](img/file226.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for any vectors x,y ∈S^⊥, z ∈S, and scalars a,b.
  prefs: []
  type: TYPE_NORMAL
- en: To see that S ∩S^⊥ = {0}, let’s take an arbitrary x ∈S ∩S^⊥. By the definition
    of S^⊥, we have ⟨x,x⟩ = 0\. As the inner product is positive definite per definition,
    x must be the zero vector 0.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the decomposition of any x ∈V into a parallel and an orthogonal component
    with respect to a fixed vector y? In terms of subspaces, we can restate this as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⊥ V = span(y) + span (y) , ](img/file227.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, V can be written as the direct sum of the vector space spanned by y,
    and its orthogonal complement. This is an extremely powerful result, as this allows
    us to decouple x from y. For instance, if we think of vectors as a collection
    of features (just like the sepal and petal width and length measurements in our
    favorite Iris dataset), y can represent a certain trait that we want to exclude
    from our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: With the notion of orthogonal complements, we can make this mathematically precise.
    We can also be more general. In fact, the decomposition
  prefs: []
  type: TYPE_NORMAL
- en: '![V = S + S⊥ ](img/file228.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for any subspace S! Let’s see the proof!
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 15\.
  prefs: []
  type: TYPE_NORMAL
- en: Let V be an arbitrary finite-dimensional inner product space and S ⊂V its subspace.
    Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![V = S + S⊥ ](img/file229.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Let v[1],…,v[k] ∈S be a basis of S. As during the Gram-Schmidt process,
    we can define the generalized orthogonal projection ([3.2.5](ch008.xhtml#the-gramschmidt-orthogonalization-process)),
    given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 ](img/file230.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this, we can decompose any x ∈V as
  prefs: []
  type: TYPE_NORMAL
- en: '![x = (x − proj_{v1,...,vk}(x)) + proj_{v1,...,vk}(x)](img/file231.png) (2.17)'
  prefs: []
  type: TYPE_IMG
- en: Since proj[v[1],…,v[k]](x) is the linear combination of v[i]-s, it belongs to
    S. On the other hand, the bilinearity of the inner product gives that x − proj[v[1],…,v[k]](x)
    ∈S^⊥. Indeed, as we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k proj (x ) = ⟨x,vi⟩vi. v1,...,vk i=1 ](img/equation.png)'
  prefs: []
  type: TYPE_IMG
- en: the vector x − proj[v[1],…,v[k]](x) is orthogonal to each v[j]. Thus, since
    v[1],…,v[k] is a basis of S, x is also orthogonal to S, hence x − proj[e[1],…,v[k]](x)
    ∈S^⊥.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that every x ∈V can be decomposed as the sum of a vector from S and
    a vector from S^⊥, as given by ([3.2.6](ch008.xhtml)), means that V = S + S^⊥,
    which is what we had to prove.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have learned that, besides the algebraic structure given
    by addition and scalar multiplication, vectors have a beautiful geometry that
    rises from the inner product. From the inner product, we have norms; from norms,
    we have metrics; and from metrics, we have geometry and topology.
  prefs: []
  type: TYPE_NORMAL
- en: Distance, similarity, angles, and orthogonality all arise from the simple concept
    of inner products. These are all extremely useful in both theory and practice.
    For instance, inner products give us a way to quantify the similarity of two vectors
    via the so-called cosine similarity, but they also provide a means to find optimal
    bases through the notion of orthogonality.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we’ve learned what norms and distances are, the definition of
    the inner product, how inner products give angles and norms, and why all of these
    are useful in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the basic definitions and properties, we’ve encountered our very first
    algorithm: the Gram-Schmidt process, turning a set of vectors into an orthonormal
    basis. This is the best kind of basis.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll take all that theory and put it into practice, taking
    our first steps in computational linear algebra. Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Problem 1\. Let V be a vector space and define the function d : V ×V → [0,∞)
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 0 if x = y, d(x,y) = |( 1 otherwise. ](img/file234.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Show that d is a metric (see Definition [8](ch008.xhtml#x1-41003r8)).
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that d cannot come from a norm.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Let S[n] be the set of all ASCII strings of n character length and
    define the Hamming distance h(x,y) for any two x,y ∈S[n] by the number of corresponding
    positions where x and y are different.
  prefs: []
  type: TYPE_NORMAL
- en: For instance,
  prefs: []
  type: TYPE_NORMAL
- en: '![h(”001101”,”101110”) = 2, h(”metal”,”petal”) = 1\. ](img/file235.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that h satisfies the three defining properties of a metric. (Note that
    S[n] is not a vector space so, technically, the Hamming distance is not a metric.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 3\. Let ∥⋅∥ be a norm on the vector space ℝ^n, and define the mapping
    f : ℝ^n →ℝ^n,'
  prefs: []
  type: TYPE_NORMAL
- en: '![f : (x1,x2,...,xn ) ↦→ (x1,2x2,...,nxn ). ](img/file236.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![∥x ∥∗ := ∥f(x)∥ ](img/file237.png)'
  prefs: []
  type: TYPE_IMG
- en: is a norm on ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let a[1],…,a[n]/span>0 be arbitrary positive numbers. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ n ⟨x,y ⟩ := aixiyi, x,y ∈ ℝ . i=1 ](img/file238.png)'
  prefs: []
  type: TYPE_IMG
- en: is an inner product, where x = (x[1],…,x[n]) and y = (y[1],…,y[n]).
  prefs: []
  type: TYPE_NORMAL
- en: Problem 5\. Let V be a finite-dimensional inner product space, let v[1],…,v[n]
    ∈V be a basis in V , and define
  prefs: []
  type: TYPE_NORMAL
- en: '![ai,j := ⟨vi,vj⟩. ](img/file239.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that for any x,y ∈V ,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ⟨x,y ⟩ = ai,jxiyj, i,j=1 ](img/file240.png)'
  prefs: []
  type: TYPE_IMG
- en: where x = ∑ [i=1]^nx[i]v[i] and y = ∑ [i=1]^ny[i]v[i].
  prefs: []
  type: TYPE_NORMAL
- en: Problem 6\. Let V be a finite-dimensional real inner product space.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Let y ∈V be an arbitrary vector. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![f : V → ℝ, x ↦→ ⟨x,y⟩ ](img/file241.png)'
  prefs: []
  type: TYPE_IMG
- en: is a linear function (that is, f(αu + βv) = αf(u) + βf(v) holds for all u,v
    ∈V and α,β ∈ℝ).
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Let f : V →ℝ an arbitrary linear function. Show that there exists a y ∈V
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = ⟨x,y⟩. ](img/file242.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that (b) is the reverse of (a), and a much more interesting result.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 7\. Let V be a real inner product space and let ∥x∥ = ![∘ ------ ⟨x,x
    ⟩](img/file243.png) be the generated norm. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 2.16](img/file244.png) (2.18)'
  prefs: []
  type: TYPE_IMG
- en: This is called the parallelogram law, because if we think of x and y as the
    two sides determining a parallelogram, ([3.4](ch008.xhtml#problems1)) relates
    the length of its sides to the length of its diagonals.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 8\. Let V be a real inner product space and let u,v ∈V . Show that if
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨u, x⟩ = ⟨v, x⟩ ](img/file245.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all x ∈V , then u = v.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 9\. Apply the Gram-Schmidt process to the input vectors
  prefs: []
  type: TYPE_NORMAL
- en: '![v1 = (2,1,1), v2 = (1,1,1), v3 = (1,0,1). ](img/file246.png)'
  prefs: []
  type: TYPE_IMG
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
