- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Algebra in Practice
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the geometric structure of vector spaces, it’s time to
    put the theory into practice once again. In this chapter, we’ll take a hands-on
    look at norms, inner products, and NumPy array operations in general. Most importantly,
    we’ll also meet matrices for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last time we translated theory to code, we left off at finding an ideal
    representation for vectors: NumPy arrays. NumPy is built for linear algebra and
    handles computations much faster than the vanilla Python objects.'
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s initialize two NumPy arrays to play around with!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In linear algebra, and in most of machine learning, almost all operations involve
    looping through the vector components one by one. For instance, addition can be
    implemented like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this is far from optimal. (It may not even work if the vectors have
    different dimensions.)
  prefs: []
  type: TYPE_NORMAL
- en: For example, addition is massively parallelizable, and our implementation does
    not take advantage of that. With two threads, we can do two additions simultaneously.
    So, adding together two-dimensional vectors would require just one step, as one
    would compute x[0] + y[0], while the other x[1] + y[1]. Raw Python does not have
    access to such high-performance computing tools, but NumPy does, through functions
    implemented in C. In turn, C uses the LAPACK (Linear Algebra PACKage) library,
    which makes calls to BLAS (Basic Linear Algebra Subprograms). BLAS is optimized
    at the assembly level.
  prefs: []
  type: TYPE_NORMAL
- en: So, whenever it is possible, we should strive to work with vectors in a NumPythonic
    way. (Yes, I just made that term up.) For vector addition, this is simply the
    + operator, as we have seen earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By the way, you shouldn’t ever compare floats with the == operator, as internal
    rounding errors can occur due to the float representation. The example below illustrates
    this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To compare arrays, NumPy provides the functions np.allclose and np.equal. These
    compare arrays elementwise, returning a Boolean array. From this, the built-in
    all function can be used to see if all the elements match.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, we’ll briefly review how to work with NumPy arrays
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Vectors in NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two operations that we definitely want to do with our vectors: apply
    a function elementwise or take the sum/product of the elements. Since the +, *,
    and ** operators are implemented for our arrays, certain functions carry over
    from scalars, as the example below shows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: However, we can’t just plug in ndarrays to every function. For instance, let’s
    take a look at Python’s built-in exp from its math module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To overcome this problem, we could manually apply the function elementwise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: (Recall that np.empty_like(x) creates an uninitialized array that matches the
    dimensions of x.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: A bit less naive implementation would use a list comprehension to achieve the
    same effect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though comprehensions are more concise and readable, they still don’t
    avoid the core issue: for loops in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: This problem is solved by NumPy’s famous ufuncs, that is, functions that operate
    element by element on whole arrays ( [https://numpy.org/doc/stable/reference/generated/numpy.ufunc.html](https://numpy.org/doc/stable/reference/generated/numpy.ufunc.html)).
    Since they are implemented in C, they are blazing fast. For instance, the exponential
    function f(x) = e^x is given by np.exp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Not surprisingly, the results of our implementations match.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Again, there are more advantages to using NumPy functions and operations than
    simplicity. In machine learning, we care a lot about speed, and as we are about
    to see, NumPy delivers once more.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For further reference, you can find the list of available ufuncs here: [https://numpy.org/doc/stable/reference/ufuncs.html\#available-ufuncs](https://numpy.org/doc/stable/reference/ufuncs.html/#available-ufuncs).'
  prefs: []
  type: TYPE_NORMAL
- en: What about operations that aggregate the elements and return a single value?
    Not surprisingly, these can be found within NumPy as well. For instance, let’s
    take a look at the sum. In terms of mathematical formulas, we are looking to implement
    the function
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ n sum (x) = xi, x = (x1,...,xn ) ∈ ℝ . i=1 ](img/file250.png)'
  prefs: []
  type: TYPE_IMG
- en: A basic approach would be something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we can use Python’s built-in summing function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The story is the same: NumPy can do this better. We can either call the function
    np.sum or use the array method np.ndarray.sum.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You know by now that I love timing functions, so let’s compare the performances
    once more.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the product
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∏n prod (x ) = xi, x = (x1,...,xn) ∈ ℝn i=1 ](img/file251.png)'
  prefs: []
  type: TYPE_IMG
- en: is implemented by the np.prod function and the np.ndarray.prod method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: On quite a few occasions, we need to find the maximum or minimum of an array.
    We can do this using the np.max and np.min functions. (Similarly to the others,
    these are also available as array methods.) The rule of thumb is if you want to
    perform any array operation, use NumPy functions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Norms, distances, and dot products
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have reviewed how to perform operations on our vectors efficiently,
    it’s time to dive deep into the really interesting part: norms and distances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the most important one: the Euclidean norm, also known as
    the 2-norm, defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∥x∥2 = ( x2i)1∕2, x = (x1,...,xn) ∈ ℝn. i=1 ](img/file252.png)'
  prefs: []
  type: TYPE_IMG
- en: A straightforward implementation would be the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that our euclidean_norm function is dimension-agnostic; that is, it works
    for arrays of every dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'But wait, didn’t I just mention that we should use NumPy functions whenever
    possible? Norms are important enough to have their own functions: np.linalg.norm.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: With a quick inspection, we can check that these match for our vector x.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: However, the Euclidean norm is just a special case of p-norms. Recall that for
    any p ∈ [0,∞), we defined the p-norm by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∥x∥ = (∑ |x |p)1∕p, x = (x ,...,x ) ∈ ℝn, p i=1 i 1 n ](img/file253.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![∥x∥∞ = max {|x1 |,...,|xn |}, x = (x1,...,xn) ∈ ℝn ](img/file254.png)'
  prefs: []
  type: TYPE_IMG
- en: 'for p = ∞. It is a good practice to keep the number of functions in a codebase
    minimal to reduce maintenance costs. Can we compact all p-norms into a single
    Python function that takes the value of p as an argument? Sure. We only have a
    small issue: representing ∞. Python and NumPy both provide their own representations,
    but we will go with NumPy’s np.inf. Surprisingly, this is a float type.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Since ∞ can have multiple other representations, such as Python’s built-in math.inf,
    we can make our function more robust by using the np.isinf function to check if
    an object represents ∞ or not.
  prefs: []
  type: TYPE_NORMAL
- en: A quick check shows that p_norm works as intended.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: However, once again, NumPy is one step ahead of us. In fact, the familiar np.linalg.norm
    already does this out of the box. We can achieve the same with less code by passing
    the value of p as the argument ord, short for order. For ord = 2, we obtain the
    good old 2-norm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Somewhat surprisingly, distances don’t have their own NumPy functions. However,
    as the most common distance metrics are generated from norms (Section [2.1.1](ch008.xhtml#defining-distances-from-norms)),
    we can often write our own. For instance, here is the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Besides norms and distances, the third component that defines the geometry of
    our vector spaces is the inner product. During our journey, we’ll almost exclusively
    use the dot product, defined in the vector space ℝ^n by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ⟨x, y⟩ = ∑ x y, x, y ∈ ℝn. i i i=1 ](img/file255.png)'
  prefs: []
  type: TYPE_IMG
- en: By now, you can easily smash out a Python function that calculates this. In
    principle, the one-liner below should work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test this out!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: When the dimension of the vectors doesn’t match, the function throws an exception
    as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: However, upon further attempts to break the code, a strange thing occurs. Our
    function dot_product should fail when called with an n-dimensional and one-dimensional
    vector, and this is not what happens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: I always advocate breaking solutions in advance to avoid later surprises, and
    the above example excellently illustrates the usefulness of this principle. If
    the previous phenomenon occurs in production, you would have code that executes
    properly but gives a totally wrong result. That’s the worst kind of bug.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, NumPy is doing something called broadcasting. When performing
    an operation on two arrays with mismatching shapes, it tries to guess the correct
    sizes and reshape them so that the operation can go through. Check out what takes
    place when calculating x*y.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: NumPy guessed that we want to multiply all elements of x by the scalar y[0],
    so it transforms y = np.array([2.0]) into np.array([2.0, 2.0, 2.0, 2.0]), then
    calculates the elementwise product.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting is extremely useful because it allows us to write much simpler
    code by automagically performing transformations. Still, if you are unaware of
    how and when broadcasting is done, it can seriously come back to bite you. Just
    like in our case, as the inner product of a four-dimensional and one-dimensional
    vector is not defined.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid writing excessive checks for edge cases (or missing them altogether),
    we calculate the inner product in practice using the np.dot function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: When attempting to call np.dot with misaligned arrays, it fails as supposed
    to, even in cases when broadcasting bails out our custom implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a basic arsenal of array operations and functions, it is time
    to do something with them!
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 The Gram-Schmidt orthogonalization process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most fundamental algorithms in linear algebra is the Gram-Schmidt
    orthogonalization process (Theorem [13](ch008.xhtml#x1-47004r13)), used to turn
    a set of linearly independent vectors into an orthonormal set.
  prefs: []
  type: TYPE_NORMAL
- en: To be more precise, for our input of a set of linearly independent vectors v[1],…,v[n]
    ∈ℝ^n, the Gram-Schmidt process finds the output set of vectors e[1],…,e[n] ∈ℝ^n
    such that ∥e[i]∥ = 1 and ⟨e[i],e[j]⟩ = 0 for all i≠j (that is, the vectors are
    orthonormal), and span(e[1],…,e[k]) = span(v[1],…,v[k]) for all k = 1,…,n.
  prefs: []
  type: TYPE_NORMAL
- en: If you are having trouble recalling how this is done, feel free to revisit Section
    2.2.5, where we first described the algorithm. The learning process is a spiral,
    where we keep revisiting old concepts from new perspectives. For the Gram-Schmidt
    process, this is our second iteration, where we put the mathematical formulation
    into code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are talking about a sequence of vectors, we need a suitable data structure
    for this purpose. There are several possibilities for this in Python. For now,
    we are going with the conceptually simplest, albeit computationally rather suboptimal,
    one: lists.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The first component of the algorithm is the orthogonal projection operator,
    defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k proj (x) = ⟨x,ei⟩ei. e1,...,ek i=1 ⟨ei,ei⟩ ](img/file456.png)'
  prefs: []
  type: TYPE_IMG
- en: With our NumPy tools, the implementation is straightforward by now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: To check if it works, let’s look at a simple example and visualize the results.
    Since this book is written in Jupyter Notebooks, we can do it right here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file257.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: The projection of x to e'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the orthogonality of e and x - x to e provides another means of verification.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: When writing code for production, a couple of visualizations and ad hoc checks
    are not enough. An extensive set of unit tests is customarily written to ensure
    that a function works as intended. We are skipping this to keep our discussion
    on track, but feel free to add some of your tests. After all, mathematics and
    programming are not a spectator’s sport.
  prefs: []
  type: TYPE_NORMAL
- en: With the projection function available to us, we are ready to knock the Gram-Schmidt
    algorithm out of the park.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Let’s quickly test out this implementation with a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: So, we have just created our first algorithm from scratch. This is like the
    base camp for Mount Everest. We have come a long way, but there is much further
    to go before we can create a neural network from scratch. Until then, the journey
    is packed with some beautiful sections, and this is one of them. Take a while
    to appreciate this, then move on when you are ready.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 4\. (Linearly dependent inputs of the Gram-Schmidt process)
  prefs: []
  type: TYPE_NORMAL
- en: Recall that if the input vectors of the Gram-Schmidt are linearly dependent,
    some vectors of the output are zero (Remark [3](ch008.xhtml#x1-47008r3)). In practice,
    this causes a lot of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we normalize the vectors in the end, using list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This can cause numerical issues. If any v is approximately 0, its norm np.linalg.norm(v, ord=2)
    is going to be really small, and division with such small numbers is problematic.
  prefs: []
  type: TYPE_NORMAL
- en: 'This issue also affects the projection function. Take a look at the definition
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: If e is (close to) 0, which can happen if the input vectors are linearly dependent,
    then e_norm_square is small. One way to solve this is to add a small float, say,
    1e-16.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s meet the single most important objects in machine learning: matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Matrices, the workhorses of linear algebra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am quite sure that you were already familiar with the notion of matrices before
    reading this book. Matrices are one of the most important data structures that
    can represent systems of equations, graphs, mappings between vector spaces, and
    many more. Matrices are the fundamental building blocks of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: At first look, we define a matrix as a table of numbers. If the matrix A has,
    for instance, n rows and m columns of real numbers, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋ | a1,1 a1,2 ... a1,m | || a2,1 a2,2 ... a2,m || | . . . . | n×m A = ||
    .. .. .. .. || ∈ ℝ . || a a ... a || ⌈ n,1 n,2 n,m ⌉](img/file258.png)'
  prefs: []
  type: TYPE_IMG
- en: (3.1)
  prefs: []
  type: TYPE_NORMAL
- en: When we don’t want to write out the entire matrix as ([3.1](ch009.xhtml#matrices-the-workhorses-of-linear-algebra)),
    we use the abbreviation A = (a[i,j])[i=1,j=1]^(n,m).
  prefs: []
  type: TYPE_NORMAL
- en: The set of all n ×m real matrices is denoted by ℝ^(n×m). We will exclusively
    talk about real matrices, but when referring to other types, this notation is
    modified accordingly. For instance, ℤ^({n×m}) denotes the set of integer matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices can be added and multiplied together, or multiplied by a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 14\. (Matrix operations)
  prefs: []
  type: TYPE_NORMAL
- en: (a) Let A ∈ℝ^(n×m) be a matrix and c ∈ℝ a real number. The multiple of A by
    the scalar c is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n,m cA := (cai,j)i,j=1 ∈ ℝn ×m. ](img/file259.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Let A,B ∈ℝ^(n×m) be two matrices of matching dimensions. Their sum A + B
    is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n,m n×m A + B := (ai,j + bi,j)i,j=1 ∈ ℝ . ](img/file260.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Let A ∈ℝ^(n×l) and B ∈ℝ^(l×m) be two matrices. Their product AB ∈ℝ^(n×m)
    is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ l ∑ n,m n×m AB := ( ai,kbk,j)i,j=1 ∈ ℝ . k=1 ](img/file261.png)'
  prefs: []
  type: TYPE_IMG
- en: Scalar multiplication and addition are clear, but matrix multiplication is not
    as simple to understand. Fortunately, visualization can help. In essence, the
    (i,j)-th element is the dot product of the i-th row of A and the j-th column of
    B.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Visualizing matrix multiplication'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides addition and multiplication, there is another operation that is worth
    mentioning: transposition.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 15\. (Matrix transposition)
  prefs: []
  type: TYPE_NORMAL
- en: Let A = (a[{]i,j})[{]i,j = 1}^({n,m}) ∈ ℝ^({n×m}) be a matrix. The matrix A^T
    , defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![AT = (aj,i)ni,,jm=1 ∈ ℝm ×n ](img/file263.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the transpose of A. The operation A→A^T is called transposition.
  prefs: []
  type: TYPE_NORMAL
- en: Transposition simply means “flipping” the matrix, replacing rows with columns.
    For example,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌈a b⌉ T ⌈a c⌉ A = c d , A = b d , ](img/file265.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊0 1⌋ ⌊ ⌋ | | 0 2 4 B = |⌈2 3|⌉ , BT = ⌈ ⌉ . 4 5 1 3 5 ](img/file266.png)'
  prefs: []
  type: TYPE_IMG
- en: As opposed to addition and multiplication, transposition is a unary operation.
    (Unary means that it takes one argument. Binary operations take two arguments,
    and so on.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take another look at matrix multiplication, one of the most frequently
    used operations in computing. As it can be performed extremely fast on modern
    computers, it is common to vectorize certain algorithms just to express it in
    terms of matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the more we know about it, the better. To get a grip on the operation
    itself, we can take a look at it from a few different angles. Let’s start with
    a special case!
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, taking the product of a matrix and a column vector is a
    fundamental building block of certain models. For instance, this is linear regression
    in itself, or the famous fully connected layer in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: To see what happens in this case, let A ∈ℝ^(n×m) be a matrix. If we treat x
    ∈ℝ^m as a column vector x ∈ℝ^(m×1), then Ax can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ∑m ⌋ a1,1 a1,2 ... a1,m x1 j=1a1,jxj || a2,1 a2,2 ... a2,m ||
    || x2|| || ∑m a2,jxj|| Ax = || . . . . || || . || = || j=1\. || . |⌈ .. .. ..
    .. |⌉ |⌈ .. |⌉ |⌈ .. |⌉ ∑m an,1 an,2 ... an,m. xm j=1 an,jxj ](img/file267.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on this, the matrix A describes a function that takes a piece of data
    x, then transforms it into the form Ax.
  prefs: []
  type: TYPE_NORMAL
- en: This is the same as taking the linear combination of the columns of A, that
    is,
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋⌊ ⌋ ⌊ ⌋ ⌊ ⌋ a1,1 a1,2 ... a1,m x1 a1,1 a1,m || |||| || || || || || ||a2,1
    a2,2 ... a2,m |||| x2|| = x1|| a2,1|| + ⋅⋅⋅+ xm ||a2,m|| |⌈ ... ... ... ... |⌉|⌈
    ...|⌉ |⌈ ... |⌉ |⌈ ... |⌉ an,1 an,2 ... an,m. xm an,1 an,m ](img/file268.png)'
  prefs: []
  type: TYPE_IMG
- en: With a bit more suggestive notation, by denoting the i-th column as a[i], we
    can write
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋ | a1,1 a1,2 ... a1,m | || a2,1 a2,2 ... a2,m || | . . . . | n×m A = ||
    .. .. .. .. || ∈ ℝ . || a a ... a || ⌈ n,1 n,2 n,m ⌉](img/equation_(1).png)'
  prefs: []
  type: TYPE_IMG
- en: (3.2)
  prefs: []
  type: TYPE_NORMAL
- en: If we replace the vector x with a matrix B, the columns in the product matrix
    AB are linear combinations of the columns of A, where the coefficients are determined
    by B.
  prefs: []
  type: TYPE_NORMAL
- en: You should really appreciate that certain operations on the data can be written
    in the form Ax. Elevating this simple property to a higher level of abstraction,
    we can say that the data has the same representation as the function. If you are
    familiar with programming languages like Lisp, you know how beautiful this is.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more way to think about the matrix product: taking the columnwise
    inner products. If a[i] = (a[i,1],…,a[i,n]) denotes the i-th column of A, then
    Ax can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋ | a1,1 a1,2 ... a1,m | || a2,1 a2,2 ... a2,m || | . . . . | n×m A = ||
    .. .. .. .. || ∈ ℝ . || a a ... a || ⌈ n,1 n,2 n,m ⌉](img/equation_(2).png)'
  prefs: []
  type: TYPE_IMG
- en: (3.3)
  prefs: []
  type: TYPE_NORMAL
- en: That is, the transformation x→Ax projects the input x to the row vectors of
    A, then compacts the results in a vector.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Manipulating matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because matrix operations are well defined, we can do algebra on matrices just
    as with numbers. However, there are some major differences. As manipulating matrix
    expressions is an essential skill, let’s take a look at its fundamental rules!
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 16\. (Properties of matrix addition and multiplication)
  prefs: []
  type: TYPE_NORMAL
- en: (a) Let A,B,C ∈ℝ^(n×l) be arbitrary matrices. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![A + (B + C ) = (A + B )+ C ](img/file276.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. That is, matrix addition is associative.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Let A ∈ℝ^(n×l), B ∈ℝ^(l×k), C ∈ℝ^(k×m) be arbitrary matrices. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![A (BC ) = (AB )C ](img/file277.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. That is, matrix multiplication is associative.
  prefs: []
  type: TYPE_NORMAL
- en: (c) Let A ∈ℝ^(n×l) and B,C ∈ℝ^(l×m) be arbitrary matrices. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![A(B + C ) = AB + AC ](img/file278.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. That is, matrix multiplication is left-distributive with respect to addition.
  prefs: []
  type: TYPE_NORMAL
- en: (d) Let A,B ∈ℝ^(n×l) and C ∈ℝ^(l×m) be arbitrary matrices. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![(A + B)C = AC + BC ](img/file279.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. That is, matrix multiplication is right-distributive with respect to
    addition.
  prefs: []
  type: TYPE_NORMAL
- en: As the proof is extremely technical and boring, we are going to skip it. However,
    there are a few things to note. Most importantly, matrix multiplication is not
    commutative; that is, AB is not always equal to BA. (It might not even be defined.)
    For instance, consider
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌈1 1 ⌉ ⌈1 0⌉ A = 1 1 , B = 0 2 . ](img/file280.png)'
  prefs: []
  type: TYPE_IMG
- en: You can verify by hand that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌈1 2⌉ ⌈1 1⌉ AB = 1 2 , BA = 2 2 , ](img/file281.png)'
  prefs: []
  type: TYPE_IMG
- en: which are not equal.
  prefs: []
  type: TYPE_NORMAL
- en: In line with this, the algebraic identities that we use for scalars are quite
    different. For instance, if A and B are matrices, then
  prefs: []
  type: TYPE_NORMAL
- en: '![(A + B)(A + B ) = A (A + B) + B (A + B ) = A2 + AB + BA + B2\. ](img/file282.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![(A + B)(A − B ) = A (A − B) + B (A − B ) = A2 − AB + BA − B2\. ](img/file283.png)'
  prefs: []
  type: TYPE_IMG
- en: Transposition also behaves nicely with respect to addition and multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 17\. (Properties of transposition)
  prefs: []
  type: TYPE_NORMAL
- en: (a) Let A,B ∈ℝ^(n×m) be arbitrary matrices. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![(A + B )T = AT + BT ](img/file284.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Let A ∈ℝ^(n×l), B ∈ℝ^(l×m) be arbitrary matrices. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![(AB )T = BT AT ](img/file285.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to prove this either, but feel free to do so as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Matrices as arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform computations with matrices inside a computer, we are looking for
    a data structure that represents a matrix A and supports
  prefs: []
  type: TYPE_NORMAL
- en: accessing elements by A[i, j],
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assigning elements by A[i, j] = value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: addition and multiplication with the + and * operators,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and works lightning fast. These requirements only specify the interface of our
    matrix data structure, not the actual implementation. An obvious choice would
    be a list of lists, but as discussed when talking about representing vectors in
    computations (Section [1.3](ch007.xhtml#vectors-in-practice)), this is highly
    suboptimal. Can we leverage the C array structure to store a matrix?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, and this is precisely what NumPy does, providing a fast and convenient
    representation for matrices in the form of multidimensional arrays. Before learning
    how to use NumPy’s machinery for our purposes, let’s look a bit deeper into the
    heart of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, there seems to be a problem: a computer’s memory is one-dimensional,
    thus addressed (indexed) by a single key, not two as we want. Thus, we can’t just
    shove a matrix into the memory. The solution is to flatten the matrix and place
    each consecutive row next to each other, like Figure [3.3](#) illustrates in the
    3 × 3 case. This is called row-major ordering.'
  prefs: []
  type: TYPE_NORMAL
- en: By storing the rows of any n×m matrix in a contiguous array, we get all the
    benefits of the array data structure at the low cost of a simple index transformation
    defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![(i,j) ↦→ im + j. ](img/file286.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that for programming languages like Fortran or MATLAB that use column-major
    ordering — i.e., the columns are concatenated — this index transformation won’t
    work. I leave figuring out the correct transformation as an exercise to check
    your understanding.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file287.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Flattening a matrix'
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate what’s happening, let’s conjure up a prototypical Matrix class
    in Python that uses a single list to store all the values, yet supports accessing
    elements by row and column indices. For the sake of illustration, let’s imagine
    that a Python list is actually a static array. (At least until this presentation
    is over.) This is for educational purposes only, as at the moment, we only want
    to understand the process, not to maximize performance.
  prefs: []
  type: TYPE_NORMAL
- en: Take a moment to review the code below. I’ll explain everything line by line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The Matrix object is initialized with the __init__ method. This is called when
    an object is created, like we are about to do now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Upon initialization, we supply the dimensions of the matrix in the form of a
    two-dimensional tuple, passed for the shape argument. In our concrete example,
    M is a 3 × 4 matrix, represented by an array of length 12\. For simplicity, our
    simple Matrix is filled up with zeros by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the __init__ method performs three main tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Validates the shape parameter to ensure correctness
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stores the shape in an instance attribute for future reference
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializes a list of size shape[0] * shape[1], which serves as the primary
    data storage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second method, suggestively named _linear_idx, is responsible for translating
    between the row-column indices of the matrix and the linear index for our internal
    one-dimensional representation. (In Python, it is customary to prefix methods
    with an underscore if they are not intended to be called externally. Many other
    languages, such as Java, support private methods. Python is not one of them, so
    we have to make do with such polite suggestions instead of strictly enforced rules.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement item retrieval via indexing by providing the __getitem__ method,
    which expects a two-dimensional integer tuple as the key. For any key = (i, j),
    the method:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the linear index using our _linear_idx method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieves the element located at the given linear index from the list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Item assignment happens similarly, as given by the __setitem__ magic method.
    Let’s try these out to see if they work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: By providing a __repr__ method, we specify how a Matrix object is represented
    as a string. So, we can print it out to the standard output in a pretty form.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Pretty awesome. Now that we understand some of the internals, it is time to
    see how much we can achieve with NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Matrices in NumPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As foreshadowed earlier, NumPy provides an excellent out-of-the-box representation
    for matrices in the form of multidimensional arrays. (These are often called tensors,
    but I’ll just stick to the naming array.)
  prefs: []
  type: TYPE_NORMAL
- en: 'I have some fantastic news: these are the same np.ndarray objects we have been
    using! We can create one by simply providing a list of lists during initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Everything works the same as we have seen so far. Operations are performed elementwise,
    and you can plug them into functions like np.exp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Since we are working with multidimensional arrays, the transposition operator
    can be defined. Here, this is conveniently implemented as the np.transpose function,
    but can also be accessed at the np.ndarray.T attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we can get and set elements with the indexing operator []. The
    indexing starts from zero. (Don’t even get me started.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Entire rows and columns can be accessed using slicing. Instead of giving the
    exact definitions, I’ll just provide a few examples and let you figure it out
    with your internal pattern matching engine. (That is, your brain.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: When used as an iterable, a two-dimensional array yields its rows at every step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Initializing arrays can be done with the familiar np.zeros, np.ones, and other
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: As you have guessed, that shape argument specifies the dimensions of the array.
    We are going to explore this property next. Let’s initialize an example multidimensional
    array with three rows and four columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: The shape of an array, stored inside the attribute np.ndarray.shape, is a tuple
    object describing its dimensions. In our example, since we have a 3 × 4 matrix,
    the shape equals (3, 4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: This innocent-looking attribute determines what kind of operations you can perform
    with your arrays. Let me tell you, as a machine learning engineer, shape mismatches
    will be the bane of your existence. You want to calculate the product of two matrices
    A and B? The second dimension of A must match the first dimension of B. Pointwise
    products? Matching or broadcastable shapes are required. Understanding shapes
    is vital.
  prefs: []
  type: TYPE_NORMAL
- en: However, we have just learned that multidimensional arrays are linear arrays
    in disguise. (See Section [3.2.2](ch009.xhtml#matrices-as-arrays).) Because of
    this, we can reshape an array by slicing the linear view differently. For example,
    A can be reshaped into arrays with shapes (12, 1), (6, 2), (4, 3), (3, 4), (2, 6),
    and (1, 12).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: The np.ndarray.reshape method returns a newly constructed array object but doesn’t
    change A. In other words, reshaping is not destructive in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Reshaping is hard to wrap your head around for the first time. To help you visualize
    the process, Figure [3.4](#) shows precisely what happens in our case.
  prefs: []
  type: TYPE_NORMAL
- en: If you are unaware of the exact dimension along a specific axis, you can get
    away by inputting -1 there during the reshaping. Since the product of dimensions
    is constant, NumPy is smart enough to figure out the missing one for you. This
    trick will get you out of trouble all the time, so it is worth taking note.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file294.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Reshaping a one-dimensional array into multiple possible shapes'
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into the details now, but as you probably guessed, multidimensional
    arrays can have more than two dimensions. The range of permitted shapes for the
    operations will be even more complicated, then. So, building a solid understanding
    now will provide a massive head start in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Matrix multiplication, revisited
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Without a doubt, one of the most important operations regarding matrices is
    multiplication. Computing determinants and eigenvalues? Matrix multiplication.
    Passing data through a fully connected layer? Matrix multiplication. Convolution?
    Matrix multiplication. We will see how these seemingly different things can be
    traced back to matrix multiplication; but first, let’s discuss the operation itself
    from a computational perspective.
  prefs: []
  type: TYPE_NORMAL
- en: First, recap the mathematical definition. For any A ∈ℝ^(n×m) and B ∈ℝ^(m×l),
    their product is defined by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ m ∑ n,l n×l AB = ( ai,kbk,j)i,j=1 ∈ ℝ . k=1 ](img/file295.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the element in the i-th row and j-th column of AB is the dot product
    of A’s i-th row and B’s j-th column.
  prefs: []
  type: TYPE_NORMAL
- en: We can put this into code using the tools we have learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test our function with an example that is easy to verify by hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: The result is correct, as we expected.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, matrix multiplication has its own NumPy function in the form of numpy.matmul.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: This yields the same result as our custom function. We can test it out by generating
    a bunch of random matrices and checking if the results match.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'According to this small test, our matrix_multiplication function yields the
    same result as NumPy’s built-in one. We are happy, but don’t forget: always use
    your chosen framework’s implementations in practice, whether it be NumPy, TensorFlow,
    or PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Since writing np.matmul is cumbersome when lots of multiplications are present,
    NumPy offers a way to abbreviate using the @ operator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 3.2.5 Matrices and data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we are familiar with matrix multiplication, it’s time to make sense
    of them outside of linear algebra. Let’s take a matrix A ∈ℝ^(n×m) and a vector
    x ∈ℝ^m. By treating x as a column vector x ∈ℝ^(m×1), the product of A and x can
    be calculated by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ a a ... a x ∑m a x | 1,1 1,2 1,m | | 1| | ∑mj=1 1,j j| || a2,1
    a2,2 ... a2,m || || x2|| || j=1a2,jxj|| Ax = || .. .. .. .. || || .. || = || ..
    || . ⌈ . . . . ⌉ ⌈ . ⌉ ⌈ ∑ . ⌉ an,1 an,2 ... an,m. xm mj=1an,jxj ](img/file296.png)'
  prefs: []
  type: TYPE_IMG
- en: Mathematically speaking, looking at x as a column vector is perfectly natural.
    Think of it as extending ℝ^m with a dummy dimension, thus obtaining ℝ^(m×1). This
    form also comes naturally by considering that the columns of a matrix are images
    of the basis vectors by their very definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, things are not as simple as they look. Implicitly, we have made
    a choice here: to represent datasets as a horizontal stack of column vectors.
    To elaborate further, let’s consider two data points with four features and a
    matrix that maps these into a three-dimensional feature space. That is, let x[1],x[2]
    ∈ℝ⁴ and A ∈ℝ^(3×4).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: (I specifically selected these numbers so that the calculations would be easily
    verifiable by hand.) To be sure, we double-check the shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: What happens when we call the np.matmul function?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: The result is correct. However, when we have a bunch of input data points, we
    prefer to calculate the images using a single operation. This way, we can take
    advantage of vectorized code, locality of reference, and all the juicy computational
    magic we have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve this by horizontally stacking the column vectors, each one representing
    a data point. Mathematically speaking, we want to perform the calculation in code.
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊0 1 2 3⌋ | 2 − 1| ⌊ 0 1⌋ | | || 0 1 || | | |⌈4 5 6 7|⌉ | | = |⌈ 8 1|⌉
    |⌈ 0 0 |⌉ 8 9 10 11 0 0 16 1 ](img/file297.png)'
  prefs: []
  type: TYPE_IMG
- en: Upon looking up the NumPy documentation, we quickly find that the np.hstack
    function might be the tool for the job, at least according to its official documentation
    ( [https://numpy.org/doc/stable/reference/generated/numpy.hstack.html](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html)).
    Yay!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Not yay. What happened? np.hstack treats one-dimensional arrays differently,
    and even though the math works out perfectly by creatively abusing the notation,
    we don’t get away that easily in the trenches of real-life computations. Thus,
    we have to reshape our inputs manually. Meet the true skill gap between junior
    and senior machine learning engineers: correctly shaping multidimensional arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try this one more time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: Yay! (For real this time.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we made an extremely impactful choice in this chapter: representing
    individual data points as column vectors. I have written this in bold to emphasize
    its importance.'
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because we could have gone the other way and treated samples as row vectors.
    With our current choice, we ended up with a multidimensional array of shape
  prefs: []
  type: TYPE_NORMAL
- en: '![number of dimensions× number of samples, ](img/file298.png)'
  prefs: []
  type: TYPE_IMG
- en: as opposed to
  prefs: []
  type: TYPE_NORMAL
- en: '![number of samples × number of dimensions. ](img/file299.png)'
  prefs: []
  type: TYPE_IMG
- en: The former is called batch-last, while the latter is called batch-first format.
    Popular frameworks like TensorFlow and PyTorch use batch-first, but we are going
    with batch-last. The reasons go back to the very definition of matrices, where
    columns are the images of basis vectors under the given linear transformation.
    This way, we can write multiplication from left to right, like Ax and AB.
  prefs: []
  type: TYPE_NORMAL
- en: Should we define matrices as rows of basis vector images, everything turns upside
    down. This way, if f and g are linear transformations with “matrices” A and B,
    the “matrix” of the composed transformation f ∘g would be BA. This makes the math
    complicated and ugly.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, batch-first makes the data easier to store and read. Think
    about a situation when you have thousands of data points in a single CSV file.
    Due to how input-output is implemented, files are read line by line, so it is
    natural and convenient to have a single line correspond to a single sample.
  prefs: []
  type: TYPE_NORMAL
- en: There are no good choices here; there are sacrifices either way. Since the math
    works out much easier for batch-last, we will use that format. However, in practice,
    you’ll find that batch-first is more common. With this textbook, I don’t intend
    to give you just a manual. My goal is to help you understand the internals of
    machine learning. If I succeed, you’ll be able to apply your knowledge to translate
    between batch-first and batch-last seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we finally dug into the trenches of practice instead of merely
    looking out from the towers of theory. Previously, we saw that NumPy arrays are
    the ideal tools for numeric computations, especially linear algebra. Now, we use
    them to provide fast and elegant implementations of what we learned in the previous
    chapter: norms, distances, dot products, and the Gram-Schmidt process.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides vectors, we also finally introduced matrices, one of the most important
    tools of machine learning. This time, we introduced, in a practical manner, viewing
    matrices as a table of numbers. Matrices can be transposed and added together,
    and unlike vectors, they can be multiplied with each other as well.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of our “from scratch” approach, before looking into how to actually
    work with matrices in practice, we created our very own Matrix implementation
    in vanilla Python. Closing the chapter, we dealt with the fundamentals and best
    practices of two-dimensional NumPy arrays, the prime matrix representation that
    Python can offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll once more take a theoretical approach. This is how
    we do it in this book: looking at both aspects at once, supercharging our understanding
    of mathematics (and machine learning, along the way). We’ll see that matrices
    are not just plain tables of numbers; they are data transformations as well. This
    property is beautiful beyond words: data and their transformations are represented
    by the same object.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get to it!
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Implement the mean squared error
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n MSE (x,y) = 1- (xi − yi)2, x, y ∈ ℝn n i=1 ](img/file300.png)'
  prefs: []
  type: TYPE_IMG
- en: both with and without using NumPy functions and methods. (The vectors x and
    y should be represented by NumPy arrays in both cases.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Compare the performances of the built-in maximum function max and
    NumPy’s np.max using timeit.timeit, like we did above. Try running a different
    number of experiments and changing the array sizes to figure out the breakeven
    point between the two performances.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Instead of implementing the general p-norm as we did earlier in
    this chapter in Section [3.1.1](ch009.xhtml#norms-distances-and-dot-products)
    , we can change things around to obtain the version below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: However, this doesn’t work for p = ∞. What is the problem with it?
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let w ∈ℝ^n be a vector with nonnegative elements. Use NumPy to implement
    the weighted p-norm by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∥x∥wp = ( wi|xi|p)1∕p, x = (x1,...,xn) ∈ ℝn. i=1 ](img/file301.png)'
  prefs: []
  type: TYPE_IMG
- en: Can you come up with a scenario where this can be useful in machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: Problem 5\. Implement the cosine similarity function, defined by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![cos(x,y ) = ⟨-x-,-y--⟩, x, y ∈ ℝn. ∥x ∥ ∥y∥ ](img/file302.png)'
  prefs: []
  type: TYPE_IMG
- en: (Whenever possible, use built-in NumPy functions.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 6\. Calculate the product of the following matrices.
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ − 1 2 | 6 − 2| A = ⌈ ⌉, B = | 2 − 6|. 1 5 ⌈ ⌉ − 3 2 ](img/file303.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ 1 2 3 7 8 A = ⌈ ⌉ , B = ⌈ ⌉ . 4 5 6 9 10 ](img/file304.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 7\. The famous Fibonacci numbers are defined by the recursive sequence
  prefs: []
  type: TYPE_NORMAL
- en: F[0]= 0,
  prefs: []
  type: TYPE_NORMAL
- en: F[1]=1,
  prefs: []
  type: TYPE_NORMAL
- en: F[n]= F[n−1] + F[n−2]
  prefs: []
  type: TYPE_NORMAL
- en: (a) Write a recursive function that computes the n-th Fibonacci number. (Expect
    it to be really slow.)
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋n ⌊ ⌋ ⌈1 1⌉ = ⌈Fn+1 Fn ⌉ , 1 0 Fn Fn− 1 ](img/file305.png)'
  prefs: []
  type: TYPE_IMG
- en: and use this identity to write a non-recursive function that computes the n-th
    Fibonacci number.
  prefs: []
  type: TYPE_NORMAL
- en: Use Python’s built-in timeit function to measure the execution of both functions.
    Which one is faster?
  prefs: []
  type: TYPE_NORMAL
- en: Problem 8\. Let A,B ∈ℝ^(n×m) be two matrices. Their Hadamard product is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | a1,1b1,1 a1,2b1,2 ... a1,nb1,n| | a2,1b2,1 a2,2b2,2 ... a2,nb2,n|
    || . . . . || A ⊙ B = || .. .. .. .. || . || || ⌈ an,1bn,1 an,2bn,2 ... an,nbn,n⌉
    ](img/file306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Implement a function that takes two identically shaped NumPy arrays, then performs
    the Hadamard product on them. (There are two ways to do this: with for loops and
    with NumPy operations. It is instructive to implement both.)'
  prefs: []
  type: TYPE_NORMAL
- en: Problem 9\. Let A ∈ℝ^(n×n) be a square matrix. Functions of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![B(x,y ) = xT Ay, x,y ∈ ℝn ](img/file307.png)'
  prefs: []
  type: TYPE_IMG
- en: are called bilinear forms. Implement a function that takes two vectors and a
    matrix (all represented by NumPy arrays), then calculates the corresponding bilinear
    form.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
