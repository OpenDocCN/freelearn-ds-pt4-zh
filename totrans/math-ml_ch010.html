<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch010.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="linear-transformations" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">4</span></span><br/>
<span id="x1-650005"></span><span class="cmss-10x-x-109">Linear Transformations</span></h1>
<blockquote class="packt_quote">
<p><span class="cmss-10x-x-109">”Why do my eyes hurt?” ”You’ve never used them before.”</span></p>
<div class="flushright">
<p><span class="cmss-10x-x-109">— Morpheus to Neo, when waking up from the Matrix for the first time</span></p>
</div>
</blockquote>
<p><span class="cmss-10x-x-109">In most linear algebra courses, the curriculum is all about matrices. In machine learning, we work with them all the time. Here is the thing: matrices don’t tell the whole story. It is hard to understand the patterns by looking only at matrices. For instance, why is matrix multiplication defined in such a complex way as it is? Why are relations like </span><span class="cmmi-10x-x-109">B </span>= <span class="cmmi-10x-x-109">T</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">AT </span><span class="cmss-10x-x-109">important? Why are some matrices invertible and some are not?</span></p>
<p><span class="cmss-10x-x-109">To </span><span class="cmssi-10x-x-109">really </span><span class="cmss-10x-x-109">understand what is going on, we have to look at what gives rise to matrices: linear transformations. Like for Neo, this might hurt a bit, but it will greatly reward us later down the line. Let’s get to it!</span></p>
<section id="what-is-a-linear-transformation" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_50"><span class="titlemark"><span class="cmss-10x-x-109">4.1 </span></span> <span id="x1-660005.1"></span><span class="cmss-10x-x-109">What is a linear transformation?</span></h2>
<p><span class="cmss-10x-x-109">With the</span><span id="dx1-66001"></span> <span class="cmss-10x-x-109">introduction of inner products, orthogonality, and orthogonal/orthonormal bases, we know everything about the structure of our feature spaces. However, in machine learning, our interest mainly lies in </span><span class="cmssi-10x-x-109">transforming </span><span class="cmss-10x-x-109">the data.</span></p>
<p><span class="cmss-10x-x-109">From this viewpoint, a neural network is just a function composed of smaller parts (known as </span><span class="cmssi-10x-x-109">layers</span><span class="cmss-10x-x-109">), transforming the data to a new feature space in every step. One of the key components of models in machine learning are </span><span class="cmssi-10x-x-109">linear transformations</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">You probably encountered them as functions of the form </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, but this is only one way to look at them. This section will start from a geometric viewpoint, then move towards the algebraic representation that you are probably already familiar with. To understand how neural networks can learn powerful high-level representations of the data, looking at the geometry of transforms is essential.</span></p>
<p><span class="cmss-10x-x-109">So, what linear transformations are? Let’s not hesitate a moment further, and jump into the definition right away!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-66002r16"></span> <span class="cmbx-10x-x-109">Definition 16.</span> </span><span class="cmbx-10x-x-109">(Linear transformations)</span></p>
<p>Let <span class="cmmi-10x-x-109">U </span>and <span class="cmmi-10x-x-109">V </span>be two vector spaces (over the same scalar field), and let <span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V</span> be a function between them. We say that <span class="cmmi-10x-x-109">f </span>is linear if</p>
<div style="display: flex; justify-content: space-between; text-align: center; width: 100%; max-width: 600px;" class="equation math-display">
  <div>
    <span class="cmmi-10x-x-109">f</span>(
    <span class="cmmi-10x-x-109">a</span><span class="cmbx-10x-x-109">x </span> + 
    <span class="cmmi-10x-x-109">b</span><span class="cmbx-10x-x-109">y</span>) = 
    <span class="cmmi-10x-x-109">af</span>(<span class="cmbx-10x-x-109">x</span>) + 
    <span class="cmmi-10x-x-109">bf</span>(<span class="cmbx-10x-x-109">y</span>)
  </div>
  <div style="padding-left: 1em;">(4.1)</div>
</div>

<p>holds for all vectors <span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">U </span>and all scalars <span class="cmmi-10x-x-109">a,b</span>.</p>
</div>
<p><span class="cmss-10x-x-109">This is why</span> <span id="dx1-66003"></span><span class="cmss-10x-x-109">linear algebra is called </span><span class="cmssi-10x-x-109">linear </span><span class="cmss-10x-x-109">algebra. In essence, a linear transformation</span><span id="dx1-66004"></span> <span class="cmss-10x-x-109">is a mapping between two vector spaces that preserves the algebraic structure: addition and scalar multiplication. (Functions between vector spaces are often called </span><span class="cmssi-10x-x-109">transformations</span><span class="cmss-10x-x-109">, so we will use this terminology.)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-66005r5"></span> <span class="cmbx-10x-x-109">Remark 5.</span> </span></p>
<p>Linearity is essentially combining two properties in one: <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">y</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) + <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">y</span>) and <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">a</span><span class="cmbx-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">af</span>(<span class="cmbx-10x-x-109">x</span>) for all vectors <span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span>and all scalars <span class="cmmi-10x-x-109">a</span>. From these two, (<a href="ch010.xhtml#x1-66002r16">4.1</a>) follows by</p>
<div class="math-display">
<img src="../media/file308.png" class="math-display" alt="f(ax + by) = f(ax) + f(by) = af(x)+ bf (y ). "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Two properties immediately jump out from the definition. First, since</span></p>

<img src="../media/file309.png" class="math-display" alt="f(x) = f(x + 0) = f(x) + f(0), " width="150"/>
<p><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">0</span>) = <span class="cmbx-10x-x-109">0 </span><span class="cmss-10x-x-109">holds for every linear transformation. In addition, the composition of linear transformations is still linear, as</span></p>
<div class="math-display">
<img src="../media/file310.png" class="math-display" alt="f (g(ax + by)) = f(ag(x)+ bg(y)) = af(g(x))+ bf(g(y)) "/>
</div>
<p><span class="cmss-10x-x-109">shows for any linear </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">g </span><span class="cmss-10x-x-109">and scalars </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">b</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">As usual, let’s see some examples to build intuition.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">For any scalar </span><span class="cmmi-10x-x-109">c</span><span class="cmss-10x-x-109">, the scaling transformation </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">c</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is linear.</span></p>
<p><span class="cmss-10x-x-109">This is probably the simplest example out there, and it can be defined in all vector spaces.</span></p>
<div class="minipage">
<p><img src="../media/file311.png" width="470" alt="PIC"/> <span id="x1-66006r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.1: Scaling as a linear transformation</span> </span>
</div>
<p><span class="cmss-10x-x-109">It’s easy to</span> <span id="dx1-66007"></span><span class="cmss-10x-x-109">see that scaling is linear:</span></p>
<div class="math-display">
<img src="../media/file312.png" class="math-display" alt="c(ax + by) = c(ax)+ c(by) = a(cx)+ b(cy). "/>
</div>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">In </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, rotations around the origin by an angle </span><span class="cmmi-10x-x-109">α </span><span class="cmss-10x-x-109">are also linear.</span></p>
<div class="minipage">
<p><img src="../media/file313.png" width="470" alt="PIC"/> <span id="x1-66008r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.2: Rotation in the Euclidean plane as a linear transformation</span> </span>
</div>
<p><span class="cmss-10x-x-109">To show that rotations are indeed linear, I pull the definition out from the hat: the rotation of a planar vector </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">with the angle </span><span class="cmmi-10x-x-109">α </span><span class="cmss-10x-x-109">is described by</span></p>
<img src="../media/file314.png" class="math-display" width="450" alt="f(x) = (x1cosα − x2 sinα, x1sinα + x2cos α), "/>

<p><span class="cmss-10x-x-109">from which (</span><a href="ch010.xhtml#x1-66002r16"><span class="cmss-10x-x-109">16</span></a><span class="cmss-10x-x-109">) is easily confirmed. I know that this looks like magic, but trust me, the rotation formula will be explained in detail. You can sweat it out with some basic trigonometry, or wait until we do this later with matrices.</span></p>
<p><span class="cmss-10x-x-109">In general, linear</span> <span id="dx1-66009"></span><span class="cmss-10x-x-109">transformations have a strong connection with the geometry of the space. Later, we are going to study the linear transformations of </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">in detail, with an emphasis on geometric ones such as this. (Note that rotations are slightly more complicated in higher dimensions, as they will require an axis to rotate around.)</span></p>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmss-10x-x-109">In any vector space </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">and a nonzero vector </span><span class="cmbx-10x-x-109">v </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, the translation defined by </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) = <span class="cmbx-10x-x-109">x </span>+ <span class="cmbx-10x-x-109">v </span><span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">not </span><span class="cmss-10x-x-109">linear, as </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">0</span>) = <span class="cmbx-10x-x-109">v</span><span class="cmmi-10x-x-109">≠</span><span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">We’ll see more examples later in the section. For now, let’s move on to some general properties of linear transformations. For any linear transformation </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, the image</span></p>
<div class="math-display">
<img src="../media/file315.png" class="math-display" alt="im(f) = {v ∈ V : v = f (u) for some u ∈ U } "/>
</div>
<p><span class="cmss-10x-x-109">is always a subspace </span><span class="cmssi-10x-x-109">Section </span><a href="ch007.xhtml#subspaces"><span class="cmssi-10x-x-109">1.2.7</span></a> <span class="cmss-10x-x-109">of </span><span class="cmbx-10x-x-109">V</span><span class="cmss-10x-x-109">. This is easy to check: if </span><img src="../media/file316.png" width="75" alt="v1, v2 ∈ im f "/><span class="cmss-10x-x-109">, then there exist </span><img src="../media/file317.png" width="75" alt="u1,u2 ∈ U "/> <span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>) = <span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub>) = <span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, as</span></p>

<img src="../media/file318.png" class="math-display" width="450" alt="av1 + bv2 = af(u1)+ bf (u2 ) = f (au1 + bu2) ∈ im f. "/>

<p><span class="cmss-10x-x-109">To add one more level of abstraction, we will see that the set of all linear transformations form a vector space.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-66010r18"></span> <span class="cmbx-10x-x-109">Theorem 18.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">U </span><span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be two vector spaces over the same field </span><span class="cmmi-10x-x-109">F</span><span class="cmti-10x-x-109">. Then the set of all linear transformations</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;">
  <div class="math-display">
    <img src="../media/file319.png" alt="L(U,V ) = {f : U → V | f is linear}"/>
  </div>
  <div style="padding-left: 1em;">
    (4.2)
  </div>
</div>

<p><span class="cmti-10x-x-109">is also a vector space over </span><span class="cmmi-10x-x-109">F</span><span class="cmti-10x-x-109">, with the usual definitions for function addition and scalar multiplication.</span></p>
</div>
<p><span class="cmss-10x-x-109">The proof of this is just a boring checklist, going through the items of the definition of vector spaces (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch007.xhtml#x1-20004r2"><span class="cmssi-10x-x-109">2</span></a><span class="cmss-10x-x-109">). I recommend you walk through it at least once to solidify your understanding of vector spaces, but there is really nothing special there.</span></p>
<section id="linear-transformations-and-matrices" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_51"><span class="titlemark"><span class="cmss-10x-x-109">4.1.1 </span></span> <span id="x1-670005.1.1"></span><span class="cmss-10x-x-109">Linear transformations and matrices</span></h3>
<p><span class="cmss-10x-x-109">The definition of linear transformations, as we saw, is a bit abstract. However, there</span> <span id="dx1-67001"></span><span class="cmss-10x-x-109">is a simple and expressive way to characterize them.</span></p>
<p><span class="cmss-10x-x-109">To see this, let </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be a linear transformation between two vector spaces </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. Suppose that </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">m</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is a basis in </span><span class="cmmi-10x-x-109">U</span><span class="cmss-10x-x-109">, while </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is a basis in </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. Since every </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">can be written in the form</span></p>
<img src="../media/file320.png" class="math-display" width="150" alt=" m ∑ x = xiui, i=1 "/>

<p><span class="cmss-10x-x-109">the linearity of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">implies</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; max-width: 600px;" class="equation math-display">
  <div>
    <span class="cmmi-10x-x-109">f</span><span class="bbig">(</span>
    <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">m</span></sup>
    <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">j</span></sub> <span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">j</span></sub>
    <span class="bbig">)</span> =
    <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">m</span></sup>
    <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">j</span></sub>
    <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">j</span></sub>),
  </div>
  <div style="padding-left: 1em;">(4.3)</div>
</div>

<p><span class="cmss-10x-x-109">meaning that </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">is a linear combination of </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">m</span></sub>)<span class="cmss-10x-x-109">. In other words, every linear transformation is completely determined by the images of basis vectors. To expand this idea, suppose that for every </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">j</span></sub><span class="cmss-10x-x-109">, we have</span></p>
<img src="../media/file323.png" class="math-display" width="150" alt=" ∑n f(uj) = ai,jvi i=1 "/>

<p><span class="cmss-10x-x-109">for some scalars </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">These </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">m </span><span class="cmss-10x-x-109">numbers completely describe </span><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">. For notational simplicity, we store these in a </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">m</span><span class="cmss-10x-x-109">-sized table called a </span><span class="cmssi-10x-x-109">matrix</span><span class="cmss-10x-x-109">, which we’ll denote by </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f</span></sub><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file324.png" class="math-display" alt=" ⌊ ⌋ a1,1 a1,2 ... a1,m | | ||a2,1 a2,2 ... a2,m|| f ↔ Af = || .. .. ... .. || , ⌈ . . . ⌉ an,1 an,2 ... an,m "/>
</div>
<p><span class="cmss-10x-x-109">meaning that linear transformations are represented by matrices. This connection is heavily utilized throughout machine learning.</span></p>
<p><span class="cmss-10x-x-109">Expanding (</span><a href="ch010.xhtml#linear-transformations-and-matrices"><span class="cmss-10x-x-109">4.3</span></a><span class="cmss-10x-x-109">) further, for every </span><span class="cmbx-10x-x-109">x </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">m</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">j</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">j</span></sub> <span class="cmss-10x-x-109">we have</span></p>
<div class="math-display">
<img src="../media/file325.png" class="math-display" alt=" ∑m f(x) = xjf(uj) j=1 ∑m ∑n = xj ai,jvi j=1 i=1 n ( m ) = ∑ ( ∑ a x ) v . i,j j i i=1 j=1 "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the image of </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be expressed as </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f</span></sub><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file326.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ ⌊∑m ⌋ |a1,1 a1,2 ... a1,m | |x1 | | j=1 a1,jxj| |a2,1 a2,2 ... a2,m | |x2 | |∑m a2,jxj| f(x) = || . . . . || || . || = || j=1. || . |⌈ .. .. .. .. |⌉ |⌈ .. |⌉ |⌈ .. |⌉ a a ... a , x ∑m a x n,1 n,2 n,m m j=1 n,j j "/>
</div>
<p><span class="cmss-10x-x-109">Two things to note here. First, we implicitly chose to represent vectors as columns</span><span id="dx1-67002"></span> <span class="cmss-10x-x-109">instead of rows. This is a seriously impactful decision and will affect many of the computations later in this book. We’ll keep pointing it out.</span></p>
<p><span class="cmss-10x-x-109">Second, the matrix representation depends on the choice of the basis! If, say, </span><span class="cmmi-10x-x-109">P </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}⊂</span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">is the basis of our matrix, we denote this dependence in the subscript, writing </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f,P</span></sub> <span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To avoid confusion, we’ll almost exclusively define linear transformations by giving their matrices in the standard orthonormal basis. In practical scenarios, this makes it much easier to understand what is going on. So, whenever I write something like “</span><span class="cmssi-10x-x-109">let </span><span class="cmmi-10x-x-109">A </span><span class="cmssi-10x-x-109">be the matrix of a linear transformation </span><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">”, it is implictly assumed that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is written in the basis </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">,…,</span>0)<span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> = (0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,…,</span>0)<span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub> = (0<span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">,…,</span>1)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">On a philosophical note, have you heard about Plato’s allegory of the cave? In this thought experiment, people are assumed to be living in a cave constantly facing a single wall, only observing their shadows projected by a fire behind them. What they observe and use to build an internal representation of the world is very different from reality. Applying this analogy to linear algebra, matrices are the shadows that we observe and use in practical scenarios. In many introductory courses, linear transformations are hidden, and only matrix calculus is taught. My first exposition into the subject was similar: the first linear algebra course I took talked exclusively about matrices. It was as complicated and confusing as a math course can be. (Which, I can assure you, can be </span><span class="cmssi-10x-x-109">very </span><span class="cmss-10x-x-109">complicated and confusing.) Later in my studies, everything clicked when I discovered that you could look at matrices from the perspective of linear transformations.</span></p>
<p><span class="cmss-10x-x-109">Without seeing what is behind matrices, it is impossible to master linear algebra. If my approach feels too abstract for you, keep this in mind: years later, when you are a practicing data scientist/machine learning engineer/researcher or whatever, going below the surface will pay huge dividends.</span></p>
<p><span class="cmss-10x-x-109">Let’s get back on track and continue our discussion about linear transformations. The most commonly used matrix is the matrix of the identity transformation</span> id : <span class="cmbx-10x-x-109">x</span>→<span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. We’ll denote this by </span><span class="cmmi-10x-x-109">I</span><span class="cmss-10x-x-109">. It is easy to see that</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(3).png" alt="L(U,V ) = {f : U → V | f is linear}"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(4.4)
  </div>
</div>

<p><span class="cmss-10x-x-109">To summarize, for a matrix </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, a linear transformation can be given by </span><span class="cmbx-10x-x-109">x</span>→<span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. In fact, the mapping</span></p>
<img src="../media/file330.png" class="math-display" width="150" alt="f ↦→ Af,P "/>

<p><span class="cmss-10x-x-109">defines a one-to-one correspondence between the space of linear transformations </span><span class="cmmi-10x-x-109">L</span>(<span class="cmmi-10x-x-109">U,V </span>) <span class="cmss-10x-x-109">defined by (</span><a href="ch010.xhtml#x1-66010r18"><span class="cmss-10x-x-109">4.2</span></a><span class="cmss-10x-x-109">) and the set of </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">m </span><span class="cmss-10x-x-109">matrices, where </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">m </span><span class="cmss-10x-x-109">are the corresponding dimensions.</span></p>
</section>
<section id="matrix-operations-revisited" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_52"><span class="titlemark"><span class="cmss-10x-x-109">4.1.2 </span></span> <span id="x1-680005.1.2"></span><span class="cmss-10x-x-109">Matrix operations revisited</span></h3>
<p><span class="cmss-10x-x-109">Functions can be added and composed. Because of the connection between linear</span> <span id="dx1-68001"></span><span class="cmss-10x-x-109">transformations and matrices, matrix operations are inherited from the corresponding function operations.</span></p>
<p><span class="cmss-10x-x-109">With this principle in mind, we defined matrix addition so that the the matrix of the sum of two linear transformations is the sum of the corresponding matrices.</span></p>
<p><span class="cmss-10x-x-109">Mathematically speaking, if </span><span class="cmmi-10x-x-109">f,g </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">are two linear transformations with matrices, </span><span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">↔︎</span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">g </span><span class="cmsy-10x-x-109">↔︎</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file331.png" class="math-display" alt=" n (f + g)(u ) = f (u )+ g(u ) = ∑ (a + b )v . j j j i=1 i,j i,j i "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the corresponding matrices can be added together elementwise:</span></p>
<div class="math-display">
<img src="../media/file332.png" class="math-display" alt="A + B = (ai,j + bi,j)n,m . i,j=1 "/>
</div>
<p><span class="cmss-10x-x-109">Multiplication between matrices is defined by the composition of the corresponding</span> <span id="dx1-68002"></span><span class="cmss-10x-x-109">transformations.</span></p>
<p><span class="cmss-10x-x-109">To see how, we study a special case first. (In general, it is a good idea to look at special cases first, as they often reduce the complexity and allow you to see patterns without information overload.) So, let </span><span class="cmmi-10x-x-109">f,g </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">be two linear transformations, mapping </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">onto itself. To determine the elements of the matrix corresponding to </span><span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmmi-10x-x-109">g</span><span class="cmss-10x-x-109">, we have to express </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">g</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">j</span></sub>)) <span class="cmss-10x-x-109">in terms of all the basis vectors </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">. For this, we have</span></p>
<div class="math-display">
<img src="../media/file333.png" class="math-display" alt=" n (f g)(u ) = f(g(u )) = f(∑ b u ) j j k,j k n k=1 ∑ = bk,jf(uk) k=n1 n ∑ ∑ = bk,j ai,kui k=1 i=1 ∑n ∑n = ( ai,kbk,j)ui. i=1 k=1 "/>
</div>
<p><span class="cmss-10x-x-109">By considering how we defined a transformation’s matrix, the scalar (</span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,k</span></sub><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">k,j</span></sub><span class="cmss-10x-x-109">) is the element in the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column of the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmmi-10x-x-109">g</span><span class="cmss-10x-x-109">. Thus, matrix multiplication can be defined by</span></p>
<div class="math-display">
<img src="../media/file334.png" class="math-display" alt=" ( )n ∑n AB = ai,kbk,j . k=1 i,j=1 "/>
</div>
<p><span class="cmss-10x-x-109">In the general case, we can only define the product of matrices if the corresponding linear transformations can be composed. That is, if </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">g </span><span class="cmss-10x-x-109">must start from </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. Translating this into the language of the matrices, the number of columns in </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">must match the number of rows in </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">. So, for any </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">l</span></sup><span class="cmss-10x-x-109">, their product is defined by</span></p>
<div class="math-display">
<img src="../media/file335.png" class="math-display" alt=" ∑m AB = ( ai,kbk,j)n,l ∈ ℝn×l. k=1 i,j=1 "/>
</div>
</section>
<section id="inverting-linear-transformations" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_53"><span class="titlemark"><span class="cmss-10x-x-109">4.1.3 </span></span> <span id="x1-690005.1.3"></span><span class="cmss-10x-x-109">Inverting linear transformations</span></h3>
<p><span class="cmss-10x-x-109">Regarding</span> <span id="dx1-69001"></span><span class="cmss-10x-x-109">linear transformations, the question of invertibility is extremely important. For example, have you encountered a system of equations like this?</span></p>
<div class="math-display">
<img src="../media/file336.png" class="math-display" alt="2x1 + x2 = 5 x1 − 3x2 = − 8 "/>
</div>
<p><span class="cmss-10x-x-109">If we define</span></p>
<div class="math-display">
<img src="../media/file337.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ 2 1 5 x1 A = ⌈ ⌉ , b = ⌈ ⌉ , x = ⌈ ⌉ , 1 − 3 − 8 x2 "/>
</div>
<p><span class="cmss-10x-x-109">the above system can be written in the form </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">. These are called </span><span class="cmssi-10x-x-109">linear equations</span><span class="cmss-10x-x-109">, modeling various processes from finance to biology.</span></p>
<p><span class="cmss-10x-x-109">How would you write the solution of such an equation? If there would be a matrix </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is the identity matrix </span><span class="cmmi-10x-x-109">I </span><span class="cmss-10x-x-109">(defined by (</span><a href="ch010.xhtml#linear-transformations-and-matrices"><span class="cmss-10x-x-109">4.4</span></a><span class="cmss-10x-x-109">)), then multiplying the equation </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b </span><span class="cmss-10x-x-109">from the left by </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">would yield the solution in the form </span><span class="cmbx-10x-x-109">x </span>= <span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">The matrix </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">inverse matrix </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. It might not always exist, but when it does, it is extremely important for several reasons. We’ll talk about linear equations later, but first, let’s study the fundamentals of invertibility! Here is the general definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-69002r17"></span> <span class="cmbx-10x-x-109">Definition 17.</span> </span><span class="cmbx-10x-x-109">(Inverse of a linear transformation)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span>be a linear transformation between the vector spaces <span class="cmmi-10x-x-109">U </span>and <span class="cmmi-10x-x-109">V </span>. We say that <span class="cmmi-10x-x-109">f </span>is invertible if there is a linear transformation <span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> such that <span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmsy-10x-x-109">∘</span><span class="cmmi-10x-x-109">f </span>and <span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> are the identity functions; that is,</p>
<img src="../media/file338.png" class="math-display" alt="f− 1(f (u )) = u, − 1 f(f (v )) = v " width="150"/>
<p>holds for all <span class="cmbx-10x-x-109">u </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">U,</span><span class="cmbx-10x-x-109">v </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span>. <span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> is called the <span class="cmti-10x-x-109">inverse </span>of <span class="cmmi-10x-x-109">f</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Not all linear transformations are invertible. For instance, if </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">maps all vectors to the zero vector, you cannot define an inverse.</span></p>
<p><span class="cmss-10x-x-109">There are certain conditions that guarantee the existence of the inverse. One of the most important ones connects the concept of basis with invertibility.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-69003r19"></span> <span class="cmbx-10x-x-109">Theorem 19.</span> </span><span class="cmbxti-10x-x-109">(Invertibility of linear transformations)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be a linear transformation and let </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">be a basis in </span><span class="cmmi-10x-x-109">U</span><span class="cmti-10x-x-109">. Then </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">is invertible if and only if </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmti-10x-x-109">is a basis in </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">The following proof is straightforward, but can be a bit overwhelming. Feel free to</span> <span id="dx1-69004"></span><span class="cmss-10x-x-109">skip this at the first reading, you can always revisit it later.</span></p>
<div id="tcolobox-143" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As usual, the proof of the </span><span class="cmssi-10x-x-109">if and only if </span><span class="cmss-10x-x-109">type theorems consist of two parts, as these statements involve two implications.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">First, we prove that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is invertible, then </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">is a basis. That is, we need to show that </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">is linearly independent and every </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">can be written as their linear combination.</span></p>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is invertible, </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">0</span>) = <span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">, moreover there are no nonzero vectors </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) = <span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">. In other words, </span><span class="cmbx-10x-x-109">0 </span><span class="cmss-10x-x-109">cannot be written as the nontrivial linear combination of </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">, from which </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch007.xhtml#x1-25003r3"><span class="cmssi-10x-x-109">3</span></a> <span class="cmss-10x-x-109">implies the linear independence.</span></p>
<p><span class="cmss-10x-x-109">On the other hand, invertibility implies that every </span><img src="../media/file339.png" class="math" alt="y ∈ V "/> <span class="cmss-10x-x-109">can be obtained as </span><img src="../media/file340.png" class="math" alt="y = f(x) "/> <span class="cmss-10x-x-109">for some </span><img src="../media/file341.png" class="math" alt="x ∈ U "/><span class="cmss-10x-x-109">. (With the choice</span> <img src="../media/file342.png" class="math" alt="x = f −1(y) "/><span class="cmss-10x-x-109">.) As </span><img src="../media/file343.png" class="math" alt="u1,...,un "/> <span class="cmss-10x-x-109">is a basis, </span><img src="../media/file344.png" class="math" alt=" ∑ x = ni=1xiui "/><span class="cmss-10x-x-109">. Thus,</span></p>

<img src="../media/file345.png" class="math-display" alt="y = f(x) ∑n = f( xiui) i=1 ∑n = xif(ui), i=1 " width="150"/>

<p><span class="cmss-10x-x-109">showing that</span> span(<span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>)) = <span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">The linear independence </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">and the fact that it spans </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">gives that it is indeed a basis.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Now we prove the other implication: if </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">is a basis, then </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is invertible.</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">,…,f</span>(<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">is indeed a basis, then every </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file346.png" class="math-display" alt=" n n ∑ ∑ y = yif (ui) = f( yiui), i=1 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">which shows the surjectivity. Regarding the injectivity, if </span><span class="cmbx-10x-x-109">y </span>= <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">b</span>) <span class="cmss-10x-x-109">for some </span><span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">U</span><span class="cmss-10x-x-109">, then, since both </span><span class="cmbx-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">b </span><span class="cmss-10x-x-109">can be written as a linear combination of the </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">basis vectors, we would have</span></p>
<div class="math-display">
<img src="../media/file347.png" class="math-display" alt=" n n ∑ ∑ y = f(a) = f( aiui) = aif (ui) i=1 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file348.png" class="math-display" alt=" ∑n ∑n y = f(b) = f( biui) = yif(ui). i=1 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Thus, </span><span class="cmbx-10x-x-109">0 </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup>(<span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">i</span></sub>)<span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">, and since </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is a basis in U, </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">must hold. Hence </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is injective.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">A consequence of this theorem is that a linear transformation </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">is not invertible if the dimensions of </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">are different. We can</span><span id="dx1-69005"></span> <span class="cmss-10x-x-109">look at invertibility from the aspect of matrices as well. For any </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, if the corresponding linear transformation is invertible, there exists a matrix </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">AA</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> = <span class="cmmi-10x-x-109">I</span><span class="cmss-10x-x-109">. If a matrix is not square, it is not invertible in the classical sense.</span></p>
</section>
<section id="the-kernel-and-the-image" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_54"><span class="titlemark"><span class="cmss-10x-x-109">4.1.4 </span></span> <span id="x1-700005.1.4"></span><span class="cmss-10x-x-109">The kernel and the image</span></h3>
<p><span class="cmss-10x-x-109">Regarding</span> <span id="dx1-70001"></span><span class="cmss-10x-x-109">the invertibility of a linear transformation, two special sets play an essential role: the </span><span class="cmssi-10x-x-109">kernel </span><span class="cmss-10x-x-109">and the </span><span class="cmssi-10x-x-109">image</span><span class="cmss-10x-x-109">. Let’s see them!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-70002r18"></span> <span class="cmbx-10x-x-109">Definition 18.</span> </span><span class="cmbx-10x-x-109">(Kernel and image of linear transformations)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span>be a linear transformation. Its image and kernel is defined by</p>
<div class="math-display">
<img src="../media/file349.png" class="math-display" alt="im f := {f(u) : u ∈ U} ⊆ V "/>
</div>
<p>and</p>
<div class="math-display">
<img src="../media/file350.png" class="math-display" alt="kerf := {u ∈ U : f(u ) = 0} ⊆ U. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Often, we write</span> im<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and</span> ker<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">for some matrix </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, referring to the linear transformation defined by </span><span class="cmbx-10x-x-109">x</span>→<span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. Due to the linearity of </span><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">, it is easy to see that</span> im<span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is a subspace of </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">and</span> ker<span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is a subspace of </span><span class="cmmi-10x-x-109">U</span><span class="cmss-10x-x-109">. As mentioned, they are closely connected with invertibility, as we shall see next.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-70003r20"></span> <span class="cmbx-10x-x-109">Theorem 20.</span> </span><span class="cmbxti-10x-x-109">(Invertibility in terms of linear transformations)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be a linear transformation.</span></p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">is injective if and only if</span> ker<span class="cmmi-10x-x-109">f </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">0</span><span class="cmsy-10x-x-109">}</span><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">(b) </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">is surjective if and only if</span> im<span class="cmmi-10x-x-109">f </span>= <span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">(c) </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">is bijective (that is, invertible) if and only if</span> ker<span class="cmmi-10x-x-109">f </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">0</span><span class="cmsy-10x-x-109">}</span><span class="cmti-10x-x-109">and</span> im<span class="cmmi-10x-x-109">f </span>= <span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-144" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. (a) </span><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is injective, there can only be one vector in </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">that is mapped to </span><span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">. Since </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">0</span>) = <span class="cmbx-10x-x-109">0 </span><span class="cmss-10x-x-109">for any linear transformation,</span> ker<span class="cmmi-10x-x-109">f </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">0</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">On the other hand, if there are two different vectors </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈ </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">y</span>)<span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">y</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">y</span>) = <span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">, so </span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">. Thus,</span> ker<span class="cmmi-10x-x-109">f </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">0</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">implies </span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, which gives the injectivity.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">This is just the definition of surjectivity.</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">This immediately follows from combining </span><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">above.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Because matrices define linear transformations, it makes sense to talk about the inverse of a matrix.</span></p>
<p><span class="cmss-10x-x-109">Algebraically</span> <span id="dx1-70004"></span><span class="cmss-10x-x-109">speaking, the inverse of an </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the matrix </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">AA</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> = <span class="cmmi-10x-x-109">I </span><span class="cmss-10x-x-109">holds. The connection between linear transforms and matrices imply that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is the matrix of </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">, so no surprise here.</span></p>
<p><span class="cmss-10x-x-109">Don’t worry if this section about invertibility feels like a bit too much algebra. Later, when talking about the determinant of a transformation, we are going to study invertibility from a geometric perspective later in this chapter. In terms of matrices, later we are going to see a general method to calculate the inverse matrix in </span><span class="cmssi-10x-x-109">Section </span><a href="ch011.xhtml#inverting-matrices"><span class="cmssi-10x-x-109">5.1.6</span></a><span class="cmss-10x-x-109">. We’ll be there soon, but first, we take a look at how the choice of basis determines the matrix representation.</span></p>
</section>
</section>
<section id="change-of-basis" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_55"><span class="titlemark"><span class="cmss-10x-x-109">4.2 </span></span> <span id="x1-710005.2"></span><span class="cmss-10x-x-109">Change of basis</span></h2>
<p><span class="cmss-10x-x-109">Previously in</span> <span id="dx1-71001"></span><span class="cmss-10x-x-109">this section, we have seen that any linear transformation can be described with the images of the basis vectors (see </span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#linear-transformations-and-matrices"><span class="cmssi-10x-x-109">4.1.1</span></a><span class="cmss-10x-x-109">). This gave us the matrix representation that we use all the time. However, this very much depends on the choice of basis. Different bases yield different matrices for the same transformation.</span></p>
<p><span class="cmss-10x-x-109">For instance, let’s take a look at </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, which maps </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>0) <span class="cmss-10x-x-109">to the vector</span> (2<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> = (0<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">to </span>(1<span class="cmmi-10x-x-109">,</span>2)<span class="cmss-10x-x-109">. Its matrix in the standard orthonormal basis </span><span class="cmmi-10x-x-109">E </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is given by</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(4).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(4.5)
  </div>
</div>
<div class="minipage">
<p><img src="../media/file353.png" width="470" alt="PIC"/> <span id="x1-71002r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.3: The linear transformation </span><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">, defined by (</span><a href="ch010.xhtml#change-of-basis"><span class="cmss-10x-x-109">5.2</span></a><span class="cmss-10x-x-109">)</span> </span>
</div>
<p><span class="cmss-10x-x-109">The effect of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is visualized in </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">4.3</span></a><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">What if we</span> <span id="dx1-71003"></span><span class="cmss-10x-x-109">select a different basis, say </span><span class="cmmi-10x-x-109">P </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>1)<span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub> = (<span class="cmsy-10x-x-109">−</span>1<span class="cmmi-10x-x-109">,</span>1)<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">? With a quick calculation, we can check that</span></p>
<div class="math-display">
<img src="../media/file354.png" class="math-display" alt="⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ |2 1| 1 3 |2 1| − 1 − 1 |⌈1 2|⌉ ⌈ ⌉ = ⌈ ⌉ , |⌈1 2|⌉ ⌈ ⌉ = ⌈ ⌉ . 1 3 1 1 "/>
</div>
<p><span class="cmss-10x-x-109">In other words, </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub>) = 3<span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub> + 0<span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub>) = 0<span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub> + <span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. This is visualized by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">4.4</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file355.png" width="170" alt="PIC"/> <span id="x1-71004r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.4: The effect of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">on </span><span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub> = (<span class="cmsy-10x-x-109">−</span>1<span class="cmmi-10x-x-109">,</span>1) </span>
</div>
<p><span class="cmss-10x-x-109">This means that if </span><span class="cmmi-10x-x-109">P </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is our basis (thus, if writing </span>(<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">means </span><span class="cmmi-10x-x-109">a</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">b</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">), the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">becomes</span></p>
<img src="../media/file356.png" class="math-display" width="150" alt=" ⌊ ⌋ |3 0| Af,P = |⌈0 1|⌉ . "/>

<p><span class="cmss-10x-x-109">In this form, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f,P</span></sub> <span class="cmss-10x-x-109">is a diagonal matrix. (That is, its elements below and above the diagonal are zero.) As you can see, having</span><span id="dx1-71005"></span> <span class="cmss-10x-x-109">the right basis can significantly simplify the linear transformation. For instance, in </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">dimensions, applying a transformation in diagonal form requires only </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">operations, as</span></p>
<div class="math-display">
<img src="../media/file357.png" class="math-display" alt="⌊ ⌋⌊ ⌋ ⌊ ⌋ d1 0 ... 0 x1 d1x1 || 0 d ... 0 |||| x || || d x || || 2 |||| 2|| = || 2 2|| |⌈ ... ... ... ... |⌉|⌈ ...|⌉ |⌈ ... |⌉ 0 0 ... dn xn dnxn "/>
</div>
<p><span class="cmss-10x-x-109">Otherwise, </span><span class="cmmi-10x-x-109">n</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">operations are needed. So, we can save a lot there.</span></p>
<section id="the-transformation-matrix" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_56"><span class="titlemark"><span class="cmss-10x-x-109">4.2.1 </span></span> <span id="x1-720005.2.1"></span><span class="cmss-10x-x-109">The transformation matrix</span></h3>
<p><span class="cmss-10x-x-109">We have</span> <span id="dx1-72001"></span><span class="cmss-10x-x-109">just seen that the matrix of a linear transformation depends on our choice of basis. However, there is a special relation between matrices of the same transformation. We’ll explore this next. Let </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">be a linear transformation, and let </span><span class="cmmi-10x-x-109">P </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Q </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">q</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">q</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">be two bases. As before, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f,S</span></sub> <span class="cmss-10x-x-109">denotes the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">in some basis </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Suppose that we know </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f,P</span></sub> <span class="cmss-10x-x-109">, but we have our vectors represented in terms of the other basis </span><span class="cmmi-10x-x-109">Q</span><span class="cmss-10x-x-109">. How do we calculate the images our vectors under the linear transformation? A natural idea is to first </span><span class="cmssi-10x-x-109">transform our vector representations </span><span class="cmss-10x-x-109">from </span><span class="cmmi-10x-x-109">Q </span><span class="cmss-10x-x-109">to </span><span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">, apply </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f,P</span></sub> <span class="cmss-10x-x-109">, then transform the representations back. In the following, we are going to make this precise.</span></p>
<p><span class="cmss-10x-x-109">Let </span><img src="../media/file358.png" class="math" alt="t : U → U "/> <span class="cmss-10x-x-109">be a transformation defined by </span><img src="../media/file359.png" class="math" alt="pi ↦→ qi "/> <span class="cmss-10x-x-109">for all </span><img src="../media/file360.png" class="math" alt="i ∈ {1,...,n } "/><span class="cmss-10x-x-109">. (In other words, </span><img src="../media/file361.png" class="math" alt="t "/> <span class="cmss-10x-x-109">maps one set of basis vectors to another.) Since </span><img src="../media/file362.png" class="math" alt="P "/> <span class="cmss-10x-x-109">and </span><img src="../media/file363.png" class="math" alt="Q "/> <span class="cmss-10x-x-109">are bases (so the sets are linearly independent), </span><img src="../media/file364.png" class="math" alt="t "/> <span class="cmss-10x-x-109">is invertible.</span></p>
<p><span class="cmss-10x-x-109">Suppose that the matrix </span><img src="../media/file365.png" class="math" alt=" Q Af,Q = (ai,j)ni,j=1 "/> <span class="cmss-10x-x-109">is known to us, that is,</span></p>
<div class="math-display">
<img src="../media/file366.png" class="math-display" alt=" ∑n Q f(qj) = Af,Qqj = ai,jqi i=1 "/>
</div>
<p><span class="cmss-10x-x-109">holds for all </span><img src="../media/file367.png" class="math" alt="j "/><span class="cmss-10x-x-109">. So, we have</span></p>
<div class="math-display">
<img src="../media/file368.png" class="math-display" alt=" −1 −1 (t ft)(pj ) = t f(qj) ∑n = t−1( aQi,jqi) i=1 ∑n = aQi,jt−1(qi) i=1 ∑n = aQi,jpi. i=1 "/>
</div>
<p><span class="cmss-10x-x-109">In other</span> <span id="dx1-72002"></span><span class="cmss-10x-x-109">words, the matrix of the composed transformation </span><span class="cmmi-10x-x-109">t</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">ft </span><span class="cmss-10x-x-109">in the basis </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is the same as the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">in </span><span class="cmmi-10x-x-109">Q</span><span class="cmss-10x-x-109">. In terms of formulas,</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; max-width: 600px;" class="math-display equation">
  <div class="math-display">
    <span class="cmmi-10x-x-109">T</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>
    <span class="cmmi-10x-x-109">A</span> <sub><span class="cmmi-8">f,P</span></sub>
    <span class="cmmi-10x-x-109">T </span>= 
    <span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">f,Q</span></sub><span class="cmmi-10x-x-109">,</span>
  </div>
  <div style="padding-left: 1em;">(4.6)</div>
</div>

<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">T </span><span class="cmss-10x-x-109">denotes the matrix of </span><span class="cmmi-10x-x-109">t </span><span class="cmss-10x-x-109">in </span><span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">. (For notational simplicity, we omit the subscript. Most often, we don’t care what base it is in.)</span></p>
<p><span class="cmss-10x-x-109">We’ll call </span><span class="cmmi-10x-x-109">T </span><span class="cmss-10x-x-109">the change of basis matrix. These types of relations are prevalent in linear algebra, so we’ll take the time to introduce a definition formally.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-72003r19"></span> <span class="cmbx-10x-x-109">Definition 19.</span> </span><span class="cmbx-10x-x-109">(Similar matrices)</span></p>
<p>Let <span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> be two arbitrary matrices. <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B </span>are called <span class="cmti-10x-x-109">similar </span>if there exists a matrix <span class="cmmi-10x-x-109">T </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> such that</p>

<img src="../media/file369.png" class="math-display" width="150" alt=" − 1 B = T AT "/>

<p>holds. We call mappings of the form <span class="cmmi-10x-x-109">A</span>→<span class="cmmi-10x-x-109">T</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">AT </span><span class="cmti-10x-x-109">similarity transformations</span>.</p>
</div>
<p><span class="cmss-10x-x-109">In these terms, (</span><a href="ch010.xhtml#the-transformation-matrix"><span class="cmss-10x-x-109">4.6</span></a><span class="cmss-10x-x-109">) says that the matrices of a given linear transformation are all similar to each other.</span></p>
<p><span class="cmss-10x-x-109">With this under our belt, we can finish up with the example (</span><a href="ch010.xhtml#change-of-basis"><span class="cmss-10x-x-109">4.5</span></a><span class="cmss-10x-x-109">). In this case, </span><span class="cmmi-10x-x-109">T </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">T</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file371.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ | 1 − 1| | 1∕2 1∕2| T = |⌈ 1 1|⌉ , T −1 = |⌈− 1∕2 1∕2|⌉ . "/>
</div>
<p><span class="cmss-10x-x-109">(Later, we’ll see a general method to compute the inverse of any matrix, but for now, you can verify this by hand.) Thus,</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(5).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(4.7)
  </div>
</div>
<p><span class="cmss-10x-x-109">or equivalently,</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(6).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(4.8)
  </div>
</div>
<p><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">4.5</span></a> <span class="cmss-10x-x-109">shows what (</span><a href="#"><span class="cmss-10x-x-109">4.8</span></a><span class="cmss-10x-x-109">) looks like in</span> <span id="dx1-72004"></span><span class="cmss-10x-x-109">geometric terms.</span></p>
<div class="minipage">
<p><img src="../media/file380.png" width="569" alt="PIC"/> <span id="x1-72005r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.5: Change of basis, illustrated</span> </span>
</div>
<p><span class="cmss-10x-x-109">From this example, we can see that a properly selected similarity transformation can diagonalize certain matrices. Is this a coincidence? Spoiler alert: no. In </span><span class="cmssi-10x-x-109">Chapter </span><a href="ch013.xhtml#matrix-factorizations"><span class="cmssi-10x-x-109">7</span></a><span class="cmss-10x-x-109">, we will see exactly when and how this can be done.</span></p>
<p><span class="cmss-10x-x-109">I know, this is a bit too abstract. As always, examples illustrate a concept best, so let’s see some!</span></p>
</section>
</section>
<section id="linear-transformations-in-the-euclidean-plane" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_57"><span class="titlemark"><span class="cmss-10x-x-109">4.3 </span></span> <span id="x1-730005.3"></span><span class="cmss-10x-x-109">Linear transformations in the Euclidean plane</span></h2>
<p><span class="cmss-10x-x-109">We have just seen that a linear transformation can be described by the image of a basis set. From a geometric viewpoint, they are functions mapping </span><a href="https://en.wikipedia.org/wiki/Parallelepiped"><span class="cmss-10x-x-109">parallelepipeds</span></a> <span class="cmss-10x-x-109">to parallelepipeds.</span></p>
<p><span class="cmss-10x-x-109">Because of the linearity, you can imagine this as distorting the </span><span class="cmssi-10x-x-109">grid </span><span class="cmss-10x-x-109">determined by the bases.</span></p>
<div class="minipage">
<p><img src="../media/file381.png" width="284" alt="PIC"/> <span id="x1-73001r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.6: How linear transforms distort the grid determined by the basis vectors</span> </span>
</div>
<p><span class="cmss-10x-x-109">In two dimensions, we</span> <span id="dx1-73002"></span><span class="cmss-10x-x-109">have seen a few examples of geometric maps such as scaling and rotation as linear transformations. Now we can put them into matrix form. There are five of them in particular that we will study: stretching, shearing, rotation, reflection, and projection.</span></p>
<p><span class="cmss-10x-x-109">These simple transformations are not only essential to build intuition, but they are also frequently applied in computer vision. Flipping, rotating, and stretching are essential parts of image augmentation pipelines, greatly enhancing the performance of models.</span></p>
<section id="stretching" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_58"><span class="titlemark"><span class="cmss-10x-x-109">4.3.1 </span></span> <span id="x1-740005.3.1"></span><span class="cmss-10x-x-109">Stretching</span></h3>
<p><span class="cmss-10x-x-109">The simplest</span> <span id="dx1-74001"></span><span class="cmss-10x-x-109">one is a generalization of scaling. We have seen a variant of this in Example 1 above (see </span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#what-is-a-linear-transformation"><span class="cmssi-10x-x-109">4.1</span></a><span class="cmss-10x-x-109">). In matrix form, this is given by</span></p>
<div class="math-display">
<img src="../media/file382.png" class="math-display" alt=" ⌊ ⌋ c1 0 A = ⌈ ⌉ , c1,c2 ∈ ℝ. 0 c2 "/>
</div>
<p><span class="cmss-10x-x-109">Linear</span> <span id="dx1-74002"></span><span class="cmss-10x-x-109">transformations</span> <span id="dx1-74003"></span><span class="cmss-10x-x-109">such as this can be visualized by plotting the image of the unit square determined by the standard basis </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>0)<span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> = (0<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file383.png" width="170" alt="PIC"/> <span id="x1-74004r7"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.7: Stretching</span> </span>
</div>
</section>
<section id="rotations" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_59"><span class="titlemark"><span class="cmss-10x-x-109">4.3.2 </span></span> <span id="x1-750005.3.2"></span><span class="cmss-10x-x-109">Rotations</span></h3>
<p><span class="cmss-10x-x-109">Rotations are given by the matrix</span></p>
<div class="math-display">
<img src="../media/file384.png" class="math-display" alt=" ⌊ ⌋ cosα − sinα R α = ⌈ ⌉. sinα cosα "/>
</div>
<p><span class="cmss-10x-x-109">To see</span> <span id="dx1-75001"></span><span class="cmss-10x-x-109">why, recall</span><span id="dx1-75002"></span> <span class="cmss-10x-x-109">that each column of the transformation’s matrix describes the </span><span class="cmssi-10x-x-109">image </span><span class="cmss-10x-x-109">of the basis vectors. The rotation of </span>(1<span class="cmmi-10x-x-109">,</span>0) <span class="cmss-10x-x-109">is given by </span>(cos<span class="cmmi-10x-x-109">α,</span>sin<span class="cmmi-10x-x-109">α</span>)<span class="cmss-10x-x-109">, while the rotation of </span>(0<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">is </span>(cos(<span class="cmmi-10x-x-109">α </span>+ <span class="cmmi-10x-x-109">π∕</span>2)<span class="cmmi-10x-x-109">,</span>sin(<span class="cmmi-10x-x-109">α </span>+ <span class="cmmi-10x-x-109">π∕</span>2))<span class="cmss-10x-x-109">. This is illustrated by </span><span class="cmssi-10x-x-109">Figure 4.8</span><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file385.png" width="341" alt="PIC"/> <span id="x1-75003r8"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.8: The rotation matrix explained</span> </span>
</div>
<p><span class="cmss-10x-x-109">Like above, we</span> <span id="dx1-75004"></span><span class="cmss-10x-x-109">can visualize the image of the unit square to gain a geometric insight into</span> <span id="dx1-75005"></span><span class="cmss-10x-x-109">what is happening.</span></p>
<div class="minipage">
<p><img src="../media/file386.png" width="170" alt="PIC"/> <span id="x1-75006r9"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.9: Rotation</span> </span>
</div>
</section>
<section id="shearing" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_60"><span class="titlemark"><span class="cmss-10x-x-109">4.3.3 </span></span> <span id="x1-760005.3.3"></span><span class="cmss-10x-x-109">Shearing</span></h3>
<p><span class="cmss-10x-x-109">Another</span><span id="dx1-76001"></span> <span class="cmss-10x-x-109">essential</span> <span id="dx1-76002"></span><span class="cmss-10x-x-109">geometric transform is </span><span class="cmssi-10x-x-109">shearing</span><span class="cmss-10x-x-109">, which is frequently applied in physics. A shearing force (</span><a href="https://en.wikipedia.org/wiki/Shear_force" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Shear_force</span></a><span class="cmss-10x-x-109">) is a pair of forces with opposite directions, acting on the same body.</span></p>
<div class="minipage">
<p><img src="../media/file387.png" width="170" alt="PIC"/> <span id="x1-76003r10"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.10: Shearing</span> </span>
</div>
<p><span class="cmss-10x-x-109">Its matrix is given in the form</span></p>
<div class="math-display">
<img src="../media/file388.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ Sx = ⌈1 kx⌉ , Sy = ⌈ 1 0⌉, S = ⌈1 kx⌉ , 0 1 ky 1 ky 1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">x</span></sub><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">y</span></sub><span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">represent shearing transformations</span> <span id="dx1-76004"></span><span class="cmss-10x-x-109">in the </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">y</span><span class="cmss-10x-x-109">, and in both directions.</span></p>
</section>
<section id="reflection" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_61"><span class="titlemark"><span class="cmss-10x-x-109">4.3.4 </span></span> <span id="x1-770005.3.4"></span><span class="cmss-10x-x-109">Reflection</span></h3>
<p><span class="cmss-10x-x-109">Until this</span><span id="dx1-77001"></span> <span class="cmss-10x-x-109">point, all the transformations we</span> <span id="dx1-77002"></span><span class="cmss-10x-x-109">have seen in the Euclidean plane preserved the “orientation” of the space. However, this is not always the case. The transformation given by the matrices</span></p>
<div class="math-display">
<img src="../media/file389.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ − 1 0 1 0 R1 = ⌈ ⌉ , R2 = ⌈ ⌉ 0 1 0 − 1 "/>
</div>
<p><span class="cmss-10x-x-109">act as reflections with respect to the </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">and the </span><span class="cmmi-10x-x-109">y </span><span class="cmss-10x-x-109">axes.</span></p>
<div class="minipage">
<p><img src="../media/file390.png" width="370" alt="PIC"/> <span id="x1-77003r11"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.11: Reflection</span> </span>
</div>
<p><span class="cmss-10x-x-109">When combined</span><span id="dx1-77004"></span> <span class="cmss-10x-x-109">with a rotation, we can use</span><span id="dx1-77005"></span> <span class="cmss-10x-x-109">reflections to flip bases. For instance, the transformation maps </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">to </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">to </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">.</span></p>
<div class="math-display">
<img src="../media/file391.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ R = ⌈0 − 1⌉ ⌈1 0⌉ = ⌈0 1⌉ 1 0 0 − 1 1 0 ◟--◝◜--◞ ◟---◝◜--◞ rotation with π∕2 =R2 "/>
</div>
<div class="minipage">
<p><img src="../media/file392.png" width="370" alt="PIC"/> <span id="x1-77006r12"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.12: Swapping </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">is a reflection and rotation</span> </span>
</div>
<p><span class="cmss-10x-x-109">These types of transformations play an essential role in understanding determinants, as we will soon see in the next chapter.</span></p>
<p><span class="cmss-10x-x-109">In general, reflections can be easily defined in higher dimensional spaces. For instance,</span></p>

<img src="../media/file393.png" class="math-display" alt=" ⌊ ⌋ 1 0 0 R = ||0 1 0 || ⌈ ⌉ 0 0 − 1 " width="150"/>

<p><span class="cmss-10x-x-109">is a reflection in </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">3</span></sup> <span class="cmss-10x-x-109">that flips </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">3</span></sub> <span class="cmss-10x-x-109">to the opposite</span> <span id="dx1-77007"></span><span class="cmss-10x-x-109">direction. It is just like looking in the mirror: it turns left to right and right to left.</span></p>
<p><span class="cmss-10x-x-109">Reflections can flip orientations multiple times. The transformation given by</span></p>

<img src="../media/file394.png" class="math-display" alt=" ⌊ ⌋ 1 0 0 R = || || ⌈0 − 1 0⌉ 0 0 − 1 " width="150"/>

<p><span class="cmss-10x-x-109">flips </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">3</span></sub><span class="cmss-10x-x-109">, changing the orientation twice. Later, we’ll see that the “number of changes in orientation” of a given transformation is one of its essential descriptors.</span></p>
</section>
<section id="orthogonal-projection" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_62"><span class="titlemark"><span class="cmss-10x-x-109">4.3.5 </span></span> <span id="x1-780005.3.5"></span><span class="cmss-10x-x-109">Orthogonal projection</span></h3>
<p><span class="cmss-10x-x-109">One of</span> <span id="dx1-78001"></span><span class="cmss-10x-x-109">the most important</span> <span id="dx1-78002"></span><span class="cmss-10x-x-109">transformations (not only in two dimensions) is the </span><span class="cmssi-10x-x-109">orthogonal projection</span><span class="cmss-10x-x-109">. We have seen this already when talking about inner products and their geometric representation in </span><span class="cmssi-10x-x-109">Section </span><a href="ch008.xhtml#the-geometric-interpretation-of-inner-products"><span class="cmssi-10x-x-109">2.2.3</span></a><span class="cmss-10x-x-109">. By taking a closer look, it turns out that they are linear transformations.</span></p>
<div class="minipage">
<p><img src="../media/file395.png" width="370" alt="PIC"/> <span id="x1-78003r13"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.13: Orthogonal projection</span> </span>
</div>
<p><span class="cmss-10x-x-109">Recall from (</span><a href="ch008.xhtml#x1-45003r3.2.3"><span class="cmss-10x-x-109">3.2.3</span></a><span class="cmss-10x-x-109">) that the orthogonal projection of </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">to some </span><span class="cmmi-10x-x-109">y </span><span class="cmss-10x-x-109">can be written as</span></p>

<img src="../media/file396.png" class="math-display" width="150" alt="proj (x) = ⟨x,y⟩y. y ⟨y,y⟩ "/>

<p><span class="cmss-10x-x-109">The bilinearity of </span><span class="cmsy-10x-x-109">⟨⋅</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">⋅⟩ </span><span class="cmss-10x-x-109">immediately implies that</span> proj<sub><span class="cmbx-8">y</span></sub>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">is also linear. With a bit of algebra, we can rewrite this in terms of matrices. We have</span></p>
<div class="math-display">
<img src="../media/file397.png" class="math-display" alt="projy(x) = ⟨x,y-⟩y ⟨y,y ⟩ ⌊ ⌋ = x1y1-+-x2y2⌈y1 ⌉ ∥y ∥2 y2 ⌊ ⌋ ⌊ ⌋ 1 y21 y1y2 x1 = ----2⌈ 2 ⌉ ⌈ ⌉ ∥y ∥ y1y2 y2 x2 1 = ----2yyT x, ∥y ∥ "/>
</div>
<p><span class="cmss-10x-x-109">thus,</span></p>
<div class="math-display">
<img src="../media/file398.png" class="math-display" alt=" ⌊ ⌋ 1 y2 y y projy = ----2⌈ 1 1 2⌉ ∥y ∥ y1y2 y22 1 = ----2yyT x ∥y ∥ "/>
</div>
<p><span class="cmss-10x-x-109">Notice that</span></p>
<div class="math-display">
<img src="../media/file399.png" class="math-display" alt=" y2 projy(e2) = y1 projy(e1), "/>
</div>
<p><span class="cmss-10x-x-109">so the images of the standard basis vectors are not linearly independent. As a consequence, the image of the plane under</span> proj<sub><span class="cmbx-8">y</span></sub> <span class="cmss-10x-x-109">is</span> span(<span class="cmbx-10x-x-109">y</span>)<span class="cmss-10x-x-109">, which is a one-dimensional subspace. From this example, we can see that the image of a vector space under a linear transformation is not necessarily of the same dimension as the starting space.</span></p>
<p><span class="cmss-10x-x-109">With these</span><span id="dx1-78004"></span> <span class="cmss-10x-x-109">examples and knowledge under our belt, we have a basic understanding of linear transformations, the most basic building blocks of neural networks. In the next section, we will study how linear transformations affect the geometric structure of the vector space.</span></p>
</section>
</section>
<section id="determinants-or-how-linear-transformations-affect-volume" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_63"><span class="titlemark"><span class="cmss-10x-x-109">4.4 </span></span> <span id="x1-790005.4"></span><span class="cmss-10x-x-109">Determinants, or how linear transformations affect volume</span></h2>
<p><span class="cmss-10x-x-109">In </span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#linear-transformations-in-the-euclidean-plane"><span class="cmssi-10x-x-109">4.3</span></a><span class="cmss-10x-x-109">, we have seen that linear transformations (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch010.xhtml#x1-66002r16"><span class="cmssi-10x-x-109">16</span></a><span class="cmss-10x-x-109">) can be thought of as distorting the grid determined by the basis vectors.</span></p>
<p><span class="cmss-10x-x-109">Following our geometric intuition, we suspect that measuring </span><span class="cmssi-10x-x-109">how much </span><span class="cmss-10x-x-109">a transformation distorts volume and distance can provide some valuable insight. As we will see in this chapter, this is exactly the case. Transformations that preserve distance or norm are special, giving</span><span id="dx1-79001"></span> <span class="cmss-10x-x-109">rise to methods such as Principal Component Analysis.</span></p>
<section id="how-linear-transformations-scale-the-area" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_64"><span class="titlemark"><span class="cmss-10x-x-109">4.4.1 </span></span> <span id="x1-800005.4.1"></span><span class="cmss-10x-x-109">How linear transformations scale the area</span></h3>
<p><span class="cmss-10x-x-109">Let’s go</span> <span id="dx1-80001"></span><span class="cmss-10x-x-109">back to the Euclidean plane one more time. Consider any linear transformation </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, mapping the unit square to a parallelogram.</span></p>
<div class="minipage">
<p><img src="../media/file400.png" width="284" alt="PIC"/> <span id="x1-80002r14"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.14: Image of the unit square under a linear transformation</span> </span>
</div>
<p><span class="cmss-10x-x-109">The area of this parallelogram describes how </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">scales the unit square. Let’s call it </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">for now; that is,</span></p>
<div class="math-display">
<img src="../media/file401.png" class="math-display" alt="area(A (C )) = λ ⋅area(C), "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">C </span>= [0<span class="cmmi-10x-x-109">,</span>1] <span class="cmsy-10x-x-109">× </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">is the unit square, and </span><span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">C</span>) <span class="cmss-10x-x-109">is its image</span></p>
<div class="math-display">
<img src="../media/file402.png" class="math-display" alt="A(C ) := {Ax : x ∈ C }. "/>
</div>
<p><span class="cmss-10x-x-109">Due to linearity, </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">also matches the scaling ratio between the area of any rectangle (with parallel sides to the coordinate axes) and its image under </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. As </span><span class="cmssi-10x-x-109">Figure 4.15 </span><span class="cmss-10x-x-109">shows, we can approximate any planar object as the union of rectangles.</span></p>
<p><span class="cmss-10x-x-109">If all rectangles are scaled by </span><span class="cmmi-10x-x-109">λ</span><span class="cmss-10x-x-109">, then unions of rectangles also scale by that factor. Thus, it follows that </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">is also the scaling ratio between </span><span class="cmssi-10x-x-109">any </span><span class="cmss-10x-x-109">planar object </span><span class="cmmi-10x-x-109">E </span><span class="cmss-10x-x-109">and its image </span><span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">E</span>) = <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>: <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">E</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file403.png" width="484" alt="PIC"/> <span id="x1-80003r15"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.15: Approximating planar objects with a union of rectangles</span> </span>
</div>
<p><span class="cmss-10x-x-109">This quantity </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">reveals a lot about the transformation itself, but</span> <span id="dx1-80004"></span><span class="cmss-10x-x-109">there is a question remaining: how can we calculate it?</span></p>
<p><span class="cmss-10x-x-109">Suppose that our linear transformation is given by</span></p>
<img src="../media/file404.png" class="math-display" width="150" alt=" ⌊ ⌋ x y A = ⌈ 1 1⌉ , x2 y2 "/>

<p><span class="cmss-10x-x-109">thus its columns </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span>= (<span class="cmmi-10x-x-109">y</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,y</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">describe the two sides of the parallelogram. This is the image of the unit square.</span></p>
<div class="minipage">
<p><img src="../media/file405.png" width="484" alt="PIC"/> <span id="x1-80005r16"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.16: Image of the unit square under a linear transformation</span> </span>
</div>
<p><span class="cmss-10x-x-109">Our area scaling factor </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">equals the area of this parallelogram, so our goal is to calculate this.</span></p>
<p><span class="cmss-10x-x-109">The area of any parallelogram can be calculated by multiplying the length of the base (</span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥ </span><span class="cmss-10x-x-109">in this case) with the height </span><span class="cmmi-10x-x-109">h</span><span class="cmss-10x-x-109">. (You can easily see this by cutting off a triangle at the right side of the parallelogram and putting it to the left side, rearranging it as a rectangle.) </span><span class="cmmi-10x-x-109">h </span><span class="cmss-10x-x-109">is unknown, but with basic trigonometry, we can see that </span><span class="cmmi-10x-x-109">h </span>= sin<span class="cmmi-10x-x-109">α</span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">α </span><span class="cmss-10x-x-109">is the angle between </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file406.png" class="math-display" alt="area = sin α∥y∥∥x ∥. "/>
</div>
<p><span class="cmss-10x-x-109">This is almost</span> <span id="dx1-80006"></span><span class="cmss-10x-x-109">the dot product of </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">. (Recall that the dot product can be written as </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥∥</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">∥</span>cos<span class="cmmi-10x-x-109">α</span><span class="cmss-10x-x-109">.) However, the</span> sin<span class="cmmi-10x-x-109">α </span><span class="cmss-10x-x-109">part is not a match.</span></p>
<p><span class="cmss-10x-x-109">Fortunately, there is a clever trick we can use to turn this into a dot product! Since</span> sin<span class="cmmi-10x-x-109">α </span>= cos<span class="bbig">(</span><span class="cmmi-10x-x-109">α </span><span class="cmsy-10x-x-109">−</span><img src="../media/file408.png" width="11" data-align="middle" alt="π 2"/><span class="bbig">)</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file410.png" class="math-display" alt=" π- area = cos(α− 2)∥x∥∥y ∥. "/>
</div>
<p><span class="cmss-10x-x-109">The issue is the angle between </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">is not </span><span class="cmmi-10x-x-109">α </span><span class="cmsy-10x-x-109">−</span><img src="../media/file411.png" width="11" data-align="middle" alt="π 2"/><span class="cmss-10x-x-109">. However, we can solve this easily by applying a rotation (</span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#rotations"><span class="cmssi-10x-x-109">4.3.2</span></a><span class="cmss-10x-x-109">). Applying the transformation</span></p>
<img src="../media/file412.png" class="math-display" width="150" alt=" ⌊ ⌋ 0 1 R = ⌈ ⌉, − 1 0 "/>

<p><span class="cmss-10x-x-109">we obtain</span></p>
<div class="math-display">
<img src="../media/file413.png" class="math-display" alt="y = Ry = (y,− y ). rot 2 1 "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><img src="../media/file414.png" class="math" alt="∥yrot∥ = ∥y∥ "/><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file415.png" class="math-display" alt="area = sin α∥y∥∥x ∥ π = cos(α − 2)∥x ∥∥y∥ π = cos(α − 2)∥x ∥∥yrot∥ = ⟨x,yrot⟩. "/>
</div>
<p><span class="cmss-10x-x-109">The quantity </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><sub><span class="cmss-10x-x-109">rot</span></sub><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">can be calculated using only the elements of the matrix </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file416.png" class="math-display" alt="⟨x, yrot⟩ = x1y2 − x2y1. "/>
</div>
<p><span class="cmss-10x-x-109">Notice that </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><sub><span class="cmss-10x-x-109">rot</span></sub><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">can be negative! This happens when the angle between </span><span class="cmbx-10x-x-109">y </span>= <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x </span>= <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, measured from a counter-clockwise direction, is larger than </span><span class="cmmi-10x-x-109">π</span><span class="cmss-10x-x-109">, as this implies</span> cos<span class="bbig">(</span><span class="cmmi-10x-x-109">α </span><span class="cmsy-10x-x-109">−</span><img src="../media/file418.png" width="11" data-align="middle" alt="π 2"/><span class="bbig">)</span><span class="cmmi-10x-x-109">/span&gt;0<span class="cmss-10x-x-109">.</span> </span></p>
<p><span class="cmss-10x-x-109">Hence, the quantity </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><sub><span class="cmss-10x-x-109">rot</span></sub><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">signed area </span><span class="cmss-10x-x-109">of the parallelogram.</span></p>
<p><span class="cmss-10x-x-109">In two dimensions, we</span> <span id="dx1-80007"></span><span class="cmss-10x-x-109">call this the </span><span class="cmssi-10x-x-109">determinant </span><span class="cmss-10x-x-109">of the linear transformation. That is, for any given linear transformation/matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span><span class="cmsy-8">×</span><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, its determinant is defined by</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(7).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(4.9)
  </div>
</div>
<p><span class="cmss-10x-x-109">The</span> <span id="dx1-80008"></span><span class="cmss-10x-x-109">determinant is often written as </span><span class="cmmi-10x-x-109">jAj</span><span class="cmss-10x-x-109">, but we’ll avoid this notation. We’ll deal with</span> <span id="dx1-80009"></span><span class="cmss-10x-x-109">determinants for any matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, but let’s stay with the </span>2 <span class="cmsy-10x-x-109">× </span>2 <span class="cmss-10x-x-109">case just a bit to build intuition.</span></p>
<p><span class="cmss-10x-x-109">The determinant also reveals the orientation of the vectors: positive determinant means positive orientation, negative determinant means negative orientation. (Intuitively, positive orientation means that the angle measured from </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">to </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">in a counter-clockwise direction is between </span>0 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">π</span><span class="cmss-10x-x-109">; equivalently, the angle measured from </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">to </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">in a clockwise direction is between </span><span class="cmmi-10x-x-109">π </span><span class="cmss-10x-x-109">and </span>2<span class="cmmi-10x-x-109">π</span><span class="cmss-10x-x-109">.) This is demonstrated in </span><span class="cmssi-10x-x-109">Figure 4.17 </span><span class="cmss-10x-x-109">below.</span></p>
<div class="minipage">
<p><img src="../media/file421.png" width="4" alt="PIC"/> <span id="x1-80010r17"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.17: Orientation of two vectors in the plane</span> </span>
</div>
<p><span class="cmss-10x-x-109">Overall,</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; max-width: 600px;" class="math-display">
  <div class="math-display">
    area<span class="big">(</span><span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">E</span>)<span class="big">)</span> = 
    <span class="cmsy-10x-x-109">|</span>det<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">|</span>area(<span class="cmmi-10x-x-109">E</span>)
  </div>
  <div style="padding-left: 1em;">(4.10)</div>
</div>

<p><span class="cmss-10x-x-109">holds, where </span><span class="cmmi-10x-x-109">E </span><span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">is a planar object, and</span></p>
<div class="math-display">
<img src="../media/file424.png" class="math-display" alt="A (E) = {Ax : x ∈ E } "/>
</div>
<p><span class="cmss-10x-x-109">is the image of </span><span class="cmmi-10x-x-109">E </span><span class="cmss-10x-x-109">under the transformation </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Even though we have only shown (</span><a href="#"><span class="cmss-10x-x-109">5.4.1</span></a><span class="cmss-10x-x-109">) in two dimensions, this holds in general. (Although we don’t know how to define the determinant there yet.)</span></p>
<p><span class="cmss-10x-x-109">So, if </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">is a basis on the plane, equations (</span><a href="#"><span class="cmss-10x-x-109">5.4.1</span></a><span class="cmss-10x-x-109">) and (</span><a href="#"><span class="cmss-10x-x-109">5.4.1</span></a><span class="cmss-10x-x-109">) tell us that the determinant in two dimensions equals to</span></p>
<div class="math-display">
<img src="../media/file425.png" class="math-display" alt="det A = orientation(Ae ,Ae )× area(Ae ,Ae ) 1 2 1 2 "/>
</div>
<p><span class="cmss-10x-x-109">Based on</span> <span id="dx1-80011"></span><span class="cmss-10x-x-109">the example of the Euclidean plane, we have built enough geometric intuition on understanding how linear transformations distort volume and change the orientation of the space. These are described by the concept of </span><span class="cmssi-10x-x-109">determinants</span><span class="cmss-10x-x-109">, which we have defined in the special case (</span><a href="#"><span class="cmss-10x-x-109">5.4.1</span></a><span class="cmss-10x-x-109">). We are going to move on</span> <span id="dx1-80012"></span><span class="cmss-10x-x-109">to study the concept in its full generality.</span></p>
<p><span class="cmss-10x-x-109">To introduce the formal definition of the determinant, we will take a route that is different from the usual. Most commonly, the determinant of a linear transformation </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is defined straight away with a complicated formula, then all of its geometric properties are shown.</span></p>
<p><span class="cmss-10x-x-109">Instead of this, we will deduce the determinant formula by generalizing the geometric notion we have learned in the previous section. Here, we are roughly going to follow the outline of </span><span class="cmssi-10x-x-109">Linear Algebra and Its Applications </span><span class="cmss-10x-x-109">by Peter D. Lax.</span></p>
<p><span class="cmss-10x-x-109">We set the foundations by introducing some key notations. Let</span></p>
<div class="math-display">
<img src="../media/file426.png" class="math-display" alt="A = (ai,j)ni,j=1 ∈ ℝn ×n "/>
</div>
<p><span class="cmss-10x-x-109">be a matrix with columns </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">. When we introduced the notion of matrices as linear transformations in </span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#linear-transformations-and-matrices"><span class="cmssi-10x-x-109">4.1.1</span></a> <span class="cmss-10x-x-109">, we saw that the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th column is the image of the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th basis vector. For simplicity, let’s assume that </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is the standard orthonormal basis, that is, </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the vector whose </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th coordinate is </span>1 <span class="cmss-10x-x-109">and the rest is</span> 0<span class="cmss-10x-x-109">. Thus, </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">During our explorations in the Euclidean plane </span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#linear-transformations-in-the-euclidean-plane"><span class="cmssi-10x-x-109">4.3</span></a><span class="cmss-10x-x-109">, we have seen that the determinant is the orientation of the images of basis vectors, times the area of the parallelogram defined by them. Following this logic, we could define the determinant for </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">matrices by</span></p>
<div class="math-display">
<img src="../media/file427.png" class="math-display" alt="det A = orientation(Ae1,...,Aen )× volume(Ae1, ...,Aen ) "/>
</div>
<p><span class="cmss-10x-x-109">Two questions surface immediately. First, how do we define the orientation of multiple vectors in the </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional space? Second, how can we even calculate the area?</span></p>
<p><span class="cmss-10x-x-109">Instead of finding the answers for these questions, we are going to add a twist into the story: first, we’ll find a convenient formula for determinants, then use it to </span><span class="cmssi-10x-x-109">define </span><span class="cmss-10x-x-109">orientation.</span></p>
</section>
<section id="the-multilinearity-of-determinants" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_65"><span class="titlemark"><span class="cmss-10x-x-109">4.4.2 </span></span> <span id="x1-810005.4.2"></span><span class="cmss-10x-x-109">The multi-linearity of determinants</span></h3>
<p><span class="cmss-10x-x-109">To make the</span> <span id="dx1-81001"></span><span class="cmss-10x-x-109">relation between the determinant and the columns of the matrix </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">more explicit, we’ll write</span></p>
<div class="math-display">
<img src="../media/file428.png" class="math-display" alt="det A = det(a1,...,an ). "/>
</div>
<p><span class="cmss-10x-x-109">Thinking about determinants this way,</span> det <span class="cmss-10x-x-109">is just a function of multiple variables:</span></p>
<div class="math-display">
<img src="../media/file429.png" class="math-display" alt="det : ℝn × ⋅⋅⋅× ℝn → ℝ. ◟-----◝◜----◞ n times "/>
</div>
<p><span class="cmss-10x-x-109">Good news:</span> det(<span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">is linear in each variable. That is,</span></p>

<img src="../media/file430.png" class="math-display" width="650" alt="det(a1,...,αai+ βbi,...an) = α det(a1,...,ai,...an )+β det(a1,...,bi,...an) "/>

<p><span class="cmss-10x-x-109">holds. We are not going to prove this, but as the determinant represents the signed volume, you can convince yourself by checking out </span><span class="cmssi-10x-x-109">Figure 4.18</span><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file431.png" width="456" alt="PIC"/> <span id="x1-81002r18"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 4.18: The multilinearity of</span> det(<span class="cmbx-10x-x-109">a</span><sub><span class="cmbx-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmbx-8">2</span></sub>) </span>
</div>
<p><span class="cmss-10x-x-109">A consequence of linearity is that we can express the determinant as a linear combination of determinants for the standard basis vectors </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">. For instance, consider the following. Since</span></p>
<div class="math-display">
<img src="../media/file432.png" class="math-display" alt=" n Ae = a = ∑ a e, 1 1 i,1 i i=1 "/>
</div>
<p><span class="cmss-10x-x-109">we have</span></p>
<img src="../media/file433.png" class="math-display" alt=" n ∑ det(a1,a2,...,an) = ai,1 det(ei,a2,...,an). i=1 " width="450"/>

<p><span class="cmss-10x-x-109">Going one</span> <span id="dx1-81003"></span><span class="cmss-10x-x-109">step further and using that</span></p>
<img src="../media/file434.png" class="math-display" alt=" ∑n a2 = aj,2ej, j=1 " width="150"/>

<p><span class="cmss-10x-x-109">we start noticing a pattern. With the linearity, we have</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; max-width: 900px;" class="equation math-display">
  <div class="math-display">
    det(<span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span>
    <span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…,</span>
    <span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>) = 
    <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup> 
    <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup>
    <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,</span><span class="cmr-8">1</span></sub>
    <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">j,</span><span class="cmr-8">2</span></sub> 
    det(<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">,</span>
    <span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">j</span></sub><span class="cmmi-10x-x-109">,</span>
    <span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,…,</span>
    <span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmmi-10x-x-109">.</span>
  </div>
  <div style="padding-left: 1em;">(4.11)</div>
</div>

<p><span class="cmss-10x-x-109">We can see that the row indices in the coefficients </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,</span><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">j,</span><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">match the indices of </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">-s in</span> det(<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">j</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">. In the general case, this pattern can be formalized in terms of permutations; that is, orderings of the set </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">You can imagine a permutation as a function </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">mapping </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">to itself in a way that for every </span><span class="cmmi-10x-x-109">j </span><span class="cmsy-10x-x-109">∈{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, there is exactly one </span><span class="cmmi-10x-x-109">i </span><span class="cmsy-10x-x-109">∈{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">with </span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">i</span>) = <span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">. In other words, you take every integer between </span>1 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">, and putting them in an order. The set of all possible permutations on </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is denoted by </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Continuing (</span><a href="#"><span class="cmss-10x-x-109">5.4.2</span></a><span class="cmss-10x-x-109">) and further expanding the determinant of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file435.png" class="math-display" alt=" ∑ ∏n det(a1,...,an) = [ aσ(i),i]det(eσ(1),...,eσ(n)). σ∈Sn i=1 "/>
</div>
<p><span class="cmss-10x-x-109">This formula is not the easiest one to understand. You can think about each term </span><span class="cmex-10x-x-109">∏</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">i</span><span class="cmr-8">)</span><span class="cmmi-8">,i</span></sub> <span class="cmss-10x-x-109">as placing </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">chess rooks on an </span><span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">board such that none of them can capture each other.</span></p>
<div id="the-anatomy-of-the-term-a-a-nn" class="minipage">
<img src="../media/file436.png" width="600" alt="PIC"/> <span id="x1-81004r19"></span>
<span class="id"><span class="cmss-10x-x-109">Figure 4.19: The anatomy of the term </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(1)1</span></sub>⋅⋅⋅<span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span><span class="cmmi-8">n</span></sub> </span>
</div>
<p><span class="cmss-10x-x-109">The formula</span></p>
<div class="math-display">
<img src="../media/file438.png" class="math-display" alt="∑ [∏n ] aσ(i),i det(eσ(1),...,e σ(n)) σ∈Sn i=1 "/>
</div>
<p><span class="cmss-10x-x-109">combines all the possible ways we can do this.</span></p>
<p><span class="cmss-10x-x-109">There is only one thing left: calculating</span> det(<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(1)</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sub>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Remember when</span> <span id="dx1-81005"></span><span class="cmss-10x-x-109">we discussed the combination of reflections and rotations in the Euclidean plane (</span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#reflection"><span class="cmssi-10x-x-109">4.3.4</span></a><span class="cmss-10x-x-109">)? The transformation determined by </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub>→<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">i</span><span class="cmr-8">)</span></sub> <span class="cmss-10x-x-109">is similar to that. When talking about permutations, it’s good to know that each one can be obtained by switching two elements at a time. The number of </span><span class="cmssi-10x-x-109">transpositions </span><span class="cmss-10x-x-109">- that is, permutations affecting two elements - in a permutation is called the </span><span class="cmssi-10x-x-109">sign </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">. In the context of our linear transformation </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub>→<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">i</span><span class="cmr-8">)</span></sub><span class="cmss-10x-x-109">, the number of transpositions in </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">is the number of reflections required and</span> sign(<span class="cmmi-10x-x-109">σ</span>) <span class="cmss-10x-x-109">is the </span><span class="cmssi-10x-x-109">orientation </span><span class="cmss-10x-x-109">of</span> (<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(1)</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sub>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Thus, with these, we can finally give a formal definition for determinants and the orientation.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-81006r20"></span> <span class="cmbx-10x-x-109">Definition 20.</span> </span><span class="cmbx-10x-x-109">(Determinants and orientation)</span></p>
<p>Let <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> be an arbitrary matrix and let <span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> be its <span class="cmmi-10x-x-109">i</span>-th column. The <span class="cmti-10x-x-109">determinant </span>of <span class="cmmi-10x-x-109">A </span>is defined by</p>
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; max-width: 900px;" class="equation math-display">
  <div class="math-display">
    det<span class="cmmi-10x-x-109">A </span>= det(
    <span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span>
    <span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>) = 
    <span class="cmex-10x-x-109">∑</span> 
    <sub><span class="cmmi-8">σ</span><span class="cmsy-8">∈</span><span class="cmmi-8">S</span><sub><span class="cmmi-6">n</span></sub></sub>
    sign(<span class="cmmi-10x-x-109">σ</span>)
    <span class="bbig">[</span>
    <span class="cmex-10x-x-109">∏</span> 
    <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup>
    <span class="cmbx-10x-x-109">a</span> 
    <sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">i</span><span class="cmr-8">)</span><span class="cmmi-8">,i</span></sub>
    <span class="bbig">]</span><span class="cmmi-10x-x-109">,</span>
  </div>
  <div style="padding-left: 1em;">(4.12)</div>
</div>

<p>and the <span class="cmti-10x-x-109">orientation </span>of the vectors <span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub> is</p>
<div class="math-display">
<img src="../media/file443.png" class="math-display" alt="orientation(a1,...,an ) := sign(detA ). "/>
</div>
</div>
<p><span class="cmss-10x-x-109">When the</span> det <span class="cmss-10x-x-109">notation is not convenient, we denote determinants by putting the elements of the matrix inside a big absolute value sign:</span></p>
<div class="math-display">
<img src="../media/file444.png" class="math-display" alt=" || || ||a1,1 a1,2 ... a1,n|| |a2,1 a2,2 ... a2,n| detA = || . . . . ||. || .. .. .. .. || || || an,1 an,2 ... an,n "/>
</div>
<p><span class="cmss-10x-x-109">When I was a young math student, the determinant formula (</span><a href="ch010.xhtml#x1-81006r20"><span class="cmss-10x-x-109">4.12</span></a><span class="cmss-10x-x-109">) was presented as-is in my first linear algebra class. Without explaining the connection</span> <span id="dx1-81007"></span><span class="cmss-10x-x-109">to volume and orientation, it took me years to properly understand it. I still think that the determinant is one of the most complex concepts in linear algebra, especially when presented without a geometric motivation for the definition.</span></p>
<p><span class="cmss-10x-x-109">Now that you have a basic understanding of the determinant, you might ask: how can we calculate it in practice? Summing over the set of all permutations and calculating their sign is not an easy operation from a computational perspective.</span></p>
<p><span class="cmss-10x-x-109">Good news: there is a</span> <span id="dx1-81008"></span><span class="cmss-10x-x-109">recursive formula for the determinant. Bad news: for an </span><span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">matrix, it involves </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">pieces of </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmsy-10x-x-109">× </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmss-10x-x-109">matrices. Still, it is a big step from the permutation formula. Let’s see it!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-81009r21"></span> <span class="cmbx-10x-x-109">Theorem 21.</span> </span><span class="cmbxti-10x-x-109">(Recursive formula for determinants)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary square matrix. Then</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; max-width: 900px;" class="equation math-display">
  <div class="math-display">
    det<span class="cmmi-10x-x-109">A </span>= 
    <span class="cmex-10x-x-109">∑</span> 
    <sub><span class="cmmi-8">j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup>
    (<span class="cmsy-10x-x-109">−</span>1)<sup><span class="cmmi-8">j</span><span class="cmr-8">+1</span></sup>
    <span class="cmmi-10x-x-109">a</span> 
    <sub><span class="cmr-8">1</span><span class="cmmi-8">,j</span></sub> 
    det<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,j</span></sub><span class="cmmi-10x-x-109">,</span>
  </div>
  <div style="padding-left: 1em;">4.13</div>
</div>

<p><span class="cmti-10x-x-109">where </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i,j</span></sub> <span class="cmti-10x-x-109">is the </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmsy-10x-x-109">× </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmti-10x-x-109">matrix obtained from </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">by removing its </span><span class="cmmi-10x-x-109">i</span><span class="cmti-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmti-10x-x-109">-th column.</span></p>
</div>
<p><span class="cmss-10x-x-109">Instead of a proof, we are going to provide an example to demonstrate the formula. For </span>3 <span class="cmsy-10x-x-109">× </span>3 <span class="cmss-10x-x-109">matrices, this is how it looks:</span></p>
<div class="math-display">
<img src="../media/file445.png" class="math-display" alt="| | ||a b c|| || || || || || || || || ||e f|| ||d f|| ||d e|| ||d e f||= a ||h i||− b||g i||+ c||g h||. |g h i| "/>
</div>
<p><span class="cmss-10x-x-109">Now that we have both the geometric intuition and the recursive formula, let’s see the most important properties of the determinants!</span></p>
</section>
<section id="fundamental-properties-of-the-determinants" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_66"><span class="titlemark"><span class="cmss-10x-x-109">4.4.3 </span></span> <span id="x1-820005.4.3"></span><span class="cmss-10x-x-109">Fundamental properties of the determinants</span></h3>
<p><span class="cmss-10x-x-109">When working</span> <span id="dx1-82001"></span><span class="cmss-10x-x-109">with determinants, we prefer to create basic building blocks and rules for combining them. (As we have seen with this pattern so many times, even when deducing the (</span><a href="ch010.xhtml#x1-81006r20"><span class="cmss-10x-x-109">4.12</span></a><span class="cmss-10x-x-109">).) These rules are manifested by the fundamental properties of determinants, which we will discuss now.</span></p>
<p><span class="cmss-10x-x-109">The first property is concerned with the relation of composition and the determinant.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82002r22"></span> <span class="cmbx-10x-x-109">Theorem 22.</span> </span><span class="cmbxti-10x-x-109">(The product of determinants)</span></p>
<p><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be two matrices. Then</span></p>
<div class="math-display" style="display: flex; justify-content: space-between; align-items: center; max-width: 600px; width: 100%;">
  <div class="math-display">det<span class="cmmi-10x-x-109">AB</span> = det<span class="cmmi-10x-x-109">A</span>det<span class="cmmi-10x-x-109">B.</span></div>
  <div style="padding-left: 1em;">(4.14)</div>
</div>

</div>
<p><span class="cmss-10x-x-109">The equation (</span><a href="ch010.xhtml#x1-82002r22"><span class="cmss-10x-x-109">4.14</span></a><span class="cmss-10x-x-109">) is called the </span><span class="cmssi-10x-x-109">determinant product rule</span><span class="cmss-10x-x-109">, and its proof involves some heavy computations based on the formulas (</span><a href="ch010.xhtml#x1-81006r20"><span class="cmss-10x-x-109">4.12</span></a><span class="cmss-10x-x-109">) and (</span><a href="ch010.xhtml#x1-81009r21"><span class="cmss-10x-x-109">4.13</span></a><span class="cmss-10x-x-109">). Instead of providing a fully fleshed-out proof, I’ll give an intuitive explanation. After all, we want to build algorithms </span><span class="cmssi-10x-x-109">using </span><span class="cmss-10x-x-109">mathematics, not building mathematics.</span></p>
<p><span class="cmss-10x-x-109">So, the explanation of</span> det<span class="cmmi-10x-x-109">AB </span>= det<span class="cmmi-10x-x-109">A</span>det<span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is quite simple. If we think about the matrices </span><span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">as linear transformations, we have just seen that</span> det<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and</span> det<span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">determine how they scale the unit cube.</span></p>
<p><span class="cmss-10x-x-109">Since the composition of these linear transformations is the matrix product </span><span class="cmmi-10x-x-109">AB</span><span class="cmss-10x-x-109">, the linear transformation </span><span class="cmmi-10x-x-109">AB </span><span class="cmss-10x-x-109">scales the unit cube to a parallelepiped with signed volume</span> det<span class="cmmi-10x-x-109">A</span>det<span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">. (Because applying </span><span class="cmmi-10x-x-109">AB </span><span class="cmss-10x-x-109">is the same as applying </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">first, then applying </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">on the result.)</span></p>
<p><span class="cmss-10x-x-109">Thus, by our understanding of the determinant, as the scaling factor of </span><span class="cmmi-10x-x-109">AB </span><span class="cmss-10x-x-109">is also</span> det<span class="cmmi-10x-x-109">AB</span><span class="cmss-10x-x-109">, (</span><a href="ch010.xhtml#x1-82002r22"><span class="cmss-10x-x-109">4.14</span></a><span class="cmss-10x-x-109">) holds.</span></p>
<p><span class="cmss-10x-x-109">We can do the actual proof of this, for example, by induction based on the recursive formula (</span><a href="ch010.xhtml#x1-81009r21"><span class="cmss-10x-x-109">4.13</span></a><span class="cmss-10x-x-109">), leading to a long and involved calculation.</span></p>
<p><span class="cmss-10x-x-109">An immediate corollary of the product rule is a special relation between the determinants of a matrix and its inverse.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82003r23"></span> <span class="cmbx-10x-x-109">Theorem 23.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary invertible matrix. Then</span></p>
<div class="math-display">
<img src="../media/file446.png" class="math-display" alt="detA −1 = (det A)−1. "/>
</div>
</div>
<div id="tcolobox-145" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Using the product rule, we have</span></p>
<div class="math-display">
<img src="../media/file447.png" class="math-display" alt="1 = detI = detAA −1 = (det A)(detA −1), "/>
</div>
<p><span class="cmss-10x-x-109">from which the theorem follows.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Because of this, we</span> <span id="dx1-82004"></span><span class="cmss-10x-x-109">can also conclude that the determinant is preserved by the similarity relation.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82005r24"></span> <span class="cmbx-10x-x-109">Theorem 24.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be two similar matrices with </span><span class="cmmi-10x-x-109">B </span>= <span class="cmmi-10x-x-109">T</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">AT </span><span class="cmti-10x-x-109">for some </span><span class="cmmi-10x-x-109">T </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file448.png" class="math-display" alt="detA = detB. "/>
</div>
</div>
<div id="tcolobox-146" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This simply follows from</span></p>
<div class="math-display">
<img src="../media/file449.png" class="math-display" alt=" −1 detB = det T AT = det T−1 detA detT = det A, "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Another important consequence is that the determinant is independent of the basis the matrix is in. If </span><span class="cmmi-10x-x-109">A </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">is a </span><span class="cmssi-10x-x-109">linear transformation</span><span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">P </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">p</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">R </span>= <span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">r</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">r</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">are two bases of </span><span class="cmmi-10x-x-109">U</span><span class="cmss-10x-x-109">, then we know that the matrices of the transformation are related (</span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#the-transformation-matrix"><span class="cmssi-10x-x-109">4.2.1</span></a><span class="cmss-10x-x-109">) by</span></p>
<div class="math-display">
<img src="../media/file450.png" class="math-display" alt="AP = T−1ART, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">S</span></sub> <span class="cmss-10x-x-109">is the matrix of the transformation </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">in a basis </span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">T </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the change of basis matrix (</span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#the-transformation-matrix"><span class="cmssi-10x-x-109">4.2.1</span></a><span class="cmss-10x-x-109">). The previous theorem implies that</span> det<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">P</span></sub> = det<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">R</span></sub><span class="cmss-10x-x-109">. Thus, the determinant is properly defined for </span><span class="cmssi-10x-x-109">linear transformations</span><span class="cmss-10x-x-109">, not just matrices!</span></p>
<p><span class="cmss-10x-x-109">There is an essential duality relation regarding determinants: you can swap the rows and columns of a matrix, keeping all determinant-related identities true.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82006r25"></span> <span class="cmbx-10x-x-109">Theorem 25.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary matrix. Then</span></p>
<div class="math-display">
<img src="../media/file451.png" class="math-display" alt="detA = detAT . "/>
</div>
</div>
<div id="tcolobox-147" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Suppose that </span><span class="cmmi-10x-x-109">A </span>= (<span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub>)<sub><span class="cmmi-8">i,j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. Let’s denote the elements of its transpose by </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub><sup><span class="cmmi-8">t</span></sup> = <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">j,i</span></sub><span class="cmss-10x-x-109">. According to (</span><a href="ch010.xhtml#x1-81006r20"><span class="cmss-10x-x-109">4.12</span></a><span class="cmss-10x-x-109">), we have</span></p>
<div class="math-display">
<img src="../media/file452.png" class="math-display" alt=" T ∑ ∏n t detA = sign (σ ) aσ(i),i σ∈Sn i=1 ∑ ∏n = sign (σ ) ai,σ(i). σ∈Sn i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Now comes the trick. Since the product </span><span class="cmex-10x-x-109">∏</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,σ</span><span class="cmr-8">(</span><span class="cmmi-8">i</span><span class="cmr-8">)</span></sub> <span class="cmss-10x-x-109">iterates through all </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-s, and the order of the terms doesn’t matter, we might as well order the terms as </span><span class="cmmi-10x-x-109">i </span>= <span class="cmmi-10x-x-109">σ</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(1)<span class="cmmi-10x-x-109">,…,σ</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">n</span>)<span class="cmss-10x-x-109">. Since</span></p>
<div class="math-display">
<img src="../media/file453.png" class="math-display" alt="sign(σ −1) = sign(σ), "/>
</div>
<p><span class="cmss-10x-x-109">by continuing the above calculation, we have</span></p>
<div class="math-display">
<img src="../media/file454.png" class="math-display" alt=" n n ∑ sign(σ)∏ a = ∑ sign(σ−1)∏ a . i,σ(i) σ− 1(j),j σ∈Sn i=1 σ∈Sn j=1 "/>
</div>
<p><span class="cmss-10x-x-109">Because every</span> <span id="dx1-82007"></span><span class="cmss-10x-x-109">permutation is invertible and </span><span class="cmmi-10x-x-109">σ</span>→<span class="cmmi-10x-x-109">σ</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is a bijection, summing over </span><span class="cmmi-10x-x-109">σ </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is the same as summing over </span><span class="cmmi-10x-x-109">σ</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Combining all of the above, we obtain that</span></p>
<div class="math-display">
<img src="../media/file456.png" class="math-display" alt=" ∑ ∏n detA = sign(σ−1) aj,σ−1(j) σ∈Sn j=1 n = ∑ sign(σ)∏ a σ(j),j σ∈Sn j=1 = detAT . "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82008r26"></span> <span class="cmbx-10x-x-109">Theorem 26.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary matrix and let </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">i,j</span></sup> <span class="cmti-10x-x-109">denote the matrix which can be obtained by swapping the </span><span class="cmmi-10x-x-109">i</span><span class="cmti-10x-x-109">-th and </span><span class="cmmi-10x-x-109">j</span><span class="cmti-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">A</span><span class="cmti-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file457.png" class="math-display" alt="detAi,j = − detA, "/>
</div>
<p><span class="cmti-10x-x-109">or in other words, swapping any two columns of </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">will change the sign of the determinant. Similarly, swapping two rows also changes the sign of the determinant.</span></p>
</div>
<div id="tcolobox-148" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This follows</span> <span id="dx1-82009"></span><span class="cmss-10x-x-109">from a clever application of (</span><a href="ch010.xhtml#x1-82002r22"><span class="cmss-10x-x-109">4.14</span></a><span class="cmss-10x-x-109">), noticing that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">i,j</span></sup> = <span class="cmmi-10x-x-109">AI</span><sup><span class="cmmi-8">i,j</span></sup><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">I</span><sup><span class="cmmi-8">i,j</span></sup> <span class="cmss-10x-x-109">is obtained from the identity matrix by swapping its </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column.</span> det<span class="cmmi-10x-x-109">I</span><sup><span class="cmmi-8">i,j</span></sup> <span class="cmss-10x-x-109">is a determinant of the form</span> det(<span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(1)</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">σ</span><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sub>)<span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">is a permutation simply swapping </span><span class="cmmi-10x-x-109">i </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">. (That is, </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">is a transposition.) Thus,</span></p>
<div class="math-display">
<img src="../media/file458.png" class="math-display" alt=" i,j i,j detA = detA det I = − detA, "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
<p><span class="cmss-10x-x-109">Regarding swapping rows, we can apply the previous result because transposing a matrix preserves the determinant.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">As a consequence, matrices with two matching rows have a zero determinant.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82010r27"></span> <span class="cmbx-10x-x-109">Theorem 27.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file459.png" class="math" alt="A ∈ ℝn ×n "/> <span class="cmti-10x-x-109">be a matrix that has two identical rows or columns. Then</span> <img src="../media/file460.png" class="math" alt="detA = 0 "/><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-149" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Suppose that the </span><img src="../media/file461.png" class="math" alt="i "/><span class="cmss-10x-x-109">-th and the </span><img src="../media/file462.png" class="math" alt="j "/><span class="cmss-10x-x-109">-th columns are matching. Since the two columns are equal, </span><img src="../media/file463.png" class="math" alt="detAi,j = det A "/><span class="cmss-10x-x-109">. However, applying the previous theorem (which states that swapping two columns changes the sign of the determinant), we obtain</span> <img src="../media/file464.png" class="math" alt=" i,j detA = − detA "/><span class="cmss-10x-x-109">. This can only be true if </span><img src="../media/file465.png" class="math" alt="detA = 0 "/><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Again, transposing the matrix gives the statement for rows.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">As yet another consequence, we obtain an essential connection between linearly dependent vector systems and determinants.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82011r28"></span> <span class="cmbx-10x-x-109">Theorem 28.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file466.png" class="math" alt="A ∈ ℝn×n "/> <span class="cmti-10x-x-109">be a matrix. Then its columns are linearly dependent if and only if </span><img src="../media/file467.png" class="math" alt="detA = 0 "/><span class="cmti-10x-x-109">. Similarly, the rows of </span><img src="../media/file468.png" class="math" alt="A "/> <span class="cmti-10x-x-109">are linearly dependent if and only if </span><img src="../media/file469.png" class="math" alt="detA = 0 "/><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-150" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">(i) First, we are going to show that linearly dependent columns (or rows) imply</span> det<span class="cmmi-10x-x-109">A </span>= 0<span class="cmss-10x-x-109">. As usual, let’s denote the columns of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">as </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">and for the sake of simplicity, assume that</span></p>
<div class="math-display">
<img src="../media/file470.png" class="math-display" alt=" n ∑ a1 = αiai. i=2 "/>
</div>
<p><span class="cmss-10x-x-109">Since the determinant is a linear function of the columns, we have</span></p>
<div class="math-display">
<img src="../media/file471.png" class="math-display" alt=" n ∑ det(a1,a2,...,an) = αidet(ai,a2,...,an ). i=2 "/>
</div>
<p><span class="cmss-10x-x-109">Because of the previous theorem, all terms</span> det(<span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">are zero, implying</span> det<span class="cmmi-10x-x-109">A </span>= 0<span class="cmss-10x-x-109">, which is what we had to show. If the rows are linearly dependent, we apply the above to obtain that</span> det<span class="cmmi-10x-x-109">A </span>= det<span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> = 0<span class="cmss-10x-x-109">.</span></p>
<ol>
<li><span id="x1-82013x2"><span class="cmss-10x-x-109">Now, let’s show that</span> det<span class="cmmi-10x-x-109">A </span>= 0 <span class="cmss-10x-x-109">means linearly dependent columns. Instead of the exact proof, which is rather involved, we should have an intuitive explanation instead.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Recall that the determinant is orientation times volume of the parallelepiped given by the columns. Since the orientation is </span><span class="cmsy-10x-x-109">±</span>1<span class="cmss-10x-x-109">,</span> det<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">implies that the volume of the parallelepiped is </span>0<span class="cmss-10x-x-109">. This can only happen if the </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">columns lie in an </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1<span class="cmss-10x-x-109">-dimensional subspace, meaning that they are linearly dependent.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">We can</span> <span id="dx1-82014"></span><span class="cmss-10x-x-109">immediately apply this to get the following result.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82015r2"></span> <span class="cmbx-10x-x-109">Corollary 2.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a matrix with a constant zero column (or row). Then</span> det<span class="cmmi-10x-x-109">A </span>= 0<span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">As the determinant is the signed volume of the basis vectors’ image, it can be zero in certain cases. These transformations are rather special. When can it happen? Let’s go back to the Euclidean plane to build some intuition.</span></p>
<p><span class="cmss-10x-x-109">There, we have</span></p>
<div class="math-display">
<img src="../media/file472.png" class="math-display" alt="|| || ||x1 y1|| ||x y || = x1y2 − x2y1 = 0, 2 2 "/>
</div>
<p><span class="cmss-10x-x-109">or in other words,</span> <img src="../media/file473.png" width="15" data-align="middle" alt="x1 y1-"/> = <img src="../media/file474.png" width="15" data-align="middle" alt="x2 y2-"/><span class="cmss-10x-x-109">. There is one more interpretation of this: the vector</span> (<span class="cmmi-10x-x-109">y</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,y</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">is a scalar multiple of </span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>)<span class="cmss-10x-x-109">; that is, they are </span><span class="cmssi-10x-x-109">colinear</span><span class="cmss-10x-x-109">, meaning that they lie on the same line through the origin. Thinking in terms of linear transformations, this means that the images of </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">lie on a subspace of </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">. As we shall see next, this is closely connected with the invertibility of the transformation.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-82016r29"></span> <span class="cmbx-10x-x-109">Theorem 29.</span> </span> <span class="cmbxti-10x-x-109">(Invertibility and the determinants) </span><span class="cmti-10x-x-109">The linear transformation </span><img src="../media/file475.png" class="math" alt="A ∈ ℝn×n "/> <span class="cmti-10x-x-109">is invertible if and only if</span> <img src="../media/file476.png" class="math" alt="detA ⁄= 0 "/><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-151" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">When we</span> <span id="dx1-82017"></span><span class="cmss-10x-x-109">introduced the concept of </span><span class="cmssi-10x-x-109">invertibility </span><span class="cmss-10x-x-109">(</span><span class="cmssi-10x-x-109">Definition </span><a href="ch010.xhtml#x1-69002r17"><span class="cmssi-10x-x-109">17</span></a><span class="cmss-10x-x-109">), we saw that </span><img src="../media/file477.png" class="math" alt="A "/> <span class="cmss-10x-x-109">is invertible if and only if its columns </span><img src="../media/file478.png" class="math" alt="a1,...,an "/> <span class="cmss-10x-x-109">form a basis. Thus, they are linearly independent.</span></p>
<p><span class="cmss-10x-x-109">Since linear independence (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch007.xhtml#x1-23002r3"><span class="cmssi-10x-x-109">3</span></a><span class="cmss-10x-x-109">) of columns is equivalent to a nonzero determinant, the result follows.</span></p>
</div>
</div>
</section>
</section>
<section id="summary3" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_67"><span class="titlemark"><span class="cmss-10x-x-109">4.5 </span></span> <span id="x1-830005.5"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">So, do your eyes hurt after finally using them for the first time? Mine sure did when I first learned about matrices as </span><span class="cmssi-10x-x-109">linear transformations</span><span class="cmss-10x-x-109">. Here’s a part where the abstract viewpoint pays off for the first time, and trust me, it’ll pay even more dividends later.</span></p>
<p><span class="cmss-10x-x-109">Let’s recap this chapter quickly.</span></p>
<p><span class="cmss-10x-x-109">We’ve learned that besides a table of numbers, a matrix can represent linear transformations:</span></p>
<div class="math-display">
<img src="../media/file479.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ ⌊ ∑m ⌋ a1,1 a1,2 ... a1,m x1 j=1a1,jxj || a a ... a || || x || || ∑m a x || Ax = || 2,.1 2.,2 2,m. || || .2|| = || j=1.2,j j|| , |⌈ .. .. ... .. |⌉ |⌈ .. |⌉ |⌈ .. |⌉ ∑m an,1 an,2 ... an,m, xm j=1an,jxj "/>
</div>
<p><span class="cmss-10x-x-109">where the columns of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">describe the images of the basis vectors under the linear transformation </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Why on Earth is this useful for us? Think of mathematics as a problem-solving tool. The crux of problem-solving is often finding the proper representation of our objects of interest. Looking at matrices as a way to transform data gives us the much-needed geometric perspective, opening up a whole new avenue of methods.</span></p>
<p><span class="cmss-10x-x-109">When looking at matrices this way, we quickly understand why matrix multiplication is defined as it is. The definition</span></p>
<div class="math-display">
<img src="../media/file480.png" class="math-display" alt=" ∑l n,m AB = ( ai,kbk,j)i,j=1 k=1 "/>
</div>
<p><span class="cmss-10x-x-109">is daunting at first, but from the perspective of linear transformations, it’s all revealed to be a simple composition: first, we apply the transformation </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Be careful, though: linear transformations and matrices are not exactly the same, as the matrix representation depends on the underlying basis of our vector space. (See, I told you that bases are going to be useful.)</span></p>
<p><span class="cmss-10x-x-109">Matrices also possess an important quantity called </span><span class="cmssi-10x-x-109">determinants</span><span class="cmss-10x-x-109">, originally defined by the mind-boggingly complex formula</span></p>
<img src="../media/file481.png" class="math-display" alt=" ∑ ∏n det(a1,...,an) = [ aσ(i),i]det(eσ(1),...,eσ(n)), σ∈Sn i=1 " width="500"/>

<p><span class="cmss-10x-x-109">but an investigation exploiting our newfound geometric perspective reveals that the determinant simply describes how much the linear transformation distorts the volume of the domain space, and how it changes the orientation of the basis vectors.</span></p>
<p><span class="cmss-10x-x-109">For us machine learning practitioners, making the conceptual jump from matrices to linear transformations is the more interesting one. (As opposed to the theory, where we often learn about linear transformations first, matrices second.) For instance, this allows us to see a layer in a neural network as stretching, rotating, shearing, and potentially reflecting the feature space.</span></p>
<p><span class="cmss-10x-x-109">In the next chapter, we’ll revisit matrices from a slightly different perspective: systems of equations. Of course, everything is connected, and we’ll end up where we started, looking at what we know from a higher perspective. This is because learning is a spiral, and we are ascending fast.</span></p>
</section>
<section id="problems3" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_68"><span class="titlemark"><span class="cmss-10x-x-109">4.6 </span></span> <span id="x1-840005.6"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Show that if </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is an invertible matrix, then</span></p>
<div class="math-display">
<img src="../media/file482.png" class="math-display" alt=" −1 T T −1 (A ) = (A ) . "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">R</span><sub><span class="cmmi-8">α</span></sub> <span class="cmss-10x-x-109">be the two-dimensional rotation matrix defined by</span></p>
<div class="math-display">
<img src="../media/file483.png" class="math-display" alt=" ⌊ ⌋ ⌈cosα − sinα ⌉ R α = sinα cosα . "/>
</div>
<p><span class="cmss-10x-x-109">Show that </span><span class="cmmi-10x-x-109">R</span><sub><span class="cmmi-8">α</span></sub><span class="cmmi-10x-x-109">R</span><sub><span class="cmmi-8">β</span></sub> = <span class="cmmi-10x-x-109">R</span><sub><span class="cmmi-8">α</span><span class="cmr-8">+</span><span class="cmmi-8">β</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span>= (<span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub>)<sub><span class="cmmi-8">i,j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a matrix and let </span><span class="cmmi-10x-x-109">D </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a </span><span class="cmssi-10x-x-109">diagonal </span><span class="cmss-10x-x-109">matrix defined by</span></p>
<div class="math-display">
<img src="../media/file484.png" class="math-display" alt=" ⌊ ⌋ | d1 0 ... 0 | D = |⌈ 0 d2 ... 0 |⌉ , 0 0 ... dn "/>
</div>
<p><span class="cmss-10x-x-109">where all of its elements are zero outside the diagonal. Show that</span></p>
<div class="math-display">
<img src="../media/file485.png" class="math-display" alt=" ⌊ ⌋ d1a1,1 d2a1,2 ... dna1,n || || DA = || d1a2,1 d2a2,2 ... dna2,n || |⌈ ... ... ... ... |⌉ d1an,1 d2an,2 ... dnan,n "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file486.png" class="math-display" alt=" ⌊ ⌋ |d1a1,1 d1a1,2 ... d1a1,n| |d2a2,1 d2a2,2 ... d2a2,n| AD = || . . . . || . |⌈ .. .. .. .. |⌉ dnan,1 dnan,2 ... dnan,n "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span><span class="cmsy-10x-x-109">∥⋅∥ </span><span class="cmss-10x-x-109">be a norm on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, and let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be an arbitrary matrix.</span></p>
<p><span class="cmss-10x-x-109">Show that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is invertible if and only if the function</span></p>
<div class="math-display">
<img src="../media/file487.png" class="math-display" alt="∥x∥∗ := ∥Ax ∥ "/>
</div>
<p><span class="cmss-10x-x-109">is a norm on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 5. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">be a normed space and </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">be a linear transformation.</span></p>
<p><span class="cmss-10x-x-109">If</span></p>
<div class="math-display">
<img src="../media/file488.png" class="math-display" alt="∥x ∥∗ := ∥f(x)∥ "/>
</div>
<p><span class="cmss-10x-x-109">is a norm, is </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">necessarily invertible?</span></p>
<p><span class="cmssi-10x-x-109">Hint: </span><span class="cmss-10x-x-109">Consider the vector space </span><span class="msbm-10x-x-109">ℝ</span>[<span class="cmmi-10x-x-109">x</span>] <span class="cmss-10x-x-109">with the norm</span></p>
<div class="math-display">
<img src="../media/file489.png" class="math-display" alt=" n n ∑ 2 1∕2 ∑ i ∥p∥ = ( pi) , p(x) = pix i=0 i=0 "/>
</div>
<p><span class="cmss-10x-x-109">and the linear transformation </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">p</span>(<span class="cmmi-10x-x-109">x</span>)→<span class="cmmi-10x-x-109">xp</span>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 6. </span><span class="cmss-10x-x-109">Let </span><span class="cmsy-10x-x-109">⟨⋅</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">⋅</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">be an inner product on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. Show that there is a matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">such that</span></p>
<div class="math-display">
<img src="../media/file491.png" class="math-display" alt="⟨x,y ⟩ = xT Ay, x,y ∈ ℝn. "/>
</div>
<p><span class="cmss-10x-x-109">(Recall that we treat vectors </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">as column vectors.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 7. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a matrix. </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is called </span><span class="cmssi-10x-x-109">positive definite </span><span class="cmss-10x-x-109">if </span><span class="cmbx-10x-x-109">x</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">/span&gt;0 <span class="cmss-10x-x-109">for every nonzero </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span> </span></p>
<p><span class="cmss-10x-x-109">Show that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is positive definite if and only if</span></p>
<div class="math-display">
<img src="../media/file492.png" class="math-display" alt="⟨x,y⟩ := xTAy "/>
</div>
<p><span class="cmss-10x-x-109">is an inner product.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 8. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">be a matrix, and denote its columns by </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Show that for all </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup><span class="cmss-10x-x-109">, we have </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span> span(<span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">B </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">k</span></sup><span class="cmss-10x-x-109">, and denote the columns of </span><span class="cmmi-10x-x-109">AB </span><span class="cmss-10x-x-109">by </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. Show that</span></p>
<div class="math-display">
<img src="../media/file493.png" class="math-display" alt="v1,...,vk ∈ span(a1,...,an). "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 9. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a matrix. Show that</span></p>
<div class="math-display">
<img src="../media/file494.png" class="math-display" alt="⟨Ax, y⟩ = ⟨x,AT y⟩ "/>
</div>
<p><span class="cmss-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, where </span><span class="cmsy-10x-x-109">⟨⋅</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">⋅⟩ </span><span class="cmss-10x-x-109">is the Euclidean inner product.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 10. </span><span class="cmss-10x-x-109">Calculate the determinant of</span></p>
<div class="math-display">
<img src="../media/file495.png" class="math-display" alt=" ⌊1 2 3⌋ | | A = |⌈4 5 6|⌉ . 7 8 9 "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 11. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a matrix and let </span><span class="cmmi-10x-x-109">c </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">be a constant.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Show that</span></p>
<div class="math-display">
<img src="../media/file496.png" class="math-display" alt="| | ||a1,1 ... ca1,i ... a1,n || ||a ... ca ... a || || 2.,1 .2,i 2.,n ||= cdetA || .. ... .. ... .. || || || an,1 ... can,i ... an,n "/>
</div>
<p><span class="cmss-10x-x-109">holds for all </span><span class="cmmi-10x-x-109">i </span>= 1<span class="cmmi-10x-x-109">,…,n</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Show that</span></p>
<div class="math-display">
<img src="../media/file497.png" class="math-display" alt="| | ||a1,1 a1,2 ... a1,n || || . . . . || | .. .. .. .. | ||ca ca ... ca ||= cdet A || .i,1 i.,2 .i,n|| || .. .. ... .. || || || an,1 an,2 ... an,n "/>
</div>
<p><span class="cmss-10x-x-109">holds for all </span><span class="cmmi-10x-x-109">i </span>= 1<span class="cmmi-10x-x-109">,…,n </span><span class="cmss-10x-x-109">and</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">Show that</span></p>
<img src="../media/file498.png" class="math-display" width="150" alt="det(cA ) = cndet A. "/>
<p><span class="cmssbx-10x-x-109">Problem 12. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be an upper triangular matrix. (That is, all elements below the diagonal are zero.) Show that</span></p>

<img src="../media/file499.png" class="math-display" width="150" alt=" ∏n detA = ai,i. i=1 "/>

<p><span class="cmss-10x-x-109">Show that the same holds for lower triangular matrices. (That is, matrices where elements above the diagonal are zero.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 13. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">M </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">be a matrix with the block structure</span></p>
<img src="../media/file500.png" class="math-display" width="150" alt=" ⌊ ⌋ ⌈A B ⌉ M = 0 C , "/>

<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">k</span><span class="cmsy-8">×</span><span class="cmmi-8">k</span></sup><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">B </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">k</span><span class="cmsy-8">×</span><span class="cmmi-8">l</span></sup><span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">C </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">l</span><span class="cmsy-8">×</span><span class="cmmi-8">l</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Show that</span></p>
<img src="../media/file501.png" class="math-display" width="150" alt="detM = detA detC. "/>
</section>
<section id="join-our-community-on-discord4" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_69"><span id="x1-85000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>