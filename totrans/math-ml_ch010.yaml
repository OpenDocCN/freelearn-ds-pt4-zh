- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Transformations
  prefs: []
  type: TYPE_NORMAL
- en: ”Why do my eyes hurt?” ”You’ve never used them before.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Morpheus to Neo, when waking up from the Matrix for the first time
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In most linear algebra courses, the curriculum is all about matrices. In machine
    learning, we work with them all the time. Here is the thing: matrices don’t tell
    the whole story. It is hard to understand the patterns by looking only at matrices.
    For instance, why is matrix multiplication defined in such a complex way as it
    is? Why are relations like B = T^(−1)AT important? Why are some matrices invertible
    and some are not?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To really understand what is going on, we have to look at what gives rise to
    matrices: linear transformations. Like for Neo, this might hurt a bit, but it
    will greatly reward us later down the line. Let’s get to it!'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 What is a linear transformation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the introduction of inner products, orthogonality, and orthogonal/orthonormal
    bases, we know everything about the structure of our feature spaces. However,
    in machine learning, our interest mainly lies in transforming the data.
  prefs: []
  type: TYPE_NORMAL
- en: From this viewpoint, a neural network is just a function composed of smaller
    parts (known as layers), transforming the data to a new feature space in every
    step. One of the key components of models in machine learning are linear transformations.
  prefs: []
  type: TYPE_NORMAL
- en: You probably encountered them as functions of the form f(x) = Ax, but this is
    only one way to look at them. This section will start from a geometric viewpoint,
    then move towards the algebraic representation that you are probably already familiar
    with. To understand how neural networks can learn powerful high-level representations
    of the data, looking at the geometry of transforms is essential.
  prefs: []
  type: TYPE_NORMAL
- en: So, what linear transformations are? Let’s not hesitate a moment further, and
    jump into the definition right away!
  prefs: []
  type: TYPE_NORMAL
- en: Definition 16\. (Linear transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let U and V be two vector spaces (over the same scalar field), and let f :
    U →V be a function between them. We say that f is linear if'
  prefs: []
  type: TYPE_NORMAL
- en: f( ax + by) = af(x) + bf(y)
  prefs: []
  type: TYPE_NORMAL
- en: (4.1)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all vectors x,y ∈U and all scalars a,b.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why linear algebra is called linear algebra. In essence, a linear transformation
    is a mapping between two vector spaces that preserves the algebraic structure:
    addition and scalar multiplication. (Functions between vector spaces are often
    called transformations, so we will use this terminology.)'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 5\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearity is essentially combining two properties in one: f(x + y) = f(x) +
    f(y) and f(ax) = af(x) for all vectors x,y and all scalars a. From these two,
    ([4.1](ch010.xhtml#x1-66002r16)) follows by'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(ax + by) = f(ax) + f(by) = af(x)+ bf (y ). ](img/file308.png)'
  prefs: []
  type: TYPE_IMG
- en: Two properties immediately jump out from the definition. First, since
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = f(x + 0) = f(x) + f(0), ](img/file309.png)'
  prefs: []
  type: TYPE_IMG
- en: f(0) = 0 holds for every linear transformation. In addition, the composition
    of linear transformations is still linear, as
  prefs: []
  type: TYPE_NORMAL
- en: '![f (g(ax + by)) = f(ag(x)+ bg(y)) = af(g(x))+ bf(g(y)) ](img/file310.png)'
  prefs: []
  type: TYPE_IMG
- en: shows for any linear f and g and scalars a and b.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, let’s see some examples to build intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. For any scalar c, the scaling transformation f(x) = cx is linear.
  prefs: []
  type: TYPE_NORMAL
- en: This is probably the simplest example out there, and it can be defined in all
    vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file311.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Scaling as a linear transformation'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s easy to see that scaling is linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![c(ax + by) = c(ax)+ c(by) = a(cx)+ b(cy). ](img/file312.png)'
  prefs: []
  type: TYPE_IMG
- en: Example 2\. In ℝ², rotations around the origin by an angle α are also linear.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file313.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Rotation in the Euclidean plane as a linear transformation'
  prefs: []
  type: TYPE_NORMAL
- en: 'To show that rotations are indeed linear, I pull the definition out from the
    hat: the rotation of a planar vector x = (x[1],x[2]) with the angle α is described
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = (x1cosα − x2 sinα, x1sinα + x2cos α), ](img/file314.png)'
  prefs: []
  type: TYPE_IMG
- en: from which ([16](ch010.xhtml#x1-66002r16)) is easily confirmed. I know that
    this looks like magic, but trust me, the rotation formula will be explained in
    detail. You can sweat it out with some basic trigonometry, or wait until we do
    this later with matrices.
  prefs: []
  type: TYPE_NORMAL
- en: In general, linear transformations have a strong connection with the geometry
    of the space. Later, we are going to study the linear transformations of ℝ² in
    detail, with an emphasis on geometric ones such as this. (Note that rotations
    are slightly more complicated in higher dimensions, as they will require an axis
    to rotate around.)
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. In any vector space V and a nonzero vector v ∈V , the translation
    defined by f(x) = x + v is not linear, as f(0) = v≠0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll see more examples later in the section. For now, let’s move on to some
    general properties of linear transformations. For any linear transformation f
    : U →V , the image'
  prefs: []
  type: TYPE_NORMAL
- en: '![im(f) = {v ∈ V : v = f (u) for some u ∈ U } ](img/file315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is always a subspace Section [1.2.7](ch007.xhtml#subspaces) of V. This is easy
    to check: if ![v1, v2 ∈ im f ](img/file316.png), then there exist ![u1,u2 ∈ U
    ](img/file317.png) such that f(u[1]) = v[1] and f(u[2]) = v[2], as'
  prefs: []
  type: TYPE_NORMAL
- en: '![av1 + bv2 = af(u1)+ bf (u2 ) = f (au1 + bu2) ∈ im f. ](img/file318.png)'
  prefs: []
  type: TYPE_IMG
- en: To add one more level of abstraction, we will see that the set of all linear
    transformations form a vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 18\.
  prefs: []
  type: TYPE_NORMAL
- en: Let U and V be two vector spaces over the same field F. Then the set of all
    linear transformations
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/file319.png)'
  prefs: []
  type: TYPE_IMG
- en: (4.2)
  prefs: []
  type: TYPE_NORMAL
- en: is also a vector space over F, with the usual definitions for function addition
    and scalar multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: The proof of this is just a boring checklist, going through the items of the
    definition of vector spaces (Definition [2](ch007.xhtml#x1-20004r2)). I recommend
    you walk through it at least once to solidify your understanding of vector spaces,
    but there is really nothing special there.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Linear transformations and matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The definition of linear transformations, as we saw, is a bit abstract. However,
    there is a simple and expressive way to characterize them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this, let f : U →V be a linear transformation between two vector spaces
    U and V . Suppose that {u[1],…,u[m]} is a basis in U, while {v[1],…,v[n]} is a
    basis in V . Since every x ∈U can be written in the form'
  prefs: []
  type: TYPE_NORMAL
- en: '![ m ∑ x = xiui, i=1 ](img/file320.png)'
  prefs: []
  type: TYPE_IMG
- en: the linearity of f implies
  prefs: []
  type: TYPE_NORMAL
- en: f( ∑ [j=1]^m x[j] u[j] ) = ∑ [j=1]^m x[j] f(u[j]),
  prefs: []
  type: TYPE_NORMAL
- en: (4.3)
  prefs: []
  type: TYPE_NORMAL
- en: meaning that f(x) is a linear combination of f(u[1]),…,f(u[m]). In other words,
    every linear transformation is completely determined by the images of basis vectors.
    To expand this idea, suppose that for every u[j], we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n f(uj) = ai,jvi i=1 ](img/file323.png)'
  prefs: []
  type: TYPE_IMG
- en: for some scalars a[i,j].
  prefs: []
  type: TYPE_NORMAL
- en: 'These n ×m numbers completely describe f. For notational simplicity, we store
    these in a n ×m-sized table called a matrix, which we’ll denote by A[f]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ a1,1 a1,2 ... a1,m | | ||a2,1 a2,2 ... a2,m|| f ↔ Af = || .. .. ...
    .. || , ⌈ . . . ⌉ an,1 an,2 ... an,m ](img/file324.png)'
  prefs: []
  type: TYPE_IMG
- en: meaning that linear transformations are represented by matrices. This connection
    is heavily utilized throughout machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding ([4.3](ch010.xhtml#linear-transformations-and-matrices)) further,
    for every x = ∑ [j=1]^mx[j]u[j] we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑m f(x) = xjf(uj) j=1 ∑m ∑n = xj ai,jvi j=1 i=1 n ( m ) = ∑ ( ∑ a x ) v
    . i,j j i i=1 j=1 ](img/file325.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the image of x can be expressed as A[f]x:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊∑m ⌋ |a1,1 a1,2 ... a1,m | |x1 | | j=1 a1,jxj| |a2,1 a2,2 ... a2,m
    | |x2 | |∑m a2,jxj| f(x) = || . . . . || || . || = || j=1\. || . |⌈ .. .. .. ..
    |⌉ |⌈ .. |⌉ |⌈ .. |⌉ a a ... a , x ∑m a x n,1 n,2 n,m m j=1 n,j j ](img/file326.png)'
  prefs: []
  type: TYPE_IMG
- en: Two things to note here. First, we implicitly chose to represent vectors as
    columns instead of rows. This is a seriously impactful decision and will affect
    many of the computations later in this book. We’ll keep pointing it out.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the matrix representation depends on the choice of the basis! If, say,
    P = {p[1],…,p[n]}⊂U is the basis of our matrix, we denote this dependence in the
    subscript, writing A[f,P] .
  prefs: []
  type: TYPE_NORMAL
- en: To avoid confusion, we’ll almost exclusively define linear transformations by
    giving their matrices in the standard orthonormal basis. In practical scenarios,
    this makes it much easier to understand what is going on. So, whenever I write
    something like “let A be the matrix of a linear transformation f”, it is implictly
    assumed that A is written in the basis e[1] = (1,0,…,0),e[2] = (0,1,…,0),…,e[n]
    = (0,0,…,1).
  prefs: []
  type: TYPE_NORMAL
- en: 'On a philosophical note, have you heard about Plato’s allegory of the cave?
    In this thought experiment, people are assumed to be living in a cave constantly
    facing a single wall, only observing their shadows projected by a fire behind
    them. What they observe and use to build an internal representation of the world
    is very different from reality. Applying this analogy to linear algebra, matrices
    are the shadows that we observe and use in practical scenarios. In many introductory
    courses, linear transformations are hidden, and only matrix calculus is taught.
    My first exposition into the subject was similar: the first linear algebra course
    I took talked exclusively about matrices. It was as complicated and confusing
    as a math course can be. (Which, I can assure you, can be very complicated and
    confusing.) Later in my studies, everything clicked when I discovered that you
    could look at matrices from the perspective of linear transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without seeing what is behind matrices, it is impossible to master linear algebra.
    If my approach feels too abstract for you, keep this in mind: years later, when
    you are a practicing data scientist/machine learning engineer/researcher or whatever,
    going below the surface will pay huge dividends.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get back on track and continue our discussion about linear transformations.
    The most commonly used matrix is the matrix of the identity transformation id
    : x→x. We’ll denote this by I. It is easy to see that'
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(3).png)(4.4)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, for a matrix A, a linear transformation can be given by x→Ax.
    In fact, the mapping
  prefs: []
  type: TYPE_NORMAL
- en: '![f ↦→ Af,P ](img/file330.png)'
  prefs: []
  type: TYPE_IMG
- en: defines a one-to-one correspondence between the space of linear transformations
    L(U,V ) defined by ([4.2](ch010.xhtml#x1-66010r18)) and the set of n ×m matrices,
    where n and m are the corresponding dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Matrix operations revisited
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functions can be added and composed. Because of the connection between linear
    transformations and matrices, matrix operations are inherited from the corresponding
    function operations.
  prefs: []
  type: TYPE_NORMAL
- en: With this principle in mind, we defined matrix addition so that the the matrix
    of the sum of two linear transformations is the sum of the corresponding matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically speaking, if f,g : U →V are two linear transformations with
    matrices, f ↔︎A and g ↔︎B, then'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n (f + g)(u ) = f (u )+ g(u ) = ∑ (a + b )v . j j j i=1 i,j i,j i ](img/file331.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the corresponding matrices can be added together elementwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A + B = (ai,j + bi,j)n,m . i,j=1 ](img/file332.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiplication between matrices is defined by the composition of the corresponding
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how, we study a special case first. (In general, it is a good idea to
    look at special cases first, as they often reduce the complexity and allow you
    to see patterns without information overload.) So, let f,g : U →U be two linear
    transformations, mapping U onto itself. To determine the elements of the matrix
    corresponding to f ∘g, we have to express f(g(u[j])) in terms of all the basis
    vectors u[1],…,u[n]. For this, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n (f g)(u ) = f(g(u )) = f(∑ b u ) j j k,j k n k=1 ∑ = bk,jf(uk) k=n1 n
    ∑ ∑ = bk,j ai,kui k=1 i=1 ∑n ∑n = ( ai,kbk,j)ui. i=1 k=1 ](img/file333.png)'
  prefs: []
  type: TYPE_IMG
- en: By considering how we defined a transformation’s matrix, the scalar (∑ [k=1]^na[i,k]b[k,j])
    is the element in the i-th row and j-th column of the matrix of f ∘g. Thus, matrix
    multiplication can be defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( )n ∑n AB = ai,kbk,j . k=1 i,j=1 ](img/file334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the general case, we can only define the product of matrices if the corresponding
    linear transformations can be composed. That is, if f : U →V , then g must start
    from V . Translating this into the language of the matrices, the number of columns
    in A must match the number of rows in B. So, for any A ∈ℝ^(n×m) and B ∈ℝ^(m×l),
    their product is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑m AB = ( ai,kbk,j)n,l ∈ ℝn×l. k=1 i,j=1 ](img/file335.png)'
  prefs: []
  type: TYPE_IMG
- en: 4.1.3 Inverting linear transformations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regarding linear transformations, the question of invertibility is extremely
    important. For example, have you encountered a system of equations like this?
  prefs: []
  type: TYPE_NORMAL
- en: '![2x1 + x2 = 5 x1 − 3x2 = − 8 ](img/file336.png)'
  prefs: []
  type: TYPE_IMG
- en: If we define
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ 2 1 5 x1 A = ⌈ ⌉ , b = ⌈ ⌉ , x = ⌈ ⌉ , 1 − 3 − 8 x2 ](img/file337.png)'
  prefs: []
  type: TYPE_IMG
- en: the above system can be written in the form Ax = b. These are called linear
    equations, modeling various processes from finance to biology.
  prefs: []
  type: TYPE_NORMAL
- en: How would you write the solution of such an equation? If there would be a matrix
    A^(−1) such that A^(−1)A is the identity matrix I (defined by ([4.4](ch010.xhtml#linear-transformations-and-matrices))),
    then multiplying the equation Ax = b from the left by A^(−1) would yield the solution
    in the form x = A^(−1)b.
  prefs: []
  type: TYPE_NORMAL
- en: The matrix A^(−1) is called the inverse matrix of A. It might not always exist,
    but when it does, it is extremely important for several reasons. We’ll talk about
    linear equations later, but first, let’s study the fundamentals of invertibility!
    Here is the general definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 17\. (Inverse of a linear transformation)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : U →V be a linear transformation between the vector spaces U and V .
    We say that f is invertible if there is a linear transformation f^(−1) such that
    f^(−1) ∘f and f ∘f^(−1) are the identity functions; that is,'
  prefs: []
  type: TYPE_NORMAL
- en: '![f− 1(f (u )) = u, − 1 f(f (v )) = v ](img/file338.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all u ∈U,v ∈V . f^(−1) is called the inverse of f.
  prefs: []
  type: TYPE_NORMAL
- en: Not all linear transformations are invertible. For instance, if f maps all vectors
    to the zero vector, you cannot define an inverse.
  prefs: []
  type: TYPE_NORMAL
- en: There are certain conditions that guarantee the existence of the inverse. One
    of the most important ones connects the concept of basis with invertibility.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 19\. (Invertibility of linear transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : U →V be a linear transformation and let u[1],…,u[n] be a basis in U.
    Then f is invertible if and only if f(u[1]),…,f(u[n]) is a basis in V .'
  prefs: []
  type: TYPE_NORMAL
- en: The following proof is straightforward, but can be a bit overwhelming. Feel
    free to skip this at the first reading, you can always revisit it later.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As usual, the proof of the if and only if type theorems consist of two
    parts, as these statements involve two implications.
  prefs: []
  type: TYPE_NORMAL
- en: (a) First, we prove that f is invertible, then f(u[1]),…,f(u[n]) is a basis.
    That is, we need to show that f(u[1]),…,f(u[n]) is linearly independent and every
    y ∈V can be written as their linear combination.
  prefs: []
  type: TYPE_NORMAL
- en: Since f is invertible, f(0) = 0, moreover there are no nonzero vectors x ∈ U
    such that f(x) = 0\. In other words, 0 cannot be written as the nontrivial linear
    combination of f(u[1]),…,f(u[n]), from which Theorem [3](ch007.xhtml#x1-25003r3)
    implies the linear independence.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, invertibility implies that every ![y ∈ V ](img/file339.png)
    can be obtained as ![y = f(x) ](img/file340.png) for some ![x ∈ U ](img/file341.png).
    (With the choice ![x = f −1(y) ](img/file342.png).) As ![u1,...,un ](img/file343.png)
    is a basis, ![ ∑ x = ni=1xiui ](img/file344.png). Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![y = f(x) ∑n = f( xiui) i=1 ∑n = xif(ui), i=1 ](img/file345.png)'
  prefs: []
  type: TYPE_IMG
- en: showing that span(f(u[1]),…,f(u[n])) = V .
  prefs: []
  type: TYPE_NORMAL
- en: The linear independence f(u[1]),…,f(u[n]) and the fact that it spans V gives
    that it is indeed a basis.
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Now we prove the other implication: if f(u[1]),…,f(u[n]) is a basis, then
    f is invertible.'
  prefs: []
  type: TYPE_NORMAL
- en: If f(u[1]),…,f(u[n]) is indeed a basis, then every y ∈V can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ n n ∑ ∑ y = yif (ui) = f( yiui), i=1 i=1 ](img/file346.png)'
  prefs: []
  type: TYPE_IMG
- en: which shows the surjectivity. Regarding the injectivity, if y = f(a) = f(b)
    for some a,b ∈U, then, since both a and b can be written as a linear combination
    of the u[i] basis vectors, we would have
  prefs: []
  type: TYPE_NORMAL
- en: '![ n n ∑ ∑ y = f(a) = f( aiui) = aif (ui) i=1 i=1 ](img/file347.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n y = f(b) = f( biui) = yif(ui). i=1 i=1 ](img/file348.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, 0 = ∑ [i=1]^n(a[i]−b[i])u[i], and since u[1],…,u[n] is a basis in U, a[i]
    = b[i] must hold. Hence f is injective.
  prefs: []
  type: TYPE_NORMAL
- en: 'A consequence of this theorem is that a linear transformation f : U →V is not
    invertible if the dimensions of U and V are different. We can look at invertibility
    from the aspect of matrices as well. For any A ∈ℝ^(n×n), if the corresponding
    linear transformation is invertible, there exists a matrix A^(−1) ∈ℝ^(n×n) such
    that A^(−1)A = AA^(−1) = I. If a matrix is not square, it is not invertible in
    the classical sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 The kernel and the image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regarding the invertibility of a linear transformation, two special sets play
    an essential role: the kernel and the image. Let’s see them!'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 18\. (Kernel and image of linear transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : U →V be a linear transformation. Its image and kernel is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![im f := {f(u) : u ∈ U} ⊆ V ](img/file349.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![kerf := {u ∈ U : f(u ) = 0} ⊆ U. ](img/file350.png)'
  prefs: []
  type: TYPE_IMG
- en: Often, we write imA and kerA for some matrix A, referring to the linear transformation
    defined by x→Ax. Due to the linearity of f, it is easy to see that imf is a subspace
    of V and kerf is a subspace of U. As mentioned, they are closely connected with
    invertibility, as we shall see next.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 20\. (Invertibility in terms of linear transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : U →V be a linear transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) A is injective if and only if kerf = {0}.
  prefs: []
  type: TYPE_NORMAL
- en: (b) A is surjective if and only if imf = V .
  prefs: []
  type: TYPE_NORMAL
- en: (c) A is bijective (that is, invertible) if and only if kerf = {0}and imf =
    V .
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (a) If f is injective, there can only be one vector in U that is mapped
    to 0\. Since f(0) = 0 for any linear transformation, kerf = {0}.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if there are two different vectors x,y ∈ U such that f(x)
    = f(y), then f(x−y) = f(x) −f(y) = 0, so x−y ∈ kerf. Thus, kerf = {0} implies
    x = y, which gives the injectivity.
  prefs: []
  type: TYPE_NORMAL
- en: (b) This is just the definition of surjectivity.
  prefs: []
  type: TYPE_NORMAL
- en: (c) This immediately follows from combining (a) and (b) above.
  prefs: []
  type: TYPE_NORMAL
- en: Because matrices define linear transformations, it makes sense to talk about
    the inverse of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Algebraically speaking, the inverse of an A ∈ℝ^(n×n) is the matrix A^(−1) ∈ℝ^(n×n)
    such that A^(−1)A = AA^(−1) = I holds. The connection between linear transforms
    and matrices imply that A^(−1) is the matrix of f^(−1), so no surprise here.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry if this section about invertibility feels like a bit too much algebra.
    Later, when talking about the determinant of a transformation, we are going to
    study invertibility from a geometric perspective later in this chapter. In terms
    of matrices, later we are going to see a general method to calculate the inverse
    matrix in Section [5.1.6](ch011.xhtml#inverting-matrices). We’ll be there soon,
    but first, we take a look at how the choice of basis determines the matrix representation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Change of basis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously in this section, we have seen that any linear transformation can
    be described with the images of the basis vectors (see Section [4.1.1](ch010.xhtml#linear-transformations-and-matrices)).
    This gave us the matrix representation that we use all the time. However, this
    very much depends on the choice of basis. Different bases yield different matrices
    for the same transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let’s take a look at f : ℝ² →ℝ², which maps e[1] = (1,0) to the
    vector (2,1) and e[2] = (0,1) to (1,2). Its matrix in the standard orthonormal
    basis E = {e[1],e[2]} is given by'
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(4).png)(4.5)'
  prefs: []
  type: TYPE_IMG
- en: '![PIC](img/file353.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: The linear transformation f, defined by ([5.2](ch010.xhtml#change-of-basis))'
  prefs: []
  type: TYPE_NORMAL
- en: The effect of f is visualized in Figure [4.3](#).
  prefs: []
  type: TYPE_NORMAL
- en: What if we select a different basis, say P = {p[1] = (1,1),p[2] = (−1,1)}? With
    a quick calculation, we can check that
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ |2 1| 1 3 |2 1| − 1 − 1 |⌈1 2|⌉ ⌈ ⌉ = ⌈ ⌉ , |⌈1 2|⌉
    ⌈ ⌉ = ⌈ ⌉ . 1 3 1 1 ](img/file354.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, f(p[1]) = 3p[1] + 0p[2] and f(p[2]) = 0p[1] + p[2]. This is
    visualized by Figure [4.4](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file355.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: The effect of f on p[1] = (1,1) and p[2] = (−1,1)'
  prefs: []
  type: TYPE_NORMAL
- en: This means that if P = {p[1],p[2]} is our basis (thus, if writing (a,b) means
    ap[1] + bp[2]), the matrix of f becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ |3 0| Af,P = |⌈0 1|⌉ . ](img/file356.png)'
  prefs: []
  type: TYPE_IMG
- en: In this form, A[f,P] is a diagonal matrix. (That is, its elements below and
    above the diagonal are zero.) As you can see, having the right basis can significantly
    simplify the linear transformation. For instance, in n dimensions, applying a
    transformation in diagonal form requires only n operations, as
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋⌊ ⌋ ⌊ ⌋ d1 0 ... 0 x1 d1x1 || 0 d ... 0 |||| x || || d x || || 2 ||||
    2|| = || 2 2|| |⌈ ... ... ... ... |⌉|⌈ ...|⌉ |⌈ ... |⌉ 0 0 ... dn xn dnxn ](img/file357.png)'
  prefs: []
  type: TYPE_IMG
- en: Otherwise, n² operations are needed. So, we can save a lot there.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 The transformation matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have just seen that the matrix of a linear transformation depends on our
    choice of basis. However, there is a special relation between matrices of the
    same transformation. We’ll explore this next. Let f : U →U be a linear transformation,
    and let P = {p[1],…,p[n]} and Q = {q[1],…,q[n] be two bases. As before, A[f,S]
    denotes the matrix of f in some basis S.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we know A[f,P] , but we have our vectors represented in terms of
    the other basis Q. How do we calculate the images our vectors under the linear
    transformation? A natural idea is to first transform our vector representations
    from Q to P, apply A[f,P] , then transform the representations back. In the following,
    we are going to make this precise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![t : U → U ](img/file358.png) be a transformation defined by ![pi ↦→ qi
    ](img/file359.png) for all ![i ∈ {1,...,n } ](img/file360.png). (In other words,
    ![t ](img/file361.png) maps one set of basis vectors to another.) Since ![P ](img/file362.png)
    and ![Q ](img/file363.png) are bases (so the sets are linearly independent), ![t
    ](img/file364.png) is invertible.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the matrix ![ Q Af,Q = (ai,j)ni,j=1 ](img/file365.png) is known
    to us, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n Q f(qj) = Af,Qqj = ai,jqi i=1 ](img/file366.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all ![j ](img/file367.png). So, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 −1 (t ft)(pj ) = t f(qj) ∑n = t−1( aQi,jqi) i=1 ∑n = aQi,jt−1(qi) i=1
    ∑n = aQi,jpi. i=1 ](img/file368.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the matrix of the composed transformation t^(−1)ft in the basis
    P is the same as the matrix of f in Q. In terms of formulas,
  prefs: []
  type: TYPE_NORMAL
- en: T^(−1) A [f,P] T = A[f,Q],
  prefs: []
  type: TYPE_NORMAL
- en: (4.6)
  prefs: []
  type: TYPE_NORMAL
- en: where T denotes the matrix of t in P. (For notational simplicity, we omit the
    subscript. Most often, we don’t care what base it is in.)
  prefs: []
  type: TYPE_NORMAL
- en: We’ll call T the change of basis matrix. These types of relations are prevalent
    in linear algebra, so we’ll take the time to introduce a definition formally.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 19\. (Similar matrices)
  prefs: []
  type: TYPE_NORMAL
- en: Let A,B ∈ℝ^(n×n) be two arbitrary matrices. A and B are called similar if there
    exists a matrix T ∈ℝ^(n×n) such that
  prefs: []
  type: TYPE_NORMAL
- en: '![ − 1 B = T AT ](img/file369.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. We call mappings of the form A→T^(−1)AT similarity transformations.
  prefs: []
  type: TYPE_NORMAL
- en: In these terms, ([4.6](ch010.xhtml#the-transformation-matrix)) says that the
    matrices of a given linear transformation are all similar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: With this under our belt, we can finish up with the example ([4.5](ch010.xhtml#change-of-basis)).
    In this case, T and T^(−1) can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ | 1 − 1| | 1∕2 1∕2| T = |⌈ 1 1|⌉ , T −1 = |⌈− 1∕2 1∕2|⌉ . ](img/file371.png)'
  prefs: []
  type: TYPE_IMG
- en: (Later, we’ll see a general method to compute the inverse of any matrix, but
    for now, you can verify this by hand.) Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(5).png)(4.7)'
  prefs: []
  type: TYPE_IMG
- en: or equivalently,
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(6).png)(4.8)'
  prefs: []
  type: TYPE_IMG
- en: Figure [4.5](#) shows what ([4.8](#)) looks like in geometric terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file380.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Change of basis, illustrated'
  prefs: []
  type: TYPE_NORMAL
- en: 'From this example, we can see that a properly selected similarity transformation
    can diagonalize certain matrices. Is this a coincidence? Spoiler alert: no. In
    Chapter [7](ch013.xhtml#matrix-factorizations), we will see exactly when and how
    this can be done.'
  prefs: []
  type: TYPE_NORMAL
- en: I know, this is a bit too abstract. As always, examples illustrate a concept
    best, so let’s see some!
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Linear transformations in the Euclidean plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just seen that a linear transformation can be described by the image
    of a basis set. From a geometric viewpoint, they are functions mapping [parallelepipeds](https://en.wikipedia.org/wiki/Parallelepiped)
    to parallelepipeds.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the linearity, you can imagine this as distorting the grid determined
    by the bases.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file381.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: How linear transforms distort the grid determined by the basis
    vectors'
  prefs: []
  type: TYPE_NORMAL
- en: 'In two dimensions, we have seen a few examples of geometric maps such as scaling
    and rotation as linear transformations. Now we can put them into matrix form.
    There are five of them in particular that we will study: stretching, shearing,
    rotation, reflection, and projection.'
  prefs: []
  type: TYPE_NORMAL
- en: These simple transformations are not only essential to build intuition, but
    they are also frequently applied in computer vision. Flipping, rotating, and stretching
    are essential parts of image augmentation pipelines, greatly enhancing the performance
    of models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Stretching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest one is a generalization of scaling. We have seen a variant of this
    in Example 1 above (see Section [4.1](ch010.xhtml#what-is-a-linear-transformation)).
    In matrix form, this is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ c1 0 A = ⌈ ⌉ , c1,c2 ∈ ℝ. 0 c2 ](img/file382.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear transformations such as this can be visualized by plotting the image
    of the unit square determined by the standard basis e[1] = (1,0),e[2] = (0,1).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file383.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Stretching'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Rotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rotations are given by the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ cosα − sinα R α = ⌈ ⌉. sinα cosα ](img/file384.png)'
  prefs: []
  type: TYPE_IMG
- en: To see why, recall that each column of the transformation’s matrix describes
    the image of the basis vectors. The rotation of (1,0) is given by (cosα,sinα),
    while the rotation of (0,1) is (cos(α + π∕2),sin(α + π∕2)). This is illustrated
    by Figure 4.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file385.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: The rotation matrix explained'
  prefs: []
  type: TYPE_NORMAL
- en: Like above, we can visualize the image of the unit square to gain a geometric
    insight into what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Rotation'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Shearing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another essential geometric transform is shearing, which is frequently applied
    in physics. A shearing force ([https://en.wikipedia.org/wiki/Shear_force](https://en.wikipedia.org/wiki/Shear_force))
    is a pair of forces with opposite directions, acting on the same body.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file387.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Shearing'
  prefs: []
  type: TYPE_NORMAL
- en: Its matrix is given in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ Sx = ⌈1 kx⌉ , Sy = ⌈ 1 0⌉, S = ⌈1 kx⌉ , 0 1 ky 1 ky 1 ](img/file388.png)'
  prefs: []
  type: TYPE_IMG
- en: where S[x], S[y], and S represent shearing transformations in the x, y, and
    in both directions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Reflection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until this point, all the transformations we have seen in the Euclidean plane
    preserved the “orientation” of the space. However, this is not always the case.
    The transformation given by the matrices
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ − 1 0 1 0 R1 = ⌈ ⌉ , R2 = ⌈ ⌉ 0 1 0 − 1 ](img/file389.png)'
  prefs: []
  type: TYPE_IMG
- en: act as reflections with respect to the x and the y axes.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file390.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Reflection'
  prefs: []
  type: TYPE_NORMAL
- en: When combined with a rotation, we can use reflections to flip bases. For instance,
    the transformation maps e[1] to e[2] and e[2] to e[1].
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ R = ⌈0 − 1⌉ ⌈1 0⌉ = ⌈0 1⌉ 1 0 0 − 1 1 0 ◟--◝◜--◞ ◟---◝◜--◞ rotation
    with π∕2 =R2 ](img/file391.png)'
  prefs: []
  type: TYPE_IMG
- en: '![PIC](img/file392.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Swapping e[1] and e[2] is a reflection and rotation'
  prefs: []
  type: TYPE_NORMAL
- en: These types of transformations play an essential role in understanding determinants,
    as we will soon see in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In general, reflections can be easily defined in higher dimensional spaces.
    For instance,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 1 0 0 R = ||0 1 0 || ⌈ ⌉ 0 0 − 1 ](img/file393.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is a reflection in ℝ³ that flips e[3] to the opposite direction. It is just
    like looking in the mirror: it turns left to right and right to left.'
  prefs: []
  type: TYPE_NORMAL
- en: Reflections can flip orientations multiple times. The transformation given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 1 0 0 R = || || ⌈0 − 1 0⌉ 0 0 − 1 ](img/file394.png)'
  prefs: []
  type: TYPE_IMG
- en: flips e[2] and e[3], changing the orientation twice. Later, we’ll see that the
    “number of changes in orientation” of a given transformation is one of its essential
    descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Orthogonal projection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most important transformations (not only in two dimensions) is the
    orthogonal projection. We have seen this already when talking about inner products
    and their geometric representation in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products).
    By taking a closer look, it turns out that they are linear transformations.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file395.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Orthogonal projection'
  prefs: []
  type: TYPE_NORMAL
- en: Recall from ([3.2.3](ch008.xhtml#x1-45003r3.2.3)) that the orthogonal projection
    of x to some y can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![proj (x) = ⟨x,y⟩y. y ⟨y,y⟩ ](img/file396.png)'
  prefs: []
  type: TYPE_IMG
- en: The bilinearity of ⟨⋅,⋅⟩ immediately implies that proj[y](x) is also linear.
    With a bit of algebra, we can rewrite this in terms of matrices. We have
  prefs: []
  type: TYPE_NORMAL
- en: '![projy(x) = ⟨x,y-⟩y ⟨y,y ⟩ ⌊ ⌋ = x1y1-+-x2y2⌈y1 ⌉ ∥y ∥2 y2 ⌊ ⌋ ⌊ ⌋ 1 y21 y1y2
    x1 = ----2⌈ 2 ⌉ ⌈ ⌉ ∥y ∥ y1y2 y2 x2 1 = ----2yyT x, ∥y ∥ ](img/file397.png)'
  prefs: []
  type: TYPE_IMG
- en: thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 1 y2 y y projy = ----2⌈ 1 1 2⌉ ∥y ∥ y1y2 y22 1 = ----2yyT x ∥y ∥ ](img/file398.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that
  prefs: []
  type: TYPE_NORMAL
- en: '![ y2 projy(e2) = y1 projy(e1), ](img/file399.png)'
  prefs: []
  type: TYPE_IMG
- en: so the images of the standard basis vectors are not linearly independent. As
    a consequence, the image of the plane under proj[y] is span(y), which is a one-dimensional
    subspace. From this example, we can see that the image of a vector space under
    a linear transformation is not necessarily of the same dimension as the starting
    space.
  prefs: []
  type: TYPE_NORMAL
- en: With these examples and knowledge under our belt, we have a basic understanding
    of linear transformations, the most basic building blocks of neural networks.
    In the next section, we will study how linear transformations affect the geometric
    structure of the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Determinants, or how linear transformations affect volume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Section [4.3](ch010.xhtml#linear-transformations-in-the-euclidean-plane),
    we have seen that linear transformations (Definition [16](ch010.xhtml#x1-66002r16))
    can be thought of as distorting the grid determined by the basis vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Following our geometric intuition, we suspect that measuring how much a transformation
    distorts volume and distance can provide some valuable insight. As we will see
    in this chapter, this is exactly the case. Transformations that preserve distance
    or norm are special, giving rise to methods such as Principal Component Analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 How linear transformations scale the area
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s go back to the Euclidean plane one more time. Consider any linear transformation
    A, mapping the unit square to a parallelogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file400.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Image of the unit square under a linear transformation'
  prefs: []
  type: TYPE_NORMAL
- en: The area of this parallelogram describes how A scales the unit square. Let’s
    call it λ for now; that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![area(A (C )) = λ ⋅area(C), ](img/file401.png)'
  prefs: []
  type: TYPE_IMG
- en: where C = [0,1] × [0,1] is the unit square, and A(C) is its image
  prefs: []
  type: TYPE_NORMAL
- en: '![A(C ) := {Ax : x ∈ C }. ](img/file402.png)'
  prefs: []
  type: TYPE_IMG
- en: Due to linearity, λ also matches the scaling ratio between the area of any rectangle
    (with parallel sides to the coordinate axes) and its image under A. As Figure
    4.15 shows, we can approximate any planar object as the union of rectangles.
  prefs: []
  type: TYPE_NORMAL
- en: 'If all rectangles are scaled by λ, then unions of rectangles also scale by
    that factor. Thus, it follows that λ is also the scaling ratio between any planar
    object E and its image A(E) = {Ax : x ∈E}.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file403.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: Approximating planar objects with a union of rectangles'
  prefs: []
  type: TYPE_NORMAL
- en: 'This quantity λ reveals a lot about the transformation itself, but there is
    a question remaining: how can we calculate it?'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that our linear transformation is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ x y A = ⌈ 1 1⌉ , x2 y2 ](img/file404.png)'
  prefs: []
  type: TYPE_IMG
- en: thus its columns x = (x[1],x[2]) and y = (y[1],y[2]) describe the two sides
    of the parallelogram. This is the image of the unit square.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file405.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: Image of the unit square under a linear transformation'
  prefs: []
  type: TYPE_NORMAL
- en: Our area scaling factor λ equals the area of this parallelogram, so our goal
    is to calculate this.
  prefs: []
  type: TYPE_NORMAL
- en: The area of any parallelogram can be calculated by multiplying the length of
    the base (∥x∥ in this case) with the height h. (You can easily see this by cutting
    off a triangle at the right side of the parallelogram and putting it to the left
    side, rearranging it as a rectangle.) h is unknown, but with basic trigonometry,
    we can see that h = sinα∥y∥, where α is the angle between x and y.
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![area = sin α∥y∥∥x ∥. ](img/file406.png)'
  prefs: []
  type: TYPE_IMG
- en: This is almost the dot product of x and y. (Recall that the dot product can
    be written as ⟨x,y⟩ = ∥x∥∥y∥cosα.) However, the sinα part is not a match.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is a clever trick we can use to turn this into a dot product!
    Since sinα = cos(α −![π 2](img/file408.png)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ π- area = cos(α− 2)∥x∥∥y ∥. ](img/file410.png)'
  prefs: []
  type: TYPE_IMG
- en: The issue is the angle between x and y is not α −![π 2](img/file411.png). However,
    we can solve this easily by applying a rotation (Section [4.3.2](ch010.xhtml#rotations)).
    Applying the transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 0 1 R = ⌈ ⌉, − 1 0 ](img/file412.png)'
  prefs: []
  type: TYPE_IMG
- en: we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![y = Ry = (y,− y ). rot 2 1 ](img/file413.png)'
  prefs: []
  type: TYPE_IMG
- en: Since ![∥yrot∥ = ∥y∥ ](img/file414.png), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![area = sin α∥y∥∥x ∥ π = cos(α − 2)∥x ∥∥y∥ π = cos(α − 2)∥x ∥∥yrot∥ = ⟨x,yrot⟩.
    ](img/file415.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The quantity ⟨x,y[rot]⟩ can be calculated using only the elements of the matrix
    A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x, yrot⟩ = x1y2 − x2y1\. ](img/file416.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that ⟨x,y[rot]⟩ can be negative! This happens when the angle between
    y = Ae[2] and x = Ae[1], measured from a counter-clockwise direction, is larger
    than π, as this implies cos(α −![π 2](img/file418.png))/span>0\.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the quantity ⟨x,y[rot]⟩ is called the signed area of the parallelogram.
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions, we call this the determinant of the linear transformation.
    That is, for any given linear transformation/matrix A ∈ℝ^(2×2), its determinant
    is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(7).png)(4.9)'
  prefs: []
  type: TYPE_IMG
- en: The determinant is often written as jAj, but we’ll avoid this notation. We’ll
    deal with determinants for any matrix A ∈ℝ^(n×n), but let’s stay with the 2 ×
    2 case just a bit to build intuition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The determinant also reveals the orientation of the vectors: positive determinant
    means positive orientation, negative determinant means negative orientation. (Intuitively,
    positive orientation means that the angle measured from x to y in a counter-clockwise
    direction is between 0 and π; equivalently, the angle measured from x to y in
    a clockwise direction is between π and 2π.) This is demonstrated in Figure 4.17
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file421.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Orientation of two vectors in the plane'
  prefs: []
  type: TYPE_NORMAL
- en: Overall,
  prefs: []
  type: TYPE_NORMAL
- en: area(A(E)) = |detA|area(E)
  prefs: []
  type: TYPE_NORMAL
- en: (4.10)
  prefs: []
  type: TYPE_NORMAL
- en: holds, where E ⊆ℝ² is a planar object, and
  prefs: []
  type: TYPE_NORMAL
- en: '![A (E) = {Ax : x ∈ E } ](img/file424.png)'
  prefs: []
  type: TYPE_IMG
- en: is the image of E under the transformation A.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we have only shown ([5.4.1](#)) in two dimensions, this holds in
    general. (Although we don’t know how to define the determinant there yet.)
  prefs: []
  type: TYPE_NORMAL
- en: So, if e[1] and e[2] is a basis on the plane, equations ([5.4.1](#)) and ([5.4.1](#))
    tell us that the determinant in two dimensions equals to
  prefs: []
  type: TYPE_NORMAL
- en: '![det A = orientation(Ae ,Ae )× area(Ae ,Ae ) 1 2 1 2 ](img/file425.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the example of the Euclidean plane, we have built enough geometric
    intuition on understanding how linear transformations distort volume and change
    the orientation of the space. These are described by the concept of determinants,
    which we have defined in the special case ([5.4.1](#)). We are going to move on
    to study the concept in its full generality.
  prefs: []
  type: TYPE_NORMAL
- en: To introduce the formal definition of the determinant, we will take a route
    that is different from the usual. Most commonly, the determinant of a linear transformation
    A is defined straight away with a complicated formula, then all of its geometric
    properties are shown.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of this, we will deduce the determinant formula by generalizing the
    geometric notion we have learned in the previous section. Here, we are roughly
    going to follow the outline of Linear Algebra and Its Applications by Peter D.
    Lax.
  prefs: []
  type: TYPE_NORMAL
- en: We set the foundations by introducing some key notations. Let
  prefs: []
  type: TYPE_NORMAL
- en: '![A = (ai,j)ni,j=1 ∈ ℝn ×n ](img/file426.png)'
  prefs: []
  type: TYPE_IMG
- en: be a matrix with columns a[1],…,a[n]. When we introduced the notion of matrices
    as linear transformations in Section [4.1.1](ch010.xhtml#linear-transformations-and-matrices)
    , we saw that the i-th column is the image of the i-th basis vector. For simplicity,
    let’s assume that e[1],e[2],…,e[n] is the standard orthonormal basis, that is,
    e[i] is the vector whose i-th coordinate is 1 and the rest is 0\. Thus, Ae[i]
    = a[i].
  prefs: []
  type: TYPE_NORMAL
- en: During our explorations in the Euclidean plane Section [4.3](ch010.xhtml#linear-transformations-in-the-euclidean-plane),
    we have seen that the determinant is the orientation of the images of basis vectors,
    times the area of the parallelogram defined by them. Following this logic, we
    could define the determinant for n ×n matrices by
  prefs: []
  type: TYPE_NORMAL
- en: '![det A = orientation(Ae1,...,Aen )× volume(Ae1, ...,Aen ) ](img/file427.png)'
  prefs: []
  type: TYPE_IMG
- en: Two questions surface immediately. First, how do we define the orientation of
    multiple vectors in the n-dimensional space? Second, how can we even calculate
    the area?
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of finding the answers for these questions, we are going to add a twist
    into the story: first, we’ll find a convenient formula for determinants, then
    use it to define orientation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 The multi-linearity of determinants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make the relation between the determinant and the columns of the matrix a[i]
    = Ae[i] more explicit, we’ll write
  prefs: []
  type: TYPE_NORMAL
- en: '![det A = det(a1,...,an ). ](img/file428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thinking about determinants this way, det is just a function of multiple variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![det : ℝn × ⋅⋅⋅× ℝn → ℝ. ◟-----◝◜----◞ n times ](img/file429.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Good news: det(a[1],…,a[n]) is linear in each variable. That is,'
  prefs: []
  type: TYPE_NORMAL
- en: '![det(a1,...,αai+ βbi,...an) = α det(a1,...,ai,...an )+β det(a1,...,bi,...an)
    ](img/file430.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. We are not going to prove this, but as the determinant represents the
    signed volume, you can convince yourself by checking out Figure 4.18.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file431.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: The multilinearity of det(a[1],a[2])'
  prefs: []
  type: TYPE_NORMAL
- en: A consequence of linearity is that we can express the determinant as a linear
    combination of determinants for the standard basis vectors e[1],…,e[n]. For instance,
    consider the following. Since
  prefs: []
  type: TYPE_NORMAL
- en: '![ n Ae = a = ∑ a e, 1 1 i,1 i i=1 ](img/file432.png)'
  prefs: []
  type: TYPE_IMG
- en: we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ det(a1,a2,...,an) = ai,1 det(ei,a2,...,an). i=1 ](img/file433.png)'
  prefs: []
  type: TYPE_IMG
- en: Going one step further and using that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n a2 = aj,2ej, j=1 ](img/file434.png)'
  prefs: []
  type: TYPE_IMG
- en: we start noticing a pattern. With the linearity, we have
  prefs: []
  type: TYPE_NORMAL
- en: det(a[1], a[2],…, a[n]) = ∑ [i=1]^n ∑ [j=1]^n a[i,1] a[j,2] det(e[i], e[j],
    a[3],…, a[n]).
  prefs: []
  type: TYPE_NORMAL
- en: (4.11)
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the row indices in the coefficients a[i,1]a[j,2] match the indices
    of e[k]-s in det(e[i],e[j],a[3],…,a[n]). In the general case, this pattern can
    be formalized in terms of permutations; that is, orderings of the set {1,2,…,n}.
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine a permutation as a function σ mapping {1,2,…,n} to itself in
    a way that for every j ∈{1,2,…,n}, there is exactly one i ∈{1,2,…,n} with σ(i)
    = j. In other words, you take every integer between 1 and n, and putting them
    in an order. The set of all possible permutations on {1,2,…,n} is denoted by S[n].
  prefs: []
  type: TYPE_NORMAL
- en: Continuing ([5.4.2](#)) and further expanding the determinant of A, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ ∏n det(a1,...,an) = [ aσ(i),i]det(eσ(1),...,eσ(n)). σ∈Sn i=1 ](img/file435.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula is not the easiest one to understand. You can think about each
    term ∏ [i=1]^na[σ(i),i] as placing n chess rooks on an n×n board such that none
    of them can capture each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file436.png) Figure 4.19: The anatomy of the term a[σ(1)1]⋅⋅⋅a[σ(n)n]'
  prefs: []
  type: TYPE_NORMAL
- en: The formula
  prefs: []
  type: TYPE_NORMAL
- en: '![∑ [∏n ] aσ(i),i det(eσ(1),...,e σ(n)) σ∈Sn i=1 ](img/file438.png)'
  prefs: []
  type: TYPE_IMG
- en: combines all the possible ways we can do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is only one thing left: calculating det(e[σ(1)],…,e[σ(n)]).'
  prefs: []
  type: TYPE_NORMAL
- en: Remember when we discussed the combination of reflections and rotations in the
    Euclidean plane (Section [4.3.4](ch010.xhtml#reflection))? The transformation
    determined by e[i]→e[σ(i)] is similar to that. When talking about permutations,
    it’s good to know that each one can be obtained by switching two elements at a
    time. The number of transpositions - that is, permutations affecting two elements
    - in a permutation is called the sign of σ. In the context of our linear transformation
    e[i]→e[σ(i)], the number of transpositions in σ is the number of reflections required
    and sign(σ) is the orientation of (e[σ(1)],…,e[σ(n)]).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, with these, we can finally give a formal definition for determinants and
    the orientation.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 20\. (Determinants and orientation)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary matrix and let a[i] ∈ℝ^n be its i-th column.
    The determinant of A is defined by
  prefs: []
  type: TYPE_NORMAL
- en: detA = det( a[1],…, a[n]) = ∑ [σ∈S[n]] sign(σ) [ ∏ [i=1]^n a [σ(i),i] ],
  prefs: []
  type: TYPE_NORMAL
- en: (4.12)
  prefs: []
  type: TYPE_NORMAL
- en: and the orientation of the vectors a[1],…,a[n] is
  prefs: []
  type: TYPE_NORMAL
- en: '![orientation(a1,...,an ) := sign(detA ). ](img/file443.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the det notation is not convenient, we denote determinants by putting
    the elements of the matrix inside a big absolute value sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ || || ||a1,1 a1,2 ... a1,n|| |a2,1 a2,2 ... a2,n| detA = || . . . . ||.
    || .. .. .. .. || || || an,1 an,2 ... an,n ](img/file444.png)'
  prefs: []
  type: TYPE_IMG
- en: When I was a young math student, the determinant formula ([4.12](ch010.xhtml#x1-81006r20))
    was presented as-is in my first linear algebra class. Without explaining the connection
    to volume and orientation, it took me years to properly understand it. I still
    think that the determinant is one of the most complex concepts in linear algebra,
    especially when presented without a geometric motivation for the definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have a basic understanding of the determinant, you might ask:
    how can we calculate it in practice? Summing over the set of all permutations
    and calculating their sign is not an easy operation from a computational perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Good news: there is a recursive formula for the determinant. Bad news: for
    an n×n matrix, it involves n pieces of (n − 1) × (n − 1) matrices. Still, it is
    a big step from the permutation formula. Let’s see it!'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 21\. (Recursive formula for determinants)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary square matrix. Then
  prefs: []
  type: TYPE_NORMAL
- en: detA = ∑ [j=1]^n (−1)^(j+1) a [1,j] detA[1,j],
  prefs: []
  type: TYPE_NORMAL
- en: '4.13'
  prefs: []
  type: TYPE_NORMAL
- en: where A[i,j] is the (n − 1) × (n − 1) matrix obtained from A by removing its
    i-th row and j-th column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of a proof, we are going to provide an example to demonstrate the formula.
    For 3 × 3 matrices, this is how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![| | ||a b c|| || || || || || || || || ||e f|| ||d f|| ||d e|| ||d e f||=
    a ||h i||− b||g i||+ c||g h||. |g h i| ](img/file445.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have both the geometric intuition and the recursive formula, let’s
    see the most important properties of the determinants!
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Fundamental properties of the determinants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with determinants, we prefer to create basic building blocks and
    rules for combining them. (As we have seen with this pattern so many times, even
    when deducing the ([4.12](ch010.xhtml#x1-81006r20)).) These rules are manifested
    by the fundamental properties of determinants, which we will discuss now.
  prefs: []
  type: TYPE_NORMAL
- en: The first property is concerned with the relation of composition and the determinant.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 22\. (The product of determinants)
  prefs: []
  type: TYPE_NORMAL
- en: ∈ℝ^(n×n) be two matrices. Then
  prefs: []
  type: TYPE_NORMAL
- en: detAB = detAdetB.
  prefs: []
  type: TYPE_NORMAL
- en: (4.14)
  prefs: []
  type: TYPE_NORMAL
- en: The equation ([4.14](ch010.xhtml#x1-82002r22)) is called the determinant product
    rule, and its proof involves some heavy computations based on the formulas ([4.12](ch010.xhtml#x1-81006r20))
    and ([4.13](ch010.xhtml#x1-81009r21)). Instead of providing a fully fleshed-out
    proof, I’ll give an intuitive explanation. After all, we want to build algorithms
    using mathematics, not building mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: So, the explanation of detAB = detAdetB is quite simple. If we think about the
    matrices A,B ∈ℝ^(n×n) as linear transformations, we have just seen that detA and
    detB determine how they scale the unit cube.
  prefs: []
  type: TYPE_NORMAL
- en: Since the composition of these linear transformations is the matrix product
    AB, the linear transformation AB scales the unit cube to a parallelepiped with
    signed volume detAdetB. (Because applying AB is the same as applying B first,
    then applying A on the result.)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, by our understanding of the determinant, as the scaling factor of AB is
    also detAB, ([4.14](ch010.xhtml#x1-82002r22)) holds.
  prefs: []
  type: TYPE_NORMAL
- en: We can do the actual proof of this, for example, by induction based on the recursive
    formula ([4.13](ch010.xhtml#x1-81009r21)), leading to a long and involved calculation.
  prefs: []
  type: TYPE_NORMAL
- en: An immediate corollary of the product rule is a special relation between the
    determinants of a matrix and its inverse.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 23\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary invertible matrix. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![detA −1 = (det A)−1\. ](img/file446.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. Using the product rule, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![1 = detI = detAA −1 = (det A)(detA −1), ](img/file447.png)'
  prefs: []
  type: TYPE_IMG
- en: from which the theorem follows.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, we can also conclude that the determinant is preserved by the
    similarity relation.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 24\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A,B ∈ ℝ^(n×n) be two similar matrices with B = T^(−1)AT for some T ∈ℝ^(n×n).
    Then
  prefs: []
  type: TYPE_NORMAL
- en: '![detA = detB. ](img/file448.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. This simply follows from
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 detB = det T AT = det T−1 detA detT = det A, ](img/file449.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important consequence is that the determinant is independent of the
    basis the matrix is in. If A : U →U is a linear transformation, and P = {p[1],…,p[n]}
    and R = {r[1],…,r[n]} are two bases of U, then we know that the matrices of the
    transformation are related (Section [4.2.1](ch010.xhtml#the-transformation-matrix))
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '![AP = T−1ART, ](img/file450.png)'
  prefs: []
  type: TYPE_IMG
- en: where A[S] is the matrix of the transformation A in a basis S and T ∈ℝ^(n×n)
    is the change of basis matrix (Section [4.2.1](ch010.xhtml#the-transformation-matrix)).
    The previous theorem implies that detA[P] = detA[R]. Thus, the determinant is
    properly defined for linear transformations, not just matrices!
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an essential duality relation regarding determinants: you can swap
    the rows and columns of a matrix, keeping all determinant-related identities true.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 25\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary matrix. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![detA = detAT . ](img/file451.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. Suppose that A = (a[i,j])[i,j=1]^n. Let’s denote the elements of its
    transpose by a[i,j]^t = a[j,i]. According to ([4.12](ch010.xhtml#x1-81006r20)),
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ T ∑ ∏n t detA = sign (σ ) aσ(i),i σ∈Sn i=1 ∑ ∏n = sign (σ ) ai,σ(i). σ∈Sn
    i=1 ](img/file452.png)'
  prefs: []
  type: TYPE_IMG
- en: Now comes the trick. Since the product ∏ [i=1]^na[i,σ(i)] iterates through all
    i-s, and the order of the terms doesn’t matter, we might as well order the terms
    as i = σ^(−1)(1),…,σ^(−1)(n). Since
  prefs: []
  type: TYPE_NORMAL
- en: '![sign(σ −1) = sign(σ), ](img/file453.png)'
  prefs: []
  type: TYPE_IMG
- en: by continuing the above calculation, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ n n ∑ sign(σ)∏ a = ∑ sign(σ−1)∏ a . i,σ(i) σ− 1(j),j σ∈Sn i=1 σ∈Sn j=1 ](img/file454.png)'
  prefs: []
  type: TYPE_IMG
- en: Because every permutation is invertible and σ→σ^(−1) is a bijection, summing
    over σ ∈S[n] is the same as summing over σ^(−1) ∈S[n].
  prefs: []
  type: TYPE_NORMAL
- en: Combining all of the above, we obtain that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ ∏n detA = sign(σ−1) aj,σ−1(j) σ∈Sn j=1 n = ∑ sign(σ)∏ a σ(j),j σ∈Sn j=1
    = detAT . ](img/file456.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 26\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary matrix and let A^(i,j) denote the matrix which
    can be obtained by swapping the i-th and j-th column of A. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![detAi,j = − detA, ](img/file457.png)'
  prefs: []
  type: TYPE_IMG
- en: or in other words, swapping any two columns of A will change the sign of the
    determinant. Similarly, swapping two rows also changes the sign of the determinant.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. This follows from a clever application of ([4.14](ch010.xhtml#x1-82002r22)),
    noticing that A^(i,j) = AI^(i,j), where I^(i,j) is obtained from the identity
    matrix by swapping its i-th and j-th column. detI^(i,j) is a determinant of the
    form det(e[σ(1)],…,e[σ(n)]), where σ is a permutation simply swapping i and j.
    (That is, σ is a transposition.) Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ i,j i,j detA = detA det I = − detA, ](img/file458.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding swapping rows, we can apply the previous result because transposing
    a matrix preserves the determinant.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, matrices with two matching rows have a zero determinant.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 27\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![A ∈ ℝn ×n ](img/file459.png) be a matrix that has two identical rows or
    columns. Then ![detA = 0 ](img/file460.png).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Suppose that the ![i ](img/file461.png)-th and the ![j ](img/file462.png)-th
    columns are matching. Since the two columns are equal, ![detAi,j = det A ](img/file463.png).
    However, applying the previous theorem (which states that swapping two columns
    changes the sign of the determinant), we obtain ![ i,j detA = − detA ](img/file464.png).
    This can only be true if ![detA = 0 ](img/file465.png).
  prefs: []
  type: TYPE_NORMAL
- en: Again, transposing the matrix gives the statement for rows.
  prefs: []
  type: TYPE_NORMAL
- en: As yet another consequence, we obtain an essential connection between linearly
    dependent vector systems and determinants.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 28\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![A ∈ ℝn×n ](img/file466.png) be a matrix. Then its columns are linearly
    dependent if and only if ![detA = 0 ](img/file467.png). Similarly, the rows of
    ![A ](img/file468.png) are linearly dependent if and only if ![detA = 0 ](img/file469.png).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (i) First, we are going to show that linearly dependent columns (or rows)
    imply detA = 0\. As usual, let’s denote the columns of A as a[1],…,a[n] and for
    the sake of simplicity, assume that
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ a1 = αiai. i=2 ](img/file470.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the determinant is a linear function of the columns, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ det(a1,a2,...,an) = αidet(ai,a2,...,an ). i=2 ](img/file471.png)'
  prefs: []
  type: TYPE_IMG
- en: Because of the previous theorem, all terms det(a[i],a[2],…,a[n]) are zero, implying
    detA = 0, which is what we had to show. If the rows are linearly dependent, we
    apply the above to obtain that detA = detA^T = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s show that detA = 0 means linearly dependent columns. Instead of the
    exact proof, which is rather involved, we should have an intuitive explanation
    instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall that the determinant is orientation times volume of the parallelepiped
    given by the columns. Since the orientation is ±1, detA implies that the volume
    of the parallelepiped is 0\. This can only happen if the n columns lie in an n
    − 1-dimensional subspace, meaning that they are linearly dependent.
  prefs: []
  type: TYPE_NORMAL
- en: We can immediately apply this to get the following result.
  prefs: []
  type: TYPE_NORMAL
- en: Corollary 2\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ ℝ^(n×n) be a matrix with a constant zero column (or row). Then detA
    = 0.
  prefs: []
  type: TYPE_NORMAL
- en: As the determinant is the signed volume of the basis vectors’ image, it can
    be zero in certain cases. These transformations are rather special. When can it
    happen? Let’s go back to the Euclidean plane to build some intuition.
  prefs: []
  type: TYPE_NORMAL
- en: There, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![|| || ||x1 y1|| ||x y || = x1y2 − x2y1 = 0, 2 2 ](img/file472.png)'
  prefs: []
  type: TYPE_IMG
- en: 'or in other words, ![x1 y1-](img/file473.png) = ![x2 y2-](img/file474.png).
    There is one more interpretation of this: the vector (y[1],y[2]) is a scalar multiple
    of (x[1],x[2]); that is, they are colinear, meaning that they lie on the same
    line through the origin. Thinking in terms of linear transformations, this means
    that the images of e[1] and e[2] lie on a subspace of ℝ². As we shall see next,
    this is closely connected with the invertibility of the transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 29\. (Invertibility and the determinants) The linear transformation
    ![A ∈ ℝn×n ](img/file475.png) is invertible if and only if ![detA ⁄= 0 ](img/file476.png).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. When we introduced the concept of invertibility (Definition [17](ch010.xhtml#x1-69002r17)),
    we saw that ![A ](img/file477.png) is invertible if and only if its columns ![a1,...,an
    ](img/file478.png) form a basis. Thus, they are linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: Since linear independence (Definition [3](ch007.xhtml#x1-23002r3)) of columns
    is equivalent to a nonzero determinant, the result follows.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, do your eyes hurt after finally using them for the first time? Mine sure
    did when I first learned about matrices as linear transformations. Here’s a part
    where the abstract viewpoint pays off for the first time, and trust me, it’ll
    pay even more dividends later.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s recap this chapter quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve learned that besides a table of numbers, a matrix can represent linear
    transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ∑m ⌋ a1,1 a1,2 ... a1,m x1 j=1a1,jxj || a a ... a || || x || ||
    ∑m a x || Ax = || 2,.1 2.,2 2,m. || || .2|| = || j=1.2,j j|| , |⌈ .. .. ... ..
    |⌉ |⌈ .. |⌉ |⌈ .. |⌉ ∑m an,1 an,2 ... an,m, xm j=1an,jxj ](img/file479.png)'
  prefs: []
  type: TYPE_IMG
- en: where the columns of A describe the images of the basis vectors under the linear
    transformation x →Ax.
  prefs: []
  type: TYPE_NORMAL
- en: Why on Earth is this useful for us? Think of mathematics as a problem-solving
    tool. The crux of problem-solving is often finding the proper representation of
    our objects of interest. Looking at matrices as a way to transform data gives
    us the much-needed geometric perspective, opening up a whole new avenue of methods.
  prefs: []
  type: TYPE_NORMAL
- en: When looking at matrices this way, we quickly understand why matrix multiplication
    is defined as it is. The definition
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑l n,m AB = ( ai,kbk,j)i,j=1 k=1 ](img/file480.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is daunting at first, but from the perspective of linear transformations, it’s
    all revealed to be a simple composition: first, we apply the transformation B,
    then A.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful, though: linear transformations and matrices are not exactly the
    same, as the matrix representation depends on the underlying basis of our vector
    space. (See, I told you that bases are going to be useful.)'
  prefs: []
  type: TYPE_NORMAL
- en: Matrices also possess an important quantity called determinants, originally
    defined by the mind-boggingly complex formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ ∏n det(a1,...,an) = [ aσ(i),i]det(eσ(1),...,eσ(n)), σ∈Sn i=1 ](img/file481.png)'
  prefs: []
  type: TYPE_IMG
- en: but an investigation exploiting our newfound geometric perspective reveals that
    the determinant simply describes how much the linear transformation distorts the
    volume of the domain space, and how it changes the orientation of the basis vectors.
  prefs: []
  type: TYPE_NORMAL
- en: For us machine learning practitioners, making the conceptual jump from matrices
    to linear transformations is the more interesting one. (As opposed to the theory,
    where we often learn about linear transformations first, matrices second.) For
    instance, this allows us to see a layer in a neural network as stretching, rotating,
    shearing, and potentially reflecting the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll revisit matrices from a slightly different perspective:
    systems of equations. Of course, everything is connected, and we’ll end up where
    we started, looking at what we know from a higher perspective. This is because
    learning is a spiral, and we are ascending fast.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Show that if A ∈ℝ^(n×n) is an invertible matrix, then
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 T T −1 (A ) = (A ) . ](img/file482.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 2\. Let R[α] be the two-dimensional rotation matrix defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌈cosα − sinα ⌉ R α = sinα cosα . ](img/file483.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that R[α]R[β] = R[α+β].
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Let A = (a[i,j])[i,j=1]^n ∈ℝ^(n×n) be a matrix and let D ∈ℝ^(n×n)
    be a diagonal matrix defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | d1 0 ... 0 | D = |⌈ 0 d2 ... 0 |⌉ , 0 0 ... dn ](img/file484.png)'
  prefs: []
  type: TYPE_IMG
- en: where all of its elements are zero outside the diagonal. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ d1a1,1 d2a1,2 ... dna1,n || || DA = || d1a2,1 d2a2,2 ... dna2,n || |⌈
    ... ... ... ... |⌉ d1an,1 d2an,2 ... dnan,n ](img/file485.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ |d1a1,1 d1a1,2 ... d1a1,n| |d2a2,1 d2a2,2 ... d2a2,n| AD = || . . .
    . || . |⌈ .. .. .. .. |⌉ dnan,1 dnan,2 ... dnan,n ](img/file486.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 4\. Let ∥⋅∥ be a norm on ℝ^n, and let A ∈ℝ^(n×n) be an arbitrary matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Show that A is invertible if and only if the function
  prefs: []
  type: TYPE_NORMAL
- en: '![∥x∥∗ := ∥Ax ∥ ](img/file487.png)'
  prefs: []
  type: TYPE_IMG
- en: is a norm on ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 5\. Let U be a normed space and f : U →U be a linear transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: If
  prefs: []
  type: TYPE_NORMAL
- en: '![∥x ∥∗ := ∥f(x)∥ ](img/file488.png)'
  prefs: []
  type: TYPE_IMG
- en: is a norm, is f necessarily invertible?
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint: Consider the vector space ℝ[x] with the norm'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n n ∑ 2 1∕2 ∑ i ∥p∥ = ( pi) , p(x) = pix i=0 i=0 ](img/file489.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and the linear transformation f : p(x)→xp(x).'
  prefs: []
  type: TYPE_NORMAL
- en: Problem 6\. Let ⟨⋅,⋅,⟩ be an inner product on ℝ^n. Show that there is a matrix
    A ∈ℝ^(n×n) such that
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x,y ⟩ = xT Ay, x,y ∈ ℝn. ](img/file491.png)'
  prefs: []
  type: TYPE_IMG
- en: (Recall that we treat vectors x,y ∈ℝ^n as column vectors.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 7\. Let A ∈ℝ^(n×n) be a matrix. A is called positive definite if x^T
    Ax/span>0 for every nonzero x ∈ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: Show that A is positive definite if and only if
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x,y⟩ := xTAy ](img/file492.png)'
  prefs: []
  type: TYPE_IMG
- en: is an inner product.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 8\. Let A ∈ℝ^(n×m) be a matrix, and denote its columns by a[1],…,a[n]
    ∈ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Show that for all x ∈ℝ^m, we have Ax ∈ span(a[1],…,a[n]).
  prefs: []
  type: TYPE_NORMAL
- en: (b) Let B ∈ℝ^(m×k), and denote the columns of AB by v[1],…,v[k] ∈ℝ^n. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![v1,...,vk ∈ span(a1,...,an). ](img/file493.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 9\. Let A ∈ℝ^(n×n) be a matrix. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨Ax, y⟩ = ⟨x,AT y⟩ ](img/file494.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all x,y ∈ℝ^n, where ⟨⋅,⋅⟩ is the Euclidean inner product.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 10\. Calculate the determinant of
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊1 2 3⌋ | | A = |⌈4 5 6|⌉ . 7 8 9 ](img/file495.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 11\. Let A ∈ℝ^(n×n) be a matrix and let c ∈ℝ be a constant.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![| | ||a1,1 ... ca1,i ... a1,n || ||a ... ca ... a || || 2.,1 .2,i 2.,n ||=
    cdetA || .. ... .. ... .. || || || an,1 ... can,i ... an,n ](img/file496.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all i = 1,…,n.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![| | ||a1,1 a1,2 ... a1,n || || . . . . || | .. .. .. .. | ||ca ca ... ca
    ||= cdet A || .i,1 i.,2 .i,n|| || .. .. ... .. || || || an,1 an,2 ... an,n ](img/file497.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all i = 1,…,n and
  prefs: []
  type: TYPE_NORMAL
- en: (c) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![det(cA ) = cndet A. ](img/file498.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 12\. Let A ∈ℝ^(n×n) be an upper triangular matrix. (That is, all elements
    below the diagonal are zero.) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∏n detA = ai,i. i=1 ](img/file499.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that the same holds for lower triangular matrices. (That is, matrices where
    elements above the diagonal are zero.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 13\. Let M ∈ℝ^(n×m) be a matrix with the block structure
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌈A B ⌉ M = 0 C , ](img/file500.png)'
  prefs: []
  type: TYPE_IMG
- en: where A ∈ℝ^(k×k), B ∈ℝ^(k×l), and C ∈ℝ^(l×l).
  prefs: []
  type: TYPE_NORMAL
- en: Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![detM = detA detC. ](img/file501.png)'
  prefs: []
  type: TYPE_IMG
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
