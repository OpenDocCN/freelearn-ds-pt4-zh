<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch011.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="matrices-and-equations" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">5</span></span><br/>
<span id="x1-860006"></span><span class="cmss-10x-x-109">Matrices and Equations</span></h1>
<p><span class="cmss-10x-x-109">So, matrices are not just tables of numbers but linear transformations; we have spent a lengthy chapter discovering this relationship.</span></p>
<p><span class="cmss-10x-x-109">Now, I want us to circle back to the good old tables of numbers once more, but representing systems of linear equations this time. Why? Simple. Because solving linear equations is the motivator behind key theoretical and technical innovations. In the previous chapter, we talked about inverse matrices but didn’t compute one in practice. With what we’re about to learn, we will be able to not only compute inverse matrices but do so blazing fast.</span></p>
<p><span class="cmss-10x-x-109">Let’s get to work!</span></p>
<section id="linear-equations" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_70"><span class="titlemark"><span class="cmss-10x-x-109">5.1 </span></span> <span id="x1-870006.1"></span><span class="cmss-10x-x-109">Linear equations</span></h2>
<p><span class="cmss-10x-x-109">In practice, we can translate several problems into linear equations. For</span><span id="dx1-87001"></span> <span class="cmss-10x-x-109">example, a cash dispenser has </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">900 in </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">20 and </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">50 bills. We know that there are twice as many </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">20 bills than </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">50. The question is, how many of each bill does the machine have?</span></p>
<p><span class="cmss-10x-x-109">If we denote the number of </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">20 bills by </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and the number of </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">50 bills by </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, we obtain the equations</span></p>
<div class="math-display">
<img src="../media/file502.png" class="math-display" alt=" x1 − 2x2 = 0 20x1 + 50x2 = 900. "/>
</div>
<p><span class="cmss-10x-x-109">For two variables, as we have now, these are easily solvable by expressing one in terms of the other. Here, the first equation would imply </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> = 2<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. Plugging it back into the second equation, we obtain </span>90<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> = 900<span class="cmss-10x-x-109">, which gives </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> = 10<span class="cmss-10x-x-109">. Coming full circle, we can substitute this into </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> = 2<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, yielding the solutions</span></p>

<img src="../media/file503.png" width="150" class="math-display" alt="x = 20 1 x2 = 10. "/>

<p><span class="cmss-10x-x-109">However, for thousands of variables like in real applications, we need a</span><span id="dx1-87002"></span> <span class="cmss-10x-x-109">bit more craft. This is where linear algebra comes in. By introducing the matrix and vectors</span></p>
<div class="math-display">
<img src="../media/file504.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ A = ⌈ 1 − 2⌉ , x = ⌈x1 ⌉, b = ⌈ 0 ⌉ , 20 50 x2 900 "/>
</div>
<p><span class="cmss-10x-x-109">the equation can be written in the form </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">. That is, in terms of linear transformations, we can reformulate the question: which vector </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is mapped to </span><span class="cmbx-10x-x-109">b </span><span class="cmss-10x-x-109">by the transformation </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">? This question is central in linear algebra, and we’ll dedicate this section to solving it.</span></p>
<section id="gaussian-elimination" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_71"><span class="titlemark"><span class="cmss-10x-x-109">5.1.1 </span></span> <span id="x1-880006.1.1"></span><span class="cmss-10x-x-109">Gaussian elimination</span></h3>
<p><span class="cmss-10x-x-109">Let’s revisit our earlier example:</span></p>
<div class="math-display">
<img src="../media/file505.png" class="math-display" alt=" x1 − 2x2 = 0 20x1 + 50x2 = 900. "/>
</div>
<p><span class="cmss-10x-x-109">We can use the first equation </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">− </span>2<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> = 0 <span class="cmss-10x-x-109">to get rid of the term </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">in the second equation </span>20<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> + 50<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> = 900<span class="cmss-10x-x-109">. We can do this by multiplying it by </span>20 <span class="cmss-10x-x-109">and subtracting it from the second row, obtaining </span>90<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> = 900<span class="cmss-10x-x-109">, from which </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> = 10 <span class="cmss-10x-x-109">is obtained. This can be substituted back into the first row, yielding </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> = 20<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">What about the general case? Would this work for a general </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">? Absolutely. So far, we have used two rules for manipulating the equations in a linear system:</span></p>
<ol>
<li><span id="x1-88002x1"><span class="cmss-10x-x-109">Multiplying an equation with a nonzero scalar won’t change the solutions.</span></span></li>
<li><span id="x1-88004x2"><span class="cmss-10x-x-109">Adding a scalar multiple of one row to another won’t change the solutions either.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Earlier, we applied these repeatedly to eliminate variables progressively in our simple example. We can easily do the same for </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">variables! First, let’s see what we are talking about!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-88005r21"></span> <span class="cmbx-10x-x-109">Definition 21.</span> </span><span class="cmbx-10x-x-109">(System of linear equations)</span></p>
<p>Let <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> be a matrix and <span class="cmbx-10x-x-109">b </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> be a vector. The collection of equations</p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
<div class="math-display">
  <img src="../media/file506.png" alt="Equation image"/>
</div>
<div style="padding-left: 1em; ">(5.1)</div>
</div>

<p>are called the <span class="cmti-10x-x-109">system of linear equations </span>determined by <span class="cmmi-10x-x-109">A </span>and <span class="cmbx-10x-x-109">b</span>.</p>
</div>
<p><span class="cmss-10x-x-109">A system of linear equations is</span> <span id="dx1-88006"></span><span class="cmss-10x-x-109">often written in the short form </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is called its </span><span class="cmssi-10x-x-109">coefficient matrix</span><span class="cmss-10x-x-109">. If the vector </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">satisfies </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">, it is</span> <span id="dx1-88007"></span><span class="cmss-10x-x-109">called a </span><span class="cmssi-10x-x-109">solution</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Speaking of solutions, are there even any, and if so, how can we find them?</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">11</span></sub> <span class="cmss-10x-x-109">is nonzero, we can multiply the first equation of (</span><a href="ch011.xhtml#x1-88005r21"><span class="cmss-10x-x-109">5.1</span></a><span class="cmss-10x-x-109">) by</span> <img src="../media/file507.png" width="15" data-align="middle" alt="aak111"/> <span class="cmss-10x-x-109">and subtract it from the </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-th equation.</span></p>
<p><span class="cmss-10x-x-109">This way, </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">will be eliminated from all but the first row, obtaining</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation">

  <img src="../media/file508.png" alt="Equation image" class="math-display" width="600"/>

<div style="padding-left: 1em; ">(5.2)</div>
</div>

<p><span class="cmss-10x-x-109">To clear up this notation a bit, let’s denote the new coefficients with </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">ij</span></sub><sup><span class="cmr-8">(1)</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmr-8">(1)</span></sup><span class="cmss-10x-x-109">. So, we have</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/file509.png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(5.3)
  </div>
</div>

<p><span class="cmss-10x-x-109">We can repeat the above process and use the second equation to get rid of the </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">variable in the third equation, and so forth.</span></p>
<p><span class="cmss-10x-x-109">This can be done </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1 <span class="cmss-10x-x-109">times in total, ultimately leading to an equation system </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> <span class="cmss-10x-x-109">where all coefficients below the diagonal of </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> <span class="cmss-10x-x-109">are 0:</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(8).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(5.4)
  </div>
</div>
<p><span class="cmss-10x-x-109">Notice that</span> <span id="dx1-88008"></span><span class="cmss-10x-x-109">the </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-th elimination step only affects the coefficients from the </span>(<span class="cmmi-10x-x-109">k </span>+ 1)<span class="cmss-10x-x-109">-th row. Now we can work backward: the last equation </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">nn</span></sub><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub> = <span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">n</span></sub><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> <span class="cmss-10x-x-109">can be used to find </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">. This can be substituted to the </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1)<span class="cmss-10x-x-109">-th equation, yielding </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">. Continuing like this, we can eventually find all </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, obtaining a solution for our linear system.</span></p>
<p><span class="cmss-10x-x-109">This process</span> <span id="dx1-88009"></span><span class="cmss-10x-x-109">is called </span><span class="cmssi-10x-x-109">Gaussian elimination</span><span class="cmss-10x-x-109">, and it’s kind of a big deal. It is not only useful for solving linear equations, but it can also be used to calculate determinants, factor matrices into the product of simpler ones, and much more. We’ll talk about all of this in detail, but let’s focus on equations a little more.</span></p>
<p><span class="cmss-10x-x-109">Unfortunately, not all linear equations can be solved. For instance, consider the system</span></p>
<div class="math-display">
<img src="../media/file511.png" class="math-display" alt=" x1 + x2 = 1 2x1 + 2x2 = − 1. "/>
</div>
<p><span class="cmss-10x-x-109">Subtracting the first equation from the second one yields</span></p>
<div class="math-display">
<img src="../media/file512.png" class="math-display" alt="x1 + x2 = 1 x1 + x2 = − 2 "/>
</div>
<p><span class="cmss-10x-x-109">in the very first step, making it apparent that the equation has no solutions.</span></p>
<p><span class="cmss-10x-x-109">Before we turn to the technical details, let’s see a simple example of how Gaussian elimination is done in practice!</span></p>
</section>
<section id="gaussian-elimination-by-hand" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_72"><span class="titlemark"><span class="cmss-10x-x-109">5.1.2 </span></span> <span id="x1-890006.1.2"></span><span class="cmss-10x-x-109">Gaussian elimination by hand</span></h3>
<p><span class="cmss-10x-x-109">To build a</span> <span id="dx1-89001"></span><span class="cmss-10x-x-109">deeper understanding of Gaussian elimination, let’s consider the simple equation system</span></p>
<div class="math-display">
<img src="../media/file513.png" class="math-display" alt=" x + 0x − 3x = 6 1 2 3 2x1 + 1x2 + 5x3 = 2 − 2x1 − 3x2 + 8x3 = 2. "/>
</div>
<p><span class="cmss-10x-x-109">To keep track of our progress (and, since we are lazy, to avoid writing too much), we record the intermediate results as</span></p>
<div class="math-display">
<img src="../media/file514.png" class="math-display" alt=" | 1 0 − 3|6 2 1 5 |2 | − 2 − 3 8 |2 "/>
</div>
<p><span class="cmss-10x-x-109">with the coefficient matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">on the left side and </span><span class="cmmi-10x-x-109">b </span><span class="cmss-10x-x-109">on the other. To get a good grip on the method, I encourage you to follow along and do the calculations yourself by hand.</span></p>
<p><span class="cmss-10x-x-109">After eliminating the first variable from the second and third equations, we have</span></p>
<div class="math-display">
<img src="../media/file515.png" class="math-display" alt=" | 1 0 − 3 | 6 0 1 11 |− 10 | , 0 − 3 2 | 14 "/>
</div>
<p><span class="cmss-10x-x-109">while the final step yields</span></p>
<div class="math-display">
<img src="../media/file516.png" class="math-display" alt=" | 1 0 − 3| 6 0 1 11 |− 10 | . 0 0 35 |− 16 "/>
</div>
<p><span class="cmss-10x-x-109">From this form, we can unravel the solutions one by one.</span></p>
<p><span class="cmss-10x-x-109">In the 21st century, your chances of having to solve a linear equation by hand are close to 0. (If you are reading this book during the 22nd century or later, I am incredibly honored and surprised at the same time. Or, at least, I would be if I were still alive.) Still, understanding the general principles behind solving linear equations can take you very far.</span></p>
</section>
<section id="when-can-we-perform-gaussian-elimination" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_73"><span class="titlemark"><span class="cmss-10x-x-109">5.1.3 </span></span> <span id="x1-900006.1.3"></span><span class="cmss-10x-x-109">When can we perform Gaussian elimination?</span></h3>
<p><span class="cmss-10x-x-109">If you followed the description of Gaussian elimination carefully, you</span><span id="dx1-90001"></span> <span class="cmss-10x-x-109">might have noticed that the process can break down. We might accidentally divide by 0 during any elimination step!</span></p>
<p><span class="cmss-10x-x-109">For instance, after the first step given by equation (</span><a href="ch011.xhtml"><span class="cmss-10x-x-109">5.2</span></a><span class="cmss-10x-x-109">), the new coefficients are of the form</span></p>
<div class="math-display">
<img src="../media/file517.png" class="math-display" alt="(aij − a1j ai1), a11 "/>
</div>
<p><span class="cmss-10x-x-109">which is invalid if </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">11</span></sub> = 0<span class="cmss-10x-x-109">. In general, the </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-th step involves division by </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">kk</span></sub><sup><span class="cmr-8">(</span><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup><span class="cmss-10x-x-109">. Since </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">kk</span></sub><sup><span class="cmr-8">(</span><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> <span class="cmss-10x-x-109">is defined recursively, describing it in terms of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is not straightforward. For this, we introduce the concept of </span><span class="cmssi-10x-x-109">principal minors</span><span class="cmss-10x-x-109">, the upper-left subdeterminants of a matrix.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-90002r22"></span> <span class="cmbx-10x-x-109">Definition 22.</span> </span><span class="cmbx-10x-x-109">(Principal minors)</span></p>
<p>Let <img src="../media/file518.png" class="math" alt=" n n×n A = (aij)i,j=1 ∈ ℝ "/> be an arbitrary square matrix. Define the submatrix <img src="../media/file519.png" class="math" alt="Ak ∈ ℝk×k "/> by omitting all rows and columns of <img src="../media/file520.png" class="math" alt="A "/> with indices larger than <img src="../media/file521.png" class="math" alt="k "/>. For instance,</p>
<div class="math-display">
<img src="../media/file522.png" class="math-display" alt=" ⌊ ⌋ [ ] ⌈a11 a12⌉ A1 = a11 , A2 = a a , 21 22 "/>
</div>
<p>and so on. The k-th principal minor of <span class="cmmi-10x-x-109">A</span>, denoted by <span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">k</span></sub>, is defined by</p>
<div class="math-display">
<img src="../media/file523.png" class="math-display" alt="Mk := detAk. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">The first and last principal minors are special, as </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmr-8">1</span></sub> = <span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">11</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">n</span></sub> = det<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. With principal minors, we can describe when Gaussian elimination is possible. In fact, it turns out that</span></p>
<div class="math-display">
<img src="../media/file524.png" class="math-display" alt=" M M a11 = M1, a(21)2 = --2, ...,a(nnn−1)= ---n-- M1 Mn −1 "/>
</div>
<p><span class="cmss-10x-x-109">and, in general, </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">kk</span></sub><sup><span class="cmr-8">(</span><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> = <img src="../media/file525.png" class="frac" data-align="middle" alt="-Mk-- Mk−1"/><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To summarize, we can state the following.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-90003r30"></span> <span class="cmbx-10x-x-109">Theorem 30.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file526.png" class="math" alt=" n×n A ∈ ℝ "/> <span class="cmti-10x-x-109">be an arbitrary square matrix, and let </span><img src="../media/file527.png" class="math" alt="Mk "/> <span class="cmti-10x-x-109">be its </span><img src="../media/file528.png" class="math" alt="k "/><span class="cmti-10x-x-109">-th principal minor. If </span><img src="../media/file529.png" class="math" alt="Mk ⁄= 0 "/> <span class="cmti-10x-x-109">for all </span><img src="../media/file530.png" class="math" alt="k = 1,2,...,n − 1 "/><span class="cmti-10x-x-109">, then Gaussian elimination can be successfully performed.</span></p>
</div>
<p><span class="cmss-10x-x-109">As the proof is a bit involved, we are not going to do it here. (The difficult step is showing </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">kk</span></sub><sup><span class="cmr-8">(</span><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> = <span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">∕M</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">;’ the rest follows immediately.) The point is, if none of the principal minors are 0, the algorithm finishes.</span></p>
<p><span class="cmss-10x-x-109">We can simplify this requirement a bit and describe the Gaussian elimination in terms of the determinant, not the principal minors.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-90004r31"></span> <span class="cmbx-10x-x-109">Theorem 31.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary square matrix. If</span> det<span class="cmmi-10x-x-109">A≠</span>0<span class="cmti-10x-x-109">, then all principal minors are nonzero as well.</span></p>
</div>
<p><span class="cmss-10x-x-109">As a consequence, if</span> <span id="dx1-90005"></span><span class="cmss-10x-x-109">the determinant is nonzero, the Gaussian elimination is successful. A simple and nice requirement.</span></p>
</section>
<section id="the-time-complexity-of-gaussian-elimination" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_74"><span class="titlemark"><span class="cmss-10x-x-109">5.1.4 </span></span> <span id="x1-910006.1.4"></span><span class="cmss-10x-x-109">The time complexity of Gaussian elimination</span></h3>
<p><span class="cmss-10x-x-109">To get a handle on</span> <span id="dx1-91001"></span><span class="cmss-10x-x-109">how fast the Gaussian elimination algorithm executes, let’s do a little complexity analysis. As described by (</span><a href="ch011.xhtml"><span class="cmss-10x-x-109">5.2</span></a><span class="cmss-10x-x-109">), the first elimination step involves an addition and a multiplication for each element, except for those in the first row. That is </span>2<span class="cmmi-10x-x-109">n</span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmss-10x-x-109">operations in total.</span></p>
<p><span class="cmss-10x-x-109">The next step is essentially the first step, done on the </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmsy-10x-x-109">× </span>(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1) <span class="cmss-10x-x-109">matrix obtained from </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(1)</span></sup> <span class="cmss-10x-x-109">by removing its first row and column. This time, we have</span> 2(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>1)(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">− </span>2) <span class="cmss-10x-x-109">operations.</span></p>
<p><span class="cmss-10x-x-109">Following this train of thought, we quickly get that the total number of operations is</span></p>
<div class="math-display">
<img src="../media/file531.png" class="math-display" alt="∑n 2(n − i+ 1)(n− i), i=1 "/>
</div>
<p><span class="cmss-10x-x-109">which doesn’t look that friendly. Since we are looking for the order of complexity instead of an exact number, we can be generous and suppose that at each elimination step, we are performing </span><span class="cmmi-10x-x-109">O</span>(<span class="cmmi-10x-x-109">n</span><sup><span class="cmr-8">2</span></sup>) <span class="cmss-10x-x-109">operations. So, we have a time complexity of</span></p>
<div class="math-display">
<img src="../media/file532.png" class="math-display" alt="∑n O(n2) = O (n3 ), i=1 "/>
</div>
<p><span class="cmss-10x-x-109">meaning that we need around </span><span class="cmmi-10x-x-109">cn</span><sup><span class="cmr-8">3</span></sup> <span class="cmss-10x-x-109">operations for Gaussian elimination, where </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">is an arbitrary positive constant. This might seem a lot, but in the beautiful domain of algorithms, this is good. </span><span class="cmmi-10x-x-109">O</span>(<span class="cmmi-10x-x-109">n</span><sup><span class="cmr-8">3</span></sup>) <span class="cmss-10x-x-109">is polynomial time, and we can have much, much worse.</span></p>
</section>
<section id="when-can-a-system-of-linear-equations-be-solved" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_75"><span class="titlemark"><span class="cmss-10x-x-109">5.1.5 </span></span> <span id="x1-920006.1.5"></span><span class="cmss-10x-x-109">When can a system of linear equations be solved?</span></h3>
<p><span class="cmss-10x-x-109">So, we have just</span> <span id="dx1-92001"></span><span class="cmss-10x-x-109">seen that for any linear equation</span></p>
<div class="math-display">
<img src="../media/file533.png" class="math-display" alt="Ax = b, A ∈ ℝn×n, x, b ∈ ℝn, "/>
</div>
<p><span class="cmss-10x-x-109">Gaussian elimination can be successfully performed if the principal minors </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,M</span><sub><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">are nonzero. Notice one caveat about the result: </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">n</span></sub> = det<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">can be zero as well. Turns out, this is quite an important detail.</span></p>
<p><span class="cmss-10x-x-109">If you have</span> <span id="dx1-92002"></span><span class="cmss-10x-x-109">closely followed the discussion leading up to this point, you will see that we missed a crucial point: are there any solutions at all for a given linear equation? There are three options:</span></p>
<ol>
<li><span id="x1-92004x1"><span class="cmss-10x-x-109">There are no solutions.</span></span></li>
<li><span id="x1-92006x2"><span class="cmss-10x-x-109">There is exactly one solution.</span></span></li>
<li><span id="x1-92008x3"><span class="cmss-10x-x-109">There are multiple solutions.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">All of these are relevant to us from a certain perspective, but let’s start with the most straightforward one: when do we have exactly one solution? The answer is simple: when </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is invertible, the solution can be explicitly written as </span><span class="cmbx-10x-x-109">x </span>= <span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">. Speaking in terms of linear transformations, we can find a unique vector </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">that is mapped to </span><span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">. We summarize this idea in the following theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-92009r32"></span> <span class="cmbx-10x-x-109">Theorem 32.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file534.png" class="math" alt="A ∈ ℝn ×n "/> <span class="cmti-10x-x-109">be an invertible matrix. Then, for any </span><img src="../media/file535.png" class="math" alt="b ∈ ℝn "/><span class="cmti-10x-x-109">, the equation </span><img src="../media/file536.png" class="math" alt="Ax = b "/> <span class="cmti-10x-x-109">has a unique solution that can be written as </span><img src="../media/file537.png" class="math" alt="x = A− 1b "/><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is invertible, then</span> det<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is nonzero. Thus, using what we have learned previously, Gaussian elimination can be performed, yielding the unique solution. Nice and simple.</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is not invertible, the two remaining possibilities are in play: no vector is mapped to </span><span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">, which means there are no solutions, or multiple vectors are mapped to </span><span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">, giving infinite solutions.</span></p>
<p><span class="cmss-10x-x-109">Do you remember how we used the kernel of a linear transformation to describe its invertibility in </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch010.xhtml#x1-70003r20"><span class="cmssi-10x-x-109">20</span></a><span class="cmss-10x-x-109">? It turns out that</span> ker<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">can also be used to find </span><span class="cmssi-10x-x-109">all </span><span class="cmss-10x-x-109">solutions for a linear system.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-92010r33"></span> <span class="cmbx-10x-x-109">Theorem 33.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary matrix and let </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a solution to the linear equation </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmti-10x-x-109">, where </span><span class="cmbx-10x-x-109">b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">. Then, the set of all solutions can be written as</span></p>
<div class="math-display">
<img src="../media/file538.png" class="math-display" alt="x0 + kerA := {x0 + y : y ∈ kerA}. "/>
</div>
</div>
<div id="tcolobox-152" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">We have to show two things: (a) if </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> + ker<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, then </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is a solution; and (b) if </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is a solution, then </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> + ker<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">(a) Suppose that </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> + ker<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, that is, </span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> + <span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">for some </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. Then,</span></p>
<div class="math-display">
<img src="../media/file539.png" class="math-display" alt="Ax = A(x0 + y) = Ax0 + Ay = b, ◟=◝b◜◞ ◟◝=◜0◞ "/>
</div>
<p><span class="cmss-10x-x-109">which shows that </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is indeed a solution.</span></p>
<p><span class="cmss-10x-x-109">(b) Now let </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">be an arbitrary solution. We have to show that </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> <span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. This is easy, since</span></p>
<div class="math-display">
<img src="../media/file540.png" class="math-display" alt="A (x− x0) = Ax − Ax0 = b − b = 0. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, (a) and (b) imply that </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> + ker<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is the set of all solutions.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">In theory, this theorem provides an excellent way of finding all solutions for linear equations, generalizing far beyond finite-dimensional vector</span><span id="dx1-92011"></span> <span class="cmss-10x-x-109">spaces. (Note that the proof goes through verbatim for </span><span class="cmssi-10x-x-109">all </span><span class="cmss-10x-x-109">vector spaces and linear transformations.) For instance, this exact result is used to describe all solutions of an inhomogeneous linear differential equation.</span></p>
</section>
<section id="inverting-matrices" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_76"><span class="titlemark"><span class="cmss-10x-x-109">5.1.6 </span></span> <span id="x1-930006.1.6"></span><span class="cmss-10x-x-109">Inverting matrices</span></h3>
<p><span class="cmss-10x-x-109">So far, we</span> <span id="dx1-93001"></span><span class="cmss-10x-x-109">have seen that the invertibility of a matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is key to solving linear equations. However, we haven’t found a way to compute the inverse of a matrix yet.</span></p>
<p><span class="cmss-10x-x-109">Let’s recap what the inverse is in terms of linear transformations. If the columns of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">are denoted by the vectors </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is the linear transformation that maps the standard basis vectors to these vectors:</span></p>
<div class="math-display">
<img src="../media/file541.png" class="math-display" alt="A : ei ↦→ ai, i = 1,...,n. "/>
</div>
<p><span class="cmss-10x-x-109">If the direction of the arrows can be reversed, that is,</span></p>
<div class="math-display">
<img src="../media/file542.png" class="math-display" alt="A −1 : ai ↦→ ei, i = 1,...,n "/>
</div>
<p><span class="cmss-10x-x-109">is a well-defined linear equation, then </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is called the inverse of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In light of all that we have seen in this chapter, the method for finding the inverse is simple: solve </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">for each </span><span class="cmmi-10x-x-109">i </span><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the standard basis.</span></p>
<p><span class="cmss-10x-x-109">Suppose that </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmbx-10x-x-109">e </span><span class="cmssi-10x-x-109">i </span><span class="cmss-10x-x-109">. Then, if </span><span class="cmbx-10x-x-109">b </span><span class="cmss-10x-x-109">can be written as </span><span class="cmbx-10x-x-109">b </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">i</span></sub><span class="cmbx-10x-x-109">e </span><span class="cmssi-10x-x-109">i</span><span class="cmss-10x-x-109">, the vector </span><span class="cmbx-10x-x-109">x </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">i</span></sub><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the solution of </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file543.png" class="math-display" alt=" ∑n ∑n A ( bixi) = biAxi i=1 i=1 ∑n = biei i=1 = b. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the inverse is the matrix whose </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th column is </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">I know, this seems paradoxical: to find the solution of </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">, we</span><span id="dx1-93002"></span> <span class="cmss-10x-x-109">need the inverse </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">. To find the inverse, we need to solve </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">equations. The answer is Gaussian elimination (</span><span class="cmssi-10x-x-109">Section </span><a href="ch011.xhtml#gaussian-elimination"><span class="cmssi-10x-x-109">5.1.1</span></a><span class="cmss-10x-x-109">), which gives us an exact computational method to obtain </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">. In the next section, we are going to put this into practice and write our matrix-inverting algorithm from scratch. Pretty awesome.</span></p>
</section>
</section>
<section id="the-lu-decomposition" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_77"><span class="titlemark"><span class="cmss-10x-x-109">5.2 </span></span> <span id="x1-940006.2"></span><span class="cmss-10x-x-109">The LU decomposition</span></h2>
<p><span class="cmss-10x-x-109">In the previous</span> <span id="dx1-94001"></span><span class="cmss-10x-x-109">chapter, I promised that you’d never have to solve a linear equation by hand. As it turns out, this task is perfectly suitable for computers. In this chapter, we will dive deep into the art of solving linear equations, developing the tools from scratch.</span></p>
<p><span class="cmss-10x-x-109">We start by describing the process of Gaussian elimination in terms of matrices. Why would we even do that? Because matrix multiplication can be performed extremely fast in modern computers. Expressing </span><span class="cmssi-10x-x-109">any </span><span class="cmss-10x-x-109">algorithm in terms of matrices is a sure way to accelerate.</span></p>
<p><span class="cmss-10x-x-109">At the start, our linear equation </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b </span><span class="cmss-10x-x-109">is given by the coefficient matrix</span></p>
<div class="math-display">
<img src="../media/file544.png" class="math-display" alt=" ⌊ ⌋ a11 a12 ... a1n || || | a21 a22 ... a2n| A = || ... ... ... ... || ∈ ℝn×n, || || |⌈ an1 an2 ... ann|⌉ "/>
</div>
<p><span class="cmss-10x-x-109">and at the end of the elimination process, </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is transformed into the form</span></p>
<div class="math-display">
<img src="../media/file545.png" class="math-display" alt=" ⌊ ⌋ | a11 a12 a13 ... a1n | || 0 a(1) a (1) ... a(1)|| (n−1) | 22 23(2) 2(n2)| A = || 0 0 a 33 ... a3n || . || ... ... ... ... ... || ⌈ ⌉ 0 0 0 ... a(nnn−1) "/>
</div>
<p><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup> <span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">upper diagonal</span><span class="cmss-10x-x-109">; that is, all elements below its diagonal are 0.</span></p>
<p><span class="cmss-10x-x-109">Gaussian elimination performs this task one step at a time, focusing on</span><span id="dx1-94002"></span> <span class="cmss-10x-x-109">consecutive columns. After the first elimination step, this is turned into the equation (</span><a href="ch011.xhtml"><span class="cmss-10x-x-109">5.1.1</span></a><span class="cmss-10x-x-109">), described by the coefficient matrix</span></p>
<div class="math-display">
<img src="../media/file546.png" class="math-display" alt=" ⌊ ⌋ | a11 a12 ... a1n| || 0 a(212) ... a(21)n|| (1) || . . . . || n×n A = | .. .. .. .. | ∈ ℝ . || 0 a(1) ... a(1)|| ⌈ n2 nn⌉ "/>
</div>
<p><span class="cmss-10x-x-109">Can we obtain </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(1)</span></sup> <span class="cmss-10x-x-109">from </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">via multiplication with some matrix; that is, can we find </span><span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(1)</span></sup> = <span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">holds?</span></p>
<p><span class="cmss-10x-x-109">Yes. By defining </span><span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">as</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(9).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(5.5)
  </div>
</div>
<p><span class="cmss-10x-x-109">we can see that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(1)</span></sup> = <span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is the same as performing the first step of Gaussian elimination. (Pick up a pen and paper and verify this by hand. It’s an excellent exercise.) </span><span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">lower diagonal</span><span class="cmss-10x-x-109">; that is, all elements above its diagonal are 0. In fact, except for the first column, all elements below the diagonal are 0 as well. (Note that </span><span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">depends on </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">By analogously defining</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(10).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(5.6)
  </div>
</div>
<p><span class="cmss-10x-x-109">we obtain </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(2)</span></sup> = <span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(1)</span></sup> = <span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, a matrix that is </span><span class="cmssi-10x-x-109">upper diagonal </span><span class="cmss-10x-x-109">in the first two columns. (That is, all elements are 0 below the diagonal, but only in the first two columns.)</span></p>
<p><span class="cmss-10x-x-109">We can continue this</span> <span id="dx1-94003"></span><span class="cmss-10x-x-109">process until we obtain the upper triangular matrix</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="math-display">
  <div class="math-display">
    A<sup>(n−1)</sup> = G<sub>n−1</sub>…G<sub>1</sub>A.
  </div>
  <label class="equation-label">(5.7)</label>
</div>

<p><span class="cmss-10x-x-109">The algorithm is starting to shape up nicely. The </span><span class="cmmi-10x-x-109">G</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">matrices are invertible, with inverses</span></p>

<img width="650" src="../media/file549.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ 1 0 0 ... 0 |1 0 0 ... 0| ||a21 || ||0 1 0 ... 0|| ||a11 1 0 ... 0|| || a(1) || |a31 0 1 ... 0| |0 3a(21) 1 ... 0| G −11= ||a1.1 . . . ||, G −21 = || . 22. . . || ,..., || .. .. .. .. || || .. .. .. .. || ||an1 0 0 ... 1|| || a(n12) || ⌈a11 ⌉ |⌈0 a(212) 0 ... 1|⌉ "/>

<p><span class="cmss-10x-x-109">and so on. Thus, by multiplying by their inverses one by one, we can express </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">as</span></p>
<div class="math-display">
<img src="../media/file550.png" class="math-display" alt=" −1 −1 (n−1) A = G 1 ...G n−1A . "/>
</div>
<p><span class="cmss-10x-x-109">Fortunately, we can calculate </span><span class="cmmi-10x-x-109">L </span>:= <span class="cmmi-10x-x-109">G</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">…G</span><sub><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">by hand. After a quick computation, we obtain</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(11).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(5.8)
  </div>
</div>
<p><span class="cmss-10x-x-109">which is lower diagonal. (Again, don’t be shy about verifying (</span><a href="ch011.xhtml#the-lu-decomposition"><span class="cmss-10x-x-109">5.8</span></a><span class="cmss-10x-x-109">) by hand.) By defining the upper diagonal matrix </span><span class="cmmi-10x-x-109">U </span>:= <span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1)</span></sup><span class="cmss-10x-x-109">, we obtain the famous LU decomposition, factoring </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">into a lower and upper diagonal matrix:</span></p>
<img src="../media/file552.png" class="math-display" alt="A = LU. " width="150"/>

<p><span class="cmss-10x-x-109">Notice that with this algorithm, we perform two tasks for the price of</span><span id="dx1-94004"></span> <span class="cmss-10x-x-109">one: factorizing </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">into the product of an upper diagonal and lower diagonal matrix, and performing Gaussian elimination.</span></p>
<p><span class="cmss-10x-x-109">From a computational standpoint, the LU decomposition is an extremely important tool. Since it is just a refashioned Gaussian elimination, its complexity is </span><span class="cmmi-10x-x-109">O</span>(<span class="cmmi-10x-x-109">n</span><sup><span class="cmr-8">3</span></sup>)<span class="cmss-10x-x-109">, just as we saw this earlier (</span><span class="cmssi-10x-x-109">Section </span><a href="ch011.xhtml#the-time-complexity-of-gaussian-elimination"><span class="cmssi-10x-x-109">5.1.4</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">Bad news: LU decomposition not always available. Since it is tied to Gaussian elimination, we can characterize its existence in similar terms. Recall that for the Gaussian elimination to successfully finish, the principal minors are required to be nonzero (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch011.xhtml#x1-90004r31"><span class="cmssi-10x-x-109">31</span></a><span class="cmss-10x-x-109">). This is directly transferred to the LU decomposition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-94005r34"></span> <span class="cmbx-10x-x-109">Theorem 34.</span> </span><span class="cmbxti-10x-x-109">(Existence of the LU decomposition)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an arbitrary square matrix, and let </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">k</span></sub> <span class="cmti-10x-x-109">be its </span><span class="cmmi-10x-x-109">k</span><span class="cmti-10x-x-109">-th principal minor. If </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">≠</span>0 <span class="cmti-10x-x-109">for all </span><span class="cmmi-10x-x-109">k </span>= 1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n </span><span class="cmsy-10x-x-109">− </span>1<span class="cmti-10x-x-109">, then </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file553.png" class="math-display" alt="A = LU, L, U ∈ ℝn×n, "/>
</div>
<p><span class="cmti-10x-x-109">where </span><span class="cmmi-10x-x-109">L </span><span class="cmti-10x-x-109">is a lower diagonal and </span><span class="cmmi-10x-x-109">U </span><span class="cmti-10x-x-109">is an upper diagonal matrix. Moreover, the elements along the diagonal of </span><span class="cmmi-10x-x-109">L </span><span class="cmti-10x-x-109">are equal to </span>1<span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">The gist is the same: everything is fine if we avoid division by 0 during the algorithm. Note that the LU algorithm doesn’t require a nonzero </span><span class="cmmi-10x-x-109">M</span><sub><span class="cmmi-8">n</span></sub> = det<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, that is, an invertible matrix!</span></p>
<p><span class="cmss-10x-x-109">After all this preparation, we are ready to put things into practice!</span></p>
<section id="implementing-the-lu-decomposition" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_78"><span class="titlemark"><span class="cmss-10x-x-109">5.2.1 </span></span> <span id="x1-950006.2.1"></span><span class="cmss-10x-x-109">Implementing the LU decomposition</span></h3>
<p><span class="cmss-10x-x-109">To summarize</span> <span id="dx1-95001"></span><span class="cmss-10x-x-109">the LU decomposition, it’s essentially the iteration of two steps:</span></p>
<ol>
<li><span id="x1-95003x1"><span class="cmss-10x-x-109">Calculate the elimination matrices of the input.</span></span></li>
<li><span id="x1-95005x2"><span class="cmss-10x-x-109">Multiply the input by the elimination matrices, feeding the output back into the first step.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">The plan is clear: first, we write a function that computes the elimination matrices and their inverses; then, we iteratively perform the elimination steps using matrix multiplication.</span></p>
<div id="tcolobox-153" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import numpy as np 
 
 
def elimination_matrix( 
    A: np.ndarray, 
    step: int, 
): 
    #x0022;"/span&gt; 
    Computes the step-th elimination matrix and its inverse. 
 
    Args: 
        A (np.ndarray): The matrix of shape (n, n) for which 
            the LU decomposition is being computed. 
        step (int): The current step of elimination, an integer 
            between 1 and n-1 
 
    Returns: 
        elim_mtx (np.ndarray): The step-th elimination matrix 
            of shape (n, n) 
        elim_mtx_inv (np.ndarray): The inverse of the 
            elimination matrix of shape (n, n) 
    #x0022;"/span&gt; 
 
    n = A.shape[0] 
    elim_mtx = np.eye(n) 
    elim_mtx_inv = np.eye(n) 
 
    if 0 /span&gt; step /span&gt; n: 
        a = A[:, step-1]/A[step-1, step-1] 
        elim_mtx[step:, step-1] = -a[step:] 
        elim_mtx_inv[step:, step-1] = a[step:] 
 
    return elim_mtx, elim_mtx_inv</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Now, we are ready to perform the elimination steps.</span></p>
<div id="tcolobox-154" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def LU(A: np.ndarray): 
    #x0022;"/span&gt; 
    Computes the LU factorization of a square matrix A. 
 
    Args: 
        A (np.ndarray): A square matrix of shape (n, n) to be factorized. 
            It must be non-singular (invertible) for the 
            decomposition to work. 
 
    Returns: 
        L (np.ndarray): A lower triangular matrix of shape (n, n) 
            with ones on the diagonal. 
        U (np.ndarray): An upper triangular matrix of shape (n, n). 
    #x0022;"/span&gt; 
 
    n = A.shape[0] 
    L = np.eye(n) 
    U = np.copy(A) 
 
    for step in range(1, n): 
        elim_mtx, elim_mtx_inv = elimination_matrix(U, step=step) 
        U = np.matmul(elim_mtx, U) 
        L = np.matmul(L, elim_mtx_inv) 
 
    return L, U</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s test</span> <span id="dx1-95064"></span><span class="cmss-10x-x-109">our function on a small matrix.</span></p>
<div id="tcolobox-155" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = 10*np.random.rand(4, 4) - 5 
A</code></pre>
</div>
</div>
<pre class="lstlisting"><code>array([[-4.61990165, -3.97616553, -1.34258661,  0.50835913], 
      [-2.39491833, -2.3919011 , -1.3266581 ,  2.8658852 ], 
      [ 4.32658116,  0.43607725,  4.41630776, -4.46731714], 
      [-0.68329877,  4.76659965, -1.13602896, -2.12305592]])</code></pre>
<div id="tcolobox-156" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>L, U = LU(A) 
 
print(f/span&gt;Lower:\n{L}\n\nUpper:\n{U}"</code></pre>
</div>
</div>
<pre class="lstlisting"><code>Lower: 
[[  1.          0.          0.          0.       ] 
 [  0.51839163   1.          0.          0.       ] 
 [ -0.93650936   9.94174964   1.          0.       ] 
 [  0.14790331 -16.19246049  -1.18248523   1.       ]] 
 
Upper: 
[[-4.61990165e+00 -3.97616553e+00 -1.34258661e+00  5.08359130e-01] 
 [ 0.00000000e+00 -3.30690182e-01 -6.30672445e-01  2.60235608e+00] 
 [ 0.00000000e+00  0.00000000e+00  9.42895038e+00 -2.98632067e+01] 
 [ 1.11022302e-16 -8.88178420e-16  1.77635684e-15  4.62750317e+00]]</code></pre>
<p><span class="cmss-10x-x-109">Is the result correct? Let’s test it by multiplying </span><span class="cmtt-10x-x-109">L </span><span class="cmss-10x-x-109">and</span><span id="dx1-95087"></span> <span class="cmtt-10x-x-109">U </span><span class="cmss-10x-x-109">together to see if it gives </span><span class="cmtt-10x-x-109">A </span><span class="cmss-10x-x-109">back.</span></p>
<div id="tcolobox-157" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>np.allclose(np.matmul(L, U), A)</code></pre>
</div>
</div>
<pre class="lstlisting"><code>True</code></pre>
<p><span class="cmss-10x-x-109">Overall, the LU decomposition is a highly versatile tool, used as a stepping stone in the implementation of essential algorithms. One of them is computing the inverse matrix, as we shall see next.</span></p>
</section>
<section id="inverting-a-matrix-for-real" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_79"><span class="titlemark"><span class="cmss-10x-x-109">5.2.2 </span></span> <span id="x1-960006.2.2"></span><span class="cmss-10x-x-109">Inverting a matrix, for real</span></h3>
<p><span class="cmss-10x-x-109">So far, we have talked a lot about the inverse matrix. We explored the</span><span id="dx1-96001"></span> <span class="cmss-10x-x-109">question of invertibility from several angles, in terms of the kernel and the image, the determinant, and the solvability of linear equations.</span></p>
<p><span class="cmss-10x-x-109">However, we haven’t yet talked about actually computing the inverse. With the LU decomposition, we obtain a tool that can be used for this purpose. How? By plugging in a lower triangular matrix into the Gaussian elimination process, we get its inverse as a side effect. So, we</span></p>
<ol>
<li><span id="x1-96003x1"><span class="cmss-10x-x-109">calculate the LU decomposition </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">LU</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-96005x2"><span class="cmss-10x-x-109">invert the lower triangular matrices </span><span class="cmmi-10x-x-109">L </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">U</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-96007x3"><span class="cmss-10x-x-109">use the identity </span>(<span class="cmmi-10x-x-109">U</span><sub><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub>)<span class="cmmi-10x-x-109">T </span>= (<span class="cmmi-10x-x-109">U</span><sub><span class="cmmi-8">T</span></sub> )<span class="cmsy-10x-x-109">−</span>1 <span class="cmss-10x-x-109">to get </span><span class="cmmi-10x-x-109">U</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-96009x4"><span class="cmss-10x-x-109">multiply </span><span class="cmmi-10x-x-109">L</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">U</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">to finally obtain </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> = <span class="cmmi-10x-x-109">U</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">L</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">That’s a plan! Let’s start with inverting lower triangular matrices.</span></p>
<p><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">L </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be an arbitrary lower triangular matrix. Following the same process that led to (</span><a href="ch011.xhtml#the-lu-decomposition"><span class="cmss-10x-x-109">5.7</span></a><span class="cmss-10x-x-109">), we obtain</span></p>
<div class="math-display">
<img src="../media/file554.png" class="math-display" alt="D = Gn −1...G1L, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">D</span><span class="cmss-10x-x-109">, the final result of Gaussian elimination, is a diagonal matrix</span></p>
<div class="math-display">
<img src="../media/file555.png" class="math-display" alt=" ⌊ ⌋ |d1 0 ... 0 | | 0 d2 ... 0 | D = diag(d1,...,dn) = || . . . . ||, |⌈ .. .. .. .. |⌉ 0 0 ... d n "/>
</div>
<p><span class="cmss-10x-x-109">and the </span><span class="cmmi-10x-x-109">G</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s are the</span> <span id="dx1-96010"></span><span class="cmss-10x-x-109">elimination matrices defined by (</span><a href="ch011.xhtml#the-lu-decomposition"><span class="cmss-10x-x-109">5.5</span></a><span class="cmss-10x-x-109">), (</span><a href="ch011.xhtml#the-lu-decomposition"><span class="cmss-10x-x-109">5.6</span></a><span class="cmss-10x-x-109">), and so on.</span></p>
<p><span class="cmss-10x-x-109">Since the inverse of </span><img src="../media/file556.png" class="math" alt="D "/> <span class="cmss-10x-x-109">is simply </span><img src="../media/file557.png" class="math" alt=" − 1 −1 −1 D = diag(d1 ,...,dn ) "/><span class="cmss-10x-x-109">, we can express</span> <img src="../media/file558.png" class="math" alt="L −1 "/> <span class="cmss-10x-x-109">as</span></p>
<div class="math-display">
<img src="../media/file559.png" class="math-display" alt="L −1 = D −1Gn− 1...G1. "/>
</div>
<p><span class="cmss-10x-x-109">We can implement this very similarly to the LU decomposition; we can even reuse our </span><span class="cmtt-10x-x-109">elimination_matrix </span><span class="cmss-10x-x-109">function.</span></p>
<div id="tcolobox-158" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def invert_lower_triangular_matrix(L: np.ndarray): 
    #x0022;"/span&gt; 
    Computes the inverse of a lower triangular matrix. 
 
    Args: 
        L (np.ndarray): A square lower triangular matrix of shape (n, n). 
                        It must have non-zero diagonal elements for the 
                        inversion to succeed. 
 
    Returns: 
        np.ndarray: The inverse of the lower triangular matrix L, with 
                        shape (n, n). 
    #x0022;"/span&gt; 
    n = L.shape[0] 
    G = np.eye(n) 
    D = np.copy(L) 
 
    for step in range(1, n): 
        elim_mtx, _ = elimination_matrix(D, step=step) 
        G = np.matmul(elim_mtx, G) 
        D = np.matmul(elim_mtx, D) 
 
    D_inv = np.eye(n)/np.diagonal(D)   # NumPy performs this operation elementwise 
 
    return np.matmul(D_inv, G)</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">With this done, we are ready to invert any matrix (that is actually invertible).</span></p>
<p><span class="cmss-10x-x-109">We are almost at the finish line. Every component is ready; the only thing left to do is to put them together. We can do this with a few lines of code.</span></p>
<div id="tcolobox-159" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def invert(A: np.ndarray): 
    #x0022;"/span&gt; 
    Computes the inverse of a square matrix using its LU decomposition. 
 
    Args: 
        A (np.ndarray): A square matrix of shape (n, n). The matrix must be 
                        non-singular (invertible) for the inversion to succeed. 
 
    Returns: 
        np.ndarray: The inverse of the input matrix A, with shape (n, n). 
    #x0022;"/span&gt; 
    L, U = LU(A) 
    L_inv = invert_lower_triangular_matrix(L) 
    U_inv = invert_lower_triangular_matrix(U.T).T 
    return np.matmul(U_inv, L_inv)</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Voilà! Witness</span><span id="dx1-96051"></span> <span class="cmss-10x-x-109">the result with your own eyes.</span></p>
<div id="tcolobox-160" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.random.rand(3, 3) 
A_inv = invert(A) 
 
print(f/span&gt;A:\n{A}\n\nA^{-1}:\n{A_inv}\n\nAA^{-1}:\n{np.matmul(A, A_inv)}"</code></pre>
</div>
</div>
<pre class="lstlisting"><code>A: 
[[0.17180745 0.79269571 0.36879642] 
 [0.37772174 0.94712553 0.55310582] 
 [0.93418085 0.38813821 0.51581695]] 
 
A^{-1}: 
[[  9.14230036  -8.87123133   2.97602074] 
 [ 10.74480189  -8.5427258    1.47801811] 
 [-24.64252111  22.49459369  -4.56327945]] 
 
AA^{-1}: 
[[ 1.00000000e+00 -1.37050081e-15  1.56962305e-17] 
 [-4.06841165e-16  1.00000000e+00 -4.50714074e-16] 
 [ 1.26848123e-14 -1.04564970e-14  1.00000000e+00]]</code></pre>
<p><span class="cmss-10x-x-109">To test the correctness of our </span><span class="cmtt-10x-x-109">invert </span><span class="cmss-10x-x-109">function, we quickly check the results on a few randomly generated matrices.</span></p>
<div id="tcolobox-161" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>for _ in range(1000): 
    n = np.random.randint(1, 10) 
    A = np.random.rand(n, n) 
    A_inv = invert(A) 
    if not np.allclose(np.matmul(A, A_inv), np.eye(n), atol=1e-5): 
        print("/span&gt;Test failed."</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Since there is no error message above, the function is (probably) correct.</span></p>
<p><span class="cmss-10x-x-109">What seemed</span> <span id="dx1-96076"></span><span class="cmss-10x-x-109">complex and abstract a few chapters ago is now in our hands. We can invert any matrix, not with built-in functions, but with one that we wrote from scratch. I love these moments when the pieces are finally put together, and everything clicks. Sit back, relax, and appreciate the journey that got us here!</span></p>
</section>
<section id="how-to-actually-invert-matrices" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_80"><span class="titlemark"><span class="cmss-10x-x-109">5.2.3 </span></span> <span id="x1-970006.2.3"></span><span class="cmss-10x-x-109">How to actually invert matrices</span></h3>
<p><span class="cmss-10x-x-109">Of course, our LU decomposition!matrices, invertingimplementation is far from optimal. When working with NumPy arrays, we can turn to the built-in functions. In NumPy, this is </span><span class="cmtt-10x-x-109">np.linalg.inv</span><span class="cmss-10x-x-109">.</span></p>
<div id="tcolobox-162" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.random.rand(3, 3) 
A_inv = np.linalg.inv(A) 
 
print(f/span&gt;A:\n{A}\n\nNumPy’s A^{-1}:\n{A_inv}\n\nAA^{-1}:\n{np.matmul(A, A_inv)}"</code></pre>
</div>
</div>
<pre class="lstlisting"><code>A: 
[[0.08503998 0.31186637 0.71032538] 
 [0.48973954 0.77358354 0.96303592] 
 [0.31250848 0.14359491 0.05593863]] 
 
NumPy’s A^{-1}: 
[[ 2.13348825 -1.89861153  5.59470678] 
 [-6.14268693  4.87769374 -5.97239945] 
 [ 3.84931433 -1.91423645  1.95236829]] 
 
AA^{-1}: 
[[ 1.00000000e+00 -1.86546922e-16  2.74435800e-16] 
 [-1.62367293e-16  1.00000000e+00  9.21871975e-17] 
 [-1.41854334e-18  3.64838601e-17  1.00000000e+00]]</code></pre>
<p><span class="cmss-10x-x-109">Let’s compare the runtime of our implementation and NumPy’s.</span></p>
<div id="tcolobox-163" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from timeit import timeit 
 
 
n_runs = 100 
size = 100 
A = np.random.rand(size, size) 
 
t_inv = timeit(lambda: invert(A), number=n_runs) 
t_np_inv = timeit(lambda: np.linalg.inv(A), number=n_runs) 
 
 
print(f/span&gt;Our invert:              \t{t_inv} s 
print(f/span&gt;NumPy’s invert:          \t{t_np_inv} s 
print(f/span&gt;Performance improvement: \t{t_inv/t_np_inv} times faster</code></pre>
</div>
</div>
<pre class="lstlisting"><code>Our invert:                    14.586225221995846 s 
NumPy’s invert:                0.46499890399718424 s 
Performance improvement:       31.368300218798304 times faster</code></pre>
<p><span class="cmss-10x-x-109">A massive improvement. Nice! (Don’t forget that the execution time isLU decomposition!matrices, inverting hardware-dependent.) Why is NumPy that much faster? There are two main reasons. First, it directly calls the </span><span class="cmtt-10x-x-109">SGETRI </span><span class="cmss-10x-x-109">function from LAPACK, which is extremely fast. Second, according to its documentation ( </span> <a href="https://www.netlib.org/lapack/explore-html/da/d28/group__getri_gaa3bf1bb1432917f0e5fdf4c48bd6998c.html" class="url"><span class="cmtt-10x-x-109">https://www.netlib.org/lapack/explore-html/da/d28/group__getri_gaa3bf1bb1432917f0e5fdf4c48bd6998c.html</span></a><span class="cmss-10x-x-109">), </span><span class="cmtt-10x-x-109">SGETRI </span><span class="cmss-10x-x-109">uses a faster algorithm:</span></p>
<div id="tcolobox-164" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>#x0022;"/span&gt; 
SGETRI computes the inverse of a matrix using the LU factorization 
computed by SGETRF. 
 
This method inverts U and then computes inv(A) by solving the system 
inv(A)*L = inv(U) for inv(A). 
 #x0022;"/span&gt;</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">So, NumPy calls the LAPACK function, which uses LU factorization in turn. (I am not particularly adept at digging through Fortran code that is older than I am, so let me know if I am wrong here. Nevertheless, the fact that state-of-the-art frameworks still make calls to this ancient library is a testament to its power. Never underestimate old technologies like LAPACK and Fortran.)</span></p>
<p><span class="cmss-10x-x-109">Are there any other applications of the powerful LU decomposition that we’ve just learned? Glad you asked! Of course there is; that’s the beauty of math. There’s always a new and unexpected application even for the oldest of tools. This time, we’ll finally see how to compute determinants fast! (And also slow.)</span></p>
</section>
</section>
<section id="determinants-in-practice" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_81"><span class="titlemark"><span class="cmss-10x-x-109">5.3 </span></span> <span id="x1-980006.3"></span><span class="cmss-10x-x-109">Determinants in practice</span></h2>
<p><span class="cmss-10x-x-109">In the theory</span><span id="dx1-98001"></span> <span class="cmss-10x-x-109">and practice of mathematics, the development of concepts usually has a simple flow. Definitions first arise from vague geometric or algebraic intuitions, eventually crystallizing in mathematical formalism.</span></p>
<p><span class="cmss-10x-x-109">However, mathematical definitions often disregard practicalities. For a very good reason, mind you! Keeping practical considerations out of sight gives us the power to reason about structure effectively. This is the strength of abstraction. Eventually, if meaningful applications are found, the development flows toward computational questions, putting speed and efficiency onto the horizon.</span></p>
<p><span class="cmss-10x-x-109">The epitome of this is neural networks themselves. From theoretical constructs to state-of-the-art algorithms that run on your smartphone, machine learning research followed this same arc.</span></p>
<p><span class="cmss-10x-x-109">This is also what we experience in this book on a microscopic level. Among many other examples, think about determinants. We introduced the determinant as the orientation of column vectors and the parallele-piped volume defined by them. Still, we haven’t really worked on computing them in practice. Sure, we gave a formula or two, but it is hard to decide which one is the most convoluted. All of them are.</span></p>
<p><span class="cmss-10x-x-109">On the other hand, the mathematical study of determinants yielded a ton of useful results: invertibility of linear transformations, characterization of Gaussian elimination, and many more (and even more to come.).</span></p>
<p><span class="cmss-10x-x-109">In this section, we are ready to pay off our debts and develop tools to actually compute determinants. As before, we will take a straightforward approach and use one of the previously derived determinant formulas. Spoiler alert: This is far from optimal, so we’ll find a way to compute the determinant with high speed.</span></p>
<section id="the-lesser-of-two-evils" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_82"><span class="titlemark"><span class="cmss-10x-x-109">5.3.1 </span></span> <span id="x1-990006.3.1"></span><span class="cmss-10x-x-109">The lesser of two evils</span></h3>
<p><span class="cmss-10x-x-109">Let’s recall</span> <span id="dx1-99001"></span><span class="cmss-10x-x-109">what we know about determinants (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch010.xhtml#x1-81006r20"><span class="cmssi-10x-x-109">20</span></a><span class="cmss-10x-x-109">) so far. Given a matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, its determinant</span> det<span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">quantifies the volume distortion of the linear transformation </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. That is, if </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is the standard orthonormal basis, then informally speaking,</span></p>
<img src="../media/file560.png" class="math-display" alt="det A = (orientation of Ae1, Ae2,...,Aen ) × (area of the parallelepiped determined by Ae1, Ae2,...,Aen ). " width="450"/>

<p><span class="cmss-10x-x-109">We have derived two formulas to compute this quantity. Initially, we described the determinant in terms of summing over all permutations:</span></p>
<div class="math-display">
<img src="../media/file561.png" class="math-display" alt="det A = ∑ sign(σ)a ...a . σ(1),1 σ(n),n σ∈Sn "/>
</div>
<p><span class="cmss-10x-x-109">This is difficult to understand, let alone to programmatically compute. So, a recursive formula is derived, which we can also use. It states that</span></p>
<div class="math-display">
<img src="../media/file562.png" class="math-display" alt=" ∑n detA = (− 1)j+1a1,j detA1,j, j=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i,j</span></sub> <span class="cmss-10x-x-109">is the matrix obtained by deleting the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. Which one would you rather use? Take a few minutes to figure out your reasoning.</span></p>
<p><span class="cmss-10x-x-109">Unfortunately, there are</span> <span id="dx1-99002"></span><span class="cmss-10x-x-109">no right choices here. With the permutation formula, one has to find a way to generate all permutations first, then calculate their signs. Moreover, there are </span><span class="cmmi-10x-x-109">n</span>! <span class="cmss-10x-x-109">unique permutations in </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, so this sum has a </span><span class="cmssi-10x-x-109">lot </span><span class="cmss-10x-x-109">of terms. Using this formula seems extremely difficult, so we will go with the recursive version. Recursion has its issues (as we are about to see very soon), but it is easy to handle from a coding standpoint. Let’s get to work!</span></p>
</section>
<section id="the-recursive-way" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_83"><span class="titlemark"><span class="cmss-10x-x-109">5.3.2 </span></span> <span id="x1-1000006.3.2"></span><span class="cmss-10x-x-109">The recursive way</span></h3>
<p><span class="cmss-10x-x-109">Let’s put the formula</span></p>
<div class="math-display">
<img src="../media/file563.png" class="math-display" alt=" ∑n detA = (− 1)j+1a1,j det A1,j j=1 "/>
</div>
<p><span class="cmss-10x-x-109">under our</span> <span id="dx1-100001"></span><span class="cmss-10x-x-109">magnifying glass. If </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is an </span><span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">matrix, then </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,j</span></sub> <span class="cmss-10x-x-109">(obtained from </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">by deleting its first row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column) is of size </span>(<span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">− </span>1) <span class="cmsy-10x-x-109">× </span>(<span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">− </span>1)<span class="cmss-10x-x-109">. This is a recursive step. For each </span><span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">determinant, we have to calculate </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">pieces of </span>(<span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">− </span>1) <span class="cmsy-10x-x-109">× </span>(<span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">− </span>1) <span class="cmss-10x-x-109">determinants, and so on.</span></p>
<p><span class="cmss-10x-x-109">By the end, we have a lots of </span>1 <span class="cmsy-10x-x-109">× </span>1 <span class="cmss-10x-x-109">determinants, which are trivial to calculate. So, we have a boundary condition, and with that, we are ready to put these together.</span></p>
<div id="tcolobox-165" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def det(A: np.ndarray): 
    #x0022;"/span&gt; 
    Recursively computes the determinant of a square matrix A. 
 
    Args: 
        A (np.ndarray): A square matrix of shape (n, n) for which the 
        determinant is to be calculated. 
 
    Returns: 
        float: The determinant of matrix A. 
 
    Raises: 
        ValueError: If A is not a square matrix. 
    #x0022;"/span&gt; 
 
    n, m = A.shape 
 
    # making sure that A is a square matrix 
    if n != m: 
        raise ValueError("/span&gt;A must be a square matrix." 
 
    if n == 1: 
        return A[0, 0] 
 
    else: 
        return sum([(-1)**j*A[0, j]*det(np.delete(A[1:], j, axis=1)) for j in range(n)])</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s test the </span><span class="cmtt-10x-x-109">det </span><span class="cmss-10x-x-109">function out on a small example. For </span>2 <span class="cmsy-10x-x-109">× </span>2 <span class="cmss-10x-x-109">matrices, we can easily calculate the determinants using the rule</span></p>
<div class="math-display">
<img src="../media/file564.png" class="math-display" alt=" ⌊ ⌋ a b det⌈ ⌉ = ad − bc. c d "/>
</div>
<div id="tcolobox-166" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.array([[1, 2], 
              [3, 4]]) 
det(A)    # should be -2</code></pre>
</div>
</div>
<pre class="lstlisting"><code>np.int64(-2)</code></pre>
<p><span class="cmss-10x-x-109">It seems to work. So far, so good. What is the issue? Recursion. Let’s calculate</span> <span id="dx1-100032"></span><span class="cmss-10x-x-109">the determinant of a small </span>10 <span class="cmsy-10x-x-109">× </span>10 <span class="cmss-10x-x-109">matrix, measuring the time it takes.</span></p>
<div id="tcolobox-167" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from timeit import timeit 
 
A = np.random.rand(10, 10) 
t_det = timeit(lambda: det(A), number=1) 
 
print(f/span&gt;The time it takes to compute the determinant of a 10 x 10 matrix: {t_det} seconds</code></pre>
</div>
</div>
<pre class="lstlisting"><code>The time it takes to compute the determinant of a 10 x 10 matrix: 
63.98369195000123 seconds</code></pre>
<p><span class="cmss-10x-x-109">That was long and unbearable. For such a simple task, this feels like an eternity.</span></p>
<p><span class="cmss-10x-x-109">For </span><span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">inputs, we call the </span><span class="cmtt-10x-x-109">det </span><span class="cmss-10x-x-109">function recursively </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times, on </span>(<span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">− </span>1) <span class="cmsy-10x-x-109">× </span>(<span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">− </span>1) <span class="cmss-10x-x-109">inputs. That is, if </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">denotes the time complexity of our algorithm for an </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">matrix, then, due to the recursive step, we have</span></p>
<div class="math-display">
<img src="../media/file565.png" class="math-display" alt="an = nan−1, "/>
</div>
<p><span class="cmss-10x-x-109">which explodes really fast. In fact, </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">n</span></sub> = <span class="cmmi-10x-x-109">O</span>(<span class="cmmi-10x-x-109">n</span>!)<span class="cmss-10x-x-109">, which is the dreaded factorial complexity. Unlike some other recursive algorithms, </span><a href="https://docs.python.org/3/library/functools.html#functools.cache"><span class="cmss-10x-x-109">caching</span></a> <span class="cmss-10x-x-109">doesn’t help either. There are two reasons for this: sub-matrices rarely match, and </span><span class="cmtt-10x-x-109">numpy.ndarray </span><span class="cmss-10x-x-109">objects are mutable, thus not hashable.</span></p>
<p><span class="cmss-10x-x-109">In practice, </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">can be in the millions, so this formula is utterly useless. What can we do? Simple: LU decomposition.</span></p>
</section>
<section id="how-to-actually-compute-determinants" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_84"><span class="titlemark"><span class="cmss-10x-x-109">5.3.3 </span></span> <span id="x1-1010006.3.3"></span><span class="cmss-10x-x-109">How to actually compute determinants</span></h3>
<p><span class="cmss-10x-x-109">Besides the two</span> <span id="dx1-101001"></span><span class="cmss-10x-x-109">formulas, we have seen lots of useful properties of matrices and determinants. Can we apply what we have learned so far to simplify the problem?</span></p>
<p><span class="cmss-10x-x-109">Let’s consider the LU decomposition. According to this, if</span> det<span class="cmmi-10x-x-109">A≠</span>0<span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">LU</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">L </span><span class="cmss-10x-x-109">is lower triangular and </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">is upper triangular. Since the determinant behaves nicely with respect to matrix multiplication (see equation (4.11)), we have</span></p>
<div class="math-display">
<img src="../media/file566.png" class="math-display" alt="detA = detL detU. "/>
</div>
<p><span class="cmss-10x-x-109">Seemingly, we made our situation worse: instead of one determinant, we have to deal with two. However, </span><span class="cmmi-10x-x-109">L </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">are rather special, as they are triangular. It turns out that computing a triangular matrix’s determinant is extremely easy. We just have to multiply the elements on the diagonal together!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-101002r35"></span> <span class="cmbx-10x-x-109">Theorem 35.</span> </span><span class="cmbxti-10x-x-109">(Determinant of a triangular matrix)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a triangular matrix. (That is, it is either lower or upper triangular.) Then,</span></p>
<div class="math-display">
<img src="../media/file567.png" class="math-display" alt=" n ∏ det A = aii. i=1 "/>
</div>
</div>
<div id="tcolobox-168" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof.</span><span class="cmss-10x-x-109">Suppose that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is lower triangular. (That is, all elements above its diagonal are 0.) According to the recursive formula for</span> det<span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file568.png" class="math-display" alt=" ∑n detA = (− 1)j+1a1,j detA1,j. j=1 "/>
</div>
<p><span class="cmss-10x-x-109">Because </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is lower triangular, </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,j</span></sub> = 0 <span class="cmss-10x-x-109">if </span><span class="cmmi-10x-x-109">j &gt;</span>1<span class="cmss-10x-x-109">. Thus,</span></p>
<div class="math-display">
<img src="../media/file569.png" class="math-display" alt="detA = a11 detA1,1. "/>
</div>
<p><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,</span><span class="cmr-8">1</span></sub> = (<span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">ij</span></sub>)<sub><span class="cmmi-8">i,j</span><span class="cmr-8">=2</span></sub><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is also lower triangular. By iterating the previous step, we obtain</span></p>
<div class="math-display">
<img src="../media/file570.png" class="math-display" alt="detA = a11a22 ...ann, "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is upper triangular, its transpose </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">is lower triangular. Thus, we can apply the previous result, so</span></p>
<div class="math-display">
<img src="../media/file571.png" class="math-display" alt="detA = detAT = a11a22...ann "/>
</div>
<p><span class="cmss-10x-x-109">holds as well.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Back to our original problem. Since the diagonal </span><span class="cmmi-10x-x-109">L </span><span class="cmss-10x-x-109">is constant </span>1 <span class="cmss-10x-x-109">(see (</span><a href="ch011.xhtml#the-lu-decomposition"><span class="cmss-10x-x-109">5.8</span></a><span class="cmss-10x-x-109">)), as guaranteed by the LU decomposition, we have</span></p>
<div class="math-display">
<img src="../media/file572.png" class="math-display" alt=" ∏n det A = detU = uii. i=1 "/>
</div>
<p><span class="cmss-10x-x-109">So, the</span> <span id="dx1-101003"></span><span class="cmss-10x-x-109">algorithm to compute the determinant is quite simple: get the LU decomposition, then calculate the product of </span><span class="cmmi-10x-x-109">U</span><span class="cmss-10x-x-109">’s diagonal. Let’s put this into practice!</span></p>
<div id="tcolobox-169" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def fast_det(A: np.ndarray): 
    #x0022;"/span&gt; 
    Computes the determinant of a square matrix using LU decomposition. 
 
    Args: 
        A (np.ndarray): A square matrix of shape (n, n) for which the determinant 
                         needs to be computed. The matrix must be non-singular (invertible). 
 
    Returns: 
        float: The determinant of the matrix A. 
    #x0022;"/span&gt; 
    L, U = LU(A) 
    return np.prod(np.diag(U))</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Yes, that simple. Let’s see how it performs!</span></p>
<div id="tcolobox-170" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.random.rand(10, 10) 
 
t_fast_det = timeit(lambda : fast_det(A), number=1) 
print(f/span&gt;The time it takes to compute the determinant of a 10 x 10 matrix: {t_fast_det} seconds</code></pre>
</div>
</div>
<p><span id="dx1-101021"></span></p>
<pre class="lstlisting"><code>The time it takes to compute the determinant of a 10 x 10 matrix: 
0.0008458310039713979 seconds</code></pre>
<p><span class="cmss-10x-x-109">It’s faster by a huge margin. How much faster?</span></p>
<div id="tcolobox-171" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>print(f/span&gt;Recursive determinant:   \t{t_det} s 
print(f/span&gt;LU determinant:          \t{t_fast_det} s 
print(f/span&gt;Performance improvement: \t{t_det/t_fast_det} times faster</code></pre>
</div>
</div>
<pre class="lstlisting"><code>Recursive determinant:         63.98369195000123 s 
LU determinant:                0.0008458310039713979 s 
Performance improvement:       75645.95250065446 times faster</code></pre>
<p><span class="cmss-10x-x-109">That’s quite an insane improvement! This can be even faster if we use a better implementation of the LU decomposition algorithm (for instance, </span><span class="cmtt-10x-x-109">scipy.linalg.lu</span><span class="cmss-10x-x-109">, which relies on our old friend LAPACK).</span></p>
<p><span class="cmss-10x-x-109">I get emotional just by looking at this result. See how far we can go with a bit of linear algebra? This is why understanding the fundamentals such as</span> <span id="dx1-101030"></span><span class="cmss-10x-x-109">Gaussian elimination is essential. Machine learning and deep learning are still very new fields, and even though an insane amount of research power is being put into it, moments like these happen all the time. Simple ideas often give birth to new paradigms.</span></p>
</section>
</section>
<section id="summary4" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_85"><span class="titlemark"><span class="cmss-10x-x-109">5.4 </span></span> <span id="x1-1020006.4"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">In this chapter, we have looked at matrices from the perspective of linear equation systems, i.e., equations of the form</span></p>
<div class="math-display">
<img src="../media/file573.png" class="math-display" alt="a11x1 + a12x2 + ⋅⋅⋅+ a1nxn = b1 a21x1 + a22x2 + ⋅⋅⋅+ a2nxn = b2 . .. an1x1 + an2x2 + ⋅⋅⋅+ annxn = bn. "/>
</div>
<p><span class="cmss-10x-x-109">Not surprisingly, these are described by matrices, and the above is equivalent to the expression </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">. Solving linear equations is an ancient art, so why are we talking about it in the age of AI?</span></p>
<p><span class="cmss-10x-x-109">Remember: It’s only AI if you are talking to investors. Deep down, it’s linear algebra, calculus, and probability theory.</span></p>
<p><span class="cmss-10x-x-109">We wanted to solve linear equations, which led us to Gaussian elimination (well, led </span><span class="cmssi-10x-x-109">Gauss </span><span class="cmss-10x-x-109">to Gaussian elimination). Which led us to the LU decomposition. Which led us to fast matrix inversion, and a bunch of other innovations on which our current technology is built on. Let me tell you, fast matrix multiplication and inversion are the bread and butter of computational linear algebra, and they all stem from that aforementioned ancient art of solving linear equations.</span></p>
<p><span class="cmss-10x-x-109">Let’s recap the feats that we’ve achieved in this chapter one by one:</span></p>
<ul>
<li><span class="cmss-10x-x-109">solving linear equations by Gaussian elimination,</span></li>
<li><span class="cmss-10x-x-109">characterizing the invertibility of matrices in terms of linear equations,</span></li>
<li><span class="cmss-10x-x-109">discovering a matrix factorization technique called LU decomposition,</span></li>
<li><span class="cmss-10x-x-109">building a crazy-fast matrix-inverting algorithm using the LU decomposition,</span></li>
<li><span class="cmss-10x-x-109">and building a crazy-fast determinant-computing algorithm using – drumroll! – the LU decomposition.</span></li>
</ul>
<p><span class="cmss-10x-x-109">However, we are not done with matrices. Recall the relationship we established between them and linear transformations, viewing matrices as data transforms that distort the underlying feature space. As it turns out, if we are looking from the right perspective, this distortion is always a stretching. Well, almost always. Well, almost a stretching.</span></p>
<p><span class="cmss-10x-x-109">Well, that was too many </span><span class="cmssi-10x-x-109">well</span><span class="cmss-10x-x-109">s, so let’s clear all of it up in the next chapter, diving into </span><span class="cmssi-10x-x-109">eigenvalues </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">eigenvectors</span><span class="cmss-10x-x-109">. Let’s go!</span></p>
</section>
<section id="problems4" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_86"><span class="titlemark"><span class="cmss-10x-x-109">5.5 </span></span> <span id="x1-1030006.5"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Show that the product of upper triangular matrices is upper triangular. Similarly, show that the product of lower triangular matrices is lower triangular. (We have used these facts extensively in this section but didn’t give a proof. So, this is an excellent time to convince yourself about this if you haven’t already.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Write a function that, given an invertible square matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">and a vector </span><span class="cmbx-10x-x-109">b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, finds the solution of the linear equation </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">b</span><span class="cmss-10x-x-109">. (This can be done with a one-liner if you use one of the tools we have built here.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Before we wrap this chapter up, let’s go back to the definition of determinants. Even though there are lots of reasons against using the determinant formula, we have one for it: it is a good exercise, and implementing it will deepen your understanding. So, in this problem, you are going to build</span></p>
<div class="math-display">
<img src="../media/file574.png" class="math-display" alt=" ∑ detA = sign(σ)aσ(1)1 ...a σ(n)n, σ∈Sn "/>
</div>
<p><span class="cmss-10x-x-109">one step at a time.</span></p>
<p><span class="cmssi-10x-x-109">(i) </span><span class="cmss-10x-x-109">Implement a function that, given an integer </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">, returns all permutations of the set </span><span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,…,n </span><span class="cmsy-10x-x-109">− </span>1<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. Represent each permutation </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">as a list. For example,</span></p>
<pre id="verbatim-2" class="verbatim"><code>[2, 0, 1]</code></pre>
<p><span class="cmss-10x-x-109">would represent the permutation </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">σ</span>(0) = 2<span class="cmmi-10x-x-109">,σ</span>(1) = 0<span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">σ</span>(2) = 1<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(ii) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">σ </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">be a permutation of the set </span><span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,…,n </span><span class="cmsy-10x-x-109">− </span>1<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. Its </span><span class="cmssi-10x-x-109">inversion number </span><span class="cmss-10x-x-109">is defined by</span></p>
<div class="math-display">
<img src="../media/file575.png" class="math-display" alt="inversion(σ) = |{(i,j) : i &lt;j and σ(i) &gt;σ (j)}|, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">j </span><span class="cmsy-10x-x-109">⋅</span><span class="cmmi-10x-x-109">j </span><span class="cmss-10x-x-109">denotes the number of elements in the set. Essentially, inversion describes the number of times a permutation reverses the order of a pair of numbers.</span></p>
<p><span class="cmss-10x-x-109">Turns out, the sign of </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file576.png" class="math-display" alt="sign(σ) = (− 1)inversion(σ). "/>
</div>
<p><span class="cmss-10x-x-109">Implement a function that first calculates the inversion number, then the sign of an arbitrary permutation. (Permutations are represented like in the previous problem.)</span></p>
<p><span class="cmssi-10x-x-109">(iii) </span><span class="cmss-10x-x-109">Put the solutions for Problem 1. and Problem 2. together and calculate the determinant of a matrix using the permutation formula. What do you think the time complexity of this algorithm is?</span></p>
</section>
<section id="join-our-community-on-discord5" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_87"><span id="x1-104000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>