- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matrices and Equations
  prefs: []
  type: TYPE_NORMAL
- en: So, matrices are not just tables of numbers but linear transformations; we have
    spent a lengthy chapter discovering this relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Now, I want us to circle back to the good old tables of numbers once more, but
    representing systems of linear equations this time. Why? Simple. Because solving
    linear equations is the motivator behind key theoretical and technical innovations.
    In the previous chapter, we talked about inverse matrices but didn’t compute one
    in practice. With what we’re about to learn, we will be able to not only compute
    inverse matrices but do so blazing fast.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get to work!
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Linear equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, we can translate several problems into linear equations. For example,
    a cash dispenser has $900 in $20 and $50 bills. We know that there are twice as
    many $20 bills than $50\. The question is, how many of each bill does the machine
    have?
  prefs: []
  type: TYPE_NORMAL
- en: If we denote the number of $20 bills by x[1] and the number of $50 bills by
    x[2], we obtain the equations
  prefs: []
  type: TYPE_NORMAL
- en: '![ x1 − 2x2 = 0 20x1 + 50x2 = 900\. ](img/file502.png)'
  prefs: []
  type: TYPE_IMG
- en: For two variables, as we have now, these are easily solvable by expressing one
    in terms of the other. Here, the first equation would imply x[1] = 2x[2]. Plugging
    it back into the second equation, we obtain 90x[2] = 900, which gives x[2] = 10\.
    Coming full circle, we can substitute this into x[1] = 2x[2], yielding the solutions
  prefs: []
  type: TYPE_NORMAL
- en: '![x = 20 1 x2 = 10\. ](img/file503.png)'
  prefs: []
  type: TYPE_IMG
- en: However, for thousands of variables like in real applications, we need a bit
    more craft. This is where linear algebra comes in. By introducing the matrix and
    vectors
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ A = ⌈ 1 − 2⌉ , x = ⌈x1 ⌉, b = ⌈ 0 ⌉ , 20 50 x2 900 ](img/file504.png)'
  prefs: []
  type: TYPE_IMG
- en: 'the equation can be written in the form Ax = b. That is, in terms of linear
    transformations, we can reformulate the question: which vector x is mapped to
    b by the transformation A? This question is central in linear algebra, and we’ll
    dedicate this section to solving it.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Gaussian elimination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s revisit our earlier example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ x1 − 2x2 = 0 20x1 + 50x2 = 900\. ](img/file505.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use the first equation x[1] − 2x[2] = 0 to get rid of the term x[1] in
    the second equation 20x[1] + 50x[2] = 900\. We can do this by multiplying it by
    20 and subtracting it from the second row, obtaining 90x[2] = 900, from which
    x[2] = 10 is obtained. This can be substituted back into the first row, yielding
    x[1] = 20.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about the general case? Would this work for a general A ∈ℝ^(n×n) and x,b
    ∈ℝ^n? Absolutely. So far, we have used two rules for manipulating the equations
    in a linear system:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying an equation with a nonzero scalar won’t change the solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding a scalar multiple of one row to another won’t change the solutions either.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Earlier, we applied these repeatedly to eliminate variables progressively in
    our simple example. We can easily do the same for n variables! First, let’s see
    what we are talking about!
  prefs: []
  type: TYPE_NORMAL
- en: Definition 21\. (System of linear equations)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ ℝ^(n×n) be a matrix and b ∈ ℝ^n be a vector. The collection of equations
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation image](img/file506.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.1)
  prefs: []
  type: TYPE_NORMAL
- en: are called the system of linear equations determined by A and b.
  prefs: []
  type: TYPE_NORMAL
- en: A system of linear equations is often written in the short form Ax = b, where
    A is called its coefficient matrix. If the vector x satisfies Ax = b, it is called
    a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of solutions, are there even any, and if so, how can we find them?
  prefs: []
  type: TYPE_NORMAL
- en: If a[11] is nonzero, we can multiply the first equation of ([5.1](ch011.xhtml#x1-88005r21))
    by ![aak111](img/file507.png) and subtract it from the k-th equation.
  prefs: []
  type: TYPE_NORMAL
- en: This way, x[1] will be eliminated from all but the first row, obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation image](img/file508.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.2)
  prefs: []
  type: TYPE_NORMAL
- en: To clear up this notation a bit, let’s denote the new coefficients with a[ij]^((1))
    and b[i]^((1)). So, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/file509.png)(5.3)'
  prefs: []
  type: TYPE_IMG
- en: We can repeat the above process and use the second equation to get rid of the
    x[2] variable in the third equation, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done n − 1 times in total, ultimately leading to an equation system
    A^((n−1))x = b^((n−1)) where all coefficients below the diagonal of A^((n−1))
    are 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(8).png)(5.4)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the k-th elimination step only affects the coefficients from the
    (k + 1)-th row. Now we can work backward: the last equation a[nn]^((n−1))x[n]
    = b[n]^((n−1)) can be used to find x[n]. This can be substituted to the (n − 1)-th
    equation, yielding x[n−1]. Continuing like this, we can eventually find all x[1],…,x[n],
    obtaining a solution for our linear system.'
  prefs: []
  type: TYPE_NORMAL
- en: This process is called Gaussian elimination, and it’s kind of a big deal. It
    is not only useful for solving linear equations, but it can also be used to calculate
    determinants, factor matrices into the product of simpler ones, and much more.
    We’ll talk about all of this in detail, but let’s focus on equations a little
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, not all linear equations can be solved. For instance, consider
    the system
  prefs: []
  type: TYPE_NORMAL
- en: '![ x1 + x2 = 1 2x1 + 2x2 = − 1\. ](img/file511.png)'
  prefs: []
  type: TYPE_IMG
- en: Subtracting the first equation from the second one yields
  prefs: []
  type: TYPE_NORMAL
- en: '![x1 + x2 = 1 x1 + x2 = − 2 ](img/file512.png)'
  prefs: []
  type: TYPE_IMG
- en: in the very first step, making it apparent that the equation has no solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we turn to the technical details, let’s see a simple example of how Gaussian
    elimination is done in practice!
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Gaussian elimination by hand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To build a deeper understanding of Gaussian elimination, let’s consider the
    simple equation system
  prefs: []
  type: TYPE_NORMAL
- en: '![ x + 0x − 3x = 6 1 2 3 2x1 + 1x2 + 5x3 = 2 − 2x1 − 3x2 + 8x3 = 2\. ](img/file513.png)'
  prefs: []
  type: TYPE_IMG
- en: To keep track of our progress (and, since we are lazy, to avoid writing too
    much), we record the intermediate results as
  prefs: []
  type: TYPE_NORMAL
- en: '![ | 1 0 − 3|6 2 1 5 |2 | − 2 − 3 8 |2 ](img/file514.png)'
  prefs: []
  type: TYPE_IMG
- en: with the coefficient matrix A on the left side and b on the other. To get a
    good grip on the method, I encourage you to follow along and do the calculations
    yourself by hand.
  prefs: []
  type: TYPE_NORMAL
- en: After eliminating the first variable from the second and third equations, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![ | 1 0 − 3 | 6 0 1 11 |− 10 | , 0 − 3 2 | 14 ](img/file515.png)'
  prefs: []
  type: TYPE_IMG
- en: while the final step yields
  prefs: []
  type: TYPE_NORMAL
- en: '![ | 1 0 − 3| 6 0 1 11 |− 10 | . 0 0 35 |− 16 ](img/file516.png)'
  prefs: []
  type: TYPE_IMG
- en: From this form, we can unravel the solutions one by one.
  prefs: []
  type: TYPE_NORMAL
- en: In the 21st century, your chances of having to solve a linear equation by hand
    are close to 0\. (If you are reading this book during the 22nd century or later,
    I am incredibly honored and surprised at the same time. Or, at least, I would
    be if I were still alive.) Still, understanding the general principles behind
    solving linear equations can take you very far.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 When can we perform Gaussian elimination?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you followed the description of Gaussian elimination carefully, you might
    have noticed that the process can break down. We might accidentally divide by
    0 during any elimination step!
  prefs: []
  type: TYPE_NORMAL
- en: For instance, after the first step given by equation ([5.2](ch011.xhtml)), the
    new coefficients are of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![(aij − a1j ai1), a11 ](img/file517.png)'
  prefs: []
  type: TYPE_IMG
- en: which is invalid if a[11] = 0\. In general, the k-th step involves division
    by a[kk]^((k−1)). Since a[kk]^((k−1)) is defined recursively, describing it in
    terms of A is not straightforward. For this, we introduce the concept of principal
    minors, the upper-left subdeterminants of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 22\. (Principal minors)
  prefs: []
  type: TYPE_NORMAL
- en: Let ![ n n×n A = (aij)i,j=1 ∈ ℝ ](img/file518.png) be an arbitrary square matrix.
    Define the submatrix ![Ak ∈ ℝk×k ](img/file519.png) by omitting all rows and columns
    of ![A ](img/file520.png) with indices larger than ![k ](img/file521.png). For
    instance,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ [ ] ⌈a11 a12⌉ A1 = a11 , A2 = a a , 21 22 ](img/file522.png)'
  prefs: []
  type: TYPE_IMG
- en: and so on. The k-th principal minor of A, denoted by M[k], is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![Mk := detAk. ](img/file523.png)'
  prefs: []
  type: TYPE_IMG
- en: The first and last principal minors are special, as M[1] = a[11] and M[n] =
    detA. With principal minors, we can describe when Gaussian elimination is possible.
    In fact, it turns out that
  prefs: []
  type: TYPE_NORMAL
- en: '![ M M a11 = M1, a(21)2 = --2, ...,a(nnn−1)= ---n-- M1 Mn −1 ](img/file524.png)'
  prefs: []
  type: TYPE_IMG
- en: and, in general, a[kk]^((k−1)) = ![-Mk-- Mk−1](img/file525.png).
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we can state the following.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 30\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![ n×n A ∈ ℝ ](img/file526.png) be an arbitrary square matrix, and let ![Mk
    ](img/file527.png) be its ![k ](img/file528.png)-th principal minor. If ![Mk ⁄=
    0 ](img/file529.png) for all ![k = 1,2,...,n − 1 ](img/file530.png), then Gaussian
    elimination can be successfully performed.
  prefs: []
  type: TYPE_NORMAL
- en: As the proof is a bit involved, we are not going to do it here. (The difficult
    step is showing a[kk]^((k−1)) = M[k]∕M[k−1];’ the rest follows immediately.) The
    point is, if none of the principal minors are 0, the algorithm finishes.
  prefs: []
  type: TYPE_NORMAL
- en: We can simplify this requirement a bit and describe the Gaussian elimination
    in terms of the determinant, not the principal minors.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 31\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary square matrix. If detA≠0, then all principal
    minors are nonzero as well.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, if the determinant is nonzero, the Gaussian elimination is
    successful. A simple and nice requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 The time complexity of Gaussian elimination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get a handle on how fast the Gaussian elimination algorithm executes, let’s
    do a little complexity analysis. As described by ([5.2](ch011.xhtml)), the first
    elimination step involves an addition and a multiplication for each element, except
    for those in the first row. That is 2n(n − 1) operations in total.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is essentially the first step, done on the (n − 1) × (n − 1) matrix
    obtained from A^((1)) by removing its first row and column. This time, we have
    2(n − 1)(n − 2) operations.
  prefs: []
  type: TYPE_NORMAL
- en: Following this train of thought, we quickly get that the total number of operations
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![∑n 2(n − i+ 1)(n− i), i=1 ](img/file531.png)'
  prefs: []
  type: TYPE_IMG
- en: which doesn’t look that friendly. Since we are looking for the order of complexity
    instead of an exact number, we can be generous and suppose that at each elimination
    step, we are performing O(n²) operations. So, we have a time complexity of
  prefs: []
  type: TYPE_NORMAL
- en: '![∑n O(n2) = O (n3 ), i=1 ](img/file532.png)'
  prefs: []
  type: TYPE_IMG
- en: meaning that we need around cn³ operations for Gaussian elimination, where c
    is an arbitrary positive constant. This might seem a lot, but in the beautiful
    domain of algorithms, this is good. O(n³) is polynomial time, and we can have
    much, much worse.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.5 When can a system of linear equations be solved?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, we have just seen that for any linear equation
  prefs: []
  type: TYPE_NORMAL
- en: '![Ax = b, A ∈ ℝn×n, x, b ∈ ℝn, ](img/file533.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gaussian elimination can be successfully performed if the principal minors
    M[1],…,M[n−1] are nonzero. Notice one caveat about the result: M[n] = detA can
    be zero as well. Turns out, this is quite an important detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have closely followed the discussion leading up to this point, you will
    see that we missed a crucial point: are there any solutions at all for a given
    linear equation? There are three options:'
  prefs: []
  type: TYPE_NORMAL
- en: There are no solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is exactly one solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are multiple solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All of these are relevant to us from a certain perspective, but let’s start
    with the most straightforward one: when do we have exactly one solution? The answer
    is simple: when A is invertible, the solution can be explicitly written as x =
    A^(−1)b. Speaking in terms of linear transformations, we can find a unique vector
    x that is mapped to b. We summarize this idea in the following theorem.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 32\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![A ∈ ℝn ×n ](img/file534.png) be an invertible matrix. Then, for any ![b
    ∈ ℝn ](img/file535.png), the equation ![Ax = b ](img/file536.png) has a unique
    solution that can be written as ![x = A− 1b ](img/file537.png).
  prefs: []
  type: TYPE_NORMAL
- en: If A is invertible, then detA is nonzero. Thus, using what we have learned previously,
    Gaussian elimination can be performed, yielding the unique solution. Nice and
    simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'If A is not invertible, the two remaining possibilities are in play: no vector
    is mapped to b, which means there are no solutions, or multiple vectors are mapped
    to b, giving infinite solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: Do you remember how we used the kernel of a linear transformation to describe
    its invertibility in Theorem [20](ch010.xhtml#x1-70003r20)? It turns out that
    kerA can also be used to find all solutions for a linear system.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 33\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary matrix and let x[0] ∈ℝ^n be a solution to the
    linear equation Ax = b, where b ∈ℝ^n. Then, the set of all solutions can be written
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![x0 + kerA := {x0 + y : y ∈ kerA}. ](img/file538.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Proof. We have to show two things: (a) if x ∈x[0] + kerA, then x is a solution;
    and (b) if x is a solution, then x ∈x[0] + kerA.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Suppose that x ∈ x[0] + kerA, that is, x = x[0] + y for some y ∈ kerA. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![Ax = A(x0 + y) = Ax0 + Ay = b, ◟=◝b◜◞ ◟◝=◜0◞ ](img/file539.png)'
  prefs: []
  type: TYPE_IMG
- en: which shows that x is indeed a solution.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Now let x be an arbitrary solution. We have to show that x −x[0] ∈ kerA.
    This is easy, since
  prefs: []
  type: TYPE_NORMAL
- en: '![A (x− x0) = Ax − Ax0 = b − b = 0\. ](img/file540.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, (a) and (b) imply that x[0] + kerA is the set of all solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, this theorem provides an excellent way of finding all solutions for
    linear equations, generalizing far beyond finite-dimensional vector spaces. (Note
    that the proof goes through verbatim for all vector spaces and linear transformations.)
    For instance, this exact result is used to describe all solutions of an inhomogeneous
    linear differential equation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.6 Inverting matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have seen that the invertibility of a matrix A ∈ℝ^(n×n) is key to
    solving linear equations. However, we haven’t found a way to compute the inverse
    of a matrix yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap what the inverse is in terms of linear transformations. If the
    columns of A are denoted by the vectors a[1],…,a[n] ∈ℝ^n, then A is the linear
    transformation that maps the standard basis vectors to these vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A : ei ↦→ ai, i = 1,...,n. ](img/file541.png)'
  prefs: []
  type: TYPE_IMG
- en: If the direction of the arrows can be reversed, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![A −1 : ai ↦→ ei, i = 1,...,n ](img/file542.png)'
  prefs: []
  type: TYPE_IMG
- en: is a well-defined linear equation, then A^(−1) is called the inverse of A.
  prefs: []
  type: TYPE_NORMAL
- en: 'In light of all that we have seen in this chapter, the method for finding the
    inverse is simple: solve Ax = e[i] for each i where e[1],…,e[n] ∈ℝ^n is the standard
    basis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that Ax[i] = e i . Then, if b can be written as b = ∑ [i=1]^nb[i]e
    i, the vector x = ∑ [i=1]^nb[i]x[i] is the solution of Ax = b:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n A ( bixi) = biAxi i=1 i=1 ∑n = biei i=1 = b. ](img/file543.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the inverse is the matrix whose i-th column is x[i].
  prefs: []
  type: TYPE_NORMAL
- en: 'I know, this seems paradoxical: to find the solution of Ax = b, we need the
    inverse A^(−1). To find the inverse, we need to solve n equations. The answer
    is Gaussian elimination (Section [5.1.1](ch011.xhtml#gaussian-elimination)), which
    gives us an exact computational method to obtain A^(−1). In the next section,
    we are going to put this into practice and write our matrix-inverting algorithm
    from scratch. Pretty awesome.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 The LU decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, I promised that you’d never have to solve a linear
    equation by hand. As it turns out, this task is perfectly suitable for computers.
    In this chapter, we will dive deep into the art of solving linear equations, developing
    the tools from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: We start by describing the process of Gaussian elimination in terms of matrices.
    Why would we even do that? Because matrix multiplication can be performed extremely
    fast in modern computers. Expressing any algorithm in terms of matrices is a sure
    way to accelerate.
  prefs: []
  type: TYPE_NORMAL
- en: At the start, our linear equation Ax = b is given by the coefficient matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ a11 a12 ... a1n || || | a21 a22 ... a2n| A = || ... ... ... ... || ∈
    ℝn×n, || || |⌈ an1 an2 ... ann|⌉ ](img/file544.png)'
  prefs: []
  type: TYPE_IMG
- en: and at the end of the elimination process, A is transformed into the form
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | a11 a12 a13 ... a1n | || 0 a(1) a (1) ... a(1)|| (n−1) | 22 23(2)
    2(n2)| A = || 0 0 a 33 ... a3n || . || ... ... ... ... ... || ⌈ ⌉ 0 0 0 ... a(nnn−1)
    ](img/file545.png)'
  prefs: []
  type: TYPE_IMG
- en: A^((n−1)) is upper diagonal; that is, all elements below its diagonal are 0.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian elimination performs this task one step at a time, focusing on consecutive
    columns. After the first elimination step, this is turned into the equation ([5.1.1](ch011.xhtml)),
    described by the coefficient matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | a11 a12 ... a1n| || 0 a(212) ... a(21)n|| (1) || . . . . || n×n A
    = | .. .. .. .. | ∈ ℝ . || 0 a(1) ... a(1)|| ⌈ n2 nn⌉ ](img/file546.png)'
  prefs: []
  type: TYPE_IMG
- en: Can we obtain A^((1)) from A via multiplication with some matrix; that is, can
    we find G[1] ∈ℝ^(n×n) such that A^((1)) = G[1]A holds?
  prefs: []
  type: TYPE_NORMAL
- en: Yes. By defining G[1] as
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(9).png)(5.5)'
  prefs: []
  type: TYPE_IMG
- en: we can see that A^((1)) = G[1]A is the same as performing the first step of
    Gaussian elimination. (Pick up a pen and paper and verify this by hand. It’s an
    excellent exercise.) G[1] is lower diagonal; that is, all elements above its diagonal
    are 0\. In fact, except for the first column, all elements below the diagonal
    are 0 as well. (Note that G[1] depends on A.)
  prefs: []
  type: TYPE_NORMAL
- en: By analogously defining
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(10).png)(5.6)'
  prefs: []
  type: TYPE_IMG
- en: we obtain A^((2)) = G[2]A^((1)) = G[2]G[1]A, a matrix that is upper diagonal
    in the first two columns. (That is, all elements are 0 below the diagonal, but
    only in the first two columns.)
  prefs: []
  type: TYPE_NORMAL
- en: We can continue this process until we obtain the upper triangular matrix
  prefs: []
  type: TYPE_NORMAL
- en: A^((n−1)) = G[n−1]…G[1]A.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is starting to shape up nicely. The G[i] matrices are invertible,
    with inverses
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ 1 0 0 ... 0 |1 0 0 ... 0| ||a21 || ||0 1 0 ... 0|| ||a11 1 0 ...
    0|| || a(1) || |a31 0 1 ... 0| |0 3a(21) 1 ... 0| G −11= ||a1.1 . . . ||, G −21
    = || . 22\. . . || ,..., || .. .. .. .. || || .. .. .. .. || ||an1 0 0 ... 1||
    || a(n12) || ⌈a11 ⌉ |⌈0 a(212) 0 ... 1|⌉ ](img/file549.png)'
  prefs: []
  type: TYPE_IMG
- en: and so on. Thus, by multiplying by their inverses one by one, we can express
    A as
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 −1 (n−1) A = G 1 ...G n−1A . ](img/file550.png)'
  prefs: []
  type: TYPE_IMG
- en: Fortunately, we can calculate L := G[1]^(−1)…G[n−1]^(−1) by hand. After a quick
    computation, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(11).png)(5.8)'
  prefs: []
  type: TYPE_IMG
- en: 'which is lower diagonal. (Again, don’t be shy about verifying ([5.8](ch011.xhtml#the-lu-decomposition))
    by hand.) By defining the upper diagonal matrix U := A^((n−1)), we obtain the
    famous LU decomposition, factoring A into a lower and upper diagonal matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A = LU. ](img/file552.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that with this algorithm, we perform two tasks for the price of one:
    factorizing A into the product of an upper diagonal and lower diagonal matrix,
    and performing Gaussian elimination.'
  prefs: []
  type: TYPE_NORMAL
- en: From a computational standpoint, the LU decomposition is an extremely important
    tool. Since it is just a refashioned Gaussian elimination, its complexity is O(n³),
    just as we saw this earlier (Section [5.1.4](ch011.xhtml#the-time-complexity-of-gaussian-elimination)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Bad news: LU decomposition not always available. Since it is tied to Gaussian
    elimination, we can characterize its existence in similar terms. Recall that for
    the Gaussian elimination to successfully finish, the principal minors are required
    to be nonzero (Theorem [31](ch011.xhtml#x1-90004r31)). This is directly transferred
    to the LU decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 34\. (Existence of the LU decomposition)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary square matrix, and let M[k] be its k-th principal
    minor. If M[k]≠0 for all k = 1,2,…,n − 1, then A can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![A = LU, L, U ∈ ℝn×n, ](img/file553.png)'
  prefs: []
  type: TYPE_IMG
- en: where L is a lower diagonal and U is an upper diagonal matrix. Moreover, the
    elements along the diagonal of L are equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gist is the same: everything is fine if we avoid division by 0 during the
    algorithm. Note that the LU algorithm doesn’t require a nonzero M[n] = detA, that
    is, an invertible matrix!'
  prefs: []
  type: TYPE_NORMAL
- en: After all this preparation, we are ready to put things into practice!
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Implementing the LU decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To summarize the LU decomposition, it’s essentially the iteration of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the elimination matrices of the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply the input by the elimination matrices, feeding the output back into
    the first step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The plan is clear: first, we write a function that computes the elimination
    matrices and their inverses; then, we iteratively perform the elimination steps
    using matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to perform the elimination steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test our function on a small matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Is the result correct? Let’s test it by multiplying L and U together to see
    if it gives A back.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Overall, the LU decomposition is a highly versatile tool, used as a stepping
    stone in the implementation of essential algorithms. One of them is computing
    the inverse matrix, as we shall see next.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Inverting a matrix, for real
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have talked a lot about the inverse matrix. We explored the question
    of invertibility from several angles, in terms of the kernel and the image, the
    determinant, and the solvability of linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: However, we haven’t yet talked about actually computing the inverse. With the
    LU decomposition, we obtain a tool that can be used for this purpose. How? By
    plugging in a lower triangular matrix into the Gaussian elimination process, we
    get its inverse as a side effect. So, we
  prefs: []
  type: TYPE_NORMAL
- en: calculate the LU decomposition A = LU,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: invert the lower triangular matrices L and U^T ,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: use the identity (U[−1])T = (U[T] )−1 to get U^(−1),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: multiply L^(−1) and U^(−1) to finally obtain A^(−1) = U^(−1)L^(−1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s a plan! Let’s start with inverting lower triangular matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Let L ∈ℝ^(n×n) be an arbitrary lower triangular matrix. Following the same process
    that led to ([5.7](ch011.xhtml#the-lu-decomposition)), we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![D = Gn −1...G1L, ](img/file554.png)'
  prefs: []
  type: TYPE_IMG
- en: where D, the final result of Gaussian elimination, is a diagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ |d1 0 ... 0 | | 0 d2 ... 0 | D = diag(d1,...,dn) = || . . . . ||, |⌈
    .. .. .. .. |⌉ 0 0 ... d n ](img/file555.png)'
  prefs: []
  type: TYPE_IMG
- en: and the G[i]-s are the elimination matrices defined by ([5.5](ch011.xhtml#the-lu-decomposition)),
    ([5.6](ch011.xhtml#the-lu-decomposition)), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Since the inverse of ![D ](img/file556.png) is simply ![ − 1 −1 −1 D = diag(d1
    ,...,dn ) ](img/file557.png), we can express ![L −1 ](img/file558.png) as
  prefs: []
  type: TYPE_NORMAL
- en: '![L −1 = D −1Gn− 1...G1\. ](img/file559.png)'
  prefs: []
  type: TYPE_IMG
- en: We can implement this very similarly to the LU decomposition; we can even reuse
    our elimination_matrix function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With this done, we are ready to invert any matrix (that is actually invertible).
  prefs: []
  type: TYPE_NORMAL
- en: We are almost at the finish line. Every component is ready; the only thing left
    to do is to put them together. We can do this with a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Voilà! Witness the result with your own eyes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To test the correctness of our invert function, we quickly check the results
    on a few randomly generated matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since there is no error message above, the function is (probably) correct.
  prefs: []
  type: TYPE_NORMAL
- en: What seemed complex and abstract a few chapters ago is now in our hands. We
    can invert any matrix, not with built-in functions, but with one that we wrote
    from scratch. I love these moments when the pieces are finally put together, and
    everything clicks. Sit back, relax, and appreciate the journey that got us here!
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 How to actually invert matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of course, our LU decomposition!matrices, invertingimplementation is far from
    optimal. When working with NumPy arrays, we can turn to the built-in functions.
    In NumPy, this is np.linalg.inv.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare the runtime of our implementation and NumPy’s.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A massive improvement. Nice! (Don’t forget that the execution time isLU decomposition!matrices,
    inverting hardware-dependent.) Why is NumPy that much faster? There are two main
    reasons. First, it directly calls the SGETRI function from LAPACK, which is extremely
    fast. Second, according to its documentation ( [https://www.netlib.org/lapack/explore-html/da/d28/group__getri_gaa3bf1bb1432917f0e5fdf4c48bd6998c.html](https://www.netlib.org/lapack/explore-html/da/d28/group__getri_gaa3bf1bb1432917f0e5fdf4c48bd6998c.html)),
    SGETRI uses a faster algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, NumPy calls the LAPACK function, which uses LU factorization in turn. (I
    am not particularly adept at digging through Fortran code that is older than I
    am, so let me know if I am wrong here. Nevertheless, the fact that state-of-the-art
    frameworks still make calls to this ancient library is a testament to its power.
    Never underestimate old technologies like LAPACK and Fortran.)
  prefs: []
  type: TYPE_NORMAL
- en: Are there any other applications of the powerful LU decomposition that we’ve
    just learned? Glad you asked! Of course there is; that’s the beauty of math. There’s
    always a new and unexpected application even for the oldest of tools. This time,
    we’ll finally see how to compute determinants fast! (And also slow.)
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Determinants in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the theory and practice of mathematics, the development of concepts usually
    has a simple flow. Definitions first arise from vague geometric or algebraic intuitions,
    eventually crystallizing in mathematical formalism.
  prefs: []
  type: TYPE_NORMAL
- en: However, mathematical definitions often disregard practicalities. For a very
    good reason, mind you! Keeping practical considerations out of sight gives us
    the power to reason about structure effectively. This is the strength of abstraction.
    Eventually, if meaningful applications are found, the development flows toward
    computational questions, putting speed and efficiency onto the horizon.
  prefs: []
  type: TYPE_NORMAL
- en: The epitome of this is neural networks themselves. From theoretical constructs
    to state-of-the-art algorithms that run on your smartphone, machine learning research
    followed this same arc.
  prefs: []
  type: TYPE_NORMAL
- en: This is also what we experience in this book on a microscopic level. Among many
    other examples, think about determinants. We introduced the determinant as the
    orientation of column vectors and the parallele-piped volume defined by them.
    Still, we haven’t really worked on computing them in practice. Sure, we gave a
    formula or two, but it is hard to decide which one is the most convoluted. All
    of them are.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the mathematical study of determinants yielded a ton of
    useful results: invertibility of linear transformations, characterization of Gaussian
    elimination, and many more (and even more to come.).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are ready to pay off our debts and develop tools to actually
    compute determinants. As before, we will take a straightforward approach and use
    one of the previously derived determinant formulas. Spoiler alert: This is far
    from optimal, so we’ll find a way to compute the determinant with high speed.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 The lesser of two evils
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s recall what we know about determinants (Definition [20](ch010.xhtml#x1-81006r20))
    so far. Given a matrix A ∈ℝ^(n×n), its determinant detA quantifies the volume
    distortion of the linear transformation x →Ax. That is, if e[1],…,e[n] is the
    standard orthonormal basis, then informally speaking,
  prefs: []
  type: TYPE_NORMAL
- en: '![det A = (orientation of Ae1, Ae2,...,Aen ) × (area of the parallelepiped
    determined by Ae1, Ae2,...,Aen ). ](img/file560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have derived two formulas to compute this quantity. Initially, we described
    the determinant in terms of summing over all permutations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![det A = ∑ sign(σ)a ...a . σ(1),1 σ(n),n σ∈Sn ](img/file561.png)'
  prefs: []
  type: TYPE_IMG
- en: This is difficult to understand, let alone to programmatically compute. So,
    a recursive formula is derived, which we can also use. It states that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n detA = (− 1)j+1a1,j detA1,j, j=1 ](img/file562.png)'
  prefs: []
  type: TYPE_IMG
- en: where A[i,j] is the matrix obtained by deleting the i-th row and j-th column
    of A. Which one would you rather use? Take a few minutes to figure out your reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there are no right choices here. With the permutation formula,
    one has to find a way to generate all permutations first, then calculate their
    signs. Moreover, there are n! unique permutations in S[n], so this sum has a lot
    of terms. Using this formula seems extremely difficult, so we will go with the
    recursive version. Recursion has its issues (as we are about to see very soon),
    but it is easy to handle from a coding standpoint. Let’s get to work!
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 The recursive way
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s put the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n detA = (− 1)j+1a1,j det A1,j j=1 ](img/file563.png)'
  prefs: []
  type: TYPE_IMG
- en: under our magnifying glass. If A is an n×n matrix, then A[1,j] (obtained from
    A by deleting its first row and j-th column) is of size (n− 1) × (n− 1). This
    is a recursive step. For each n×n determinant, we have to calculate n pieces of
    (n− 1) × (n− 1) determinants, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: By the end, we have a lots of 1 × 1 determinants, which are trivial to calculate.
    So, we have a boundary condition, and with that, we are ready to put these together.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test the det function out on a small example. For 2 × 2 matrices, we can
    easily calculate the determinants using the rule
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ a b det⌈ ⌉ = ad − bc. c d ](img/file564.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It seems to work. So far, so good. What is the issue? Recursion. Let’s calculate
    the determinant of a small 10 × 10 matrix, measuring the time it takes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That was long and unbearable. For such a simple task, this feels like an eternity.
  prefs: []
  type: TYPE_NORMAL
- en: For n×n inputs, we call the det function recursively n times, on (n− 1) × (n−
    1) inputs. That is, if a[n] denotes the time complexity of our algorithm for an
    n ×n matrix, then, due to the recursive step, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![an = nan−1, ](img/file565.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which explodes really fast. In fact, a[n] = O(n!), which is the dreaded factorial
    complexity. Unlike some other recursive algorithms, [caching](https://docs.python.org/3/library/functools.html#functools.cache)
    doesn’t help either. There are two reasons for this: sub-matrices rarely match,
    and numpy.ndarray objects are mutable, thus not hashable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, n can be in the millions, so this formula is utterly useless.
    What can we do? Simple: LU decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 How to actually compute determinants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the two formulas, we have seen lots of useful properties of matrices
    and determinants. Can we apply what we have learned so far to simplify the problem?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the LU decomposition. According to this, if detA≠0, then A =
    LU, where L is lower triangular and U is upper triangular. Since the determinant
    behaves nicely with respect to matrix multiplication (see equation (4.11)), we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![detA = detL detU. ](img/file566.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Seemingly, we made our situation worse: instead of one determinant, we have
    to deal with two. However, L and U are rather special, as they are triangular.
    It turns out that computing a triangular matrix’s determinant is extremely easy.
    We just have to multiply the elements on the diagonal together!'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 35\. (Determinant of a triangular matrix)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be a triangular matrix. (That is, it is either lower or upper
    triangular.) Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∏ det A = aii. i=1 ](img/file567.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof.Suppose that A is lower triangular. (That is, all elements above its diagonal
    are 0.) According to the recursive formula for detA, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n detA = (− 1)j+1a1,j detA1,j. j=1 ](img/file568.png)'
  prefs: []
  type: TYPE_IMG
- en: Because A is lower triangular, a[1,j] = 0 if j >1\. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![detA = a11 detA1,1\. ](img/file569.png)'
  prefs: []
  type: TYPE_IMG
- en: A[1,1] = (a[ij])[i,j=2]^n is also lower triangular. By iterating the previous
    step, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![detA = a11a22 ...ann, ](img/file570.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: If A is upper triangular, its transpose A^T is lower triangular. Thus, we can
    apply the previous result, so
  prefs: []
  type: TYPE_NORMAL
- en: '![detA = detAT = a11a22...ann ](img/file571.png)'
  prefs: []
  type: TYPE_IMG
- en: holds as well.
  prefs: []
  type: TYPE_NORMAL
- en: Back to our original problem. Since the diagonal L is constant 1 (see ([5.8](ch011.xhtml#the-lu-decomposition))),
    as guaranteed by the LU decomposition, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∏n det A = detU = uii. i=1 ](img/file572.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the algorithm to compute the determinant is quite simple: get the LU decomposition,
    then calculate the product of U’s diagonal. Let’s put this into practice!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Yes, that simple. Let’s see how it performs!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It’s faster by a huge margin. How much faster?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: That’s quite an insane improvement! This can be even faster if we use a better
    implementation of the LU decomposition algorithm (for instance, scipy.linalg.lu,
    which relies on our old friend LAPACK).
  prefs: []
  type: TYPE_NORMAL
- en: I get emotional just by looking at this result. See how far we can go with a
    bit of linear algebra? This is why understanding the fundamentals such as Gaussian
    elimination is essential. Machine learning and deep learning are still very new
    fields, and even though an insane amount of research power is being put into it,
    moments like these happen all the time. Simple ideas often give birth to new paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have looked at matrices from the perspective of linear equation
    systems, i.e., equations of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![a11x1 + a12x2 + ⋅⋅⋅+ a1nxn = b1 a21x1 + a22x2 + ⋅⋅⋅+ a2nxn = b2 . .. an1x1
    + an2x2 + ⋅⋅⋅+ annxn = bn. ](img/file573.png)'
  prefs: []
  type: TYPE_IMG
- en: Not surprisingly, these are described by matrices, and the above is equivalent
    to the expression Ax = b. Solving linear equations is an ancient art, so why are
    we talking about it in the age of AI?
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember: It’s only AI if you are talking to investors. Deep down, it’s linear
    algebra, calculus, and probability theory.'
  prefs: []
  type: TYPE_NORMAL
- en: We wanted to solve linear equations, which led us to Gaussian elimination (well,
    led Gauss to Gaussian elimination). Which led us to the LU decomposition. Which
    led us to fast matrix inversion, and a bunch of other innovations on which our
    current technology is built on. Let me tell you, fast matrix multiplication and
    inversion are the bread and butter of computational linear algebra, and they all
    stem from that aforementioned ancient art of solving linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap the feats that we’ve achieved in this chapter one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: solving linear equations by Gaussian elimination,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: characterizing the invertibility of matrices in terms of linear equations,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discovering a matrix factorization technique called LU decomposition,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: building a crazy-fast matrix-inverting algorithm using the LU decomposition,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and building a crazy-fast determinant-computing algorithm using – drumroll!
    – the LU decomposition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we are not done with matrices. Recall the relationship we established
    between them and linear transformations, viewing matrices as data transforms that
    distort the underlying feature space. As it turns out, if we are looking from
    the right perspective, this distortion is always a stretching. Well, almost always.
    Well, almost a stretching.
  prefs: []
  type: TYPE_NORMAL
- en: Well, that was too many wells, so let’s clear all of it up in the next chapter,
    diving into eigenvalues and eigenvectors. Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Show that the product of upper triangular matrices is upper triangular.
    Similarly, show that the product of lower triangular matrices is lower triangular.
    (We have used these facts extensively in this section but didn’t give a proof.
    So, this is an excellent time to convince yourself about this if you haven’t already.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Write a function that, given an invertible square matrix A ∈ℝ^(n×n)
    and a vector b ∈ℝ^n, finds the solution of the linear equation Ax = b. (This can
    be done with a one-liner if you use one of the tools we have built here.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 3\. Before we wrap this chapter up, let’s go back to the definition
    of determinants. Even though there are lots of reasons against using the determinant
    formula, we have one for it: it is a good exercise, and implementing it will deepen
    your understanding. So, in this problem, you are going to build'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ detA = sign(σ)aσ(1)1 ...a σ(n)n, σ∈Sn ](img/file574.png)'
  prefs: []
  type: TYPE_IMG
- en: one step at a time.
  prefs: []
  type: TYPE_NORMAL
- en: (i) Implement a function that, given an integer n, returns all permutations
    of the set {0,1,…,n − 1}. Represent each permutation σ as a list. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: would represent the permutation σ, where σ(0) = 2,σ(1) = 0, and σ(2) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: (ii) Let σ ∈S[n] be a permutation of the set {0,1,…,n − 1}. Its inversion number
    is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![inversion(σ) = |{(i,j) : i <j and σ(i) >σ (j)}|, ](img/file575.png)'
  prefs: []
  type: TYPE_IMG
- en: where j ⋅j denotes the number of elements in the set. Essentially, inversion
    describes the number of times a permutation reverses the order of a pair of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Turns out, the sign of σ can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![sign(σ) = (− 1)inversion(σ). ](img/file576.png)'
  prefs: []
  type: TYPE_IMG
- en: Implement a function that first calculates the inversion number, then the sign
    of an arbitrary permutation. (Permutations are represented like in the previous
    problem.)
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Put the solutions for Problem 1\. and Problem 2\. together and calculate
    the determinant of a matrix using the permutation formula. What do you think the
    time complexity of this algorithm is?
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
