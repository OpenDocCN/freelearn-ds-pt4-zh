- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eigenvalues and Eigenvectors
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have seen three sides of linear transformations: functions, matrices,
    and transforms that distort the grid of the underlying vector space. In the Euclidean
    plane, we saw some examples (Section [4.3](ch010.xhtml#linear-transformations-in-the-euclidean-plane))
    that shed some light on the geometric nature of them.'
  prefs: []
  type: TYPE_NORMAL
- en: Following this line of thought, let’s consider the linear transformation given
    by the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(12).png)6.1'
  prefs: []
  type: TYPE_NORMAL
- en: Since the columns of A are the images of the standard basis vectors e[1] = (1,0)
    and e[2] = (0,1), we can visualize the effect of A on Figure [6.1](#). (Check
    Section [4.1.1](ch010.xhtml#linear-transformations-and-matrices) if you don’t
    recall this fact.)
  prefs: []
  type: TYPE_NORMAL
- en: This seems to shear, stretch, and rotate the entire grid. However, there are
    special directions along which A is simply a stretching. For instance, consider
    the vector u[1] = (1,1). By a simple calculation, you can verify that Au[1] =
    3u[1].
  prefs: []
  type: TYPE_NORMAL
- en: Because of the linearity, this means that if a vector x is in span(u[1]), its
    image under A is 3x.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file578.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Images of the standard basis vectors under the linear transformation
    given by A'
  prefs: []
  type: TYPE_NORMAL
- en: Another one is u[2] = (−1,1), where we have Au[2] = u[2]. Thus, any x ∈ span(u[2])
    is left in place.
  prefs: []
  type: TYPE_NORMAL
- en: If we select u[1],u[2] as our base, the matrix of this transformation is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ Au ,u = ⌈3 0⌉ , 1 2 0 1 ](img/file579.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, A[u[1],u[2]] is diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file580.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Images of u[1] = (1,1) and u[2] = (−1,1) under the linear transformation
    given by A'
  prefs: []
  type: TYPE_NORMAL
- en: We love diagonal matrices in practice because multiplication with a diagonal
    matrix is much faster, as it requires O(n) operations, opposed to O(n²).
  prefs: []
  type: TYPE_NORMAL
- en: Is this a general phenomena? Are these even useful? The answer is yes to both
    questions. What we have just seen is formalized by the concept of eigenvalues
    and eigenvectors. The terminology originates from the german word “eigen” meaning
    “own,” resulting in one of the ugliest naming conventions in mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 23\. (Eigenvalues and eigenvectors)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : V → V be an arbitrary linear transformation. We say that the λ scalar
    and the x ∈V ∖{0} nonzero vector is an eigenvalue-eigenvector pair of f if f(x)
    = λx holds.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Eigenvalues of matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we have formally defined eigenvalues and eigenvectors for linear transformations,
    we often talk about them in context of matrices. (Because, as we have seen, matrices
    and linear transformations are two faces of the same coin.) Let’s start by translating
    the definition into the language of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'If A ∈ℝ^(n×n) is a matrix, Definition [23](ch012.xhtml#x1-105003r23) translates
    to the following: the scalar λ and the vector x ∈ℝ ∖{0} is an eigenvalue-eigenvector
    pair of the matrix if'
  prefs: []
  type: TYPE_NORMAL
- en: A**x** = λ**x**
  prefs: []
  type: TYPE_NORMAL
- en: (6.2)
  prefs: []
  type: TYPE_NORMAL
- en: 'holds. This can be simplified: as the linear transformation x→λx corresponds
    to the matrix λI, ([6.2](ch012.xhtml#eigenvalues-of-matrices)) is equivalent to'
  prefs: []
  type: TYPE_NORMAL
- en: (A − λI)**x** = **0**
  prefs: []
  type: TYPE_NORMAL
- en: (6.3)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall Chapter [4](ch010.xhtml#linear-transformations), Section [4.1.1](ch010.xhtml#linear-transformations-and-matrices),
    where we learned how matrices arise from linear transformations, you might ask
    the question: won’t the eigenvalues depend on the choice of the matrix?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following theorem states that this is not the case: the eigenvalues of
    a linear transformation and its matrices are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 36\. (Eigenvalues of similar matrices)
  prefs: []
  type: TYPE_NORMAL
- en: Let A,B ∈ℝ^(n×n) be two similar matrices, that is, suppose that there exists
    an invertible T ∈ℝ^(n×n) such that B = T^(−1)AT. Then, if Ax = λx holds for some
    scalar λ and vector x ∈ℝ^n,
  prefs: []
  type: TYPE_NORMAL
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ ′ Bx = λx ](img/file582.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for some x^′∈ℝ^n as well.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Let’s massage the eigenvalue ([6.3](ch012.xhtml#eigenvalues-of-matrices))
    a bit! We have
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 (A − λI)x = (A − λT T )x = T (T−1AT − λI)T −1x = 0\. ](img/file583.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since T is invertible, T[(T^(−1)AT −λI)T^(−1)x] = 0 can only happen if (T^(−1)AT
    −λI)T^(−1)x = 0\. (Recall the relation of the kernel and invertibility in Theorem [20](ch010.xhtml#x1-70003r20).)
    This looks almost like ([6.3](ch012.xhtml#eigenvalues-of-matrices)), just a bit
    more complicated. Let me use some suggestive parentheses to highlight the similarities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 − 1 [T AT − λI ][T x] = 0\. ](img/file586.png)'
  prefs: []
  type: TYPE_IMG
- en: So, with the selection x^′ = T^(−1)x, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 ′ ′ T AT x = λx , ](img/file587.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the eigenvalues of similar matrices are the same. Consequently,
    we can talk about the eigenvalues of matrices, not just linear transformations.
    The above theorem implies that the eigenvalues of a transformation and its corresponding
    matrix are the same. Moreover, the eigenvalues of the matrix don’t depend on the
    choice of basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more precise, suppose that A : U →U is a linear transformation and P,Q
    are bases of U. The matrix of A in some basis S is denoted by A[Q]. We know that
    there is a transformation matrix T ∈ℝ^(n×n) such that'
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 AQ = T AP T. ](img/file588.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the eigenvalues are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the above begs the question: how do we actually find eigenvalues? Let’s
    talk about this next.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Finding eigenvalue-eigenvector pairs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though the definition of eigenvalue-eigenvector pairs is easy to understand
    given the geometric interpretation we just saw, it does not give us any tools
    to find them in practice. Using them to get simpler representations of matrices
    is one thing, but we are stuck at square one without a method to find them.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s focus on the eigenvalues. Suppose that for some λ, there is a
    nonzero vector x such that Ax = λx. The transformation defined by x →λx is a linear
    one, and its matrix is diagonal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ λ 0 ... 0⌋ ⌊ x ⌋ | | | 1| || 0 λ ... 0|| || x2|| λx = || .. .. .. ..||
    || .. || , ⌈ . . . .⌉ ⌈ . ⌉ 0 0 ... λ xn ](img/file589.png)'
  prefs: []
  type: TYPE_IMG
- en: where the matrix with λ-s in the diagonal is λI, that is, λ times the identity
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Because linear transformations can be added and subtracted (as we saw in Section [4.1.2](ch010.xhtml#matrix-operations-revisited)),
    the defining equation Ax = λx is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '![(A − λI)x = 0, ](img/file590.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where I denotes the identity transformation, as defined by equation (4.3).
    In other words, the transformation A−λI maps a nonzero vector to 0, meaning that
    it is not invertible, as Theorem [20](ch010.xhtml#x1-70003r20) implies. We can
    characterize this with determinants: we need to find all λ-s such that'
  prefs: []
  type: TYPE_NORMAL
- en: '![det(A − λI ) = 0\. ](img/file591.png)'
  prefs: []
  type: TYPE_IMG
- en: We can summarize the above findings in the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 37\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![A : U → U ](img/file592.png) be an arbitrary linear transformation. Then
    ![λ ](img/file593.png) is its eigenvalue if and only if'
  prefs: []
  type: TYPE_NORMAL
- en: '![det(A − λI ) = 0\. ](img/file594.png)'
  prefs: []
  type: TYPE_IMG
- en: Although we are one step closer, finding eigenvalues based on this still seems
    complicated. In the following, we are going to see what det(A−λI) really is and
    how we can find the solutions of det(A −λI) = 0 in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Before going into the generalities, let’s revisit the example ([6.1](ch012.xhtml#eigenvalues-and-eigenvectors)).
    There, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ | | | | det(A − λI) = ||2 − λ 1 || || 1 2− λ|| = (2 − λ)2 − 1 = λ2 − 4λ
    + 3\. ](img/file595.png)'
  prefs: []
  type: TYPE_IMG
- en: To find the eigenvalues, we have to solve the quadratic equation
  prefs: []
  type: TYPE_NORMAL
- en: '![λ2 − 4λ + 3 = 0, ](img/file596.png)'
  prefs: []
  type: TYPE_IMG
- en: which we can do easily. Recall that the solutions of any quadratic equation
    ax² + bx + c = 0 are
  prefs: []
  type: TYPE_NORMAL
- en: '![ √ 2------- x1,2 = − b-±--b-−-4ac. 2a ](img/file597.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying this, we have λ[1] = 3 and λ[2] = 1 as solutions. There are no other
    ones, so 1 and 3 are the only two eigenvalues for A.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what happens in the general case!
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 The characteristic polynomial
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the example above suggests, if the underlying vector space U is n-dimensional,
    that is, A is an n ×n matrix, det(A −λI) is an n-th degree polynomial in λ.
  prefs: []
  type: TYPE_NORMAL
- en: To see this, let’s write det(A−λI) explicitly in terms of matrices. With this
    in mind, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ | | |a − λ a ... a | ||11 12 1n || || a21 a22 − λ ... a2n || det(A − λI)
    = || .. .. .. .. ||. || . . . . || | an1 an2 ... ann − λ | ](img/file598.png)'
  prefs: []
  type: TYPE_IMG
- en: If you consider the formula to calculate the determinant given by ([4.12](ch010.xhtml#x1-81006r20)),
    you can see that every term is a polynomial. Depending on how many fixed points
    σ has (that is, points where σ(i) = i), the degree of this polynomial varies between
    0 and n.
  prefs: []
  type: TYPE_NORMAL
- en: (Alternatively, you can see that det(A−λI) is a polynomial of degree n by using
    the recursive formula ([4.13](ch010.xhtml#x1-81009r21)) and applying induction.)
  prefs: []
  type: TYPE_NORMAL
- en: Definition 24\. (Characteristic polynomial of matrices)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×n) be an arbitrary matrix. The polynomial
  prefs: []
  type: TYPE_NORMAL
- en: '![p(λ) = det(A − λI) ](img/file599.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the characteristic polynomial of A.
  prefs: []
  type: TYPE_NORMAL
- en: The roots of the characteristic polynomial are the eigenvalues. If U is an n-dimensional
    complex vector space (that is, the set of scalars is ℂ), the fundamental theorem
    of algebra (Theorem [156](ch038.xhtml#x1-385002r156)) guarantees that det(A −λI)
    = 0 has exactly n roots.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, every matrix A ∈ℂ^(n×n) has at least one eigenvalue. Note
    that roots can have higher algebraic multiplicity. For instance, the characteristic
    polynomial for the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | 1 0 0| B = | 0 1 0| ⌈ ⌉ 0 0 2 ](img/file600.png)'
  prefs: []
  type: TYPE_IMG
- en: is (1 −λ)²(2 −λ). So, its roots are 1 (with algebraic multiplicity 2) and 2.
  prefs: []
  type: TYPE_NORMAL
- en: If we restrict ourselves to real matrices and real vector spaces, the existence
    of eigenvalues and eigenvectors are not guaranteed. For instance, consider
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 0 − 1 C = ||1 0 ||. ⌈ ⌉ ](img/file601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Its characteristic polynomial is λ² + 1, which doesn’t have any real roots,
    only complex ones: λ[1] = i and λ[2] = −i. Mathematically speaking, if we want
    to stay within the confines of real vector spaces, C has no eigenvalues. However,
    we are here to do machine learning, not algebra. Thus, we are going to be a bit
    imprecise and treat real matrices as complex ones. We don’t often need complex
    numbers to describe mathematical models of a dataset, but they frequently appear
    during the analysis of matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Finding eigenvectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When an eigenvalue λ is identified, we can set out to find the corresponding
    eigenvectors; that is, vectors x where (A−λI)x = 0\. In more precise terms, we
    are looking for ker(A −λI).
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned before in Section [4.1.4](ch010.xhtml#the-kernel-and-the-image),
    the kernel of any linear transformation is a subspace. As it might be more than
    one-dimensional, identifying it often involves an implicit description like x[1]
    + x[2] = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check what happens with our recurring example
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉ . 1 2 ](img/file602.png)'
  prefs: []
  type: TYPE_IMG
- en: Previously, we have seen that λ[1] = 3 and λ[2] = 1 are the eigenvalues. To
    identify the corresponding eigenvectors for, say, λ[1], we have to find all solutions
    for the linear equation (A −λ[1]I)x = 0\. Expanding this, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![− x1 + x2 = 0 x1 − x2 = 0\. ](img/file603.png)'
  prefs: []
  type: TYPE_IMG
- en: Both equations imply that all x = (x[1],x[2]) are solutions where x[1] = x[2].
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Eigenvectors, eigenspaces, and their bases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Definition 25\. (Eigenspaces)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : V →V be an arbitrary linear transformation, and λ its eigenvalue. The
    subspace of eigenvectors defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![Uλ = {x : Ax = λx} ](img/file604.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the eigenspace of λ.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenspaces play an important role in understanding the structure of linear
    transformations. First, we note that a linear transformation keeps its eigenspaces
    invariant. (That is, if x is in the U[λ] eigenspace, then f(x) ∈U[λ] as well.)
    This property makes it possible for us to restrict linear transformations to their
    eigenspaces.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of eigenspaces, let’s revisit the already familiar
    matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉ 1 2 ](img/file605.png)'
  prefs: []
  type: TYPE_IMG
- en: one more time. Its eigenvalues are λ[1] = 3 and λ[2] = 1, and by solving the
    equation (A −λ[1]I)x = 0, we get that the eigenspace of λ[1] is
  prefs: []
  type: TYPE_NORMAL
- en: '![U = {x ∈ ℝ2 : x = x }. λ1 1 2 ](img/file606.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, you can check that U[λ[2]] = {x ∈ℝ² : x[1] = −x[2]}. (If you go
    back to Figure 6.2, you can visualize U[λ[1]] and U[λ[2]].)'
  prefs: []
  type: TYPE_NORMAL
- en: Eigenspaces are not necessarily one-dimensional. For instance, consider one
    of the the previous examples
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 1 0 0 || || B = ⌈0 1 0⌉ , 0 0 2 ](img/file607.png)'
  prefs: []
  type: TYPE_IMG
- en: with two eigenvalues λ[1] = 1 and λ[2] = 2\. Substituting λ[1] back into the
    equation and solving for (B −I)x = 0 for x, we obtain that
  prefs: []
  type: TYPE_NORMAL
- en: '![U = {x ∈ ℝ3 : x = 0}, λ1 3 ](img/file608.png)'
  prefs: []
  type: TYPE_IMG
- en: which is simply the plane determined by the first two axes.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of eigenspaces determines whether or not we can diagonalize the
    matrix A with a change of basis (Section [4.2](ch010.xhtml#change-of-basis)) transformation
    Λ = T^(−1)AU. The following general theorem establishes this connection.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 38\. (Diagonalization and eigenspaces)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : V → V be a linear transformation, let A ∈ ℝ^(n×n) be its matrix in
    some basis, and let U[λ1,…,U]{λ[k]} be the eigenspaces of f. The following are
    equivalent.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) There is a matrix T ∈ℝ^(n×n) such that
  prefs: []
  type: TYPE_NORMAL
- en: '![ −1 Λ = T AT, ](img/file609.png)'
  prefs: []
  type: TYPE_IMG
- en: where Λ is a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: (b) There is a basis u[1],…,u[n] for V that can be selected from the eigenvectors
    of f.
  prefs: []
  type: TYPE_NORMAL
- en: (c) V can be written as the direct sum of the eigenspaces, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![V = Uλ1 + ⋅⋅⋅+ Uλk. ](img/file610.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that k, the number of eigenspaces, is not necessarily n.)
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (a) ⇒ (b). If ![A ](img/file612.png) is the matrix of ![f ](img/file613.png)
    in some basis, then a similarity transformation is equivalent to a change of basis.
  prefs: []
  type: TYPE_NORMAL
- en: That is, the new matrix Λ = T^(−1)AT is the matrix of f in a different basis,
    say u[1],…,u[n].
  prefs: []
  type: TYPE_NORMAL
- en: If Λ is diagonal, it can be written in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ |λ1 0 ... 0 | || 0 λ2 ... 0 || Λ = | . . . .| . |⌈ .. .. .. ..|⌉ 0 0
    ... λ n ](img/file614.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that the λ[i]-s are not necessarily mutually different.) Thus, Λu[i] =
    λ[i]u[i], meaning that u[1],…,u[n] is a basis from the eigenvectors of f.
  prefs: []
  type: TYPE_NORMAL
- en: (b) ⇒ (a). If u[1],…,u[n] is a basis from the eigenvectors of f, then its matrix
    Λ in that basis is diagonal. Thus, A is similar to Λ, which is what we had to
    show.
  prefs: []
  type: TYPE_NORMAL
- en: (b) ⇒ (c). By definition, the direct sum (Definition [6](ch007.xhtml#x1-29004r6))
    of the eigenspaces contains all linear combinations of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![ n x = ∑ xu . i i i=1 ](img/file617.png)'
  prefs: []
  type: TYPE_IMG
- en: Since u[1],…,u[n] is a basis, V = U[λ[1]] + ⋅⋅⋅ + U[λ[k]] holds.
  prefs: []
  type: TYPE_NORMAL
- en: (c) ⇒ (b). From each eigenspace U[λ[i]], we can select a basis. Due to the construction
    of U[λ[i]], its basis will consist of eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Since V = U[λ[1]] + ⋅⋅⋅ + U[λ[k]], the union of of such bases u[1],…,u[n] will
    be a basis for V .
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though this theorem does not give us any useful recipes on how to diagonalize
    a matrix, it provides us with an extremely valuable insight: diagonalization is
    equivalent to finding an eigenvector basis. This is not always possible, but when
    it is, we are cooking with gas.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a deep dive into this topic, providing multiple
    ways to simplify matrices. If our journey in linear algebra is akin to a mountain
    climb, we will reach the peak soon.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we’ve veered into the theory side of math once again. This
    time, it was about eigenvalues and eigenvectors of a matrix, that is, scalars
    λ and vectors x for which
  prefs: []
  type: TYPE_NORMAL
- en: '![ n×n Ax = λx, A ∈ ℝ ](img/file621.png)'
  prefs: []
  type: TYPE_IMG
- en: hold.
  prefs: []
  type: TYPE_NORMAL
- en: Just like most mathematical objects, this might seem daunting at first, but
    geometrically, this means that in the direction x, the linear transformation A
    is the same as a stretching by λ. In practice, we can find eigenvectors by solving
    the so-called characteristic equation
  prefs: []
  type: TYPE_NORMAL
- en: '![det(A − λI) = 0 ](img/file622.png)'
  prefs: []
  type: TYPE_IMG
- en: for λ.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are eigenvalues used for? There are tons of applications, but one stands
    out: according to Theorem [38](ch012.xhtml#x1-110004r38), if you can build a basis
    from the eigenvectors of the matrix A ∈ℝ^(n×n), then you can find a T ∈ℝ^(n×n)
    such that T^(−1)AT is diagonal. This process is extremely useful. For one, multiplication
    with diagonal matrices is fast and simple, and we prefer to do it whenever we
    can. For another, diagonalization reveals a ton about the internal structure of
    the underlying linear transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve left this chapter with a multitude of questions. How do we find eigenvalues?
    What kind of matrices are diagonalizable? If a matrix is diagonalizable, how can
    we find such a form?
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll answer all of these in the next chapter. Be warned: we are approaching
    the pinnacle of linear algebra. The next chapter might be our heaviest one yet.
    Just like the final stretch before reaching the peaks of Mount Everest. However,
    you have my total confidence. If you are here, you can climb it.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Compute the eigenvalues of the matrices
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ | 4 1 − 1| | 2 1 1| A = |⌈ 1 3 1 |⌉ , B = |⌈ 1 2 1|⌉, − 1 1 2 1
    1 2 ](img/file623.png)'
  prefs: []
  type: TYPE_IMG
- en: and find an eigenvector for every eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Let A ∈ℝ^(n×n) be an upper or lower triangular matrix. Show that
    the eigenvalues of A are its diagonal elements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Let A ∈ℝ^(n×n) be a square matrix. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![det(A − λI ) ](img/file624.png)'
  prefs: []
  type: TYPE_IMG
- en: is a polynomial of degree n in λ.
  prefs: []
  type: TYPE_NORMAL
- en: This is the characteristic polynomial that we have talked about, and we have
    even mentioned this fact. However, we omitted the proof, so here’s your chance
    to fill the gap.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let A ∈ℝ^(n×n), B ∈ℝ^(n×m), and C ∈ℝ^(m×m) arbitrary matrices, and
    we define the so-called block matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌈A B ⌉ (n+m )× (n+m ) D = 0 C ∈ ℝ . ](img/file625.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that if λ is an eigenvalue of A or an eigenvalue of B, then it’s also an
    eigenvalue of C.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
