<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch013.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="matrix-factorizations" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">7</span></span><br/>
<span id="x1-1140008"></span><span class="cmss-10x-x-109">Matrix Factorizations</span></h1>
<p><span class="cmss-10x-x-109">One of the recurring thoughts in this book is that problem-solving is</span><span id="dx1-114001"></span> <span class="cmss-10x-x-109">about finding the best representations of your objects of study. Say linear transformations of a vector space are represented by matrices. Studying one is the same as studying the other, but each perspective comes with its own set of tools. Linear transformations are geometric, while matrices are algebraic sides of the same coin.</span></p>
<p><span class="cmss-10x-x-109">This thought can be applied on a smaller scale as well. Recall the LU decomposition from </span><span class="cmssi-10x-x-109">Chapter </span><a href="ch011.xhtml#matrices-and-equations"><span class="cmssi-10x-x-109">6</span></a><span class="cmss-10x-x-109">. You can think of this as another view of matrices.</span></p>
<p><span class="cmss-10x-x-109">Guess what: It’s not the only one. This chapter is dedicated to the three most important ones:</span></p>
<ul>
<li><span class="cmss-10x-x-109">the spectral decomposition,</span></li>
<li><span class="cmss-10x-x-109">the singular value decomposition,</span></li>
<li><span class="cmss-10x-x-109">and the QR decomposition.</span></li>
</ul>
<p><span class="cmss-10x-x-109">Buckle up. It’s our most challenging adventure yet.</span></p>
<section id="special-transformations" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_96"><span class="titlemark"><span class="cmss-10x-x-109">7.1 </span></span> <span id="x1-1150008.1"></span><span class="cmss-10x-x-109">Special transformations</span></h2>
<p><span class="cmss-10x-x-109">So far, we have</span> <span id="dx1-115001"></span><span class="cmss-10x-x-109">aspired to develop a geometric view of linear algebra. Vectors</span> <span id="dx1-115002"></span><span class="cmss-10x-x-109">are mathematical objects defined by their direction and magnitude. In the spaces of vectors, the concept of distance and orthogonality gives rise to a geometric structure.</span></p>
<p><span class="cmss-10x-x-109">Linear transformations, the building blocks of machine learning, are just mappings that distort this structure: rotating, stretching, and skewing the geometry. However, there are types of transformations that </span><span class="cmssi-10x-x-109">preserve </span><span class="cmss-10x-x-109">some of the structure. In practice, these provide valuable insights, and additionally, they are much easier to work with. In this section, we will take a look at the most important ones, those that we’ll encounter in machine learning.</span></p>
<section id="the-adjoint-transformation" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_97"><span class="titlemark"><span class="cmss-10x-x-109">7.1.1 </span></span> <span id="x1-1160008.1.1"></span><span class="cmss-10x-x-109">The adjoint transformation</span></h3>
<p><span class="cmss-10x-x-109">In machine learning, the</span> <span id="dx1-116001"></span><span class="cmss-10x-x-109">most important stage is the Euclidean space </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. This is where data is represented and manipulated. There, the</span> <span id="dx1-116002"></span><span class="cmss-10x-x-109">entire geometric structure is defined by the inner product</span></p>
<div class="math-display">
<img src="../media/file626.png" class="math-display" alt=" ∑n ⟨x,y ⟩ = xiyi, i=1 "/>
</div>
<p><span class="cmss-10x-x-109">giving rise to the notion of magnitude, direction (in the form of </span><span class="cmssi-10x-x-109">angles</span><span class="cmss-10x-x-109">), and orthogonality. Because of this, transformations that can be related to the inner product are special. For instance, if </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmmi-10x-x-109">,f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">and the linear transformation </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, we know that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">leaves the norm invariant. That is, distance in the original and the transformed feature space have the same meaning.</span></p>
<p><span class="cmss-10x-x-109">First, we will establish a general relation between images of vectors under a transform and their inner product. This is going to be the foundation for our discussions in this chapter.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-116003r39"></span> <span class="cmbx-10x-x-109">Theorem 39.</span> </span><span class="cmbxti-10x-x-109">(The adjoint transformation)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a linear transformation. Then, there exists a linear transformation </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">∗</span></sup> : <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">for which</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="math-display">
  <div class="math-display">
    ⟨f(<strong>x</strong>), <strong>y</strong>⟩ = ⟨<strong>x</strong>, f<sup>*</sup>(<strong>y</strong>)⟩
  </div>
  <div class="equation-label">(7.1)</div>
</div>

<p><span class="cmti-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">. </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">∗</span></sup> <span class="cmti-10x-x-109">is called the adjoint transformation* of </span><span class="cmmi-10x-x-109">f</span><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">Moreover, if </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">is the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">in the standard orthonormal basis, then the matrix of </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">∗</span></sup> <span class="cmti-10x-x-109">is </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmti-10x-x-109">. That is,</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="math-display">
  <div class="math-display">
    ⟨A<strong>x</strong>, <strong>y</strong>⟩ = ⟨<strong>x</strong>, A<sup>T</sup><strong>y</strong>⟩.
  </div>
  <div class="equation-label">(7.2)</div>
</div>

</div>
<div id="tcolobox-174" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Suppose that </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">in the standard orthonormal basis. For any </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span>= (<span class="cmmi-10x-x-109">y</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,y</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">, the inner product is defined by</span></p>
<div class="math-display">
<img src="../media/file627.png" class="math-display" alt=" ∑n ⟨x,y ⟩ = xiyi, i=1 "/>
</div>
<p><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file628.png" class="math-display" alt="⌊ ⌋ |a11 a12 ... a1n |⌊ ⌋ ⌊ ∑n ⌋ |a a ... a || x1| | ∑ j=1a1jxj| || 21. 2.2 . 2n. |||| x2|| || nj=1a2jxj|| || .. .. .. .. |||| ..|| = || .. || . || ||⌈ .⌉ ⌈ . ⌉ ⌈an1 an2 ... ann ⌉ xn ∑n anjxj j=1 "/>
</div>
<p><span class="cmss-10x-x-109">Using this form, we can express </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">in terms of </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">ij</span></sub><span class="cmss-10x-x-109">-s, </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s, and </span><span class="cmmi-10x-x-109">y</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s. For this, we have</span></p>
<div class="math-display">
<img src="../media/file629.png" class="math-display" alt=" ∑n ∑n ⟨Ax, y⟩ = ( aijxj)yi i=1 j=1 ∑n ∑n = ( aijyi) xj j=1 i=1 ◟--◝◜---◞ T j- th component of A y = ⟨x,AT y⟩. "/>
</div>
<p><span class="cmss-10x-x-109">This shows that</span> <span id="dx1-116004"></span><span class="cmss-10x-x-109">the transformation given by </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">∗</span></sup> : <span class="cmmi-10x-x-109">x</span>→<span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">satisfies (</span><a href="ch013.xhtml#x1-116003r39"><span class="cmss-10x-x-109">7.1</span></a><span class="cmss-10x-x-109">) and (</span><a href="ch013.xhtml#x1-116003r39"><span class="cmss-10x-x-109">7.2</span></a><span class="cmss-10x-x-109">), which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Why is the quantity </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">that important to us? Because inner products define the geometric structure</span><span id="dx1-116005"></span> <span class="cmss-10x-x-109">of a vector space. Recall the equation (</span><a href="ch008.xhtml#x1-46005r12"><span class="cmss-10x-x-109">2.12</span></a><span class="cmss-10x-x-109">), allowing us to fully describe any vector using only the inner products with respect to an orthonormal basis. In addition, </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">defines the notion of distance and magnitude. Because of this, (</span><a href="ch013.xhtml#x1-116003r39"><span class="cmss-10x-x-109">7.1</span></a><span class="cmss-10x-x-109">) and (</span><a href="ch013.xhtml#x1-116003r39"><span class="cmss-10x-x-109">7.2</span></a><span class="cmss-10x-x-109">) will be quite useful for us.</span></p>
<p><span class="cmss-10x-x-109">As we are about to see, transformations that preserve the inner product are rather special, and these relationships provide us with a way to characterize them both algebraically and geometrically.</span></p>
</section>
<section id="orthogonal-transformations" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_98"><span class="titlemark"><span class="cmss-10x-x-109">7.1.2 </span></span> <span id="x1-1170008.1.2"></span><span class="cmss-10x-x-109">Orthogonal transformations</span></h3>
<p><span class="cmss-10x-x-109">Let’s</span> <span id="dx1-117001"></span><span class="cmss-10x-x-109">jump straight into the definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-117002r26"></span> <span class="cmbx-10x-x-109">Definition 26.</span> </span><span class="cmbx-10x-x-109">(Orthogonal transformations)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> be an arbitrary linear transformation. <span class="cmmi-10x-x-109">f </span>is called <span class="cmti-10x-x-109">orthogonal</span> if</p>
<div class="math-display">
<img src="../media/file631.png" class="math-display" alt="⟨f(x),f(y)⟩ = ⟨x,y ⟩ "/>
</div>
<p>holds for all <span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>.</p>
</div>
<p><span class="cmss-10x-x-109">As a consequence, an orthogonal </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">preserves the norm: </span><span class="cmsy-10x-x-109">∥</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> = <span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmmi-10x-x-109">,f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">. Because the angle enclosed by two vectors is defined by their inner product, see equation (2.9), the property </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmmi-10x-x-109">,f</span>(<span class="cmbx-10x-x-109">y</span>)<span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">means that an orthogonal transform also preserves angles.</span></p>
<p><span class="cmss-10x-x-109">We can translate the definition to the language of matrices as well. In practice, we are always going to work with matrices, so this characterization is essential.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-117003r40"></span> <span class="cmbx-10x-x-109">Theorem 40.</span> </span><span class="cmbxti-10x-x-109">(Matrices of orthogonal transformations)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a linear transformation and </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be its matrix in the standard orthonormal basis. Then, </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">is orthogonal if, and only if, </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> = <span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-175" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As usual, we have to show the implication in both ways.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Suppose that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is orthogonal. Then, (</span><a href="ch013.xhtml#x1-116003r39"><span class="cmss-10x-x-109">7.2</span></a><span class="cmss-10x-x-109">) gives</span></p>
<div class="math-display">
<img src="../media/file632.png" class="math-display" alt=" T ⟨x, y⟩ = ⟨Ax, Ay ⟩ = ⟨x, A Ay ⟩. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, for any given </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file633.png" class="math-display" alt=" T ⟨x,(A A − I)y ⟩ = 0 "/>
</div>
<p><span class="cmss-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. By letting </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">I</span>)<span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">, the positive definiteness of the inner product implies that </span>(<span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">I</span>)<span class="cmbx-10x-x-109">y </span>= <span class="cmbx-10x-x-109">0 </span><span class="cmss-10x-x-109">for all </span><span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">. Thus, </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">I</span><span class="cmss-10x-x-109">, which means that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">is the inverse of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> = <span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file634.png" class="math-display" alt="⟨Ax, Ay ⟩ = ⟨x,AT Ay ⟩ −1 = ⟨x,A Ay ⟩ = ⟨x,y ⟩, "/>
</div>
<p><span class="cmss-10x-x-109">showing that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is orthogonal.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">The</span> <span id="dx1-117004"></span><span class="cmss-10x-x-109">fact that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> = <span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">has a profound implication regarding</span> <span id="dx1-117005"></span><span class="cmss-10x-x-109">the columns of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. If you think back to the definition of matrix multiplication in </span><span class="cmssi-10x-x-109">Section </span><a href="ch010.xhtml#matrix-operations-revisited"><span class="cmssi-10x-x-109">4.1.2</span></a><span class="cmss-10x-x-109">, the element in the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">AB </span><span class="cmss-10x-x-109">is the inner product of the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and the </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To be more precise, if the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th column is denoted by </span><span class="cmbx-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub> = (<span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,i</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmr-8">2</span><span class="cmmi-8">,i</span></sub><span class="cmmi-10x-x-109">,…,a</span><sub><span class="cmmi-8">n,i</span></sub>)<span class="cmss-10x-x-109">, then we have</span></p>
<div class="math-display">
<img src="../media/file635.png" class="math-display" alt="AT A = (⟨ai,aj⟩)n = I, i,j=1 "/>
</div>
<p><span class="cmss-10x-x-109">that is,</span></p>
<div class="math-display">
<img src="../media/file636.png" class="math-display" alt=" ( |{ ⟨a ,a ⟩ = 1 if i = j, i j |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">In other words, the columns of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">form an orthonormal system. This fact should</span> <span id="dx1-117006"></span><span class="cmss-10x-x-109">not come as a surprise since orthogonal transformations preserve magnitude</span> <span id="dx1-117007"></span><span class="cmss-10x-x-109">and orthogonality, and the columns of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">are the images of the standard orthonormal basis </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In machine learning, performing an orthogonal transformation on our features is equivalent to looking at them from another perspective, without distortion. You might know it already, but this is what Principal Component Analysis (PCA) is doing.</span></p>
</section>
</section>
<section id="selfadjoint-transformations-and-the-spectral-decomposition-theorem" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_99"><span class="titlemark"><span class="cmss-10x-x-109">7.2 </span></span> <span id="x1-1180008.2"></span><span class="cmss-10x-x-109">Self-adjoint transformations and the spectral decomposition theorem</span></h2>
<p><span class="cmss-10x-x-109">Besides</span> <span id="dx1-118001"></span><span class="cmss-10x-x-109">orthogonal transformations, there is another important class: transformations</span><span id="dx1-118002"></span> <span class="cmss-10x-x-109">whose adjoints are themselves. Bear with me a bit, and we’ll see an example soon.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-118003r27"></span> <span class="cmbx-10x-x-109">Definition 27.</span> </span><span class="cmbx-10x-x-109">(Self-adjoint transformations)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> be a linear transformation. <span class="cmmi-10x-x-109">f </span>is <span class="cmti-10x-x-109">self-adjoint </span>if <span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">∗</span></sup> = <span class="cmmi-10x-x-109">f</span>, that is,</p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="math-display">
  <div class="math-display">
    ⟨f(<strong>x</strong>), <strong>y</strong>⟩ = ⟨<strong>x</strong>, f(<strong>y</strong>)⟩
  </div>
  <div class="equation-label">(7.3)</div>
</div>

<p>holds for all <span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>.</p>
</div>
<p><span class="cmss-10x-x-109">As always, we are going to translate this into the language of matrices. If </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is the matrix of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">in the standard orthonormal basis, we know that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">is the matrix of the adjoint. For self-adjoint transformations, it implies that </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> = <span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. Matrices such as these are called </span><span class="cmssi-10x-x-109">symmetric</span><span class="cmss-10x-x-109">, and they have a lot of pleasant properties.</span></p>
<p><span class="cmss-10x-x-109">For us, the most important one is that symmetric matrices can be diagonalized! (That is,they can be transformed into a diagonal matrix with a check reference) The following theorem makes this precise.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-118004r41"></span> <span class="cmbx-10x-x-109">Theorem 41.</span> </span><span class="cmbxti-10x-x-109">(Spectral decomposition of real symmetric matrices)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a real symmetric matrix. Then, </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">has exactly </span><span class="cmmi-10x-x-109">n </span><span class="cmti-10x-x-109">real eigenvalues </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">≥</span>⋅⋅⋅<span class="cmsy-10x-x-109">≥</span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">n</span></sub><span class="cmti-10x-x-109">, and the corresponding eigenvectors </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">can be selected such that they form an orthonormal basis.</span></p>
<p><span class="cmti-10x-x-109">Moreover, if we let </span>Λ = diag(<span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,λ</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">U </span><span class="cmti-10x-x-109">be the orthogonal matrix whose columns are </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub><span class="cmti-10x-x-109">, then</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="math-display">
  <div class="math-display">
    A = UΛU<sup>T</sup>
  </div>
  <div class="equation-label">(7.4)</div>
</div>

<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<p><span class="cmss-10x-x-109">Note that the eigenvalues </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">≥</span>⋅⋅⋅<span class="cmsy-10x-x-109">≥</span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">are not necessarily distinct from each other.</span></p>
<div id="tcolobox-176" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">(Sketch) Since the proof is pretty involved, we are better off getting to know the main ideas behind it, without all the mathematical details.</span></p>
<p><span class="cmss-10x-x-109">The main steps are the following.</span></p>
<ol>
<li><span id="x1-118006x1"><span class="cmss-10x-x-109">If the matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is symmetric, all of its eigenvalues are real.</span></span></li>
<li><span id="x1-118008x2"><span class="cmss-10x-x-109">Using this, it can be shown that an orthonormal basis can be formed from the </span><span class="cmssi-10x-x-109">eigenvectors </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-118010x3"><span class="cmss-10x-x-109">Writing the matrix of the transformation </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">in this orthonormal basis yields a diagonal matrix. Hence, a change of basis yields (</span><a href="ch013.xhtml#x1-118004r41"><span class="cmss-10x-x-109">7.4</span></a><span class="cmss-10x-x-109">).</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Showing that the eigenvalues are real requires some complex number magic (which is beyond the scope of this chapter). The tough part is the second step. Once that has been done, moving to the third one is straightforward, as we have seen when talking about eigenspaces and their bases (</span><span class="cmssi-10x-x-109">Section </span><a href="ch012.xhtml#eigenvectors-eigenspaces-and-their-bases"><span class="cmssi-10x-x-109">6.3</span></a><span class="cmss-10x-x-109">).</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">We still don’t have a hands-on way to diagonalize matrices, but this theorem gets us one step closer: at least we know it is possible for symmetric matrices. This is an important stepping stone, as we’ll be able to reduce the general case to the symmetric one.</span></p>
<p><span class="cmss-10x-x-109">The requirement for a matrix to be symmetric seems like a very special one. However, in practice, we can </span><span class="cmssi-10x-x-109">symmetrize </span><span class="cmss-10x-x-109">matrices in several different ways. For any matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup><span class="cmss-10x-x-109">, the products </span><span class="cmmi-10x-x-109">AA</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">will be symmetric. For square matrices, the average</span> <img src="../media/file639.png" class="frac" data-align="middle" alt="A+AT- 2"/> <span class="cmss-10x-x-109">also works. So, symmetric matrices are more common than you think.</span></p>
<p><span class="cmss-10x-x-109">The orthogonal matrix </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and the corresponding orthonormal basis </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">that diagonalizes a symmetric matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">has a special property that is going to be very important in machine learning for the principal component analysis of data samples.</span></p>
<p><span class="cmss-10x-x-109">Before looking at the next theorem, we introduce the</span> argmax <span class="cmss-10x-x-109">notation. Recall that the expression</span> max<sub><span class="cmmi-8">x</span><span class="cmsy-8">∈</span><span class="cmmi-8">A</span></sub><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">denotes the maximum value of the function over the set </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. Often, we would like to know </span><span class="cmssi-10x-x-109">where </span><span class="cmss-10x-x-109">that maximum is attained, which is defined by the</span> argmax<span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file640.png" class="math-display" alt=" ∗ x = argmaxx ∈Af (x), f(x∗) = max f(x). x∈A "/>
</div>
<p><span class="cmss-10x-x-109">Now, let’s see</span> <span id="dx1-118011"></span><span class="cmss-10x-x-109">the fundamentals of PCA!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-118012r42"></span> <span class="cmbx-10x-x-109">Theorem 42.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a real symmetric matrix and let </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">≥</span> ⋅⋅⋅ <span class="cmsy-10x-x-109">≥ </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">be its real eigenvalues in decreasing order. Moreover, let </span><span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be the orthogonal matrix that diagonalizes </span><span class="cmmi-10x-x-109">A</span><span class="cmti-10x-x-109">, with the corresponding orthonormal basis </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">Then,</span></p>
<div class="math-display">
<img src="../media/file642.png" class="math-display" alt=" T arg m∥axx∥=1 x Ax = u1, "/>
</div>
<p><span class="cmti-10x-x-109">and</span></p>
<div class="math-displa">
<img src="../media/file643.png" width="150" class="math-display" alt=" max xT Ax = λ1. ∥x∥=1 "/>
</div>
</div>
<div id="tcolobox-177" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Since </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is an orthonormal basis, any </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be expressed as a linear combination of them:</span></p>
<div class="math-display">
<img src="../media/file644.png" class="math-display" alt=" n ∑ x = xiui, xi ∈ ℝ. i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Thus, since the </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">are eigenvectors of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file645.png" class="math-display" alt=" ∑n ∑n ∑n Ax = A( xiui) = xiAui = xiλiui. i=1 i=1 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Plugging it back into </span><span class="cmbx-10x-x-109">x</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file646.png" class="math-display" alt=" T ∑n T ∑n x Ax = ( xjuj) A ( xiui) j=1 i=1 ∑n ∑n = ( xjuTj )A ( xiui) j=1 i=1 ∑n ∑n = ( xjuTj )( xiλiui) j=1 i=1 "/>
</div>
<div class="math-display">
<img src="../media/file647.png" class="math-display" alt=" ∑n T = xixjλiuj ui. i,j=1 "/>
</div>
<p><span class="cmss-10x-x-109">Since the </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s form an orthonormal basis,</span></p>
<div class="math-display">
<img src="../media/file648.png" class="math-display" alt=" (| T {1 if i = j, uj ui = ⟨ui,uj⟩ = | (0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">In other words, </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">j</span></sub><sup><span class="cmmi-8">T</span></sup> <span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">vanishes when </span><span class="cmmi-10x-x-109">i≠j</span><span class="cmss-10x-x-109">. Continuing the above calculation with this observation,</span></p>
<div class="math-display">
<img src="../media/file649.png" class="math-display" alt=" "/>
</div>
<p><span class="cmss-10x-x-109">When </span><img src="../media/file650.png" class="math" alt=" "/><span class="cmss-10x-x-109">, the sum </span><img src="../media/file651.png" class="math" alt=" "/> <span class="cmss-10x-x-109">is a weighted average of the eigenvalues </span><img src="../media/file652.png" class="math" alt=" "/><span class="cmss-10x-x-109">. So,</span></p>
<div class="math-display">
<img src="../media/file653.png" class="math-display" alt=" "/>
</div>
<p><span class="cmss-10x-x-109">from which </span><span class="cmbx-10x-x-109">x</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">follows. (Recall that we can assume without loss in generality that the eigenvalues are decreasing.) On the other hand, by plugging in </span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, we can see that </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> = <span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, so the maximum is indeed attained. From these two, the theorem</span> <span id="dx1-118013"></span><span class="cmss-10x-x-109">follows.</span></p>
</div>
</div>
<div class="newtheorem">
<p><span class="head"> <span id="x1-118014r6"></span> <span class="cmbx-10x-x-109">Remark 6.</span> </span></p>
<p>In other words, <span class="cmti-10x-x-109">Theorem </span><a href="ch013.xhtml#x1-118012r42"><span class="cmti-10x-x-109">42</span></a> gives that the function <span class="cmbx-10x-x-109">x</span>→<span class="cmbx-10x-x-109">x</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>assumes its maximum value at <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>, and that maximum value is <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> = <span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub>. The quantity <span class="cmbx-10x-x-109">x</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span>seems quite mysterious as well, so let’s clarify this a bit. If we think in terms of features, the vectors <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub> can be thought of as mixtures of the “old” features <span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub>. When we have actual observations (that is, data), we can use the above process to diagonalize the covariance matrix. So, if <span class="cmmi-10x-x-109">A </span>denotes this covariance matrix, <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> is the <span class="cmti-10x-x-109">variance </span>of the new feature <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>.</p>
<p>Thus, this theorem says that <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> is the unique feature that maximizes the variance. So, among all the possible choices for new features, <span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> conveys the most information about the data.</p>
<p>At this point, we don’t have all the tools to see, but in connection to the principal component analysis, this says that the first principal vector is the <span id="dx1-118015"></span>one that maximizes variance.</p>
</div>
<p><span class="cmssi-10x-x-109">Theorem </span><a href="ch013.xhtml#x1-118012r42"><span class="cmssi-10x-x-109">42</span></a> <span class="cmss-10x-x-109">is just a special case of the following general theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-118016r43"></span> <span class="cmbx-10x-x-109">Theorem 43.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a real symmetric matrix, let </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">≥</span> ⋅⋅⋅ <span class="cmsy-10x-x-109">≥ </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">be its real eigenvalues in decreasing order. Moreover, let </span><span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be the orthogonal matrix that diagonalizes </span><span class="cmmi-10x-x-109">A</span><span class="cmti-10x-x-109">, with the corresponding orthonormal basis </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">Then, for all </span><span class="cmmi-10x-x-109">k </span>= 1<span class="cmmi-10x-x-109">,…,n</span><span class="cmti-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file656.png" class="math-display" alt="u = argmax{xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}, k 1 k−1 "/>
</div>
<p><span class="cmti-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file657.png" class="math-display" alt="λ max {xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}. k 1 k− 1 "/>
</div>
</div>
<p><span class="cmss-10x-x-109">(Sometimes, when the conditions are too complicated, we write</span> max<span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>) : <span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">instead of</span> max<sub><span class="cmmi-8">x</span><span class="cmsy-8">∈</span><span class="cmmi-8">A</span></sub><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">.)</span></p>
<div id="tcolobox-178" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">The proof is almost identical to the previous one. Since </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is required to be orthogonal to </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, it can be expressed as</span></p>
<div class="math-displa">
<img src="../media/file658.png" class="math-display" alt=" n ∑ x = xiui. i=k " width="150"/>
</div>
<p><span class="cmss-10x-x-109">Following the calculations in the proof of the previous theorem, we have</span></p>
<div class="math-displa">
<img src="../media/file659.png" width="150" class="math-display" alt=" ∑n xTAx = x2iλi ≤ λk. i=k "/>
</div>
<p><span class="cmss-10x-x-109">On the other hand, similar to before, </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub> = <span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">, so the theorem follows.</span></p>
</div>
</div>
</section>
<section id="the-singular-value-decomposition" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_100"><span class="titlemark"><span class="cmss-10x-x-109">7.3 </span></span> <span id="x1-1190008.3"></span><span class="cmss-10x-x-109">The singular value decomposition</span></h2>
<p><span class="cmss-10x-x-109">So, we</span> <span id="dx1-119001"></span><span class="cmss-10x-x-109">can diagonalize any real symmetric matrix with an orthogonal transformation. That’s great, but what if our matrix is not symmetric? After all, this is a rather special case.</span></p>
<p><span class="cmss-10x-x-109">How can we do the same for a general matrix? We’ll use a very strong tool, straight from the mathematician’s toolkit: wishful thinking. We pretend to have the solution, then reverse engineer it. To be specific, let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">be any real matrix. (It might not be square.) Since </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is not symmetric, we have to relax our wishes for factoring it into the form </span><span class="cmmi-10x-x-109">U</span>Λ<span class="cmmi-10x-x-109">U</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">. The most straightforward way is to assume that the orthogonal matrices to the left and to the right are not each other’s transposes.</span></p>
<p><span class="cmss-10x-x-109">Thus, we are looking for orthogonal matrices </span><span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">such that</span></p>
<div class="math-displa">
<img src="../media/file660.png" width="150" class="math-display" alt="A = U ΣV T "/>
</div>
<p><span class="cmss-10x-x-109">holds for some diagonal </span>Σ <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup><span class="cmss-10x-x-109">. (A non-square matrix </span>Σ = (<span class="cmmi-10x-x-109">σ</span><sub><span class="cmmi-8">i,j</span></sub>)<sub><span class="cmmi-8">i,j</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n,m</span></sup> <span class="cmss-10x-x-109">is diagonal if </span><span class="cmmi-10x-x-109">σ</span><sub><span class="cmmi-8">i,j</span></sub> <span class="cmss-10x-x-109">is 0 when </span><span class="cmmi-10x-x-109">i≠j</span><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">You might be</span> <span id="dx1-119002"></span><span class="cmss-10x-x-109">wondering about the notational switch from </span>Λ <span class="cmss-10x-x-109">to </span>Σ<span class="cmss-10x-x-109">. This is because </span>Σ <span class="cmss-10x-x-109">will not necessarily contain eigenvalues, but </span><span class="cmssi-10x-x-109">singular values</span><span class="cmss-10x-x-109">. We’ll explain soon.</span></p>
<p><span class="cmss-10x-x-109">Here comes the reverse-engineering part. First, as we discussed earlier, </span><span class="cmmi-10x-x-109">AA</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">are symmetric matrices. Second, we can simplify them by using the orthogonality of </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, obtaining</span></p>
<div class="math-display">
<img src="../media/file661.png" class="math-display" alt=" T T T AA = (UΣV )(VΣU ) 2 T = U Σ U . "/>
</div>
<p><span class="cmss-10x-x-109">Similarly, we have </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">V </span>Σ<sup><span class="cmr-8">2</span></sup><span class="cmmi-10x-x-109">V</span> <sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">. Good news: We can actually find </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">by applying the spectral decomposition theorem (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch013.xhtml#x1-118004r41"><span class="cmssi-10x-x-109">41</span></a><span class="cmss-10x-x-109">) to </span><span class="cmmi-10x-x-109">AA</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, respectively. Thus, the factorization </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">U</span>Σ<span class="cmmi-10x-x-109">V</span> <sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">is valid! This form is called the singular value decomposition (SVD), one of the pinnacle achievements of linear algebra.</span></p>
<p><span class="cmss-10x-x-109">Of course, we are not done yet; we just know where to look. Let’s make this mathematically precise!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-119003r44"></span> <span class="cmbx-10x-x-109">Theorem 44.</span> </span><span class="cmbxti-10x-x-109">(Singular value decomposition)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmti-10x-x-109">be an arbitrary matrix. Then, there exists a diagonal matrix</span> Σ <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmti-10x-x-109">and orthogonal matrices </span><span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup><span class="cmti-10x-x-109">, such that</span></p>
<div class="math-displa">
<img src="../media/file662.png" width="150" class="math-display" alt="A = U ΣV T. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">What is a non-square diagonal matrix? Let me give you two examples, and you’ll immediately get the gist:</span></p>
<div class="math-display">
<img src="../media/file663.png" class="math-display" alt="⌊ ⌋ ⌊ ⌋ 1 0 1 0 0 ||0 2|| , ||0 2 0|| . ⌈ ⌉ ⌈ ⌉ 0 0 "/>
</div>
<p><span class="cmss-10x-x-109">We can always write rectangular matrices </span><span class="cmmi-10x-x-109">M </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">in the forms</span></p>
<div class="math-display">
<img src="../media/file664.png" class="math-display" alt=" ⌊ ⌋ M1 m×m (n−m)×m M = ⌈ ⌉ , M1 ∈ ℝ , M2 ∈ ℝ M2 "/>
</div>
<p><span class="cmss-10x-x-109">if </span><span class="cmmi-10x-x-109">m/span&gt;<span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">, and</span> </span></p>
<div class="math-display">
<img src="../media/file665.png" class="math-display" alt=" [ ] n×n n×(m−n) M = M1 M2 , M1 ∈ ℝ , M2 ∈ ℝ "/>
</div>
<p><span class="cmss-10x-x-109">otherwise. Now, let’s</span> <span id="dx1-119004"></span><span class="cmss-10x-x-109">see the proof of the singular value decomposition!</span></p>
<div id="tcolobox-179" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">(Sketch.) To illustrate the main ideas of the proof, we assume that 1) </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is square, and 2) </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is invertible; that is, </span>0 <span class="cmss-10x-x-109">is not an eigenvalue of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">,</span></p>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">is a real symmetric matrix, we can apply the spectral decomposition theorem to obtain a diagonal </span>Σ <span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">and orthogonal </span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">such that</span></p>
<div class="math-displa">
<img src="../media/file666.png" width="150" class="math-display" alt=" T 2 T A A = V Σ V "/>
</div>
<p><span class="cmss-10x-x-109">holds. (Recall that the eigenvalues of a symmetric matrix are nonnegative, thus we can write the eigenvalues of </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">in the form</span> Σ<sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">As </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is invertible, </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is invertible as well; thus, </span>0 <span class="cmss-10x-x-109">is not an eigenvalue of </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. As a consequence, </span>Σ<span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is well defined. Now, by defining </span><span class="cmmi-10x-x-109">U </span>:= <span class="cmmi-10x-x-109">AV </span>Σ<span class="cmmi-10x-x-109">A</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">, the orthogonality of </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">gives that</span></p>
<div class="math-display">
<img src="../media/file667.png" class="math-display" alt=" T −1 T U ΣV = (AV Σ )ΣV T = AV V = A, "/>
</div>
<p><span class="cmss-10x-x-109">Thus, we are almost finished. The only thing left to show is that </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">is indeed orthogonal, that is, </span><span class="cmmi-10x-x-109">U</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">U </span>= <span class="cmmi-10x-x-109">I</span><span class="cmss-10x-x-109">. Here we go:</span></p>
<div class="math-display">
<img src="../media/file668.png" class="math-display" alt="U TU = (AV Σ− 1)TAV Σ−1 −1 T T −1 = Σ V A◟◝◜A◞ V Σ =VΣ2VT = Σ−1(V TV )Σ2(V TV)Σ −1 = Σ−1Σ2 Σ− 1 = I, "/>
</div>
<p><span class="cmss-10x-x-109">With that, have the singular value decomposition for the special case of square and invertible </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To keep the complexity bearable, we won’t work the rest of the details out; I’ll leave that to you as an exercise. You have all the tools by now.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s take a moment to appreciate the power of the singular value</span><span id="dx1-119005"></span> <span class="cmss-10x-x-109">decomposition. The columns of </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">are orthogonal matrices, which are rather special transformations. As they leave the inner products and the norm invariant, the structure of the underlying vector spaces is preserved. The diagonal </span>Σ <span class="cmss-10x-x-109">is also special, as it is just a stretching in the direction of the bases. It is very surprising that </span><span class="cmssi-10x-x-109">any </span><span class="cmss-10x-x-109">linear transformation is the composition of these three special ones.</span></p>
<p><span class="cmss-10x-x-109">Besides mapping out the fine structure of linear transformations, SVD offers a lot more. For instance, it generalizes the notion of eigenvectors, a concept that was defined only for square matrices. With this, we have</span></p>
<div class="math-displa">
<img src="../media/file669.png" width="150" class="math-display" alt="AV = U Σ, "/>
</div>
<p><span class="cmss-10x-x-109">which we can take a look at column-wise. Here, </span>Σ <span class="cmss-10x-x-109">is diagonal, but its number of elements depends on the smaller one of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">or </span><span class="cmmi-10x-x-109">m</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">So, if </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">U</span><span class="cmss-10x-x-109">, and </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, the identity </span><span class="cmmi-10x-x-109">AV </span>= <span class="cmmi-10x-x-109">U</span>Σ <span class="cmss-10x-x-109">is translated to</span></p>
<div class="math-display">
<img src="../media/file670.png" class="math-display" alt="Av = σ u , 0 ≤ i ≤ min (n,m ). i ii "/>
</div>
<p><span class="cmss-10x-x-109">This closely resembles the definition of eigenvalue-eigenvector pairs, except that instead of one vector, we have </span><span class="cmssi-10x-x-109">two</span><span class="cmss-10x-x-109">. The </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">are the so-called left and right singular vectors, while the scalars </span><span class="cmmi-10x-x-109">σ</span><sub><span class="cmmi-8">i</span></sub> = <img src="../media/file671.png" class="sqrt" alt="√λ- i"/> <span class="cmss-10x-x-109">are called singular values.</span></p>
<p><span class="cmss-10x-x-109">To sum up, orthogonal transformations give us the singular value decomposition, but is that all? Are there any other special transformations and matrix decompositions? You bet there is.</span></p>
<p><span class="cmss-10x-x-109">Enter </span><span class="cmssi-10x-x-109">orthogonal projections</span><span class="cmss-10x-x-109">.</span></p>
</section>
<section id="orthogonal-projections" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_101"><span class="titlemark"><span class="cmss-10x-x-109">7.4 </span></span> <span id="x1-1200008.4"></span><span class="cmss-10x-x-109">Orthogonal projections</span></h2>
<p><span class="cmss-10x-x-109">Linear transformations are essentially manipulations of data, revealing</span><span id="dx1-120001"></span> <span class="cmss-10x-x-109">other (hopefully more useful) representations. Intuitively, we think about them as one-to-one mappings, faithfully preserving all the “information” from the input.</span></p>
<p><span class="cmss-10x-x-109">This is often not the case, to such an extent that sometimes a lossy compression of the data is highly beneficial. To give you a concrete example, consider a dataset with a million features, out of which only a couple hundred are useful. What we can do is identify the important features and throw away the rest, obtaining a representation that is more compact, thus easier to work with.</span></p>
<p><span class="cmss-10x-x-109">This notion is formalized by the concept of orthogonal projections. We already met them upon our first encounter with the inner products (see (</span><a href="ch008.xhtml#x1-45003r3.2.3"><span class="cmss-10x-x-109">2.7</span></a><span class="cmss-10x-x-109">)).</span></p>
<p><span class="cmss-10x-x-109">Projections also play a fundamental role in the Gram-Schmidt process (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch008.xhtml#x1-47004r13"><span class="cmssi-10x-x-109">13</span></a><span class="cmss-10x-x-109">), used to orthogonalize an arbitrary basis. Because we are already somewhat familiar with orthogonal projections, a formal definition is due.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-120002r28"></span> <span class="cmbx-10x-x-109">Definition 28.</span> </span><span class="cmbx-10x-x-109">(Projections and orthogonal projections)</span></p>
<p>Let <span class="cmmi-10x-x-109">V </span>be an arbitrary inner product space and <span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">V </span>be a linear transformation. <span class="cmmi-10x-x-109">P </span>is a <span class="cmti-10x-x-109">projection </span>if <span class="cmmi-10x-x-109">P</span><sup><span class="cmr-8">2</span></sup> = <span class="cmmi-10x-x-109">P</span>.</p>
<p>A projection <span class="cmmi-10x-x-109">P </span>is <span class="cmti-10x-x-109">orthogonal </span>if the subspaces ker<span class="cmmi-10x-x-109">P </span>and im<span class="cmmi-10x-x-109">P </span>are orthogonal to each other. (That is, for every pair of <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">P </span>and <span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span> im<span class="cmmi-10x-x-109">P</span>, we have <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= 0.)</p>
</div>
<p><span class="cmss-10x-x-109">Let’s revisit the examples we have seen so far to get a grip on the definition!</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">The simplest one is the orthogonal projection to a single vector. That is, if </span><span class="cmmi-10x-x-109">u </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is an arbitrary vector, the transformation</span></p>
<div class="math-display">
<img src="../media/file672.png" class="math-display" alt="proju(x) = ⟨x,u⟩-u ⟨u,u ⟩ "/>
</div>
<p><span class="cmss-10x-x-109">is the orthogonal projection to (the subspace spanned by) </span><span class="cmmi-10x-x-109">u</span><span class="cmss-10x-x-109">. (We talked about this when discussing the geometric interpretation of inner products in </span><span class="cmssi-10x-x-109">Section </span><a href="ch008.xhtml#the-geometric-interpretation-of-inner-products"><span class="cmssi-10x-x-109">2.2.3</span></a><span class="cmss-10x-x-109">, where this definition was deduced from a geometric intuition.) Applying this transformation repeatedly, we get</span></p>
<div class="math-display">
<img src="../media/file673.png" class="math-display" alt=" ⟨x,u⟩ ⟨⟨u,u⟩u,-u⟩ proju(proju (x )) = ⟨u,u ⟩ u ⟨x,u⟩ -⟨u,u⟩⟨u,-u⟩ = ⟨u,u ⟩ u = ⟨x,u-⟩u ⟨u,u ⟩ = proj (x). u "/>
</div>
<p><span class="cmss-10x-x-109">Thus, faithfully to its name, </span><img src="../media/file674.png" class="math" alt="proju "/> <span class="cmss-10x-x-109">is indeed a projection. To see that it is orthogonal, let’s examine its kernel and image! Since the value of </span><img src="../media/file675.png" class="math" alt="proju(x ) "/> <span class="cmss-10x-x-109">is a scalar multiple of </span><img src="../media/file676.png" class="math" alt="u "/><span class="cmss-10x-x-109">, its image is</span></p>
<div class="math-display">
<img src="../media/file677.png" class="math-display" alt="im (proj) = span(u). u "/>
</div>
<p><span class="cmss-10x-x-109">Its kernel, the set of vectors mapped to </span><span class="cmbx-10x-x-109">0 </span><span class="cmss-10x-x-109">by</span> proj<sub><span class="cmbx-8">u</span></sub><span class="cmss-10x-x-109">, is also easy to find, as</span> <img src="../media/file678.png" class="frac" data-align="middle" alt="⟨⟨xu,,uu⟩⟩"/><span class="cmbx-10x-x-109">u </span>= <span class="cmbx-10x-x-109">0 </span><span class="cmss-10x-x-109">can only happen if </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">u</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">, that is, if </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">u</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In other words,</span></p>
<div class="math-display">
<img src="../media/file679.png" class="math-display" alt="ker(proju) = span (u)⊥, "/>
</div>
<p><span class="cmss-10x-x-109">where</span> span(<span class="cmbx-10x-x-109">u</span>)<sup><span class="cmsy-8">⊥</span></sup> <span class="cmss-10x-x-109">denotes the orthogonal complement (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch008.xhtml#x1-48004r13"><span class="cmssi-10x-x-109">13</span></a><span class="cmss-10x-x-109">) of</span> span(<span class="cmbx-10x-x-109">u</span>)<span class="cmss-10x-x-109">. This means that</span> proj<sub><span class="cmbx-8">u</span></sub> <span class="cmss-10x-x-109">is indeed an</span> <span id="dx1-120003"></span><span class="cmss-10x-x-109">orthogonal projection.</span></p>
<p><span class="cmss-10x-x-109">We can also describe </span><img src="../media/file680.png" class="math" alt="proju(x) "/> <span class="cmss-10x-x-109">in terms of matrices. By writing out </span><img src="../media/file681.png" class="math" alt="proju (x ) "/> <span class="cmss-10x-x-109">component-wise, we have</span></p>
<div class="math-display">
<img src="../media/file682.png" class="math-display" alt=" ⌊ ⌋ | ⟨x,u⟩u1| ⟨x,u⟩ 1 | ⟨x,u⟩u2| proju(x) = -----u = ----2|| . || , ⟨u, u⟩ ∥u ∥ |⌈ .. |⌉ ⟨x,u⟩u n "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">u </span>= (<span class="cmmi-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,u</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">. This looks like some kind of matrix multiplication! As we saw earlier, multiplying a matrix and a vector can be described in terms of rowwise dot products. (See (</span><a href="#"><span class="cmss-10x-x-109">3.3</span></a><span class="cmss-10x-x-109">).)</span></p>
<p><span class="cmss-10x-x-109">So, according to this interpretation of matrix multiplication, we have</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(13).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(7.5)
  </div>
</div>
<p><span class="cmss-10x-x-109">Note that the scaling with </span><span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">u</span><span class="cmsy-10x-x-109">∥</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">can be incorporated into the “matrix” product by writing</span></p>
<div class="math-display">
<img src="../media/file686.png" class="math-display" alt=" T T uu---= -u--⋅ u--, ∥u∥2 ∥u ∥ ∥u∥ "/>
</div>
<p><span class="cmss-10x-x-109">The matrix </span><span class="cmbx-10x-x-109">uu</span><sup><span class="cmmi-8">T</span></sup> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, obtained from the product of the vector </span><span class="cmbx-10x-x-109">u </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmr-8">(</span><span class="cmsy-8">×</span><span class="cmr-8">1)</span></sup> <span class="cmss-10x-x-109">and its transpose </span><span class="cmbx-10x-x-109">u</span><sup><span class="cmmi-8">T</span></sup> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">1</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, is a rather special one. They</span> <span id="dx1-120004"></span><span class="cmss-10x-x-109">are called rank-1 projection matrices, and</span> <span id="dx1-120005"></span><span class="cmss-10x-x-109">they frequently appear in mathematics.</span></p>
<p><span class="cmss-10x-x-109">(In general, the matrix </span><span class="cmbx-10x-x-109">uv</span><sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">outer product </span><span class="cmss-10x-x-109">of the vectors </span><span class="cmbx-10x-x-109">u </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">v</span><span class="cmss-10x-x-109">. We won’t use this extensively, but it appears frequently throughout linear algebra.)</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">As we saw when introducing the Gram-Schmidt orthogonalization process (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch008.xhtml#x1-47004r13"><span class="cmssi-10x-x-109">13</span></a><span class="cmss-10x-x-109">), the previous example can be generalized by projecting to multiple vectors. If </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is a set of linearly independent and pairwise orthogonal vectors, then the linear transformation</span></p>
<div class="math-display">
<img src="../media/file687.png" class="math-display" alt=" ∑k ⟨x, ui⟩ proju1,...,uk(x) = -------ui i=1 ⟨ui,ui⟩ "/>
</div>
<p><span class="cmss-10x-x-109">is an orthogonal projection onto the subspace</span> span(<span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmss-10x-x-109">. This is easy to see, and I recommend the reader to do this as an exercise. (This can be found in the problems section as well.)</span></p>
<p><span class="cmss-10x-x-109">From (</span><a href="#"><span class="cmss-10x-x-109">7.5</span></a><span class="cmss-10x-x-109">), we can determine the matrix form of</span> proj<sub><span class="cmbx-8">u</span><sub><span class="cmr-6">1</span></sub><span class="cmmi-8">,…,</span><span class="cmbx-8">u</span><sub><span class="cmmi-6">k</span></sub></sub> <span class="cmss-10x-x-109">as well:</span></p>
<div class="math-display">
<img src="../media/file688.png" class="math-display" alt=" ∑k T proj (x) = ( uiu-i) x. u1,...,uk i=1 ∥ui∥2 ◟----◝◜---◞ ∈ℝn×n "/>
</div>
<p><span class="cmss-10x-x-109">This is good to know, as projection matrices are often needed in the implementation of certain algorithms.</span></p>
<section id="properties-of-orthogonal-projections" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_102"><span class="titlemark"><span class="cmss-10x-x-109">7.4.1 </span></span> <span id="x1-1210008.4.1"></span><span class="cmss-10x-x-109">Properties of orthogonal projections</span></h3>
<p><span class="cmss-10x-x-109">Now that we have seen a few examples, it is time to discuss orthogonal projections in</span> <span id="dx1-121001"></span><span class="cmss-10x-x-109">more general terms. There are lots of reasons why these special transformations are useful, and we’ll explore them in this section. First, let’s start with the most important one: orthogonal projections also enable the decomposition of vectors in terms of a given subspace plus an orthogonal vector.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-121002r45"></span> <span class="cmbx-10x-x-109">Theorem 45.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space and </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be a projection. Then, </span><span class="cmmi-10x-x-109">V </span>= ker<span class="cmmi-10x-x-109">P </span>+ im<span class="cmmi-10x-x-109">P</span><span class="cmti-10x-x-109">; that is, every vector </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file689.png" class="math-display" alt="x = x + x , x ∈ ker P, x ∈ im P. ker im ker im "/>
</div>
<p><span class="cmti-10x-x-109">If </span><span class="cmmi-10x-x-109">P </span><span class="cmti-10x-x-109">is an orthogonal projection, then </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">im</span></sub> <span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">ker</span></sub><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-180" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Every </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file690.png" class="math-display" alt="x = (x − Px )+ Px. "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is idempotent, that is, </span><span class="cmmi-10x-x-109">P</span><sup><span class="cmr-8">2</span></sup> = <span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file691.png" class="math-display" alt="P (x − P x) = Px − P (Px) = Px − P x = 0, "/>
</div>
<p><span class="cmss-10x-x-109">that is, </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">. By definition, </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span> im<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">, so </span><span class="cmmi-10x-x-109">V </span>= ker<span class="cmmi-10x-x-109">V </span>+ im<span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, which proves our main proposition.</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is an orthogonal projection, then again, by definition, </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">im</span></sub> <span class="cmsy-10x-x-109">⊥</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">ker</span></sub><span class="cmss-10x-x-109">, which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">In addition, orthogonal projections are self-adjoint. This might not sound like a big deal, but self-adjointness leads to several very pleasant properties.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-121003r46"></span> <span class="cmbx-10x-x-109">Theorem 46.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space and </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an orthogonal projection. Then, </span><span class="cmmi-10x-x-109">P </span><span class="cmti-10x-x-109">is self-adjoint.</span></p>
</div>
<div id="tcolobox-181" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">According to the definition (</span><a href="ch013.xhtml#x1-118003r27"><span class="cmss-10x-x-109">27</span></a><span class="cmss-10x-x-109">), all we need to show is that</span></p>
<div class="math-display">
<img src="../media/file692.png" class="math-display" alt="⟨Px, y⟩ = ⟨x, Py⟩ "/>
</div>
<p><span class="cmss-10x-x-109">holds for any </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. In the previous result, we have seen that </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">can be written as</span></p>
<div style="display: flex; justify-content: space-between; align-items: flex-start; gap: 20px;">
  <div class="math-display">
    <div><b>x</b> = <b>x</b><sub>ker P</sub> + <b>x</b><sub>im P</sub>, <b>x</b><sub>ker P</sub> ∈ ker P, <b>x</b><sub>im P</sub> ∈ im P</div>
    <div><b>y</b> = <b>y</b><sub>ker P</sub> + <b>y</b><sub>im P</sub>, <b>y</b><sub>ker P</sub> ∈ ker P, <b>y</b><sub>im P</sub> ∈ im P</div>
  </div>
 
</div>

<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">P</span><sup><span class="cmr-8">2</span></sup> = <span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file693.png" class="math-display" alt="⟨P x,y⟩ = ⟨PxkerP + Pxim P,ykerP + yim P⟩ = ⟨xim P,ykerP + yim P⟩ = ⟨x ,y ⟩+ ⟨x ,y ⟩ ◟-im-P◝◜kerP◞ im P im P =0 = ⟨xim P,yimP ⟩. "/>
</div>
<p><span class="cmss-10x-x-109">Similarly, it can be shown that </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,P</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">im </span><span class="cmmi-8">P</span></sub> <span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><sub><span class="cmr-8">im </span><span class="cmmi-8">P</span></sub> <span class="cmsy-10x-x-109">⟩</span><span class="cmss-10x-x-109">. These two identities imply </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= <span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,P</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩</span><span class="cmss-10x-x-109">, which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">One</span> <span id="dx1-121004"></span><span class="cmss-10x-x-109">straightforward consequence of self-adjointness is that the kernel of orthogonal projections is the orthogonal complement of its image.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-121005r47"></span> <span class="cmbx-10x-x-109">Theorem 47.</span> </span> <span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space and </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an orthogonal projection. Then,</span></p>
<div class="math-display">
<img src="../media/file694.png" class="math-display" alt=" ⊥ ker P = (im P ) . "/>
</div>
</div>
<div id="tcolobox-182" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">To prove the equality of these two sets, we need to show that </span><span class="cmssi-10x-x-109">(a)</span> ker<span class="cmmi-10x-x-109">P </span><span class="cmsy-10x-x-109">⊆ </span>(im<span class="cmmi-10x-x-109">P</span>)<sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">, and </span><span class="cmssi-10x-x-109">(b) </span>(im<span class="cmmi-10x-x-109">P</span>)<sup><span class="cmsy-8">⊥</span></sup><span class="cmsy-10x-x-109">⊆</span> ker<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Let </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">; that is, suppose that </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">. We need to show that for any </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span> im<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">, we have </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. For this, let </span><span class="cmbx-10x-x-109">y</span><sub><span class="cmr-8">0</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">such that </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">y</span><sub><span class="cmr-8">0</span></sub> = <span class="cmbx-10x-x-109">y</span><span class="cmss-10x-x-109">. (This is guaranteed to exist, since we took </span><span class="cmbx-10x-x-109">y </span><span class="cmss-10x-x-109">from the image of </span><span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">.) Then,</span></p>
<div class="math-dispay">
<img src="../media/file695.png" width="150" class="math-display" alt="⟨x,y ⟩ = ⟨x,P y0⟩ = ⟨P x,y0⟩ = ⟨0,y0⟩ = 0, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is self-adjoint. Thus, </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span>(im<span class="cmmi-10x-x-109">P</span>)<sup><span class="cmsy-8">⊥</span></sup> <span class="cmss-10x-x-109">also holds, implying</span> ker<span class="cmmi-10x-x-109">P </span><span class="cmsy-10x-x-109">⊆ </span>(im<span class="cmmi-10x-x-109">P</span>)<sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Now, let </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span>(im<span class="cmmi-10x-x-109">P</span>)<sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">. Then, for any </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, we have </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,P</span><span class="cmbx-10x-x-109">y</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. However,</span></p>
<div class="math-display">
<img src="../media/file696.png" class="math-display" alt="⟨Px,y ⟩ = ⟨x,P y⟩ = 0. "/>
</div>
<p><span class="cmss-10x-x-109">Especially, with the choice </span><span class="cmbx-10x-x-109">y </span>= <span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, we have </span><span class="cmsy-10x-x-109">⟨</span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,P</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">. Due to the positive definiteness of the inner product, this implies that </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">, that is, </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span> ker<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Summing up all of the above, if </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is an orthogonal projection of the inner product space </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file697.png" class="math-display" alt="V = im P + (im P )⊥. "/>
</div>
<p><span class="cmss-10x-x-109">Do you recall that</span> <span id="dx1-121006"></span><span class="cmss-10x-x-109">when we first encountered the concept of orthogonal complements (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch008.xhtml#x1-48004r13"><span class="cmssi-10x-x-109">13</span></a><span class="cmss-10x-x-109">), we proved that </span><span class="cmmi-10x-x-109">V </span>= <span class="cmmi-10x-x-109">S </span>+ <span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup> <span class="cmss-10x-x-109">for any finite-dimensional inner product space </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">and its subspace </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">? We did this with the use of a special orthogonal projection. We are getting close to seeing the general pattern here.</span></p>
<p><span class="cmss-10x-x-109">Because the kernel of an orthogonal projection </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is an orthogonal complement of the image, the transformation </span><span class="cmmi-10x-x-109">I </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is an orthogonal projection as well, with the roles of image and kernel reversed.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-121007r48"></span> <span class="cmbx-10x-x-109">Theorem 48.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space and </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an orthogonal projection. Then, </span><span class="cmmi-10x-x-109">I </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">P </span><span class="cmti-10x-x-109">is an orthogonal projection as well, and</span></p>
<div class="math-display">
<img src="../media/file698.png" class="math-display" alt="ker(I − P ) = im P, im (I − P ) = ker P. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">The proof is so simple that this is left as an exercise for the reader.</span></p>
<p><span class="cmss-10x-x-109">One more thing to mention. If the image spaces of two orthogonal projections match, then the projections themselves are equal. This is a very strong uniqueness property, as if you think about it, this is not true for other classes of linear transformations.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-121008r49"></span> <span class="cmbx-10x-x-109">Theorem 49.</span> </span><span class="cmbxti-10x-x-109">(Uniqueness of orthogonal projections)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be an inner product space and </span><span class="cmmi-10x-x-109">P,Q </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be two orthogonal projections. If</span> im<span class="cmmi-10x-x-109">P </span>= im<span class="cmmi-10x-x-109">Q</span><span class="cmti-10x-x-109">, then </span><span class="cmmi-10x-x-109">P </span>= <span class="cmmi-10x-x-109">Q</span><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-183" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Because of</span> ker<span class="cmmi-10x-x-109">P </span>= (im<span class="cmmi-10x-x-109">P</span>)<sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">, the equality of the image spaces also imply that</span> ker<span class="cmmi-10x-x-109">P </span>= ker<span class="cmmi-10x-x-109">Q</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">V </span>= ker<span class="cmmi-10x-x-109">P </span>+ im<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">, every </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">can be decomposed as</span></p>
<div class="math-display">
<img src="../media/file699.png" class="math-display" alt="x = xkerP + xim P, xkerP ∈ kerP, xim P ∈ im P. "/>
</div>
<p><span class="cmss-10x-x-109">This decomposition and the equality of the kernel and image spaces give that</span></p>
<div class="math-display">
<img src="../media/file700.png" class="math-display" alt="Px = P xkerP + P ximP = xim P. "/>
</div>
<p><span class="cmss-10x-x-109">With an identical argument, we have </span><span class="cmmi-10x-x-109">Q</span><span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">im </span><span class="cmmi-8">P</span></sub> <span class="cmss-10x-x-109">, thus </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span>= <span class="cmmi-10x-x-109">Q</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">on all vectors </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">. This proves </span><span class="cmmi-10x-x-109">P </span>= <span class="cmmi-10x-x-109">Q</span><span class="cmss-10x-x-109">.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">In other words, given a</span> <span id="dx1-121009"></span><span class="cmss-10x-x-109">subspace, there can be only one orthogonal projection to it. But is there any at all? Yes, and in the next section, we will see that it can be described in geometric terms.</span></p>
</section>
<section id="orthogonal-projections-are-the-optimal-projections" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_103"><span class="titlemark"><span class="cmss-10x-x-109">7.4.2 </span></span> <span id="x1-1220008.4.2"></span><span class="cmss-10x-x-109">Orthogonal projections are the optimal projections</span></h3>
<p><span class="cmss-10x-x-109">Orthogonal projections</span> <span id="dx1-122001"></span><span class="cmss-10x-x-109">have an extremely pleasant and mathematically useful property. In some sense, if </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">is an orthogonal projection, </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">provides the optimal approximation of </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">among all vectors in</span> im<span class="cmmi-10x-x-109">P</span><span class="cmss-10x-x-109">. To make this precise, we can state the following.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-122002r50"></span> <span class="cmbx-10x-x-109">Theorem 50.</span> </span><span class="cmbxti-10x-x-109">(Construction of orthogonal transformations)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">be a finite-dimensional inner product space and </span><span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">its subspace. Then, the transformation </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmti-10x-x-109">, defined by</span></p>
<div class="math-display">
<img src="../media/file701.png" class="math-display" alt="P : x → argmyi∈nS ∥x − y∥ "/>
</div>
<p><span class="cmti-10x-x-109">is an orthogonal projection to </span><span class="cmmi-10x-x-109">S</span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">In other words, since orthogonal projections to a given subspace are unique (as implied by </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch013.xhtml#x1-121008r49"><span class="cmssi-10x-x-109">49</span></a><span class="cmss-10x-x-109">), </span><span class="cmmi-10x-x-109">P</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is the closest vector to </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">in the subspace </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">. Thus, we can denote this as </span><span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">S</span></sub><span class="cmss-10x-x-109">, emphasizing the uniqueness.</span></p>
<p><span class="cmss-10x-x-109">Besides having an explicit way to describe orthogonal projections, there is one extra benefit. Recall that previously, we showed that</span></p>
<div class="math-display">
<img src="../media/file702.png" class="math-display" alt="V = im P + (im P)⊥ "/>
</div>
<p><span class="cmss-10x-x-109">holds. Since</span> <span id="dx1-122003"></span><span class="cmss-10x-x-109">for any subspace </span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">an orthogonal projection </span><span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">S</span></sub> <span class="cmss-10x-x-109">exists whose image set is </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, it also follows that </span><span class="cmmi-10x-x-109">V </span>= <span class="cmmi-10x-x-109">S </span>+ <span class="cmmi-10x-x-109">S</span><sup><span class="cmsy-8">⊥</span></sup><span class="cmss-10x-x-109">. Although we saw this earlier when talking about orthogonal complements (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch008.xhtml#x1-48004r13"><span class="cmssi-10x-x-109">13</span></a><span class="cmss-10x-x-109">), it is interesting to see a proof that doesn’t require the construction of an orthonormal basis in </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Interestingly, this is the point where mathematical analysis and linear algebra intersect. We don’t have the tools for it yet, but using the concept of convergence, the above theorems can be generalized to infinite-dimensional spaces. Infinite-dimensional spaces are not particularly relevant to machine learning in practice, yet they provide a beautiful mathematical framework for the study of functions. Who knows, one day these advanced tools may provide a significant breakthrough in machine learning.</span></p>
</section>
</section>
<section id="computing-eigenvalues" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_104"><span class="titlemark"><span class="cmss-10x-x-109">7.5 </span></span> <span id="x1-1230008.5"></span><span class="cmss-10x-x-109">Computing eigenvalues</span></h2>
<p><span class="cmss-10x-x-109">In the last chapter, we reached the singular value decomposition, one of the</span> <span id="dx1-123001"></span><span class="cmss-10x-x-109">pinnacle results of linear algebra. We laid out the theoretical groundwork to get us to this point.</span></p>
<p><span class="cmss-10x-x-109">However, one thing is missing: computing the singular value decomposition in practice. Without this, we can’t reap all the rewards this powerful tool offers. In this section, we’ll develop two methods for this purpose. One offers a deep insight into the behavior of eigenvectors, but it doesn’t work in practice. The other offers excellent performance, but it is hard to understand what is happening behind the formulas. Let’s start with the first one, illuminating how the eigenvectors determine the effects of a linear transformation!</span></p>
<section id="power-iteration-for-calculating-the-eigenvectors-of-real-symmetric-matrices" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_105"><span class="titlemark"><span class="cmss-10x-x-109">7.5.1 </span></span> <span id="x1-1240008.5.1"></span><span class="cmss-10x-x-109">Power iteration for calculating the eigenvectors of real symmetric matrices</span></h3>
<p><span class="cmss-10x-x-109">If you</span> <span id="dx1-124001"></span><span class="cmss-10x-x-109">recall, we discovered the singular value decomposition by tracing the problem back to the spectral decomposition of symmetric matrices. In turn, we can obtain the spectral decomposition by finding an orthonormal basis from the eigenvectors of our matrix. The plan is the following: first, we define a procedure that finds an orthonormal set of eigenvectors for symmetric matrices. Then, use this to compute the singular value decomposition for arbitrary matrices.</span></p>
<p><span class="cmss-10x-x-109">A naive way would be to find the eigenvalues by solving the polynomial equation</span> det(<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">λI</span>) = 0 <span class="cmss-10x-x-109">for </span><span class="cmmi-10x-x-109">λ</span><span class="cmss-10x-x-109">, then compute the corresponding eigenvectors by solving the linear equations </span>(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">λI</span>)<span class="cmbx-10x-x-109">x </span>= <span class="cmbx-10x-x-109">0</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">However, there are problems with this approach. For an </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">matrix, the characteristic polynomial </span><span class="cmmi-10x-x-109">p</span>(<span class="cmmi-10x-x-109">λ</span>) = det(<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">λI</span>) <span class="cmss-10x-x-109">is a polynomial of degree </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">. Even if we could effectively evaluate</span> det(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">λI</span>) <span class="cmss-10x-x-109">for any lambda, there are serious issues. Unfortunately, unlike for the quadratic equation </span><span class="cmmi-10x-x-109">ax</span><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">bx </span>+ <span class="cmmi-10x-x-109">c </span>= 0<span class="cmss-10x-x-109">, there are no formulas for finding the solutions when </span><span class="cmmi-10x-x-109">n/span&gt;4<span class="cmss-10x-x-109">. (It is not that mathematicians were just not clever enough to find them. No such formula exists.)</span> </span></p>
<p><span class="cmss-10x-x-109">How can we find an alternative approach? Once again, we use the wishful thinking approach that worked so well before. Let’s pretend that we know</span><span id="dx1-124002"></span> <span class="cmss-10x-x-109">the eigenvalues, play around with them, and see if this gives us some useful insight.</span></p>
<p><span class="cmss-10x-x-109">For the sake of simplicity, assume that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is a small symmetric </span>2 <span class="cmsy-10x-x-109">× </span>2 <span class="cmss-10x-x-109">matrix, say with eigenvalues </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> = 4 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">2</span></sub> = 2<span class="cmss-10x-x-109">. Since </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is symmetric, we can even find a set of corresponding eigenvectors </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">such that </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">form an orthonormal basis. (That is, both have a unit norm and they are orthogonal to each other.) This is guaranteed by the spectral decomposition theorem (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch013.xhtml#x1-118004r41"><span class="cmssi-10x-x-109">41</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">Thus, any </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">can be written as </span><span class="cmbx-10x-x-109">x </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">for some nonzero scalars </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. What happens if we apply the transformation </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">to our vector </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">? Because </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is eigenvectors, we have</span></p>
<div class="math-display">
<img src="../media/file703.png" class="math-display" alt="Ax = A (x1u1 + x2u2 ) = x1Au1 + x2Au2 = x1λ1u1 + x2λ2u2 = 4x1u1 + 2x2u2. "/>
</div>
<p><span class="cmss-10x-x-109">By applying </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">one more time, we obtain</span></p>
<div class="math-display">
<img src="../media/file704.png" class="math-display" alt=" 2 2 2 A x = x1λ1u1 + x2λ2u2 2 2 = 4x1u1 + 2 x2u2. "/>
</div>
<p><span class="cmss-10x-x-109">A pattern starts to emerge. In general, the </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-th iteration of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">yields</span></p>
<div class="math-display">
<img src="../media/file705.png" class="math-display" alt="Akx = x1 λk1u1 + x2λk2u2 = 4kx1u1 + 2kx2u2. "/>
</div>
<p><span class="cmss-10x-x-109">By taking an inquisitive look at </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">k</span></sup><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, we can note that the contribution of </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">much </span><span class="cmss-10x-x-109">more significant than </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. Why? Because the coefficient </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">k</span></sup> = 4<sup><span class="cmmi-8">k</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">grows faster than </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">2</span></sub><sup><span class="cmmi-8">k</span></sup> = 2<sup><span class="cmmi-8">k</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, regardless of the value of </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. In technical terms, we say that </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">dominates </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Now, by scaling things down with </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">k</span></sup><span class="cmss-10x-x-109">, we can extract the eigenvector </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">! That is,</span></p>
<div class="math-display">
<img src="../media/file706.png" class="math-display" alt="Akx- λ2- k λk = x1u1 + x2(λ1 ) u2 1 = x1u1 + (something very small)k. "/>
</div>
<p><span class="cmss-10x-x-109">If we</span> <span id="dx1-124003"></span><span class="cmss-10x-x-109">let </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">grow infinitely, the contribution of </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">to</span> <img src="../media/file707.png" class="frac" data-align="middle" alt="Akkx- λ1"/> <span class="cmss-10x-x-109">vanishes. If you are familiar with the concept of </span><span class="cmssi-10x-x-109">limits</span><span class="cmss-10x-x-109">, you could write</span></p>
<div style="display: flex; justify-content: space-between; align-items: flex-start;">
  <div>
    <span>
      lim<sub>k → ∞</sub>
      <img src="../media/file708.png" alt="(A_k x - λ_k 1)" width="15" style="vertical-align: middle;"/>
      = <i>x</i><sub>1</sub><b>u</b><sub>1</sub>.
    </span>
  </div>
</div>
<div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(7.6)
  </div>

<div class="newtheorem">
<p><span class="head"> <span id="x1-124004r7"></span> <span class="cmbx-10x-x-109">Remark 7.</span> </span><span class="cmbx-10x-x-109">(A primer on limits)</span></p>
<p>If you are not familiar with limits, here is a quick explanation. The identity</p>
<div class="math-display">
<img src="../media/file709.png" class="math-display" alt=" Akx lim --k- = x1u1 k→ ∞ λ1 "/>
</div>
<p>means that as <span class="cmmi-10x-x-109">k </span>grows, the quantity <img src="../media/file710.png" class="frac" data-align="middle" alt="Akx- λk1"/> gets closer and closer to <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub>, until the difference between them is infinitesimal. In practice, this means that we can approximate <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> by <img src="../media/file711.png" class="frac" data-align="middle" alt="Akx- λk1"/> by selecting a very large <span class="cmmi-10x-x-109">k</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Equation (</span><a href="ch013.xhtml#power-iteration-for-calculating-the-eigenvectors-of-real-symmetric-matrices"><span class="cmss-10x-x-109">7.6</span></a><span class="cmss-10x-x-109">) is great news for us! All we have to do is repeatedly apply the transformation </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">to identify the eigenvector for the dominant eigenvalue </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">. There is one small caveat, though: we have to know the value of </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">. We’ll deal with this later, but first, let’s record this milestone in the form of a theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-124005r51"></span> <span class="cmbx-10x-x-109">Theorem 51.</span> </span></p>
<p><span class="cmti-10x-x-109">Finding the eigenvector for the dominant eigenvalue with power iteration.</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a real symmetric matrix. Suppose that:</span></p>
<p><span class="cmti-10x-x-109">(a) The eigenvalues of </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">are </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">…/span&gt;<span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">(that is, </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmti-10x-x-109">is the dominant eigenvalue).</span> </span></span></p>
<p><span class="cmti-10x-x-109">(b) The corresponding eigenvectors </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmbx-8">n</span></sub> <span class="cmti-10x-x-109">form an orthonormal basis.</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a vector such that when written as the linear combination </span><span class="cmbx-10x-x-109">x </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub><span class="cmti-10x-x-109">, the coefficient </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">is nonzero. Then,</span></p>
<div style="display: flex; justify-content: space-between; align-items: baseline;">
  <div>
    <span>
      lim<sub>k → ∞</sub>
      <img src="../media/file712.png" alt="(A_k x - λ_k 1)" style="vertical-align: middle;" height="35"/>
      = <i>x</i><sub>1</sub><b>u</b><sub>1</sub>.
    </span>
  </div>
  <div class="equation-label">(7.7)</div>
 <!-- Optional label -->
</div>

</div>
<p><span class="cmss-10x-x-109">Before we jump into the proof, some explanations are in order. Recall that if </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is symmetric, the spectral decomposition theorem (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch013.xhtml#x1-118004r41"><span class="cmssi-10x-x-109">41</span></a><span class="cmss-10x-x-109">) guarantees that it can be diagonalized with a similarity transformation. In its proof (sketch), we mentioned that a symmetric matrix has:</span></p>
<ul>
<li><span class="cmss-10x-x-109">Real eigenvalues</span></li>
<li><span class="cmss-10x-x-109">An orthonormal basis from its eigenvectors</span></li>
</ul>
<p><span class="cmss-10x-x-109">Thus, the assumptions </span><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">are guaranteed, except</span><span id="dx1-124006"></span> <span class="cmss-10x-x-109">for one caveat: the eigenvalues are not necessarily distinct. However, this rarely causes problems in practice. There are multiple reasons for this, but most importantly, matrices with repeated eigenvalues are so rare that they form a zero-probability set. (We’ll learn about probability later in the book. For now, we can assume that randomly picking a matrix with repeated eigenvalues is impossible.) Thus, stumbling upon one is highly unlikely.</span></p>
<div id="tcolobox-184" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Because </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">is the eigenvector for the eigenvalue </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
  <span>
    <i>A</i><sup>k</sup><b>x</b> = 
    <span style="font-size: 1.2em;">∑</span><sub>i=1</sub><sup>n</sup>
    <i>x</i><sub>i</sub><i>λ</i><sub>i</sub><sup>k</sup><b>u</b><sub>i</sub>.
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(7.8)</span>
</div>

<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file713.png" class="math-display" alt=" n Akx- ∑ λi-k λk = x1u1 + xi(λ1) ui. 1 i=2 "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">is the dominant eigenvalue, </span><span class="cmsy-10x-x-109">∥</span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">∕λ</span><sub><span class="cmr-8">1</span></sub><span class="cmsy-10x-x-109">∥</span><span class="cmmi-10x-x-109">/span&gt;1 <span class="cmss-10x-x-109">for </span><span class="cmmi-10x-x-109">i </span>= 2<span class="cmmi-10x-x-109">,…,n</span><span class="cmss-10x-x-109">, so</span> (<span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">∕λ</span><sub><span class="cmr-8">1</span></sub>)<sup><span class="cmmi-8">k</span></sup> <span class="cmsy-10x-x-109">→ </span>0 <span class="cmss-10x-x-109">as </span><span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">→∞</span><span class="cmss-10x-x-109">. Hence,</span> </span></p>
<div class="math-display">
<img src="../media/file714.png" class="math-display" alt=" Akx- kli→m∞ λk = x1u1. 1 "/>
</div>
<p><span class="cmss-10x-x-109">This is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Now, let’s fix the small issue that requires us to know </span><img src="../media/file715.png" class="math" alt="λ1 "/><span class="cmss-10x-x-109">. Since </span><img src="../media/file716.png" class="math" alt="λ1 "/> <span class="cmss-10x-x-109">is the largest eigenvalue, the previous theorem shows that </span><img src="../media/file717.png" class="math" alt="Akx "/> <span class="cmss-10x-x-109">equals </span><img src="../media/file718.png" class="math" alt="x1 λk1u1 "/> <span class="cmss-10x-x-109">plus some term that is much smaller, at least compared to this dominant term. We can extract this quantity by taking the supremum norm </span><img src="../media/file719.png" class="math" alt="∥Akx ∥∞ "/><span class="cmss-10x-x-109">. (Recall that for any </span><img src="../media/file720.png" class="math" alt="y = (y1,...,yn) "/><span class="cmss-10x-x-109">, the supremum norm is defined by</span> <img src="../media/file721.png" class="math" alt="∥y ∥∞ = max {|y1|,...,|yn|} "/><span class="cmss-10x-x-109">. Keep in mind that the </span><img src="../media/file722.png" class="math" alt="yi "/><span class="cmss-10x-x-109">-s are the coefficients of</span> <img src="../media/file723.png" class="math" alt="y "/> <span class="cmss-10x-x-109">in the original basis of our vector space, which is not necessarily our eigenvector basis </span><img src="../media/file724.png" class="math" alt="u1,...,un "/><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">By factoring out </span><span class="cmmi-10x-x-109">jλ</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">j</span><sup><span class="cmmi-8">k</span></sup> <span class="cmss-10x-x-109">from </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">k</span></sup><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file725.png" class="math-display" alt=" ∑n ∥Akx ∥∞ = |λ1|k∥x1u1 + xi(λi)kui∥∞. i=2 λ1 "/>
</div>
<p><span class="cmss-10x-x-109">Intuitively speaking, the remainder term </span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=2</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="bbig">(</span><img src="../media/file727.png" width="15" data-align="middle" alt="λi λ1"/><span class="bbig">)</span><sup><span class="cmmi-8">k</span></sup><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is small, thus we can approximate the norm as</span></p>
<div class="math-display">
<img src="../media/file729.png" class="math-display" alt="∥Akx ∥∞ ≈ |λ1 |k∥x1u1∥ ∞. "/>
</div>
<p><span class="cmss-10x-x-109">In other words, instead of scaling with </span><img src="../media/file730.png" class="math" alt="λk 1 "/><span class="cmss-10x-x-109">, we can scale with </span><img src="../media/file731.png" class="math" alt="∥Akx ∥∞ "/><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">So, we are</span> <span id="dx1-124007"></span><span class="cmss-10x-x-109">ready to describe our general eigenvector-finding procedure fully. First, we initialize a vector </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> <span class="cmss-10x-x-109">randomly, then we define the recursive sequence</span></p>
<div class="math-display">
<img src="../media/file732.png" class="math-display" alt=" Axk −1 xk = ----------, k = 1,2,... ∥Axk −1∥∞ "/>
</div>
<p><span class="cmss-10x-x-109">Using the linearity of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, we can see that, in fact,</span></p>
<div class="math-display">
<img src="../media/file733.png" class="math-display" alt=" k xk = --A--x0--, ∥Akx0 ∥∞ "/>
</div>
<p><span class="cmss-10x-x-109">but scaling has an additional side benefit, as we don’t have to use large numbers at any computational step. With this, (</span><a href="ch013.xhtml#x1-124005r51"><span class="cmss-10x-x-109">51</span></a><span class="cmss-10x-x-109">) implies that</span></p>
<div class="math-display">
<img src="../media/file734.png" class="math-display" alt=" --Akx0--- lki→m∞ xk = kl→im∞ ∥Akx0 ∥∞ = u1. "/>
</div>
<p><span class="cmss-10x-x-109">That is, we can extract the eigenvector for the dominant eigenvalue without actually knowing the eigenvalue itself.</span></p>
</section>
<section id="power-iteration-in-practice" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_106"><span class="titlemark"><span class="cmss-10x-x-109">7.5.2 </span></span> <span id="x1-1250008.5.2"></span><span class="cmss-10x-x-109">Power iteration in practice</span></h3>
<p><span class="cmss-10x-x-109">Let’s put the power</span><span id="dx1-125001"></span> <span class="cmss-10x-x-109">iteration method into practice! The input of our </span><span class="cmtt-10x-x-109">power_iteration </span><span class="cmss-10x-x-109">function is a square matrix </span><span class="cmtt-10x-x-109">A</span><span class="cmss-10x-x-109">, and we expect the output to be an eigenvector corresponding to the dominant eigenvalue.</span></p>
<p><span class="cmss-10x-x-109">Since this is an iterative process, we should define a condition that defines when the process should terminate. If the consecutive members of the sequence </span><span class="cmsy-10x-x-109">{</span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup> <span class="cmss-10x-x-109">are sufficiently close together, we arrived at a solution. That is, if, say,</span></p>
<div class="math-display">
<img src="../media/file735.png" class="math-display" alt="∥xk+1 − xk∥2 &lt;1× 10−10, "/>
</div>
<p><span class="cmss-10x-x-109">we can stop and return the current value. However, this might never happen. For those cases, we define a cutoff point, say, </span><span class="cmmi-10x-x-109">k </span>= 100<span class="cmmi-10x-x-109">,</span>000<span class="cmss-10x-x-109">, when we terminate the computation, even if there is no convergence.</span></p>
<p><span class="cmss-10x-x-109">To give</span> <span id="dx1-125002"></span><span class="cmss-10x-x-109">us a bit more control, we can also manually define the initialization vector </span><span class="cmtt-10x-x-109">x_init</span><span class="cmss-10x-x-109">.</span></p>
<div id="tcolobox-185" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import numpy as np 
def power_iteration( 
    A: np.ndarray, 
    n_max_steps: int = 100000, 
    convergence_threshold: float = 1e-10, 
    x_init: np.ndarray = None, 
    normalize: bool = False 
): 
    #x0022;"/span&gt; 
    Performs the power iteration method to find an approximation of the dominant eigenvector 
    of a square matrix. 
 
    Parameters 
    ---------- 
    A : np.ndarray 
        A square matrix whose dominant eigenvector is to be computed. 
    n_max_steps : int, optional 
        The maximum number of iterations to perform. Default is 100000. 
    convergence_threshold : float, optional 
        The convergence threshold for the difference between successive approximations. 
        Default is 1e-10. 
    x_init : np.ndarray, optional 
        The initial guess for the eigenvector. If None, a random vector is used. 
        Default is None. 
    normalize : bool, optional 
        If True, the resulting vector is normalized to unit length. Default is False. 
 
    Returns 
    ------- 
    np.ndarray 
        The approximate dominant eigenvector of the matrix ‘A‘. 
 
    Raises 
    ------ 
    ValueError 
        If the input matrix ‘A‘ is not square. 
    #x0022;"/span&gt; 
 
    n, m = A.shape 
 
    # checking the validity of the input 
    if n != m: 
        raise ValueError("/span&gt;the matrix A must be square 
 
    # reshaping or defining the initial vector 
    if x_init is not None: 
        x = x_init.reshape(-1, 1) 
    else: 
        x = np.random.normal(size=(n, 1)) 
 
    # performing the iteration 
    for step in range(n_max_steps): 
        x_transformed = A @ x    # applying the transform 
        x_new = x_transformed / np.linalg.norm(x_transformed, ord=np.inf)    # scaling the result 
 
        # quantifying the difference between the new and old vector 
        diff = np.linalg.norm(x - x_new) 
        x = x_new 
 
        # stopping the iteration in case of convergence 
        if diff /span&gt; convergence_threshold: 
            break 
 
    # normalizing the result if required 
    if normalize: 
        return x / np.linalg.norm(x) 
 
    return x</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">To test</span> <span id="dx1-125071"></span><span class="cmss-10x-x-109">the method, we should use an input for which the correct output is easy to calculate by hand. Our usual recurring example</span></p>
<div class="math-displa">
<img src="../media/file736.png" width="150" class="math-display" alt=" ⌊ ⌋ 2 1 A = ⌈ ⌉ . 1 2 "/>
</div>
<p><span class="cmss-10x-x-109">should be perfect, as we already know a lot about it.</span></p>
<p><span class="cmss-10x-x-109">Previously, we have seen in </span><span class="cmssi-10x-x-109">Section </span><a href="ch012.xhtml#finding-eigenvectors"><span class="cmssi-10x-x-109">6.2.2</span></a> <span class="cmss-10x-x-109">that its eigenvalues are </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> = 3 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">2</span></sub> = 1<span class="cmss-10x-x-109">, with corresponding eigenvectors </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub> = (<span class="cmsy-10x-x-109">−</span>1<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Let’s see if our function correctly recovers (a scalar multiple of) </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> = (1<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">!</span></p>
<div id="tcolobox-186" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.array([[2, 1], [1, 2]]) 
u_1 = power_iteration(A, normalize=True) 
u_1</code></pre>
</div>
</div>
<pre class="lstlisting"><code>array([[0.70710678], 
      [0.70710678]])</code></pre>
<p><span class="cmss-10x-x-109">Success! To recover the eigenvalue, we can simply apply the linear</span><span id="dx1-125077"></span> <span class="cmss-10x-x-109">transformation and compute the proportions.</span></p>
<div id="tcolobox-187" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A @ u_1 / u_1</code></pre>
</div>
</div>
<pre class="lstlisting"><code>array([[3.], 
      [3.]])</code></pre>
<p><span class="cmss-10x-x-109">The result is </span>3<span class="cmss-10x-x-109">, as expected.</span></p>
</section>
<section id="power-iteration-for-the-rest-of-the-eigenvectors" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_107"><span class="titlemark"><span class="cmss-10x-x-109">7.5.3 </span></span> <span id="x1-1260008.5.3"></span><span class="cmss-10x-x-109">Power iteration for the rest of the eigenvectors</span></h3>
<p><span class="cmss-10x-x-109">Can we modify the</span> <span id="dx1-126001"></span><span class="cmss-10x-x-109">power iteration algorithm to recover the other eigenvalues as well? In theory, yes. In practice, no. Let me elaborate!</span></p>
<p><span class="cmss-10x-x-109">To get a grip on how to generalize the idea, let’s take another look at the equation (</span><a href="#"><span class="cmss-10x-x-109">7.8</span></a><span class="cmss-10x-x-109">), saying that</span></p>
<div class="math-display">
<img src="../media/file737.png" class="math-display" alt=" ∑n Akx = xiλkiui. i=1 "/>
</div>
<p><span class="cmss-10x-x-109">One of the conditions for</span> <img src="../media/file738.png" width="15" data-align="middle" alt=" k Aλxk- 1"/> <span class="cmss-10x-x-109">to converge was that </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">should have a nonzero component of the eigenvector </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, that is, </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">≠</span>0<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">What if </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> = 0<span class="cmss-10x-x-109">? In that case, we have</span></p>
<div class="math-display">
<img src="../media/file739.png" class="math-display" alt="Akx = x2λk2u2 + ⋅⋅⋅+ xnλk2un, "/>
</div>
<p><span class="cmss-10x-x-109">with </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">2</span></sub><sup><span class="cmmi-8">k</span></sup><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">becoming the dominant term.</span></p>
<p><span class="cmss-10x-x-109">Thus, we have</span></p>
<div class="math-display">
<img src="../media/file740.png" class="math-display" alt="Akx ∑n λi k --k-= x2u2 + xi( --) uk λ 2 i=3 λ2 = x u + (something very small), 2 2 k "/>
</div>
<p><span class="cmss-10x-x-109">implying that</span></p>
<div class="math-display">
<img src="../media/file741.png" class="math-display" alt=" Akx lim --k-= x2u2. k→∞ λ 2 "/>
</div>
<p><span class="cmss-10x-x-109">Let’s make this mathematically precise in the following theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-126002r52"></span> <span class="cmbx-10x-x-109">Theorem 52.</span> </span><span class="cmbxti-10x-x-109">(Generalized power iteration)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a real symmetric matrix. Suppose that:</span></p>
<p><span class="cmti-10x-x-109">(a) The eigenvalues of </span><span class="cmmi-10x-x-109">A </span><span class="cmti-10x-x-109">are </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">…/span&gt;<span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">(that is, </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmti-10x-x-109">is the dominant eigenvalue).</span> </span></span></p>
<p><span class="cmti-10x-x-109">(b) The corresponding eigenvectors </span><span class="cmmi-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,u</span><sub><span class="cmmi-8">n</span></sub> <span class="cmti-10x-x-109">form an orthonormal basis.</span></p>
<p><span class="cmti-10x-x-109">Let </span><img src="../media/file742.png" class="math" alt="x ∈ ℝn "/> <span class="cmti-10x-x-109">be a vector such that, when written as a linear combination of the basis </span><img src="../media/file743.png" class="math" alt="u1,...,un "/><span class="cmti-10x-x-109">, its first nonzero component is along </span><img src="../media/file744.png" class="math" alt="ul "/> <span class="cmti-10x-x-109">for some</span> <img src="../media/file745.png" class="math" alt="l = 1,2,...,n "/> <span class="cmti-10x-x-109">(that is, </span><img src="../media/file746.png" class="math" alt=" ∑ x = ni=l xiui "/><span class="cmti-10x-x-109">).</span></p>
<p><span class="cmti-10x-x-109">Then,</span></p>
<div class="math-display">
<img src="../media/file747.png" class="math-display" alt=" k lim A--x = xlul k→ ∞ λkl "/>
</div>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<p><span class="cmss-10x-x-109">The proof looks just like what we have seen a few times already. The question is, how can we eliminate the </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">l</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">components from any vector? The answer is simple: orthogonal projections (</span><span class="cmssi-10x-x-109">Section </span><a href="ch013.xhtml#orthogonal-projections"><span class="cmssi-10x-x-109">7.4</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">For the sake of</span> <span id="dx1-126003"></span><span class="cmss-10x-x-109">simplicity, let’s take a look at extracting the second dominant eigenvector with power iteration. Recall that the transformation</span></p>
<div class="math-display">
<img src="../media/file748.png" class="math-display" alt="proj (x) = ⟨x, u ⟩u u1 1 1 "/>
</div>
<p><span class="cmss-10x-x-109">describes the orthogonal projection of any </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">to </span><span class="cmbx-10x-x-109">u</span><span class="cmssi-10x-x-109">1</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In concrete terms, if </span><span class="cmbx-10x-x-109">x </span>= <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file749.png" class="math-display" alt=" ∑n proju (x) = proju ( xiui) 1 1 i=1 ∑n = xiproj (ui) i=1 u1 = x1u1. "/>
</div>
<p><span class="cmss-10x-x-109">This is the exact opposite of what we are looking for! However, at this point, we can see that </span><span class="cmmi-10x-x-109">I </span><span class="cmsy-10x-x-109">−</span> proj<sub><span class="cmbx-8">u</span><sub><span class="cmr-6">1</span></sub></sub> <span class="cmss-10x-x-109">is going to be suitable for our purposes. This is still an orthogonal projection. Moreover, we have</span></p>
<div class="math-display">
<img src="../media/file750.png" class="math-display" alt=" ∑n ∑n (I − proju1)( xiui) = xiui, i=1 i=2 "/>
</div>
<p><span class="cmss-10x-x-109">That is, </span><span class="cmmi-10x-x-109">I </span><span class="cmsy-10x-x-109">−</span> proj<sub><span class="cmbx-8">u</span><sub><span class="cmr-6">1</span></sub></sub> <span class="cmss-10x-x-109">eliminates the </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">component of </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. Thus, if we initialize the power iteration with </span><span class="cmbx-10x-x-109">x</span><sup><span class="cmsy-8">∗</span></sup> = (<span class="cmmi-10x-x-109">I </span><span class="cmsy-10x-x-109">−</span> proj<sub><span class="cmbx-8">u</span><sub><span class="cmr-6">1</span></sub></sub>)(<span class="cmbx-10x-x-109">x</span>)<span class="cmss-10x-x-109">, the sequence</span> <img src="../media/file751.png" class="frac" data-align="middle" alt="--Akx∗-- ∥Akx∗∥∞"/> <span class="cmss-10x-x-109">will converge to </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, the second dominant eigenvector.</span></p>
<p><span class="cmss-10x-x-109">How do we compute </span><img src="../media/file752.png" width="75" alt="(I − proj )(x) u1 "/> <span class="cmss-10x-x-109">in practice? Recall that in the standard orthonormal basis, the matrix of </span><img src="../media/file753.png" width="45" alt="proju1 "/> <span class="cmss-10x-x-109">can be written as:</span></p>
<img src="../media/file754.png" class="math-display" alt="proju1 = u1uT1. " width="150"/>
<p><span class="cmss-10x-x-109">(Keep in mind that the </span><img src="../media/file755.png" class="math" alt="ui "/> <span class="cmss-10x-x-109">vectors form an orthonormal basis, so </span><img src="../media/file756.png" width="45" alt="∥u1∥ = 1 "/><span class="cmss-10x-x-109">.) Thus, the matrix of </span><img src="../media/file757.png" class="math" alt="I − proju1 "/> <span class="cmss-10x-x-109">is:</span></p>
<img src="../media/file758.png" width="150" class="math-display" alt="I − u1uT1, "/>
<p><span class="cmss-10x-x-109">which we can</span> <span id="dx1-126004"></span><span class="cmss-10x-x-109">easily compute.</span></p>
<p><span class="cmss-10x-x-109">For a general vector </span><span class="cmbx-10x-x-109">u</span><span class="cmss-10x-x-109">, this is how we can do this in NumPy.</span></p>
<div id="tcolobox-188" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def get_orthogonal_complement_projection(u: np.ndarray): 
    #x0022;"/span&gt; 
    Compute the projection matrix onto the orthogonal complement of the vector u. 
 
    This function returns a projection matrix P such that for any vector v, 
    P @ v is the projection of v onto the subspace orthogonal to u. 
 
    Parameters 
    ---------- 
    u : np.ndarray 
        A 1D or 2D array representing the vector u. It will be reshaped to a column vector. 
 
    Returns 
    ------- 
    np.ndarray 
        The projection matrix onto the orthogonal complement of u. This matrix 
        has shape (n, n), where n is the length of u. 
    #x0022;"/span&gt; 
 
    u = u.reshape(-1, 1) 
    n, _ = u.shape 
    return np.eye(n) - u @ u.T / np.linalg.norm(u, ord=2)**2</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">So, the procedure to find </span><span class="cmssi-10x-x-109">all </span><span class="cmss-10x-x-109">the eigenvectors is the following.</span></p>
<ol>
<li><span id="x1-126028x1"><span class="cmss-10x-x-109">Initialize a random </span><img src="../media/file759.png" class="math" alt=" (1) x "/> <span class="cmss-10x-x-109">and use the power iteration to find </span><img src="../media/file760.png" class="math" alt="u1 "/><span class="cmss-10x-x-109">.</span></span></li>
<li><div id="x1-126030x2">
<p><span class="cmss-10x-x-109">Project </span><img src="../media/file761.png" class="math" alt="x (1) "/> <span class="cmss-10x-x-109">to the orthogonal complement of the subspace spanned by</span> <img src="../media/file762.png" class="math" alt="u 1 "/><span class="cmss-10x-x-109">, thus obtaining</span></p>
<div class="math-display">
<img src="../media/file763.png" class="math-display" alt="x (2) := (I − proju1)(x(1)), "/>
</div>
<p><span class="cmss-10x-x-109">which we</span> <span id="dx1-126031"></span><span class="cmss-10x-x-109">use as the initial vector of the second round of power iteration, yielding the second dominant eigenvector </span><img src="../media/file764.png" class="math" alt="u2 "/><span class="cmss-10x-x-109">.</span></p>
</div></li>
<li><div id="x1-126033x3">
<p><span class="cmss-10x-x-109">Project </span><img src="../media/file765.png" class="math" alt="x (2) "/> <span class="cmss-10x-x-109">to the orthogonal complement of the subspace spanned by</span> <img src="../media/file766.png" class="math" alt="u 1 "/> <span class="cmss-10x-x-109">and </span><img src="../media/file767.png" class="math" alt="u 2 "/><span class="cmss-10x-x-109">, thus obtaining</span></p>
<div class="math-display">
<img src="../media/file768.png" class="math-display" alt="x(3) = (I − proju2)(x(2)) (1) = (I − proju1,u2)(x ), "/>
</div>
<p><span class="cmss-10x-x-109">which we use as the initial vector of the third round of power iteration, yielding the third dominant eigenvector </span><img src="../media/file769.png" class="math" alt="u3 "/><span class="cmss-10x-x-109">.</span></p>
</div></li>
<li><span id="x1-126035x4"><span class="cmss-10x-x-109">Project </span><img src="../media/file770.png" class="math" alt=" (3) x "/> <span class="cmss-10x-x-109">to…</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">You get the pattern. To implement this in practice, we add the </span><span class="cmtt-10x-x-109">find_eigenvectors </span><span class="cmss-10x-x-109">function.</span></p>
<div id="tcolobox-189" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def find_eigenvectors(A: np.ndarray, x_init: np.ndarray): 
    #x0022;"/span&gt; 
    Find the eigenvectors of the matrix A using the power iteration method. 
 
    This function computes the eigenvectors of the matrix A by iteratively 
    applying the power iteration method and projecting out previously found 
    eigenvectors to find orthogonal eigenvectors. 
 
    Parameters 
    ---------- 
    A : np.ndarray 
        A square matrix of shape (n, n) for which eigenvectors are to be computed. 
 
    x_init : np.ndarray 
        A 1D array representing the initial vector used for the power iteration. 
 
    Returns 
    ------- 
    List[np.ndarray] 
        A list of eigenvectors, each represented as a 1D numpy array of length n. 
    #x0022;"/span&gt; 
 
    n, _ = A.shape 
    eigenvectors = [] 
 
    for _ in range(n): 
        ev = power_iteration(A, x_init=x_init) 
        proj = get_orthogonal_complement_projection(ev) 
        x_init = proj @ x_init 
        x_init = x_init / np.linalg.norm(x_init, ord=np.inf) 
        eigenvectors.append(ev) 
 
    return eigenvectors</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s test </span><span class="cmtt-10x-x-109">find_eigenvectors </span><span class="cmss-10x-x-109">out on our old friend</span></p>
<img src="../media/file771.png" width="150" class="math-display" alt=" ⌊ ⌋ 2 1 A = ⌈ ⌉! 1 2 "/>
<div id="tcolobox-190" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.array([[2.0, 1.0], [1.0, 2.0]]) 
x_init = np.random.rand(2, 1) 
find_eigenvectors(A, x_init)</code></pre>
</div>
</div>
<pre class="lstlisting"><code>[array([[1.], 
       [1.]]), 
 array([[ 1.], 
       [-1.]])]</code></pre>
<p><span class="cmss-10x-x-109">The result is as we expected. (Don’t be surprised that the eigenvectors are not normalized, as we haven’t explicitly done so in the </span><span class="cmtt-10x-x-109">find_eigenvectors </span><span class="cmss-10x-x-109">function.)</span></p>
<p><span class="cmss-10x-x-109">We are ready</span> <span id="dx1-126076"></span><span class="cmss-10x-x-109">to actually diagonalize symmetric matrices. Recall that the diagonalizing orthogonal matrix </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">can be obtained by vertically stacking the eigenvectors one by one.</span></p>
<div id="tcolobox-191" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def diagonalize_symmetric_matrix(A: np.ndarray, x_init: np.ndarray): 
    #x0022;"/span&gt; 
    Diagonalize a symmetric matrix A using its eigenvectors. 
 
    Parameters 
    ---------- 
    A : np.ndarray 
        A symmetric matrix of shape (n, n) to be diagonalized. The matrix should 
        be square and symmetric. 
 
    x_init : np.ndarray 
        A 1D array representing the initial guess for the power iteration. 
 
    Returns 
    ------- 
    Tuple[np.ndarray, np.ndarray] containing: 
        - U : np.ndarray 
            A matrix of shape (n, n) whose columns are the normalized eigenvectors 
            of A. 
        - np.ndarray 
            A diagonal matrix (n, n) of the eigenvalues of A, computed as U @ A @ U.T. 
    #x0022;"/span&gt; 
 
    eigenvectors = find_eigenvectors(A, x_init) 
    U = np.hstack(eigenvectors) / np.linalg.norm(np.hstack(eigenvectors), axis=0, ord=2) 
    return U, U @ A @ U.T 
 
diagonalize_symmetric_matrix(A, x_init)</code></pre>
</div>
</div>
<pre class="lstlisting"><code>(array([[ 0.70710678,  0.70710678], 
       [ 0.70710678, -0.70710678]]), 
 array([[ 3.00000000e+00, -3.57590301e-11], 
       [-3.57589164e-11,  1.00000000e+00]]))</code></pre>
<p><span class="cmss-10x-x-109">Awesome!</span></p>
<p><span class="cmss-10x-x-109">What’s the problem? Unfortunately, power iteration is numerically unstable. For </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">matrices, where </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">can be in the millions, this is a serious issue.</span></p>
<p><span class="cmss-10x-x-109">Why did we</span> <span id="dx1-126109"></span><span class="cmss-10x-x-109">talk so much about power iteration, then? Besides being the simplest, it offers a deep insight into how linear transformations work.</span></p>
<p><span class="cmss-10x-x-109">The identity</span></p>
<div class="math-dispay">
<img src="../media/file772.png" width="150" class="math-display" alt=" ∑n Ax = xiλiui, i=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">are eigenvalue-eigenvector pairs of the symmetric matrix </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, reflecting how eigenvectors and eigenvalues determine the behavior of the transformation.</span></p>
<p><span class="cmss-10x-x-109">If the power iteration is not usable in practice, how can we compute the eigenvalues? We will see this in the next section.</span></p>
</section>
</section>
<section id="the-qr-algorithm" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_108"><span class="titlemark"><span class="cmss-10x-x-109">7.6 </span></span> <span id="x1-1270008.6"></span><span class="cmss-10x-x-109">The QR algorithm</span></h2>
<p><span class="cmss-10x-x-109">The algorithm</span> <span id="dx1-127001"></span><span class="cmss-10x-x-109">used in practice to compute the eigenvalues is the so-called QR algorithm, proposed independently by John G. R. Francis and the Soviet mathematician Vera Kublanovskaya. This is where all of the lessons we have learned in linear algebra converge. Describing the QR algorithm is very simple, as it is the iteration of a matrix decomposition and a multiplication step.</span></p>
<p><span class="cmss-10x-x-109">However, understanding </span><span class="cmssi-10x-x-109">why </span><span class="cmss-10x-x-109">it works is a different question. Behind the scenes, the QR algorithm combines many tools we have learned earlier. To start, let’s revisit the good old Gram-Schmidt orthogonalization process.</span></p>
<section id="the-qr-decomposition" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_109"><span class="titlemark"><span class="cmss-10x-x-109">7.6.1 </span></span> <span id="x1-1280008.6.1"></span><span class="cmss-10x-x-109">The QR decomposition</span></h3>
<p><span class="cmss-10x-x-109">If you recall, we</span> <span id="dx1-128001"></span><span class="cmss-10x-x-109">encountered the Gram-Schmidt orthogonalization process (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch008.xhtml#x1-47004r13"><span class="cmssi-10x-x-109">13</span></a><span class="cmss-10x-x-109">) when introducing the concept of orthogonal bases.</span></p>
<p><span class="cmss-10x-x-109">In essence, this algorithm takes an arbitrary basis </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">and turns it into an orthonormal one </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, such that </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">spans the same subspace as </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">for all </span>1 <span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">. Since we last met this, we have gained a lot of perspective on linear algebra, so we are ready to see the bigger picture.</span></p>
<p><span class="cmss-10x-x-109">With the orthogonal projections defined by</span></p>
<div class="math-display">
<img src="../media/file773.png" class="math-display" alt=" ∑k ⟨x,ei⟩ proje1,...,ek(x) = ------ei, i=1 ⟨ei,ei⟩ "/>
</div>
<p><span class="cmss-10x-x-109">we can describe the Gram-Schmidt process recursively as</span></p>
<div class="math-display">
<img src="../media/file774.png" class="math-display" alt="e1 = v1, ek = vk − proje1,...,ek−1(vk), "/>
</div>
<p><span class="cmss-10x-x-109">where the </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">vectors are normalized after.</span></p>
<p><span class="cmss-10x-x-109">By expanding this and writing out </span><span class="cmmi-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">explicitly, we have</span></p>
<div class="math-display">
<img src="../media/file775.png" class="math-display" alt="e1 = v1 e2 = v2 − ⟨v2,e1⟩e1 ⟨e1,e1⟩ .. . ⟨vn, e1⟩ ⟨vn, en−1⟩ en = vn − ⟨e-,e-⟩e1 − ⋅⋅⋅− ⟨e----,e---⟩en−1. 1 1 n− 1 n−1 "/>
</div>
<p><span class="cmss-10x-x-109">A pattern is starting to emerge. By arranging the </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">terms on one side, we obtain</span></p>
<div class="math-display">
<img src="../media/file776.png" class="math-display" alt="v1 = e1 ⟨v2,e1⟩ v2 = ⟨e-,e-⟩e1 + e2 1 1 ... vn = ⟨vn,e1⟩e1 + ⋅⋅⋅ + -⟨vn,en−1⟩-en−1 + en. ⟨e1,e1⟩ ⟨en−1,en−1⟩ "/>
</div>
<p><span class="cmss-10x-x-109">This is starting to resemble some kind of matrix multiplication! Recall that</span> <span id="dx1-128002"></span><span class="cmss-10x-x-109">matrix multiplication can be viewed as taking the linear combination of columns. (Check (</span><a href="#"><span class="cmss-10x-x-109">4.2</span></a><span class="cmss-10x-x-109">) if you are uncertain about this.)</span></p>
<p><span class="cmss-10x-x-109">By horizontally concatenating the column vectors </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">to form the matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and similarly defining the vector </span><span class="cmmi-10x-x-109">Q </span><span class="cmss-10x-x-109">from the </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">-s, we obtain that</span></p>
<img src="../media/file777.png" class="math-display" alt="A = Q∗R ∗ " width="75"/>
<p><span class="cmss-10x-x-109">for some upper triangular </span><span class="cmmi-10x-x-109">R</span><span class="cmss-10x-x-109">, defined by the coefficients of </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">in </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">according to the Gram-Schmidt orthogonalization. To be more precise, define</span></p>
<div class="math-display">
<img src="../media/file778.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ A = ||v ... v || , Q ∗ = || e ... e || , ⌈ 1 n⌉ ⌈ 1 n⌉ "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file779.png" class="math-display" alt=" ⌊ ⟨v2,e1⟩ ⟨vn,e1⟩⌋ |1 ⟨e1,e1⟩ ... ⟨e1,e1⟩| ||0 1 ... ⟨vn,e2⟩|| R ∗ = |. . ⟨e2,.e2⟩|. ||.. .. ... .. || ⌈ .. ⌉ 0 0 . 1 "/>
</div>
<p><span class="cmss-10x-x-109">The result </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">Q</span><sup><span class="cmsy-8">∗</span></sup><span class="cmmi-10x-x-109">R</span><sup><span class="cmsy-8">∗</span></sup> <span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">almost </span><span class="cmss-10x-x-109">what we call the QR factorization. The columns of </span><span class="cmmi-10x-x-109">Q</span><sup><span class="cmsy-8">∗</span></sup> <span class="cmss-10x-x-109">are orthogonal (but not orthonormal), while </span><span class="cmmi-10x-x-109">R</span><sup><span class="cmsy-8">∗</span></sup> <span class="cmss-10x-x-109">is upper triangular. We can easily orthonormalize </span><span class="cmmi-10x-x-109">Q</span><sup><span class="cmsy-8">∗</span></sup> <span class="cmss-10x-x-109">by factoring out the norms columnwise, thus obtaining</span></p>
<div class="math-dispay">
<img src="../media/file780.png" width="450" class="math-display" alt=" ⌊ ⌋ ∥e ∥ √⟨v2,e1⟩-- ... √⟨vn,e1⟩- ⌊ ⌋ | 1 ⟨e1,e1⟩ ⟨e1,e1⟩| | | || 0 ∥e2∥ ... √⟨vn,e2⟩|| Q = |⌈ e1-- ... en-|⌉ , R = || ⟨e2,e2⟩|| . ∥e1∥ ∥en∥ || ... ... ... ... || ⌈ . ⌉ 0 0 .. ∥en∥ "/>
</div>
<p><span class="cmss-10x-x-109">It is</span> <span id="dx1-128003"></span><span class="cmss-10x-x-109">easy to see that </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">QR </span><span class="cmss-10x-x-109">still holds. This result is called the </span><span class="cmssi-10x-x-109">QR decomposition</span><span class="cmss-10x-x-109">, and we have just proved the following theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-128004r53"></span> <span class="cmbx-10x-x-109">Theorem 53.</span> </span><span class="cmbxti-10x-x-109">(QR decomposition)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be an invertible matrix. Then, there exists an orthogonal matrix </span><span class="cmmi-10x-x-109">Q </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">and an upper triangular matrix </span><span class="cmmi-10x-x-109">R </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">such that</span></p>
<img src="../media/file781.png" width="75" class="math-display" alt="A = QR "/>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<p><span class="cmss-10x-x-109">As we are about to see, the QR decomposition is an extremely useful and versatile tool (like all other matrix decompositions are). Before we move forward to</span> <span id="dx1-128005"></span><span class="cmss-10x-x-109">discuss how it can be used to compute the eigenvalues in practice, let’s put what we have seen so far into code!</span></p>
<p><span class="cmss-10x-x-109">The QR decomposition algorithm is essentially Gram-Schmidt orthogonalization, where we explicitly memorize some coefficients and form a matrix from them. (Recall our earlier implementation in </span><span class="cmssi-10x-x-109">Section </span><a href="ch009.xhtml#the-gramschmidt-orthogonalization-process1"><span class="cmssi-10x-x-109">3.1.2</span></a> <span class="cmss-10x-x-109">if you feel overwhelmed.)</span></p>
<div id="tcolobox-192" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def projection_coeff(x: np.ndarray, to: np.ndarray): 
    #x0022;"/span&gt; 
    Compute the scalar coefficient for the projection of vector x onto vector to. 
 
    Parameters 
    ---------- 
    x : np.ndarray 
        A 1D array representing the vector onto which the projection is computed. 
 
    to : np.ndarray 
        A 1D array representing the vector onto which x is being projected. 
 
    Returns 
    ------- 
    float 
        The scalar coefficient representing the projection of x onto to. 
    #x0022;"/span&gt; 
    return np.dot(x, to)/np.dot(to, to)</code></pre>
</div>
</div>
<pre class="lstinputlisting"><code>from typing import List

def projection(x: np.ndarray, to: List[np.ndarray], return_coeffs: bool = True):
    """
    Computes the orthogonal projection of a vector `x` onto the subspace 
    spanned by a set of vectors `to`.

    Parameters
    ----------
    x : np.ndarray
        A 1D array representing the vector to be projected onto the subspace.
    
    to : List[np.ndarray]
        A list of 1D arrays, each representing a vector spanning the subspace
        onto which `x` is projected.
    
    return_coeffs : bool, optional, default=True
        If True, the function returns the list of projection coefficients.
        If False, only the projected vector is returned.

    Returns
    -------
    Tuple[np.ndarray, List[float]] or np.ndarray
        - If `return_coeffs` is True, returns a tuple where the first element
        is the projected vector and
          the second element is a list of the projection coefficients
          for each vector in `to`.
        - If `return_coeffs` is False, returns only the projected vector.
    """

    p_x = np.zeros_like(x)
    coeffs = []
    
    for e in to:
        coeff = projection_coeff(x, e)
        coeffs.append(coeff)
        p_x += coeff*e
    
    if return_coeffs:
        return p_x, coeffs
    else:
        return p_x</code></pre>
<p><span class="cmss-10x-x-109">Now we can put these together to obtain the QR factorization of an</span><span id="dx1-128066"></span> <span class="cmss-10x-x-109">arbitrary square matrix. (Surprisingly, this works for non-square matrices as well, but we won’t be concerned with this.)</span></p>
<div id="tcolobox-193" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def QR(A: np.ndarray): 
    #x0022;"/span&gt; 
    Computes the QR decomposition of matrix A using the Gram-Schmidt 
    orthogonalization process. 
 
    Parameters 
    ---------- 
    A : np.ndarray 
        A 2D array of shape (n, m) representing the matrix to be decomposed. 
        The matrix A should have full column rank for a valid QR decomposition. 
 
    Returns 
    ------- 
    Tuple[np.ndarray, np.ndarray] 
        - Q : np.ndarray 
            An orthogonal matrix of shape (n, m), whose columns are orthonormal. 
        - R : np.ndarray 
            An upper triangular matrix of shape (m, m), representing the coefficients of the 
            linear combinations of the columns of A. 
    #x0022;"/span&gt; 
    n, m = A.shape 
 
    A_columns = [A[:, i] for i in range(A.shape[1])] 
    Q_columns, R_columns = [], [] 
 
    Q_columns.append(A_columns[0]) 
    R_columns.append([1] + (m-1)*[0]) 
 
    for i, a in enumerate(A_columns[1:]): 
        p, coeffs = projection(a, Q_columns, return_coeffs=True) 
        next_q = a - p 
        next_r = coeffs + [1] + max(0, m - i - 2)*[0] 
 
        Q_columns.append(next_q) 
        R_columns.append(next_r) 
 
    # assembling Q and R from its columns 
    Q, R = np.array(Q_columns).T, np.array(R_columns).T 
 
    # normalizing Q’s columns 
    Q_norms = np.linalg.norm(Q, axis=0) 
    Q = Q/Q_norms 
    R = np.diag(Q_norms) @ R 
    return Q, R</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s try it out on a random </span>3 <span class="cmsy-10x-x-109">× </span>3 <span class="cmss-10x-x-109">matrix.</span></p>
<div id="tcolobox-194" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.random.rand(3, 3) 
Q, R = QR(A)</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">There are three things to check: </span><span class="cmssi-10x-x-109">(a) </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">QR</span><span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">(b) </span><span class="cmmi-10x-x-109">Q </span><span class="cmss-10x-x-109">is an orthogonal matrix, and </span><span class="cmssi-10x-x-109">(c) </span><span class="cmmi-10x-x-109">R </span><span class="cmss-10x-x-109">is upper</span> <span id="dx1-128113"></span><span class="cmss-10x-x-109">triangular.</span></p>
<div id="tcolobox-195" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>np.allclose(A, Q @ R)</code></pre>
</div>
</div>
<pre class="lstlisting"><code>True</code></pre>
<div id="tcolobox-196" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>np.allclose(Q.T @ Q, np.eye(3))</code></pre>
</div>
</div>
<pre class="lstlisting"><code>True</code></pre>
<div id="tcolobox-197" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>np.allclose(R, np.triu(R))</code></pre>
</div>
</div>
<pre class="lstlisting"><code>True</code></pre>
<p><span class="cmss-10x-x-109">Success! There is only one more question left. How does this help us in calculating the eigenvalues? Let’s see that now.</span></p>
</section>
<section id="iterating-the-qr-decomposition" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_110"><span class="titlemark"><span class="cmss-10x-x-109">7.6.2 </span></span> <span id="x1-1290008.6.2"></span><span class="cmss-10x-x-109">Iterating the QR decomposition</span></h3>
<p><span class="cmss-10x-x-109">Surprisingly, we</span> <span id="dx1-129001"></span><span class="cmss-10x-x-109">can discover the eigenvalues of a matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">by a simple iterative process. First, we find the QR decomposition</span></p>
<img src="../media/file783.png" class="math-display" alt="A = Q R , 1 1 " width="75"/>
<p><span class="cmss-10x-x-109">and define the matrix </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">by</span></p>
<img src="../media/file784.png" class="math-display" alt="A1 = R1Q1, " width="75"/>
<p><span class="cmss-10x-x-109">That is, we simply reverse the order of </span><span class="cmmi-10x-x-109">Q </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">R</span><span class="cmss-10x-x-109">. Then, we start it all over and find the QR decomposition of </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, and so on, defining the sequence</span></p>


<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/file785.png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(7.9)
  </div>
</div>
<p><span class="cmss-10x-x-109">In the long run, the diagonal elements of </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">will get closer and</span><span id="dx1-129002"></span> <span class="cmss-10x-x-109">closer to the eigenvalues of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. This is called the QR algorithm, which is so simple that I didn’t believe it when I first saw it.</span></p>
<p><span class="cmss-10x-x-109">With all of our tools, we can implement the QR algorithm in a few lines.</span></p>
<div id="tcolobox-198" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def QR_algorithm(A: np.ndarray, n_iter: int = 1000): 
    #x0022;"/span&gt; 
    Computes the QR algorithm to find the eigenvalues of a matrix A. 
 
    Parameters 
    ---------- 
    A : np.ndarray 
        A square matrix of shape (n, n) for which the eigenvalues are to be computed. 
 
    n_iter : int, optional, default=1000 
        The number of iterations to run the QR algorithm. 
        More iterations may lead to more accurate results, 
        but the algorithm typically converges quickly. 
 
    Returns 
    ------- 
    np.ndarray 
        A matrix that has converged, where the diagonal elements are the eigenvalues of the 
        original matrix A. 
        The off-diagonal elements should be close to zero. 
    #x0022;"/span&gt; 
 
    for _ in range(n_iter): 
        Q, R = QR(A) 
        A = R @ Q 
 
    return A</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s test it right away.</span></p>
<div id="tcolobox-199" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.array([[2.0, 1.0], [1.0, 2.0]]) 
QR_algorithm(A)</code></pre>
</div>
</div>
<pre class="lstlisting"><code>array([[3.00000000e+00, 2.39107046e-16], 
      [0.00000000e+00, 1.00000000e+00]])</code></pre>
<p><span class="cmss-10x-x-109">We are almost at the state of the art. Unfortunately, the vanilla QR algorithm has some issues, as it can fail to converge. A simple example</span><span id="dx1-129034"></span> <span class="cmss-10x-x-109">is given by the matrix</span></p>
<img src="../media/file786.png" class="math-display" alt=" ⌊ ⌋ A = ⌈0 1⌉ . 1 0 " width="75"/>

<div id="tcolobox-200" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>A = np.array([[0.0, 1.0], [1.0, 0.0]]) 
QR_algorithm(A)</code></pre>
</div>
</div>
<pre class="lstlisting"><code>array([[0., 1.], 
      [1., 0.]])</code></pre>
<p><span class="cmss-10x-x-109">In practice, we can solve this with the introduction of shifts:</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(14).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(7.10)
  </div>
</div>

<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">α</span><sub><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">is some scalar. There are multiple approaches to defining the shifts themselves (Rayleigh quotient shift, Wilkinson shift, etc.), but the details lie much deeper than our study.</span></p>
</section>
</section>
<section id="summary6" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_111"><span class="titlemark"><span class="cmss-10x-x-109">7.7 </span></span> <span id="x1-1300008.7"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">I told you that climbing the peak is not easy: so far, this was our hardest chapter yet. However, the tools we’ve learned are at the pinnacle of linear algebra. We started by studying two special transformations: the self-adjoint and orthogonal ones. The former ones gave the </span><span class="cmssi-10x-x-109">spectral decomposition theorem</span><span class="cmss-10x-x-109">, while the latter ones gave the </span><span class="cmssi-10x-x-109">singular value decomposition</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Undoubtedly, the SVD is one of the most important results in linear algebra, stating that every rectangular matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">can be written in the form</span></p>
<img src="../media/file787.png" width="150" class="math-display" alt="A = U ΣV T, "/>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, </span>Σ <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup><span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">are rather special: </span>Σ <span class="cmss-10x-x-109">is diagonal, while </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">are orthogonal.</span></p>
<p><span class="cmss-10x-x-109">When viewing </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">as a linear transformation, the singular value decomposition tells us that it can be written as the composition of two distance-preserving transformations (the orthogonal ones) and a simple scaling. That’s quite a characterization!</span></p>
<p><span class="cmss-10x-x-109">Speaking of singular values and eigenvalues, how do we find them in practice? Definitely not by solving the polynomial equation</span></p>
<div class="math-display">
<img src="../media/file788.png" class="math-display" alt="det(A − λI ) = 0, "/>
</div>
<p><span class="cmss-10x-x-109">which is a computationally painful problem.</span></p>
<p><span class="cmss-10x-x-109">We’ve seen two actual methods for the task. One is the complicated, slow, unstable, but illuminating algorithm of power iteration, yielding eigenvectors for the dominant eigenvalue of a real and symmetric matrix </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">via the limit</span></p>
<div class="math-display">
<img src="../media/file789.png" class="math-display" alt=" Akx0 lk→im∞ ∥Akx--∥-- = u1. 0 ∞ "/>
</div>
<p><span class="cmss-10x-x-109">Although power iteration gives us valuable insight into the structure of such matrices, the real deal is the QR algorithm (unrelated to QR codes), originating from the vectorized version of the Gram-Schmidt algorithm. The QR algorithm is hard to intuitively understand, but despite its mystery, it provides a blazing-fast method for computing the eigenvalues in practice.</span></p>
<p><span class="cmss-10x-x-109">What’s next? Now that we are at the peak, it’s time to relax a bit and enjoy the beautiful view. The next chapter does just that.</span></p>
<p><span class="cmss-10x-x-109">You see, one of the most beautiful and useful topics in linear algebra is the connection between matrices and graphs. Because from the right perspective, a matrix is a graph, and we can utilize this relationship to study the structure of both. Let’s see!</span></p>
</section>
<section id="problems6" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_112"><span class="titlemark"><span class="cmss-10x-x-109">7.8 </span></span> <span id="x1-1310008.8"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Let </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a set of linearly independent and pairwise orthogonal vectors. Show that the linear transformation</span></p>
<div class="math-display">
<img src="../media/file790.png" class="math-display" alt=" ∑k ⟨x, ui⟩ proju1,...,uk(x) = ⟨u-,u-⟩ui i=1 i i "/>
</div>
<p><span class="cmss-10x-x-109">is an orthogonal projection.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span><span class="cmbx-10x-x-109">u</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,</span><span class="cmbx-10x-x-109">u</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be a set of linearly independent vectors, and define the linear transformation</span></p>
<div class="math-display">
<img src="../media/file791.png" class="math-display" alt=" ∑k ⟨x,ui⟩- fakeproju1,...,uk(x ) = ⟨ui,ui⟩ui. i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Is this a projection? (Hint: Study the special case </span><span class="cmmi-10x-x-109">k </span>= 2 <span class="cmss-10x-x-109">and </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">3</span></sup><span class="cmss-10x-x-109">. You can visualize this if needed.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be an inner product space and </span><span class="cmmi-10x-x-109">P </span>: <span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be an orthogonal projection. Show that </span><span class="cmmi-10x-x-109">I </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is an orthogonal projection as well, and</span></p>
<div class="math-display">
<img src="../media/file792.png" class="math-display" alt="ker(I − P ) = im P, im (I − P ) = ker P "/>
</div>
<p><span class="cmss-10x-x-109">holds.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be two square matrix that are written in the block matrix form</span></p>
<div class="math-display">
<img src="../media/file793.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ A1,1 A1,2 B1,1 B1,2 A = ||A A ||, B = ||B B ||, ⌈ 2,1 2,2⌉ ⌈ 2,1 2,2⌉ "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,</span><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,B</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,</span><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">k</span><span class="cmsy-8">×</span><span class="cmmi-8">k</span></sup><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,</span><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,B</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,</span><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">k</span><span class="cmsy-8">×</span><span class="cmmi-8">l</span></sup><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span><span class="cmmi-8">,</span><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,B</span><sub><span class="cmr-8">2</span><span class="cmmi-8">,</span><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">l</span><span class="cmsy-8">×</span><span class="cmmi-8">k</span></sup><span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span><span class="cmmi-8">,</span><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,B</span><sub><span class="cmr-8">2</span><span class="cmmi-8">,</span><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">l</span><span class="cmsy-8">×</span><span class="cmmi-8">l</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Show that</span></p>
<div class="math-display">
<img src="../media/file794.png" class="math-display" alt=" ⌊ ⌋ A1,1B1,1 + A1,2B2,1 A1,1B1,2 + A1,2B2,2 || || AB = ⌈A2,1B1,1 + A2,2B2,1 A2,1B1,2 + A2,2B2,2⌉ . "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 5. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span><span class="cmsy-8">×</span><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">be the square matrix defined by</span></p>
<div class="math-display">
<img src="../media/file795.png" class="math-display" alt=" ⌊ ⌋ A = ⌈1 1⌉ . 1 0 "/>
</div>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Show that the two eigenvalues of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">are</span></p>
<div class="math-display">
<img src="../media/file796.png" class="math-display" alt=" − 1 λ1 = φ, λ2 = − φ , "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">φ </span>= <img src="../media/file797.png" class="frac" data-align="middle" alt="1+√5 --2--"/> <span class="cmss-10x-x-109">is the golden ratio.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Show that the eigenvectors of </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">λ</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">are</span></p>
<div class="math-display">
<img src="../media/file798.png" class="math-display" alt=" ⌊ ⌋ ⌊ −1⌋ f = ⌈φ ⌉, f = ⌈− φ ⌉ , 1 1 2 1 "/>
</div>
<p><span class="cmss-10x-x-109">and show that they are orthogonal; that is, </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">f</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">f</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">⟩ </span>= 0<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">U </span><span class="cmss-10x-x-109">be the matrix formed by the eigenvectors of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, defined by</span></p>
<div class="math-display">
<img src="../media/file799.png" class="math-display" alt=" ⌊ ⌋ φ − φ−1 U = ⌈ ⌉ . 1 1 "/>
</div>
<p><span class="cmss-10x-x-109">Show that</span></p>
<div class="math-display">
<img src="../media/file800.png" class="math-display" alt=" ⌊ ⌋ 1 φ −1 U −1 = √1-⌈ ⌉ . 5 − 1 φ "/>
</div>
<p><span class="cmssi-10x-x-109">(d) </span><span class="cmss-10x-x-109">Show that</span></p>
<img src="../media/file801.png" width="75" class="math-display" alt=" − 1 A = UΛU , "/>
<p><span class="cmss-10x-x-109">where</span></p>
<div class="math-display">
<img src="../media/file802.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ λ1 0 φ 0 Λ = ⌈ ⌉ = ⌈ −1⌉ . 0 λ2 0 − φ "/>
</div>
<p><span class="cmssi-10x-x-109">(e) </span><span class="cmss-10x-x-109">Were you wondering the purpose of all these mundane computations? Here comes the punchline. First, show that</span></p>
<div class="math-display">
<img src="../media/file803.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ n An = ⌈1 1⌉ = ⌈Fn+1 Fn ⌉ , 1 0 Fn Fn−1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is the </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-th Fibonacci number, defined by the recursive sequence</span></p>
<div class="math-display">
<img src="../media/file804.png" class="math-display" alt="F0 = 0, F1 = 1, Fn = Fn−1 + Fn−2. "/>
</div>
<p><span class="cmss-10x-x-109">Consequently, show that </span><span class="cmmi-10x-x-109">A </span>= <span class="cmmi-10x-x-109">U</span>Λ<span class="cmmi-10x-x-109">U</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">implies that</span></p>
<div class="math-display">
<img src="../media/file805.png" class="math-display" alt=" n −n Fn = φ--−-(√−-φ)-- . 5 "/>
</div>
<p><span class="cmss-10x-x-109">That’s pretty cool!</span></p>
</section>
<section id="join-our-community-on-discord7" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_113"><span id="x1-132000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>