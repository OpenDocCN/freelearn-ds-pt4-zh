- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matrix Factorizations
  prefs: []
  type: TYPE_NORMAL
- en: One of the recurring thoughts in this book is that problem-solving is about
    finding the best representations of your objects of study. Say linear transformations
    of a vector space are represented by matrices. Studying one is the same as studying
    the other, but each perspective comes with its own set of tools. Linear transformations
    are geometric, while matrices are algebraic sides of the same coin.
  prefs: []
  type: TYPE_NORMAL
- en: This thought can be applied on a smaller scale as well. Recall the LU decomposition
    from Chapter [6](ch011.xhtml#matrices-and-equations). You can think of this as
    another view of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Guess what: It’s not the only one. This chapter is dedicated to the three most
    important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: the spectral decomposition,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the singular value decomposition,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and the QR decomposition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buckle up. It’s our most challenging adventure yet.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Special transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have aspired to develop a geometric view of linear algebra. Vectors
    are mathematical objects defined by their direction and magnitude. In the spaces
    of vectors, the concept of distance and orthogonality gives rise to a geometric
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear transformations, the building blocks of machine learning, are just mappings
    that distort this structure: rotating, stretching, and skewing the geometry. However,
    there are types of transformations that preserve some of the structure. In practice,
    these provide valuable insights, and additionally, they are much easier to work
    with. In this section, we will take a look at the most important ones, those that
    we’ll encounter in machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 The adjoint transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In machine learning, the most important stage is the Euclidean space ℝ^n. This
    is where data is represented and manipulated. There, the entire geometric structure
    is defined by the inner product
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ⟨x,y ⟩ = xiyi, i=1 ](img/file626.png)'
  prefs: []
  type: TYPE_IMG
- en: 'giving rise to the notion of magnitude, direction (in the form of angles),
    and orthogonality. Because of this, transformations that can be related to the
    inner product are special. For instance, if ⟨f(x),f(x)⟩ = ⟨x,x⟩ holds for all
    x ∈ℝ^n and the linear transformation f : ℝ^n →ℝ^n, we know that f leaves the norm
    invariant. That is, distance in the original and the transformed feature space
    have the same meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will establish a general relation between images of vectors under
    a transform and their inner product. This is going to be the foundation for our
    discussions in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 39\. (The adjoint transformation)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ^n be a linear transformation. Then, there exists a linear transformation
    f^∗ : ℝ^n →ℝ^n for which'
  prefs: []
  type: TYPE_NORMAL
- en: ⟨f(**x**), **y**⟩ = ⟨**x**, f^*(**y**)⟩
  prefs: []
  type: TYPE_NORMAL
- en: (7.1)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all x,y ∈ℝ^n. f^∗ is called the adjoint transformation* of f.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if A ∈ℝ^(n×n) is the matrix of f in the standard orthonormal basis,
    then the matrix of f^∗ is A^T . That is,
  prefs: []
  type: TYPE_NORMAL
- en: ⟨A**x**, **y**⟩ = ⟨**x**, A^T**y**⟩.
  prefs: []
  type: TYPE_NORMAL
- en: (7.2)
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Suppose that A ∈ ℝ^(n×n) is the matrix of f in the standard orthonormal
    basis. For any x = (x[1],…,x[n]) and y = (y[1],…,y[n]), the inner product is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ⟨x,y ⟩ = xiyi, i=1 ](img/file627.png)'
  prefs: []
  type: TYPE_IMG
- en: and Ax can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋ |a11 a12 ... a1n |⌊ ⌋ ⌊ ∑n ⌋ |a a ... a || x1| | ∑ j=1a1jxj| || 21\.
    2.2 . 2n. |||| x2|| || nj=1a2jxj|| || .. .. .. .. |||| ..|| = || .. || . || ||⌈
    .⌉ ⌈ . ⌉ ⌈an1 an2 ... ann ⌉ xn ∑n anjxj j=1 ](img/file628.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this form, we can express ⟨Ax,y⟩ in terms of a[ij]-s, x[i]-s, and y[i]-s.
    For this, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n ⟨Ax, y⟩ = ( aijxj)yi i=1 j=1 ∑n ∑n = ( aijyi) xj j=1 i=1 ◟--◝◜---◞
    T j- th component of A y = ⟨x,AT y⟩. ](img/file629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This shows that the transformation given by f^∗ : x→A^T x satisfies ([7.1](ch013.xhtml#x1-116003r39))
    and ([7.2](ch013.xhtml#x1-116003r39)), which is what we had to show.'
  prefs: []
  type: TYPE_NORMAL
- en: Why is the quantity ⟨Ax,y⟩ that important to us? Because inner products define
    the geometric structure of a vector space. Recall the equation ([2.12](ch008.xhtml#x1-46005r12)),
    allowing us to fully describe any vector using only the inner products with respect
    to an orthonormal basis. In addition, ⟨x,x⟩ = ∥x∥² defines the notion of distance
    and magnitude. Because of this, ([7.1](ch013.xhtml#x1-116003r39)) and ([7.2](ch013.xhtml#x1-116003r39))
    will be quite useful for us.
  prefs: []
  type: TYPE_NORMAL
- en: As we are about to see, transformations that preserve the inner product are
    rather special, and these relationships provide us with a way to characterize
    them both algebraically and geometrically.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Orthogonal transformations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s jump straight into the definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 26\. (Orthogonal transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ^n be an arbitrary linear transformation. f is called orthogonal
    if'
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨f(x),f(y)⟩ = ⟨x,y ⟩ ](img/file631.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all x,y ∈ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a consequence, an orthogonal f preserves the norm: ∥f(x)∥² = ⟨f(x),f(x)⟩
    = ⟨x,x⟩ = ∥x∥². Because the angle enclosed by two vectors is defined by their
    inner product, see equation (2.9), the property ⟨f(x),f(y)⟩ = ⟨x,y⟩ means that
    an orthogonal transform also preserves angles.'
  prefs: []
  type: TYPE_NORMAL
- en: We can translate the definition to the language of matrices as well. In practice,
    we are always going to work with matrices, so this characterization is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 40\. (Matrices of orthogonal transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n → ℝ^n be a linear transformation and A ∈ ℝ^(n×n) be its matrix
    in the standard orthonormal basis. Then, f is orthogonal if, and only if, A^T
    = A^(−1).'
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As usual, we have to show the implication in both ways.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Suppose that f is orthogonal. Then, ([7.2](ch013.xhtml#x1-116003r39)) gives
  prefs: []
  type: TYPE_NORMAL
- en: '![ T ⟨x, y⟩ = ⟨Ax, Ay ⟩ = ⟨x, A Ay ⟩. ](img/file632.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, for any given y,
  prefs: []
  type: TYPE_NORMAL
- en: '![ T ⟨x,(A A − I)y ⟩ = 0 ](img/file633.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all x. By letting x = (A^T A −I)y, the positive definiteness of the
    inner product implies that (A^T A −I)y = 0 for all y. Thus, A^T A = I, which means
    that A^T is the inverse of A.
  prefs: []
  type: TYPE_NORMAL
- en: (b) If A^T = A^(−1), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨Ax, Ay ⟩ = ⟨x,AT Ay ⟩ −1 = ⟨x,A Ay ⟩ = ⟨x,y ⟩, ](img/file634.png)'
  prefs: []
  type: TYPE_IMG
- en: showing that f is orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that A^T = A^(−1) has a profound implication regarding the columns
    of A. If you think back to the definition of matrix multiplication in Section [4.1.2](ch010.xhtml#matrix-operations-revisited),
    the element in the i-th row and j-th column of AB is the inner product of the
    i-th row of A and the j-th column of B.
  prefs: []
  type: TYPE_NORMAL
- en: To be more precise, if the i-th column is denoted by a[i] = (a[1,i],a[2,i],…,a[n,i]),
    then we have
  prefs: []
  type: TYPE_NORMAL
- en: '![AT A = (⟨ai,aj⟩)n = I, i,j=1 ](img/file635.png)'
  prefs: []
  type: TYPE_IMG
- en: that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ ⟨a ,a ⟩ = 1 if i = j, i j |( 0 otherwise. ](img/file636.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the columns of A form an orthonormal system. This fact should
    not come as a surprise since orthogonal transformations preserve magnitude and
    orthogonality, and the columns of A are the images of the standard orthonormal
    basis e[1],…,e[n].
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, performing an orthogonal transformation on our features
    is equivalent to looking at them from another perspective, without distortion.
    You might know it already, but this is what Principal Component Analysis (PCA)
    is doing.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Self-adjoint transformations and the spectral decomposition theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides orthogonal transformations, there is another important class: transformations
    whose adjoints are themselves. Bear with me a bit, and we’ll see an example soon.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 27\. (Self-adjoint transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ^n be a linear transformation. f is self-adjoint if f^∗ = f,
    that is,'
  prefs: []
  type: TYPE_NORMAL
- en: ⟨f(**x**), **y**⟩ = ⟨**x**, f(**y**)⟩
  prefs: []
  type: TYPE_NORMAL
- en: (7.3)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all x,y ∈ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: As always, we are going to translate this into the language of matrices. If
    A is the matrix of f in the standard orthonormal basis, we know that A^T is the
    matrix of the adjoint. For self-adjoint transformations, it implies that A^T =
    A. Matrices such as these are called symmetric, and they have a lot of pleasant
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: For us, the most important one is that symmetric matrices can be diagonalized!
    (That is,they can be transformed into a diagonal matrix with a check reference)
    The following theorem makes this precise.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 41\. (Spectral decomposition of real symmetric matrices)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ ℝ^(n×n) be a real symmetric matrix. Then, A has exactly n real eigenvalues
    λ[1] ≥⋅⋅⋅≥λ[n], and the corresponding eigenvectors u[1],…,u[n] can be selected
    such that they form an orthonormal basis.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if we let Λ = diag(λ[1],…,λ[n]) and U be the orthogonal matrix whose
    columns are u[1],…,u[n], then
  prefs: []
  type: TYPE_NORMAL
- en: A = UΛU^T
  prefs: []
  type: TYPE_NORMAL
- en: (7.4)
  prefs: []
  type: TYPE_NORMAL
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the eigenvalues λ[1] ≥⋅⋅⋅≥λ[n] are not necessarily distinct from each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (Sketch) Since the proof is pretty involved, we are better off getting
    to know the main ideas behind it, without all the mathematical details.
  prefs: []
  type: TYPE_NORMAL
- en: The main steps are the following.
  prefs: []
  type: TYPE_NORMAL
- en: If the matrix A is symmetric, all of its eigenvalues are real.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using this, it can be shown that an orthonormal basis can be formed from the
    eigenvectors of A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing the matrix of the transformation x → Ax in this orthonormal basis yields
    a diagonal matrix. Hence, a change of basis yields ([7.4](ch013.xhtml#x1-118004r41)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Showing that the eigenvalues are real requires some complex number magic (which
    is beyond the scope of this chapter). The tough part is the second step. Once
    that has been done, moving to the third one is straightforward, as we have seen
    when talking about eigenspaces and their bases (Section [6.3](ch012.xhtml#eigenvectors-eigenspaces-and-their-bases)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We still don’t have a hands-on way to diagonalize matrices, but this theorem
    gets us one step closer: at least we know it is possible for symmetric matrices.
    This is an important stepping stone, as we’ll be able to reduce the general case
    to the symmetric one.'
  prefs: []
  type: TYPE_NORMAL
- en: The requirement for a matrix to be symmetric seems like a very special one.
    However, in practice, we can symmetrize matrices in several different ways. For
    any matrix A ∈ℝ^(n×m), the products AA^T and A^T A will be symmetric. For square
    matrices, the average ![A+AT- 2](img/file639.png) also works. So, symmetric matrices
    are more common than you think.
  prefs: []
  type: TYPE_NORMAL
- en: The orthogonal matrix U and the corresponding orthonormal basis {u[1],…,u[n]}
    that diagonalizes a symmetric matrix A has a special property that is going to
    be very important in machine learning for the principal component analysis of
    data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before looking at the next theorem, we introduce the argmax notation. Recall
    that the expression max[x∈A]f(x) denotes the maximum value of the function over
    the set A. Often, we would like to know where that maximum is attained, which
    is defined by the argmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∗ x = argmaxx ∈Af (x), f(x∗) = max f(x). x∈A ](img/file640.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s see the fundamentals of PCA!
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 42\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ ℝ^(n×n) be a real symmetric matrix and let λ[1] ≥ ⋅⋅⋅ ≥ λ[n] be its
    real eigenvalues in decreasing order. Moreover, let U ∈ ℝ^(n×n) be the orthogonal
    matrix that diagonalizes A, with the corresponding orthonormal basis {u[1],…,u[n]}.
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![ T arg m∥axx∥=1 x Ax = u1, ](img/file642.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ max xT Ax = λ1\. ∥x∥=1 ](img/file643.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Proof. Since {u[1],…,u[n]} is an orthonormal basis, any x can be expressed
    as a linear combination of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ x = xiui, xi ∈ ℝ. i=1 ](img/file644.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, since the u[i] are eigenvectors of A,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n ∑n Ax = A( xiui) = xiAui = xiλiui. i=1 i=1 i=1 ](img/file645.png)'
  prefs: []
  type: TYPE_IMG
- en: Plugging it back into x^T Ax, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ T ∑n T ∑n x Ax = ( xjuj) A ( xiui) j=1 i=1 ∑n ∑n = ( xjuTj )A ( xiui) j=1
    i=1 ∑n ∑n = ( xjuTj )( xiλiui) j=1 i=1 ](img/file646.png)![ ∑n T = xixjλiuj ui.
    i,j=1 ](img/file647.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the u[i]-s form an orthonormal basis,
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| T {1 if i = j, uj ui = ⟨ui,uj⟩ = | (0 otherwise. ](img/file648.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, u[j]^T u[i] vanishes when i≠j. Continuing the above calculation
    with this observation,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](img/file649.png)'
  prefs: []
  type: TYPE_IMG
- en: When ![ ](img/file650.png), the sum ![ ](img/file651.png) is a weighted average
    of the eigenvalues ![ ](img/file652.png). So,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](img/file653.png)'
  prefs: []
  type: TYPE_IMG
- en: from which x^T Ax ≤ λ[1] follows. (Recall that we can assume without loss in
    generality that the eigenvalues are decreasing.) On the other hand, by plugging
    in x = u[1], we can see that u[1]^T Au[1] = λ[1], so the maximum is indeed attained.
    From these two, the theorem follows.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 6\.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, Theorem [42](ch013.xhtml#x1-118012r42) gives that the function
    x→x^T Ax assumes its maximum value at u[1], and that maximum value is u[1]^T Au[1]
    = λ[1]. The quantity x^T Ax seems quite mysterious as well, so let’s clarify this
    a bit. If we think in terms of features, the vectors u[1],…,u[n] can be thought
    of as mixtures of the “old” features e[1],…,e[n]. When we have actual observations
    (that is, data), we can use the above process to diagonalize the covariance matrix.
    So, if A denotes this covariance matrix, u[1]^T Au[1] is the variance of the new
    feature u[1].
  prefs: []
  type: TYPE_NORMAL
- en: Thus, this theorem says that u[1] is the unique feature that maximizes the variance.
    So, among all the possible choices for new features, u[1] conveys the most information
    about the data.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we don’t have all the tools to see, but in connection to the
    principal component analysis, this says that the first principal vector is the
    one that maximizes variance.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem [42](ch013.xhtml#x1-118012r42) is just a special case of the following
    general theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 43\.
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ ℝ^(n×n) be a real symmetric matrix, let λ[1] ≥ ⋅⋅⋅ ≥ λ[n] be its real
    eigenvalues in decreasing order. Moreover, let U ∈ ℝ^(n×n) be the orthogonal matrix
    that diagonalizes A, with the corresponding orthonormal basis {u[1],…,u[n]}.
  prefs: []
  type: TYPE_NORMAL
- en: Then, for all k = 1,…,n, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![u = argmax{xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}, k 1 k−1 ](img/file656.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![λ max {xT Ax : ∥x∥ = 1,x ⊥ {u ,...,u }}. k 1 k− 1 ](img/file657.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Sometimes, when the conditions are too complicated, we write max{f(x) : x
    ∈A} instead of max[x∈A]f(x).)'
  prefs: []
  type: TYPE_NORMAL
- en: Proof. The proof is almost identical to the previous one. Since x is required
    to be orthogonal to u[1],…,u[k−1], it can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ x = xiui. i=k ](img/file658.png)'
  prefs: []
  type: TYPE_IMG
- en: Following the calculations in the proof of the previous theorem, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n xTAx = x2iλi ≤ λk. i=k ](img/file659.png)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, similar to before, u[k]^T Au[k] = λ[k], so the theorem follows.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 The singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we can diagonalize any real symmetric matrix with an orthogonal transformation.
    That’s great, but what if our matrix is not symmetric? After all, this is a rather
    special case.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we do the same for a general matrix? We’ll use a very strong tool,
    straight from the mathematician’s toolkit: wishful thinking. We pretend to have
    the solution, then reverse engineer it. To be specific, let A ∈ℝ^(n×m) be any
    real matrix. (It might not be square.) Since A is not symmetric, we have to relax
    our wishes for factoring it into the form UΛU^T . The most straightforward way
    is to assume that the orthogonal matrices to the left and to the right are not
    each other’s transposes.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we are looking for orthogonal matrices U ∈ℝ^(n×n) and V ∈ℝ^(m×m) such
    that
  prefs: []
  type: TYPE_NORMAL
- en: '![A = U ΣV T ](img/file660.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for some diagonal Σ ∈ℝ^(n×m). (A non-square matrix Σ = (σ[i,j])[i,j=1]^(n,m)
    is diagonal if σ[i,j] is 0 when i≠j.)
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering about the notational switch from Λ to Σ. This is because
    Σ will not necessarily contain eigenvalues, but singular values. We’ll explain
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the reverse-engineering part. First, as we discussed earlier, AA^T
    and A^T A are symmetric matrices. Second, we can simplify them by using the orthogonality
    of U and V , obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '![ T T T AA = (UΣV )(VΣU ) 2 T = U Σ U . ](img/file661.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we have A^T A = V Σ²V ^T . Good news: We can actually find U and
    V by applying the spectral decomposition theorem (Theorem [41](ch013.xhtml#x1-118004r41))
    to AA^T and A^T A, respectively. Thus, the factorization A = UΣV ^T is valid!
    This form is called the singular value decomposition (SVD), one of the pinnacle
    achievements of linear algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are not done yet; we just know where to look. Let’s make this
    mathematically precise!
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 44\. (Singular value decomposition)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ℝ^(n×m) be an arbitrary matrix. Then, there exists a diagonal matrix
    Σ ∈ℝ^(n×m) and orthogonal matrices U ∈ℝ^(n×n) and V ∈ℝ^(m×m), such that
  prefs: []
  type: TYPE_NORMAL
- en: '![A = U ΣV T. ](img/file662.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What is a non-square diagonal matrix? Let me give you two examples, and you’ll
    immediately get the gist:'
  prefs: []
  type: TYPE_NORMAL
- en: '![⌊ ⌋ ⌊ ⌋ 1 0 1 0 0 ||0 2|| , ||0 2 0|| . ⌈ ⌉ ⌈ ⌉ 0 0 ](img/file663.png)'
  prefs: []
  type: TYPE_IMG
- en: We can always write rectangular matrices M ∈ℝ^(n×m) in the forms
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ M1 m×m (n−m)×m M = ⌈ ⌉ , M1 ∈ ℝ , M2 ∈ ℝ M2 ](img/file664.png)'
  prefs: []
  type: TYPE_IMG
- en: if m/span>n, and
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ ] n×n n×(m−n) M = M1 M2 , M1 ∈ ℝ , M2 ∈ ℝ ](img/file665.png)'
  prefs: []
  type: TYPE_IMG
- en: otherwise. Now, let’s see the proof of the singular value decomposition!
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (Sketch.) To illustrate the main ideas of the proof, we assume that 1)
    A is square, and 2) A is invertible; that is, 0 is not an eigenvalue of A,
  prefs: []
  type: TYPE_NORMAL
- en: Since A^T A ∈ ℝ^(m×m) is a real symmetric matrix, we can apply the spectral
    decomposition theorem to obtain a diagonal Σ ∈ ℝ^(m×m) and orthogonal V ∈ℝ^(m×m)
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '![ T 2 T A A = V Σ V ](img/file666.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. (Recall that the eigenvalues of a symmetric matrix are nonnegative, thus
    we can write the eigenvalues of A^T A in the form Σ².)
  prefs: []
  type: TYPE_NORMAL
- en: As A is invertible, A^T A is invertible as well; thus, 0 is not an eigenvalue
    of A^T A. As a consequence, ΣA^(−1) is well defined. Now, by defining U := AV
    ΣA^(−1), the orthogonality of V gives that
  prefs: []
  type: TYPE_NORMAL
- en: '![ T −1 T U ΣV = (AV Σ )ΣV T = AV V = A, ](img/file667.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we are almost finished. The only thing left to show is that U is indeed
    orthogonal, that is, U^T U = I. Here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![U TU = (AV Σ− 1)TAV Σ−1 −1 T T −1 = Σ V A◟◝◜A◞ V Σ =VΣ2VT = Σ−1(V TV )Σ2(V
    TV)Σ −1 = Σ−1Σ2 Σ− 1 = I, ](img/file668.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, have the singular value decomposition for the special case of square
    and invertible A.
  prefs: []
  type: TYPE_NORMAL
- en: To keep the complexity bearable, we won’t work the rest of the details out;
    I’ll leave that to you as an exercise. You have all the tools by now.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to appreciate the power of the singular value decomposition.
    The columns of U and V are orthogonal matrices, which are rather special transformations.
    As they leave the inner products and the norm invariant, the structure of the
    underlying vector spaces is preserved. The diagonal Σ is also special, as it is
    just a stretching in the direction of the bases. It is very surprising that any
    linear transformation is the composition of these three special ones.
  prefs: []
  type: TYPE_NORMAL
- en: Besides mapping out the fine structure of linear transformations, SVD offers
    a lot more. For instance, it generalizes the notion of eigenvectors, a concept
    that was defined only for square matrices. With this, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![AV = U Σ, ](img/file669.png)'
  prefs: []
  type: TYPE_IMG
- en: which we can take a look at column-wise. Here, Σ is diagonal, but its number
    of elements depends on the smaller one of n or m.
  prefs: []
  type: TYPE_NORMAL
- en: So, if u[i] is the i-th column of U, and v[i] is the i-th column of V , the
    identity AV = UΣ is translated to
  prefs: []
  type: TYPE_NORMAL
- en: '![Av = σ u , 0 ≤ i ≤ min (n,m ). i ii ](img/file670.png)'
  prefs: []
  type: TYPE_IMG
- en: This closely resembles the definition of eigenvalue-eigenvector pairs, except
    that instead of one vector, we have two. The u[i] and v[i] are the so-called left
    and right singular vectors, while the scalars σ[i] = ![√λ- i](img/file671.png)
    are called singular values.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, orthogonal transformations give us the singular value decomposition,
    but is that all? Are there any other special transformations and matrix decompositions?
    You bet there is.
  prefs: []
  type: TYPE_NORMAL
- en: Enter orthogonal projections.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Orthogonal projections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear transformations are essentially manipulations of data, revealing other
    (hopefully more useful) representations. Intuitively, we think about them as one-to-one
    mappings, faithfully preserving all the “information” from the input.
  prefs: []
  type: TYPE_NORMAL
- en: This is often not the case, to such an extent that sometimes a lossy compression
    of the data is highly beneficial. To give you a concrete example, consider a dataset
    with a million features, out of which only a couple hundred are useful. What we
    can do is identify the important features and throw away the rest, obtaining a
    representation that is more compact, thus easier to work with.
  prefs: []
  type: TYPE_NORMAL
- en: This notion is formalized by the concept of orthogonal projections. We already
    met them upon our first encounter with the inner products (see ([2.7](ch008.xhtml#x1-45003r3.2.3))).
  prefs: []
  type: TYPE_NORMAL
- en: Projections also play a fundamental role in the Gram-Schmidt process (Theorem [13](ch008.xhtml#x1-47004r13)),
    used to orthogonalize an arbitrary basis. Because we are already somewhat familiar
    with orthogonal projections, a formal definition is due.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 28\. (Projections and orthogonal projections)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be an arbitrary inner product space and P : V → V be a linear transformation.
    P is a projection if P² = P.'
  prefs: []
  type: TYPE_NORMAL
- en: A projection P is orthogonal if the subspaces kerP and imP are orthogonal to
    each other. (That is, for every pair of x ∈ kerP and y ∈ imP, we have ⟨x,y⟩ =
    0.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit the examples we have seen so far to get a grip on the definition!
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. The simplest one is the orthogonal projection to a single vector.
    That is, if u ∈ℝ^n is an arbitrary vector, the transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![proju(x) = ⟨x,u⟩-u ⟨u,u ⟩ ](img/file672.png)'
  prefs: []
  type: TYPE_IMG
- en: is the orthogonal projection to (the subspace spanned by) u. (We talked about
    this when discussing the geometric interpretation of inner products in Section [2.2.3](ch008.xhtml#the-geometric-interpretation-of-inner-products),
    where this definition was deduced from a geometric intuition.) Applying this transformation
    repeatedly, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⟨x,u⟩ ⟨⟨u,u⟩u,-u⟩ proju(proju (x )) = ⟨u,u ⟩ u ⟨x,u⟩ -⟨u,u⟩⟨u,-u⟩ = ⟨u,u
    ⟩ u = ⟨x,u-⟩u ⟨u,u ⟩ = proj (x). u ](img/file673.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, faithfully to its name, ![proju ](img/file674.png) is indeed a projection.
    To see that it is orthogonal, let’s examine its kernel and image! Since the value
    of ![proju(x ) ](img/file675.png) is a scalar multiple of ![u ](img/file676.png),
    its image is
  prefs: []
  type: TYPE_NORMAL
- en: '![im (proj) = span(u). u ](img/file677.png)'
  prefs: []
  type: TYPE_IMG
- en: Its kernel, the set of vectors mapped to 0 by proj[u], is also easy to find,
    as ![⟨⟨xu,,uu⟩⟩](img/file678.png)u = 0 can only happen if ⟨x,u⟩ = 0, that is,
    if x ⊥u.
  prefs: []
  type: TYPE_NORMAL
- en: In other words,
  prefs: []
  type: TYPE_NORMAL
- en: '![ker(proju) = span (u)⊥, ](img/file679.png)'
  prefs: []
  type: TYPE_IMG
- en: where span(u)^⊥ denotes the orthogonal complement (Definition [13](ch008.xhtml#x1-48004r13))
    of span(u). This means that proj[u] is indeed an orthogonal projection.
  prefs: []
  type: TYPE_NORMAL
- en: We can also describe ![proju(x) ](img/file680.png) in terms of matrices. By
    writing out ![proju (x ) ](img/file681.png) component-wise, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | ⟨x,u⟩u1| ⟨x,u⟩ 1 | ⟨x,u⟩u2| proju(x) = -----u = ----2|| . || , ⟨u,
    u⟩ ∥u ∥ |⌈ .. |⌉ ⟨x,u⟩u n ](img/file682.png)'
  prefs: []
  type: TYPE_IMG
- en: where u = (u[1],…,u[n]). This looks like some kind of matrix multiplication!
    As we saw earlier, multiplying a matrix and a vector can be described in terms
    of rowwise dot products. (See ([3.3](#)).)
  prefs: []
  type: TYPE_NORMAL
- en: So, according to this interpretation of matrix multiplication, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(13).png)(7.5)'
  prefs: []
  type: TYPE_IMG
- en: Note that the scaling with ∥u∥² can be incorporated into the “matrix” product
    by writing
  prefs: []
  type: TYPE_NORMAL
- en: '![ T T uu---= -u--⋅ u--, ∥u∥2 ∥u ∥ ∥u∥ ](img/file686.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix uu^T ∈ℝ^(n×n), obtained from the product of the vector u ∈ℝ^(n(×1))
    and its transpose u^T ∈ℝ^(1×n), is a rather special one. They are called rank-1
    projection matrices, and they frequently appear in mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: (In general, the matrix uv^T is called the outer product of the vectors u and
    v. We won’t use this extensively, but it appears frequently throughout linear
    algebra.)
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. As we saw when introducing the Gram-Schmidt orthogonalization process
    (Theorem [13](ch008.xhtml#x1-47004r13)), the previous example can be generalized
    by projecting to multiple vectors. If u[1],…,u[k] ∈ℝ^n is a set of linearly independent
    and pairwise orthogonal vectors, then the linear transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k ⟨x, ui⟩ proju1,...,uk(x) = -------ui i=1 ⟨ui,ui⟩ ](img/file687.png)'
  prefs: []
  type: TYPE_IMG
- en: is an orthogonal projection onto the subspace span(u[1],…,u[k]). This is easy
    to see, and I recommend the reader to do this as an exercise. (This can be found
    in the problems section as well.)
  prefs: []
  type: TYPE_NORMAL
- en: 'From ([7.5](#)), we can determine the matrix form of proj[u[1],…,u[k]] as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k T proj (x) = ( uiu-i) x. u1,...,uk i=1 ∥ui∥2 ◟----◝◜---◞ ∈ℝn×n ](img/file688.png)'
  prefs: []
  type: TYPE_IMG
- en: This is good to know, as projection matrices are often needed in the implementation
    of certain algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Properties of orthogonal projections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have seen a few examples, it is time to discuss orthogonal projections
    in more general terms. There are lots of reasons why these special transformations
    are useful, and we’ll explore them in this section. First, let’s start with the
    most important one: orthogonal projections also enable the decomposition of vectors
    in terms of a given subspace plus an orthogonal vector.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 45\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be an inner product space and P : V → V be a projection. Then, V = kerP
    + imP; that is, every vector x ∈V can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: '![x = x + x , x ∈ ker P, x ∈ im P. ker im ker im ](img/file689.png)'
  prefs: []
  type: TYPE_IMG
- en: If P is an orthogonal projection, then x[im] ⊥x[ker].
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Every x can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![x = (x − Px )+ Px. ](img/file690.png)'
  prefs: []
  type: TYPE_IMG
- en: Since P is idempotent, that is, P² = P, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P (x − P x) = Px − P (Px) = Px − P x = 0, ](img/file691.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, x −Px ∈ kerP. By definition, Px ∈ imP, so V = kerV + imV , which proves
    our main proposition.
  prefs: []
  type: TYPE_NORMAL
- en: If P is an orthogonal projection, then again, by definition, x[im] ⊥x[ker],
    which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, orthogonal projections are self-adjoint. This might not sound like
    a big deal, but self-adjointness leads to several very pleasant properties.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 46\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be an inner product space and P : V →V be an orthogonal projection. Then,
    P is self-adjoint.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof. According to the definition ([27](ch013.xhtml#x1-118003r27)), all we
    need to show is that
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨Px, y⟩ = ⟨x, Py⟩ ](img/file692.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for any x,y ∈V . In the previous result, we have seen that x and y can
    be written as
  prefs: []
  type: TYPE_NORMAL
- en: '**x** = **x**[ker P] + **x**[im P], **x**[ker P] ∈ ker P, **x**[im P] ∈ im P**y**
    = **y**[ker P] + **y**[im P], **y**[ker P] ∈ ker P, **y**[im P] ∈ im P'
  prefs: []
  type: TYPE_NORMAL
- en: Since P² = P, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨P x,y⟩ = ⟨PxkerP + Pxim P,ykerP + yim P⟩ = ⟨xim P,ykerP + yim P⟩ = ⟨x ,y
    ⟩+ ⟨x ,y ⟩ ◟-im-P◝◜kerP◞ im P im P =0 = ⟨xim P,yimP ⟩. ](img/file693.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, it can be shown that ⟨x,Py⟩ = ⟨x[im P] ,y[im P] ⟩. These two identities
    imply ⟨Px,y⟩ = ⟨x,Py⟩, which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: One straightforward consequence of self-adjointness is that the kernel of orthogonal
    projections is the orthogonal complement of its image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theorem 47\. Let V be an inner product space and P : V → V be an orthogonal
    projection. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⊥ ker P = (im P ) . ](img/file694.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. To prove the equality of these two sets, we need to show that (a) kerP
    ⊆ (imP)^⊥, and (b) (imP)^⊥⊆ kerP.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Let x ∈ kerP; that is, suppose that Px = 0\. We need to show that for any
    y ∈ imP, we have ⟨x,y⟩ = 0\. For this, let y[0] ∈V such that Py[0] = y. (This
    is guaranteed to exist, since we took y from the image of P.) Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨x,y ⟩ = ⟨x,P y0⟩ = ⟨P x,y0⟩ = ⟨0,y0⟩ = 0, ](img/file695.png)'
  prefs: []
  type: TYPE_IMG
- en: where P is self-adjoint. Thus, x ∈ (imP)^⊥ also holds, implying kerP ⊆ (imP)^⊥.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Now, let x ∈ (imP)^⊥. Then, for any y ∈V , we have ⟨x,Py⟩ = 0\. However,
  prefs: []
  type: TYPE_NORMAL
- en: '![⟨Px,y ⟩ = ⟨x,P y⟩ = 0\. ](img/file696.png)'
  prefs: []
  type: TYPE_IMG
- en: Especially, with the choice y = Px, we have ⟨Px,Px⟩ = 0\. Due to the positive
    definiteness of the inner product, this implies that Px = 0, that is, x ∈ kerP.
  prefs: []
  type: TYPE_NORMAL
- en: Summing up all of the above, if P is an orthogonal projection of the inner product
    space V , then
  prefs: []
  type: TYPE_NORMAL
- en: '![V = im P + (im P )⊥. ](img/file697.png)'
  prefs: []
  type: TYPE_IMG
- en: Do you recall that when we first encountered the concept of orthogonal complements
    (Definition [13](ch008.xhtml#x1-48004r13)), we proved that V = S + S^⊥ for any
    finite-dimensional inner product space V and its subspace S? We did this with
    the use of a special orthogonal projection. We are getting close to seeing the
    general pattern here.
  prefs: []
  type: TYPE_NORMAL
- en: Because the kernel of an orthogonal projection P is an orthogonal complement
    of the image, the transformation I −P is an orthogonal projection as well, with
    the roles of image and kernel reversed.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 48\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be an inner product space and P : V →V be an orthogonal projection. Then,
    I −P is an orthogonal projection as well, and'
  prefs: []
  type: TYPE_NORMAL
- en: '![ker(I − P ) = im P, im (I − P ) = ker P. ](img/file698.png)'
  prefs: []
  type: TYPE_IMG
- en: The proof is so simple that this is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing to mention. If the image spaces of two orthogonal projections
    match, then the projections themselves are equal. This is a very strong uniqueness
    property, as if you think about it, this is not true for other classes of linear
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 49\. (Uniqueness of orthogonal projections)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be an inner product space and P,Q : V → V be two orthogonal projections.
    If imP = imQ, then P = Q.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Because of kerP = (imP)^⊥, the equality of the image spaces also imply
    that kerP = kerQ.
  prefs: []
  type: TYPE_NORMAL
- en: Since V = kerP + imP, every x ∈V can be decomposed as
  prefs: []
  type: TYPE_NORMAL
- en: '![x = xkerP + xim P, xkerP ∈ kerP, xim P ∈ im P. ](img/file699.png)'
  prefs: []
  type: TYPE_IMG
- en: This decomposition and the equality of the kernel and image spaces give that
  prefs: []
  type: TYPE_NORMAL
- en: '![Px = P xkerP + P ximP = xim P. ](img/file700.png)'
  prefs: []
  type: TYPE_IMG
- en: With an identical argument, we have Qx = x[im P] , thus Px = Qx on all vectors
    x ∈V . This proves P = Q.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, given a subspace, there can be only one orthogonal projection
    to it. But is there any at all? Yes, and in the next section, we will see that
    it can be described in geometric terms.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Orthogonal projections are the optimal projections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Orthogonal projections have an extremely pleasant and mathematically useful
    property. In some sense, if P : V →V is an orthogonal projection, Px provides
    the optimal approximation of x among all vectors in imP. To make this precise,
    we can state the following.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 50\. (Construction of orthogonal transformations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be a finite-dimensional inner product space and S ⊆V its subspace. Then,
    the transformation P : V →V , defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![P : x → argmyi∈nS ∥x − y∥ ](img/file701.png)'
  prefs: []
  type: TYPE_IMG
- en: is an orthogonal projection to S.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, since orthogonal projections to a given subspace are unique
    (as implied by Theorem [49](ch013.xhtml#x1-121008r49)), Px is the closest vector
    to x in the subspace S. Thus, we can denote this as P[S], emphasizing the uniqueness.
  prefs: []
  type: TYPE_NORMAL
- en: Besides having an explicit way to describe orthogonal projections, there is
    one extra benefit. Recall that previously, we showed that
  prefs: []
  type: TYPE_NORMAL
- en: '![V = im P + (im P)⊥ ](img/file702.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. Since for any subspace S an orthogonal projection P[S] exists whose image
    set is S, it also follows that V = S + S^⊥. Although we saw this earlier when
    talking about orthogonal complements (Definition [13](ch008.xhtml#x1-48004r13)),
    it is interesting to see a proof that doesn’t require the construction of an orthonormal
    basis in S.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this is the point where mathematical analysis and linear algebra
    intersect. We don’t have the tools for it yet, but using the concept of convergence,
    the above theorems can be generalized to infinite-dimensional spaces. Infinite-dimensional
    spaces are not particularly relevant to machine learning in practice, yet they
    provide a beautiful mathematical framework for the study of functions. Who knows,
    one day these advanced tools may provide a significant breakthrough in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Computing eigenvalues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last chapter, we reached the singular value decomposition, one of the
    pinnacle results of linear algebra. We laid out the theoretical groundwork to
    get us to this point.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one thing is missing: computing the singular value decomposition in
    practice. Without this, we can’t reap all the rewards this powerful tool offers.
    In this section, we’ll develop two methods for this purpose. One offers a deep
    insight into the behavior of eigenvectors, but it doesn’t work in practice. The
    other offers excellent performance, but it is hard to understand what is happening
    behind the formulas. Let’s start with the first one, illuminating how the eigenvectors
    determine the effects of a linear transformation!'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1 Power iteration for calculating the eigenvectors of real symmetric matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you recall, we discovered the singular value decomposition by tracing the
    problem back to the spectral decomposition of symmetric matrices. In turn, we
    can obtain the spectral decomposition by finding an orthonormal basis from the
    eigenvectors of our matrix. The plan is the following: first, we define a procedure
    that finds an orthonormal set of eigenvectors for symmetric matrices. Then, use
    this to compute the singular value decomposition for arbitrary matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: A naive way would be to find the eigenvalues by solving the polynomial equation
    det(A−λI) = 0 for λ, then compute the corresponding eigenvectors by solving the
    linear equations (A −λI)x = 0.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are problems with this approach. For an n ×n matrix, the characteristic
    polynomial p(λ) = det(A−λI) is a polynomial of degree n. Even if we could effectively
    evaluate det(A −λI) for any lambda, there are serious issues. Unfortunately, unlike
    for the quadratic equation ax² + bx + c = 0, there are no formulas for finding
    the solutions when n/span>4\. (It is not that mathematicians were just not clever
    enough to find them. No such formula exists.)
  prefs: []
  type: TYPE_NORMAL
- en: How can we find an alternative approach? Once again, we use the wishful thinking
    approach that worked so well before. Let’s pretend that we know the eigenvalues,
    play around with them, and see if this gives us some useful insight.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, assume that A is a small symmetric 2 × 2 matrix,
    say with eigenvalues λ[1] = 4 and λ[2] = 2\. Since A is symmetric, we can even
    find a set of corresponding eigenvectors u[1],u[2] such that u[1] and u[2] form
    an orthonormal basis. (That is, both have a unit norm and they are orthogonal
    to each other.) This is guaranteed by the spectral decomposition theorem (Theorem [41](ch013.xhtml#x1-118004r41)).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, any x ∈ℝ² can be written as x = x[1]u[1] + x[2]u[2] for some nonzero scalars
    x[1],x[2]. What happens if we apply the transformation A to our vector x? Because
    u[i] is eigenvectors, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Ax = A (x1u1 + x2u2 ) = x1Au1 + x2Au2 = x1λ1u1 + x2λ2u2 = 4x1u1 + 2x2u2\.
    ](img/file703.png)'
  prefs: []
  type: TYPE_IMG
- en: By applying A one more time, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ 2 2 2 A x = x1λ1u1 + x2λ2u2 2 2 = 4x1u1 + 2 x2u2\. ](img/file704.png)'
  prefs: []
  type: TYPE_IMG
- en: A pattern starts to emerge. In general, the k-th iteration of A yields
  prefs: []
  type: TYPE_NORMAL
- en: '![Akx = x1 λk1u1 + x2λk2u2 = 4kx1u1 + 2kx2u2\. ](img/file705.png)'
  prefs: []
  type: TYPE_IMG
- en: By taking an inquisitive look at A^kx, we can note that the contribution of
    u[1] is much more significant than u[2]. Why? Because the coefficient x[1]λ[1]^k
    = 4^kx[1] grows faster than x[2]λ[2]^k = 2^kx[2], regardless of the value of x[1]
    and x[2]. In technical terms, we say that λ[1] dominates λ[2].
  prefs: []
  type: TYPE_NORMAL
- en: Now, by scaling things down with λ[1]^k, we can extract the eigenvector u[1]!
    That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![Akx- λ2- k λk = x1u1 + x2(λ1 ) u2 1 = x1u1 + (something very small)k. ](img/file706.png)'
  prefs: []
  type: TYPE_IMG
- en: If we let k grow infinitely, the contribution of u[2] to ![Akkx- λ1](img/file707.png)
    vanishes. If you are familiar with the concept of limits, you could write
  prefs: []
  type: TYPE_NORMAL
- en: lim[k → ∞] ![(A_k x - λ_k 1)](img/file708.png) = *x*[1]**u**[1].
  prefs: []
  type: TYPE_NORMAL
- en: (7.6)
  prefs: []
  type: TYPE_NORMAL
- en: Remark 7\. (A primer on limits)
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar with limits, here is a quick explanation. The identity
  prefs: []
  type: TYPE_NORMAL
- en: '![ Akx lim --k- = x1u1 k→ ∞ λ1 ](img/file709.png)'
  prefs: []
  type: TYPE_IMG
- en: means that as k grows, the quantity ![Akx- λk1](img/file710.png) gets closer
    and closer to x[1]u[1], until the difference between them is infinitesimal. In
    practice, this means that we can approximate x[1]u[1] by ![Akx- λk1](img/file711.png)
    by selecting a very large k.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation ([7.6](ch013.xhtml#power-iteration-for-calculating-the-eigenvectors-of-real-symmetric-matrices))
    is great news for us! All we have to do is repeatedly apply the transformation
    A to identify the eigenvector for the dominant eigenvalue λ[1]. There is one small
    caveat, though: we have to know the value of λ[1]. We’ll deal with this later,
    but first, let’s record this milestone in the form of a theorem.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 51\.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the eigenvector for the dominant eigenvalue with power iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let A ∈ℝ^(n×n) be a real symmetric matrix. Suppose that:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) The eigenvalues of A are λ[1]/span>…/span>λ[n] (that is, λ[1] is the dominant
    eigenvalue).
  prefs: []
  type: TYPE_NORMAL
- en: (b) The corresponding eigenvectors u[1],…,u[n] form an orthonormal basis.
  prefs: []
  type: TYPE_NORMAL
- en: Let x ∈ℝ^n be a vector such that when written as the linear combination x =
    ∑ [i=1]^nx[i]u[i], the coefficient x[1] ∈ℝ is nonzero. Then,
  prefs: []
  type: TYPE_NORMAL
- en: lim[k → ∞] ![(A_k x - λ_k 1)](img/file712.png) = *x*[1]**u**[1].
  prefs: []
  type: TYPE_NORMAL
- en: (7.7)
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we jump into the proof, some explanations are in order. Recall that
    if A is symmetric, the spectral decomposition theorem (Theorem [41](ch013.xhtml#x1-118004r41))
    guarantees that it can be diagonalized with a similarity transformation. In its
    proof (sketch), we mentioned that a symmetric matrix has:'
  prefs: []
  type: TYPE_NORMAL
- en: Real eigenvalues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An orthonormal basis from its eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, the assumptions (a) and (b) are guaranteed, except for one caveat: the
    eigenvalues are not necessarily distinct. However, this rarely causes problems
    in practice. There are multiple reasons for this, but most importantly, matrices
    with repeated eigenvalues are so rare that they form a zero-probability set. (We’ll
    learn about probability later in the book. For now, we can assume that randomly
    picking a matrix with repeated eigenvalues is impossible.) Thus, stumbling upon
    one is highly unlikely.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Because u[k] is the eigenvector for the eigenvalue λ[k], we have
  prefs: []
  type: TYPE_NORMAL
- en: '*A*^k**x** = ∑[i=1]^n *x*[i]*λ*[i]^k**u**[i]. (7.8)'
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ n Akx- ∑ λi-k λk = x1u1 + xi(λ1) ui. 1 i=2 ](img/file713.png)'
  prefs: []
  type: TYPE_IMG
- en: Since λ[1] is the dominant eigenvalue, ∥λ[i]∕λ[1]∥/span>1 for i = 2,…,n, so
    (λ[i]∕λ[1])^k → 0 as k →∞. Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '![ Akx- kli→m∞ λk = x1u1\. 1 ](img/file714.png)'
  prefs: []
  type: TYPE_IMG
- en: This is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s fix the small issue that requires us to know ![λ1 ](img/file715.png).
    Since ![λ1 ](img/file716.png) is the largest eigenvalue, the previous theorem
    shows that ![Akx ](img/file717.png) equals ![x1 λk1u1 ](img/file718.png) plus
    some term that is much smaller, at least compared to this dominant term. We can
    extract this quantity by taking the supremum norm ![∥Akx ∥∞ ](img/file719.png).
    (Recall that for any ![y = (y1,...,yn) ](img/file720.png), the supremum norm is
    defined by ![∥y ∥∞ = max {|y1|,...,|yn|} ](img/file721.png). Keep in mind that
    the ![yi ](img/file722.png)-s are the coefficients of ![y ](img/file723.png) in
    the original basis of our vector space, which is not necessarily our eigenvector
    basis ![u1,...,un ](img/file724.png).)
  prefs: []
  type: TYPE_NORMAL
- en: By factoring out jλ[1]j^k from A^kx, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∥Akx ∥∞ = |λ1|k∥x1u1 + xi(λi)kui∥∞. i=2 λ1 ](img/file725.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively speaking, the remainder term ∑ [i=2]^nx[i](![λi λ1](img/file727.png))^ku[i]
    is small, thus we can approximate the norm as
  prefs: []
  type: TYPE_NORMAL
- en: '![∥Akx ∥∞ ≈ |λ1 |k∥x1u1∥ ∞. ](img/file729.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, instead of scaling with ![λk 1 ](img/file730.png), we can scale
    with ![∥Akx ∥∞ ](img/file731.png).
  prefs: []
  type: TYPE_NORMAL
- en: So, we are ready to describe our general eigenvector-finding procedure fully.
    First, we initialize a vector x[0] randomly, then we define the recursive sequence
  prefs: []
  type: TYPE_NORMAL
- en: '![ Axk −1 xk = ----------, k = 1,2,... ∥Axk −1∥∞ ](img/file732.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the linearity of A, we can see that, in fact,
  prefs: []
  type: TYPE_NORMAL
- en: '![ k xk = --A--x0--, ∥Akx0 ∥∞ ](img/file733.png)'
  prefs: []
  type: TYPE_IMG
- en: but scaling has an additional side benefit, as we don’t have to use large numbers
    at any computational step. With this, ([51](ch013.xhtml#x1-124005r51)) implies
    that
  prefs: []
  type: TYPE_NORMAL
- en: '![ --Akx0--- lki→m∞ xk = kl→im∞ ∥Akx0 ∥∞ = u1\. ](img/file734.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, we can extract the eigenvector for the dominant eigenvalue without
    actually knowing the eigenvalue itself.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2 Power iteration in practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s put the power iteration method into practice! The input of our power_iteration
    function is a square matrix A, and we expect the output to be an eigenvector corresponding
    to the dominant eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an iterative process, we should define a condition that defines
    when the process should terminate. If the consecutive members of the sequence
    {x[k]}[k=1]^∞ are sufficiently close together, we arrived at a solution. That
    is, if, say,
  prefs: []
  type: TYPE_NORMAL
- en: '![∥xk+1 − xk∥2 <1× 10−10, ](img/file735.png)'
  prefs: []
  type: TYPE_IMG
- en: we can stop and return the current value. However, this might never happen.
    For those cases, we define a cutoff point, say, k = 100,000, when we terminate
    the computation, even if there is no convergence.
  prefs: []
  type: TYPE_NORMAL
- en: To give us a bit more control, we can also manually define the initialization
    vector x_init.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To test the method, we should use an input for which the correct output is easy
    to calculate by hand. Our usual recurring example
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉ . 1 2 ](img/file736.png)'
  prefs: []
  type: TYPE_IMG
- en: should be perfect, as we already know a lot about it.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we have seen in Section [6.2.2](ch012.xhtml#finding-eigenvectors)
    that its eigenvalues are λ[1] = 3 and λ[2] = 1, with corresponding eigenvectors
    u[1] = (1,1) and u[2] = (−1,1).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if our function correctly recovers (a scalar multiple of) u[1] = (1,1)!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Success! To recover the eigenvalue, we can simply apply the linear transformation
    and compute the proportions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The result is 3, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.3 Power iteration for the rest of the eigenvectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Can we modify the power iteration algorithm to recover the other eigenvalues
    as well? In theory, yes. In practice, no. Let me elaborate!
  prefs: []
  type: TYPE_NORMAL
- en: To get a grip on how to generalize the idea, let’s take another look at the
    equation ([7.8](#)), saying that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n Akx = xiλkiui. i=1 ](img/file737.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the conditions for ![ k Aλxk- 1](img/file738.png) to converge was that
    x should have a nonzero component of the eigenvector u[1], that is, x[1]≠0.
  prefs: []
  type: TYPE_NORMAL
- en: What if x[1] = 0? In that case, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Akx = x2λk2u2 + ⋅⋅⋅+ xnλk2un, ](img/file739.png)'
  prefs: []
  type: TYPE_IMG
- en: with x[2]λ[2]^ku[2] becoming the dominant term.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Akx ∑n λi k --k-= x2u2 + xi( --) uk λ 2 i=3 λ2 = x u + (something very small),
    2 2 k ](img/file740.png)'
  prefs: []
  type: TYPE_IMG
- en: implying that
  prefs: []
  type: TYPE_NORMAL
- en: '![ Akx lim --k-= x2u2\. k→∞ λ 2 ](img/file741.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s make this mathematically precise in the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 52\. (Generalized power iteration)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let A ∈ℝ^(n×n) be a real symmetric matrix. Suppose that:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) The eigenvalues of A are λ[1]/span>…/span>λ[n] (that is, λ[1] is the dominant
    eigenvalue).
  prefs: []
  type: TYPE_NORMAL
- en: (b) The corresponding eigenvectors u[1],…,u[n] form an orthonormal basis.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![x ∈ ℝn ](img/file742.png) be a vector such that, when written as a linear
    combination of the basis ![u1,...,un ](img/file743.png), its first nonzero component
    is along ![ul ](img/file744.png) for some ![l = 1,2,...,n ](img/file745.png) (that
    is, ![ ∑ x = ni=l xiui ](img/file746.png)).
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![ k lim A--x = xlul k→ ∞ λkl ](img/file747.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof looks just like what we have seen a few times already. The question
    is, how can we eliminate the u[1],…,u[l−1] components from any vector? The answer
    is simple: orthogonal projections (Section [7.4](ch013.xhtml#orthogonal-projections)).'
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, let’s take a look at extracting the second dominant
    eigenvector with power iteration. Recall that the transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![proj (x) = ⟨x, u ⟩u u1 1 1 ](img/file748.png)'
  prefs: []
  type: TYPE_IMG
- en: describes the orthogonal projection of any x to u1.
  prefs: []
  type: TYPE_NORMAL
- en: In concrete terms, if x = ∑ [i=1]^nx[i]u[i], then
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n proju (x) = proju ( xiui) 1 1 i=1 ∑n = xiproj (ui) i=1 u1 = x1u1\. ](img/file749.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the exact opposite of what we are looking for! However, at this point,
    we can see that I − proj[u[1]] is going to be suitable for our purposes. This
    is still an orthogonal projection. Moreover, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n (I − proju1)( xiui) = xiui, i=1 i=2 ](img/file750.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, I − proj[u[1]] eliminates the u[1] component of x. Thus, if we initialize
    the power iteration with x^∗ = (I − proj[u[1]])(x), the sequence ![--Akx∗-- ∥Akx∗∥∞](img/file751.png)
    will converge to u[2], the second dominant eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we compute ![(I − proj )(x) u1 ](img/file752.png) in practice? Recall
    that in the standard orthonormal basis, the matrix of ![proju1 ](img/file753.png)
    can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![proju1 = u1uT1\. ](img/file754.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Keep in mind that the ![ui ](img/file755.png) vectors form an orthonormal
    basis, so ![∥u1∥ = 1 ](img/file756.png).) Thus, the matrix of ![I − proju1 ](img/file757.png)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![I − u1uT1, ](img/file758.png)'
  prefs: []
  type: TYPE_IMG
- en: which we can easily compute.
  prefs: []
  type: TYPE_NORMAL
- en: For a general vector u, this is how we can do this in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, the procedure to find all the eigenvectors is the following.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a random ![ (1) x ](img/file759.png) and use the power iteration
    to find ![u1 ](img/file760.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project ![x (1) ](img/file761.png) to the orthogonal complement of the subspace
    spanned by ![u 1 ](img/file762.png), thus obtaining
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![x (2) := (I − proju1)(x(1)), ](img/file763.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: which we use as the initial vector of the second round of power iteration, yielding
    the second dominant eigenvector ![u2 ](img/file764.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Project ![x (2) ](img/file765.png) to the orthogonal complement of the subspace
    spanned by ![u 1 ](img/file766.png) and ![u 2 ](img/file767.png), thus obtaining
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![x(3) = (I − proju2)(x(2)) (1) = (I − proju1,u2)(x ), ](img/file768.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: which we use as the initial vector of the third round of power iteration, yielding
    the third dominant eigenvector ![u3 ](img/file769.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Project ![ (3) x ](img/file770.png) to…
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You get the pattern. To implement this in practice, we add the find_eigenvectors
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test find_eigenvectors out on our old friend
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 2 1 A = ⌈ ⌉! 1 2 ](img/file771.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result is as we expected. (Don’t be surprised that the eigenvectors are
    not normalized, as we haven’t explicitly done so in the find_eigenvectors function.)
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to actually diagonalize symmetric matrices. Recall that the diagonalizing
    orthogonal matrix U can be obtained by vertically stacking the eigenvectors one
    by one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Awesome!
  prefs: []
  type: TYPE_NORMAL
- en: What’s the problem? Unfortunately, power iteration is numerically unstable.
    For n ×n matrices, where n can be in the millions, this is a serious issue.
  prefs: []
  type: TYPE_NORMAL
- en: Why did we talk so much about power iteration, then? Besides being the simplest,
    it offers a deep insight into how linear transformations work.
  prefs: []
  type: TYPE_NORMAL
- en: The identity
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n Ax = xiλiui, i=1 ](img/file772.png)'
  prefs: []
  type: TYPE_IMG
- en: where λ[i] and u[i] are eigenvalue-eigenvector pairs of the symmetric matrix
    A, reflecting how eigenvectors and eigenvalues determine the behavior of the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: If the power iteration is not usable in practice, how can we compute the eigenvalues?
    We will see this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 The QR algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm used in practice to compute the eigenvalues is the so-called QR
    algorithm, proposed independently by John G. R. Francis and the Soviet mathematician
    Vera Kublanovskaya. This is where all of the lessons we have learned in linear
    algebra converge. Describing the QR algorithm is very simple, as it is the iteration
    of a matrix decomposition and a multiplication step.
  prefs: []
  type: TYPE_NORMAL
- en: However, understanding why it works is a different question. Behind the scenes,
    the QR algorithm combines many tools we have learned earlier. To start, let’s
    revisit the good old Gram-Schmidt orthogonalization process.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.1 The QR decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you recall, we encountered the Gram-Schmidt orthogonalization process (Theorem [13](ch008.xhtml#x1-47004r13))
    when introducing the concept of orthogonal bases.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, this algorithm takes an arbitrary basis v[1],…,v[n] and turns it
    into an orthonormal one e[1],…,e[n], such that e[1],…,e[k] spans the same subspace
    as v[1],…,v[k] for all 1 ≤k ≤n. Since we last met this, we have gained a lot of
    perspective on linear algebra, so we are ready to see the bigger picture.
  prefs: []
  type: TYPE_NORMAL
- en: With the orthogonal projections defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k ⟨x,ei⟩ proje1,...,ek(x) = ------ei, i=1 ⟨ei,ei⟩ ](img/file773.png)'
  prefs: []
  type: TYPE_IMG
- en: we can describe the Gram-Schmidt process recursively as
  prefs: []
  type: TYPE_NORMAL
- en: '![e1 = v1, ek = vk − proje1,...,ek−1(vk), ](img/file774.png)'
  prefs: []
  type: TYPE_IMG
- en: where the e[k] vectors are normalized after.
  prefs: []
  type: TYPE_NORMAL
- en: By expanding this and writing out e[k] explicitly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![e1 = v1 e2 = v2 − ⟨v2,e1⟩e1 ⟨e1,e1⟩ .. . ⟨vn, e1⟩ ⟨vn, en−1⟩ en = vn − ⟨e-,e-⟩e1
    − ⋅⋅⋅− ⟨e----,e---⟩en−1\. 1 1 n− 1 n−1 ](img/file775.png)'
  prefs: []
  type: TYPE_IMG
- en: A pattern is starting to emerge. By arranging the e[1],…,e[n] terms on one side,
    we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![v1 = e1 ⟨v2,e1⟩ v2 = ⟨e-,e-⟩e1 + e2 1 1 ... vn = ⟨vn,e1⟩e1 + ⋅⋅⋅ + -⟨vn,en−1⟩-en−1
    + en. ⟨e1,e1⟩ ⟨en−1,en−1⟩ ](img/file776.png)'
  prefs: []
  type: TYPE_IMG
- en: This is starting to resemble some kind of matrix multiplication! Recall that
    matrix multiplication can be viewed as taking the linear combination of columns.
    (Check ([4.2](#)) if you are uncertain about this.)
  prefs: []
  type: TYPE_NORMAL
- en: By horizontally concatenating the column vectors v[k] to form the matrix A and
    similarly defining the vector Q from the e[k]-s, we obtain that
  prefs: []
  type: TYPE_NORMAL
- en: '![A = Q∗R ∗ ](img/file777.png)'
  prefs: []
  type: TYPE_IMG
- en: for some upper triangular R, defined by the coefficients of e[k] in v[k] according
    to the Gram-Schmidt orthogonalization. To be more precise, define
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ A = ||v ... v || , Q ∗ = || e ... e || , ⌈ 1 n⌉ ⌈ 1 n⌉ ](img/file778.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⟨v2,e1⟩ ⟨vn,e1⟩⌋ |1 ⟨e1,e1⟩ ... ⟨e1,e1⟩| ||0 1 ... ⟨vn,e2⟩|| R ∗ = |.
    . ⟨e2,.e2⟩|. ||.. .. ... .. || ⌈ .. ⌉ 0 0 . 1 ](img/file779.png)'
  prefs: []
  type: TYPE_IMG
- en: The result A = Q^∗R^∗ is almost what we call the QR factorization. The columns
    of Q^∗ are orthogonal (but not orthonormal), while R^∗ is upper triangular. We
    can easily orthonormalize Q^∗ by factoring out the norms columnwise, thus obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ∥e ∥ √⟨v2,e1⟩-- ... √⟨vn,e1⟩- ⌊ ⌋ | 1 ⟨e1,e1⟩ ⟨e1,e1⟩| | | || 0 ∥e2∥
    ... √⟨vn,e2⟩|| Q = |⌈ e1-- ... en-|⌉ , R = || ⟨e2,e2⟩|| . ∥e1∥ ∥en∥ || ... ...
    ... ... || ⌈ . ⌉ 0 0 .. ∥en∥ ](img/file780.png)'
  prefs: []
  type: TYPE_IMG
- en: It is easy to see that A = QR still holds. This result is called the QR decomposition,
    and we have just proved the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 53\. (QR decomposition)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ∈ ℝ^(n×n) be an invertible matrix. Then, there exists an orthogonal matrix
    Q ∈ℝ^(n×n) and an upper triangular matrix R ∈ℝ^(n×n) such that
  prefs: []
  type: TYPE_NORMAL
- en: '![A = QR ](img/file781.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: As we are about to see, the QR decomposition is an extremely useful and versatile
    tool (like all other matrix decompositions are). Before we move forward to discuss
    how it can be used to compute the eigenvalues in practice, let’s put what we have
    seen so far into code!
  prefs: []
  type: TYPE_NORMAL
- en: The QR decomposition algorithm is essentially Gram-Schmidt orthogonalization,
    where we explicitly memorize some coefficients and form a matrix from them. (Recall
    our earlier implementation in Section [3.1.2](ch009.xhtml#the-gramschmidt-orthogonalization-process1)
    if you feel overwhelmed.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we can put these together to obtain the QR factorization of an arbitrary
    square matrix. (Surprisingly, this works for non-square matrices as well, but
    we won’t be concerned with this.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try it out on a random 3 × 3 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three things to check: (a) A = QR, (b) Q is an orthogonal matrix,
    and (c) R is upper triangular.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Success! There is only one more question left. How does this help us in calculating
    the eigenvalues? Let’s see that now.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6.2 Iterating the QR decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Surprisingly, we can discover the eigenvalues of a matrix A by a simple iterative
    process. First, we find the QR decomposition
  prefs: []
  type: TYPE_NORMAL
- en: '![A = Q R , 1 1 ](img/file783.png)'
  prefs: []
  type: TYPE_IMG
- en: and define the matrix A[1] by
  prefs: []
  type: TYPE_NORMAL
- en: '![A1 = R1Q1, ](img/file784.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, we simply reverse the order of Q and R. Then, we start it all over
    and find the QR decomposition of A[1], and so on, defining the sequence
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/file785.png)(7.9)'
  prefs: []
  type: TYPE_IMG
- en: In the long run, the diagonal elements of A[k] will get closer and closer to
    the eigenvalues of A. This is called the QR algorithm, which is so simple that
    I didn’t believe it when I first saw it.
  prefs: []
  type: TYPE_NORMAL
- en: With all of our tools, we can implement the QR algorithm in a few lines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test it right away.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We are almost at the state of the art. Unfortunately, the vanilla QR algorithm
    has some issues, as it can fail to converge. A simple example is given by the
    matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ A = ⌈0 1⌉ . 1 0 ](img/file786.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, we can solve this with the introduction of shifts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(14).png)(7.10)'
  prefs: []
  type: TYPE_IMG
- en: where α[k] is some scalar. There are multiple approaches to defining the shifts
    themselves (Rayleigh quotient shift, Wilkinson shift, etc.), but the details lie
    much deeper than our study.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I told you that climbing the peak is not easy: so far, this was our hardest
    chapter yet. However, the tools we’ve learned are at the pinnacle of linear algebra.
    We started by studying two special transformations: the self-adjoint and orthogonal
    ones. The former ones gave the spectral decomposition theorem, while the latter
    ones gave the singular value decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: Undoubtedly, the SVD is one of the most important results in linear algebra,
    stating that every rectangular matrix A can be written in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![A = U ΣV T, ](img/file787.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where U ∈ℝ^(n×n), Σ ∈ℝ^(n×m), and V ∈ℝ^(m×m) are rather special: Σ is diagonal,
    while U and V are orthogonal.'
  prefs: []
  type: TYPE_NORMAL
- en: When viewing A as a linear transformation, the singular value decomposition
    tells us that it can be written as the composition of two distance-preserving
    transformations (the orthogonal ones) and a simple scaling. That’s quite a characterization!
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of singular values and eigenvalues, how do we find them in practice?
    Definitely not by solving the polynomial equation
  prefs: []
  type: TYPE_NORMAL
- en: '![det(A − λI ) = 0, ](img/file788.png)'
  prefs: []
  type: TYPE_IMG
- en: which is a computationally painful problem.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen two actual methods for the task. One is the complicated, slow, unstable,
    but illuminating algorithm of power iteration, yielding eigenvectors for the dominant
    eigenvalue of a real and symmetric matrix A via the limit
  prefs: []
  type: TYPE_NORMAL
- en: '![ Akx0 lk→im∞ ∥Akx--∥-- = u1\. 0 ∞ ](img/file789.png)'
  prefs: []
  type: TYPE_IMG
- en: Although power iteration gives us valuable insight into the structure of such
    matrices, the real deal is the QR algorithm (unrelated to QR codes), originating
    from the vectorized version of the Gram-Schmidt algorithm. The QR algorithm is
    hard to intuitively understand, but despite its mystery, it provides a blazing-fast
    method for computing the eigenvalues in practice.
  prefs: []
  type: TYPE_NORMAL
- en: What’s next? Now that we are at the peak, it’s time to relax a bit and enjoy
    the beautiful view. The next chapter does just that.
  prefs: []
  type: TYPE_NORMAL
- en: You see, one of the most beautiful and useful topics in linear algebra is the
    connection between matrices and graphs. Because from the right perspective, a
    matrix is a graph, and we can utilize this relationship to study the structure
    of both. Let’s see!
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Let u[1],…,u[k] ∈ℝ^n be a set of linearly independent and pairwise
    orthogonal vectors. Show that the linear transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k ⟨x, ui⟩ proju1,...,uk(x) = ⟨u-,u-⟩ui i=1 i i ](img/file790.png)'
  prefs: []
  type: TYPE_IMG
- en: is an orthogonal projection.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Let u[1],…,u[k] ∈ℝ^n be a set of linearly independent vectors, and
    define the linear transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k ⟨x,ui⟩- fakeproju1,...,uk(x ) = ⟨ui,ui⟩ui. i=1 ](img/file791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Is this a projection? (Hint: Study the special case k = 2 and ℝ³. You can visualize
    this if needed.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 3\. Let V be an inner product space and P : V →V be an orthogonal projection.
    Show that I −P is an orthogonal projection as well, and'
  prefs: []
  type: TYPE_NORMAL
- en: '![ker(I − P ) = im P, im (I − P ) = ker P ](img/file792.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let A,B ∈ℝ^(n×n) be two square matrix that are written in the block
    matrix form
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ A1,1 A1,2 B1,1 B1,2 A = ||A A ||, B = ||B B ||, ⌈ 2,1 2,2⌉ ⌈ 2,1
    2,2⌉ ](img/file793.png)'
  prefs: []
  type: TYPE_IMG
- en: where A[1,1],B[1,1] ∈ℝ^(k×k), A[1,2],B[1,2] ∈ℝ^(k×l), A[2,1],B[2,1] ∈ℝ^(l×k),
    and A[2,2],B[2,2] ∈ℝ^(l×l).
  prefs: []
  type: TYPE_NORMAL
- en: Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ A1,1B1,1 + A1,2B2,1 A1,1B1,2 + A1,2B2,2 || || AB = ⌈A2,1B1,1 + A2,2B2,1
    A2,1B1,2 + A2,2B2,2⌉ . ](img/file794.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 5\. Let A ∈ℝ^(2×2) be the square matrix defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ A = ⌈1 1⌉ . 1 0 ](img/file795.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Show that the two eigenvalues of A are
  prefs: []
  type: TYPE_NORMAL
- en: '![ − 1 λ1 = φ, λ2 = − φ , ](img/file796.png)'
  prefs: []
  type: TYPE_IMG
- en: where φ = ![1+√5 --2--](img/file797.png) is the golden ratio.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that the eigenvectors of λ[1] and λ[2] are
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ −1⌋ f = ⌈φ ⌉, f = ⌈− φ ⌉ , 1 1 2 1 ](img/file798.png)'
  prefs: []
  type: TYPE_IMG
- en: and show that they are orthogonal; that is, ⟨f[1],f[2]⟩ = 0.
  prefs: []
  type: TYPE_NORMAL
- en: (c) Let U be the matrix formed by the eigenvectors of A, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ φ − φ−1 U = ⌈ ⌉ . 1 1 ](img/file799.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 1 φ −1 U −1 = √1-⌈ ⌉ . 5 − 1 φ ](img/file800.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ − 1 A = UΛU , ](img/file801.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ λ1 0 φ 0 Λ = ⌈ ⌉ = ⌈ −1⌉ . 0 λ2 0 − φ ](img/file802.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Were you wondering the purpose of all these mundane computations? Here comes
    the punchline. First, show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ n An = ⌈1 1⌉ = ⌈Fn+1 Fn ⌉ , 1 0 Fn Fn−1 ](img/file803.png)'
  prefs: []
  type: TYPE_IMG
- en: where F[n] is the n-th Fibonacci number, defined by the recursive sequence
  prefs: []
  type: TYPE_NORMAL
- en: '![F0 = 0, F1 = 1, Fn = Fn−1 + Fn−2\. ](img/file804.png)'
  prefs: []
  type: TYPE_IMG
- en: Consequently, show that A = UΛU^(−1) implies that
  prefs: []
  type: TYPE_NORMAL
- en: '![ n −n Fn = φ--−-(√−-φ)-- . 5 ](img/file805.png)'
  prefs: []
  type: TYPE_IMG
- en: That’s pretty cool!
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
