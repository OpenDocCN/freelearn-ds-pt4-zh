<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch014.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="matrices-and-graphs" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">8</span></span><br/>
<span id="x1-1330009"></span><span class="cmss-10x-x-109">Matrices and Graphs</span></h1>
<p><span class="cmss-10x-x-109">Now that we have gotten past the hard part (that is, the singular value decomposition and other matrix factorizations), it’s time to finish our journey through linear algebra with a bang. In my teaching experience, one of students’ most common concerns is the apparent disconnect between practice and theory. Among machine learning practitioners and software engineers, there’s often a reluctance to touch anything that is not immediately valuable in practice.</span></p>
<p><span class="cmss-10x-x-109">As a mathematician, I completely get where this dread comes from. We are often taught arcane topics of no practical importance, taking valuable time away from hacking and slashing our way through data.</span></p>
<p><span class="cmss-10x-x-109">In this chapter, we’ll look at a subject that is not immediately useful for your machine learning practice, but will pay serious dividends in the future. Considering how beautiful it is, it might be the inspiration for your next genius idea. (No promises, though.)</span></p>
<p><span class="cmss-10x-x-109">Let me introduce you to the single most undervalued fact of linear algebra: matrices are graphs, and graphs are matrices. Encoding matrices</span><span id="dx1-133001"></span> <span class="cmss-10x-x-109">as graphs is a cheat code, making complex behavior simple to study.</span></p>
<p><span class="cmss-10x-x-109">Check out </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">8.1</span></a> <span class="cmss-10x-x-109">below.</span></p>
<div class="minipage">
<p><img src="../media/file806.png" width="341" alt="PIC"/> <span id="x1-133002r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.1: A matrix and its directed graph</span> </span>
</div>
<p><span class="cmss-10x-x-109">Can you figure out how it was constructed? Can you guess why it is useful? We’ll answer these questions in the next couple of pages. Specifically, we’ll see:</span></p>
<ul>
<li><span class="cmss-10x-x-109">What the relationship between graphs and matrices is</span></li>
<li><span class="cmss-10x-x-109">How matrix multiplication can be translated to walks on the graph</span></li>
<li><span class="cmss-10x-x-109">What the connectivity structure of a graph reveals about its corresponding matrix</span></li>
</ul>
<p><span class="cmss-10x-x-109">Let’s get to it!</span></p>
<section id="the-directed-graph-of-a-nonnegative-matrix" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_114"><span class="titlemark"><span class="cmss-10x-x-109">8.1 </span></span> <span id="x1-1340009.1"></span><span class="cmss-10x-x-109">The directed graph of a nonnegative matrix</span></h2>
<p><span class="cmss-10x-x-109">If you look</span> <span id="dx1-134001"></span><span class="cmss-10x-x-109">carefully at </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">8.1</span></a><span class="cmss-10x-x-109">, you can probably figure out how to construct a weighted graph from a matrix. Just compare each row and the outgoing edge weights for nodes.</span></p>
<p><span class="cmss-10x-x-109">Each row is a node, and each element represents a directed and weighted edge. Edges of zero elements are omitted. The element in the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column corresponds to an edge going from </span><span class="cmmi-10x-x-109">i </span><span class="cmss-10x-x-109">to </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">. The resulting graph is called the </span><span class="cmssi-10x-x-109">directed graph </span><span class="cmss-10x-x-109">(or digraph) </span><span class="cmssi-10x-x-109">of the matrix</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To unwrap the definition a bit, let’s check out the previous graph of the matrix</span></p>
<div class="math-display">
<img src="../media/file807.png" class="math-display" alt=" ⌊ ⌋ 0.5 1 0 | | 3×3 A = |⌈0.2 0 2.2|⌉ ∈ ℝ . 1.8 2 0 "/>
</div>
<p><span class="cmss-10x-x-109">Here’s the first row, corresponding to the edges coming out from the first node.</span></p>
<div class="minipage">
<p><img src="../media/file808.png" width="341" alt="PIC"/> <span id="x1-134002r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.2: The first row corresponds to the edges coming out from the first node</span> </span>
</div>
<p><span class="cmss-10x-x-109">Similarly, the</span> <span id="dx1-134003"></span><span class="cmss-10x-x-109">first column corresponds to the edges coming into the first node.</span></p>
<div class="minipage">
<p><img src="../media/file809.png" width="341" alt="PIC"/> <span id="x1-134004r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.3: The first column corresponds to the edges coming into the first node</span> </span>
</div>
<p><span class="cmss-10x-x-109">Now, we can put all of this together. </span><span class="cmssi-10x-x-109">Figure 8.4 </span><span class="cmss-10x-x-109">shows the full picture, with the nodes explicitly labeled.</span></p>
<div class="minipage">
<p><img src="../media/file810.png" width="341" alt="PIC"/> <span id="x1-134005r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.4: Constructing the graph of a matrix</span> </span>
</div>
<p><span class="cmss-10x-x-109">It’s time to</span> <span id="dx1-134006"></span><span class="cmss-10x-x-109">check the formal definition, which we’ll split into two parts. First, let’s talk about weighted and directed graphs.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-134007r29"></span> <span class="cmbx-10x-x-109">Definition 29.</span> </span></p>
<p>Let <span class="cmmi-10x-x-109">V </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,v</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…,v</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span>be an arbitrary finite set. We say that <span class="cmmi-10x-x-109">G </span>= (<span class="cmmi-10x-x-109">V,E,w</span>) is a <span class="cmti-10x-x-109">weighted and directed graph</span>, if</p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">V </span>represents the set of <span class="cmti-10x-x-109">vertices </span>(also called nodes). <span class="cmti-10x-x-109">(b) </span><span class="cmmi-10x-x-109">E </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">V </span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">V </span>represents the set of <span class="cmti-10x-x-109">directed edges</span>. <span class="cmti-10x-x-109">(c) </span>The function <span class="cmmi-10x-x-109">w </span>: <span class="cmmi-10x-x-109">E </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>represents the edge <span class="cmti-10x-x-109">weights</span>.</p>
</div>
<p><span class="cmss-10x-x-109">For example, check out </span><span class="cmssi-10x-x-109">Figure 8.4</span><span class="cmss-10x-x-109">, which we can formalize as </span>(<span class="cmmi-10x-x-109">V,E,w</span>) <span class="cmss-10x-x-109">with </span><span class="cmmi-10x-x-109">V </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">v</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,v</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,v</span><sub><span class="cmr-8">3</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file811.png" class="math-display" alt="E = {(v1,v1),(v1,v2), (v2,v1),(v2,v3), (v3,v1),(v3,v2)}, "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file812.png" class="math-display" alt="w(v1,v1) = 0.5, w (v1,v2) = 1, w(v2,v1) = 0.2, w (v2,v2) = 2.2, w(v3,v1) = 1.8, w (v3,v2) = 2. "/>
</div>
<p><span class="cmss-10x-x-109">Now, we are ready to talk about matrices.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-134008r30"></span> <span class="cmbx-10x-x-109">Definition 30.</span> </span><span class="cmbx-10x-x-109">(Irreducible and reducible matrices)</span></p>
<p>Let <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> be a nonnegative matrix, i.e., a matrix with only nonnegative elements. The directed weighted graph <span class="cmmi-10x-x-109">G </span>= (<span class="cmmi-10x-x-109">V,E,w</span>) is said to be the <span class="cmti-10x-x-109">directed graph </span>(or digraph for short) <span class="cmti-10x-x-109">of </span><span class="cmmi-10x-x-109">A </span>if:</p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">V </span>= <span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">}</span>, <span class="cmti-10x-x-109">(b) </span>(<span class="cmmi-10x-x-109">i,j</span>) <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">E </span>if, and only if, <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub><span class="cmmi-10x-x-109">/span&gt;0, <span class="cmti-10x-x-109">(c) </span><span class="cmmi-10x-x-109">w</span>(<span class="cmmi-10x-x-109">i,j</span>) = <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub>. </span></p>
</div>
<p><span class="cmss-10x-x-109">(Sometimes, for illustrative purposes, we’ll just omit the weights and assume all of them to be equal to 1.) Again, why is this useful? Because</span> <span id="dx1-134009"></span><span class="cmss-10x-x-109">this way, we can translate algebraic questions into graph-theoretic ones. Thus, we gain access to the vast toolkit of graph theory.</span></p>
</section>
<section id="benefits-of-the-graph-representation" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_115"><span class="titlemark"><span class="cmss-10x-x-109">8.2 </span></span> <span id="x1-1350009.2"></span><span class="cmss-10x-x-109">Benefits of the graph representation</span></h2>
<p><span class="cmss-10x-x-109">Let’s talk about the</span> <span id="dx1-135001"></span><span class="cmss-10x-x-109">concrete advantages that the graph representation offers. For one, the powers of the matrix correspond to walks in the graph. Say, for any let</span> <img src="../media/file813.png" class="math" alt="A = (ai,j)ni,j=1 ∈ ℝn×n "/><span class="cmss-10x-x-109">. Its square is denoted by </span><img src="../media/file814.png" class="math" alt="A2 = (a(2))ni,j=1 ∈ ℝn ×n i,j "/><span class="cmss-10x-x-109">, where the elements </span><img src="../media/file815.png" class="math" alt=" (2) ai,j "/> <span class="cmss-10x-x-109">are defined by</span></p>
<div class="math-display">
<img src="../media/file816.png" class="math-display" alt=" n a(2) = ∑ a a . i,j i,k k,j k=1 "/>
</div>
<p><span class="cmss-10x-x-109">(Note that the </span>(2) <span class="cmss-10x-x-109">in the superscript of </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,j</span></sub><sup><span class="cmr-8">(2)</span></sup> <span class="cmss-10x-x-109">is not an exponent; this is just an index indicating that </span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i,</span></sub><sup><span class="cmr-8">(2)</span></sup> <span class="cmss-10x-x-109">is the element of </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmssi-10x-x-109">Figure 8.5 </span><span class="cmss-10x-x-109">shows the elements of the square matrix and its graph: all possible two-step walks are accounted for in the sum defining the elements of </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file817.png" width="284" alt="PIC"/> <span id="x1-135002r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.5: Powers of the matrix describe walks on its directed graph</span> </span>
</div>
<p><span class="cmss-10x-x-109">There is much more to this connection; for instance, it gives us a deep insight into the structure of nonnegative matrices. To see how, let’s talk about the concept of strongly connected components.</span></p>
<section id="the-connectivity-of-graphs" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_116"><span class="titlemark"><span class="cmss-10x-x-109">8.2.1 </span></span> <span id="x1-1360009.2.1"></span><span class="cmss-10x-x-109">The connectivity of graphs</span></h3>
<p><span class="cmss-10x-x-109">Intuitively, we can think of</span> <span id="dx1-136001"></span><span class="cmss-10x-x-109">connectivity as the ability to reach every node from the others. To formalize this, we’ll need a couple of definitions. First, the </span><span class="cmssi-10x-x-109">“reach every node from the others” </span><span class="cmss-10x-x-109">part.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-136002r31"></span> <span class="cmbx-10x-x-109">Definition 31.</span> </span><span class="cmbx-10x-x-109">(Walks on a graph)</span></p>
<p>Let <span class="cmmi-10x-x-109">G </span>= (<span class="cmmi-10x-x-109">V,E,w</span>) be a weighted and directed graph. The sequence <span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmr-6">1</span></sub></sub><span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmr-6">2</span></sub></sub><span class="cmmi-10x-x-109">…v</span><sub><span class="cmmi-8">k</span><sub><span class="cmmi-6">l</span></sub></sub> is a (directed) <span class="cmti-10x-x-109">walk </span>on <span class="cmmi-10x-x-109">G </span>if (<span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmmi-6">i</span></sub></sub><span class="cmmi-10x-x-109">,v</span><sub><span class="cmmi-8">k</span><sub><span class="cmmi-6">i</span><span class="cmr-6">+1</span></sub></sub>) <span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">E </span>for all <span class="cmmi-10x-x-109">i</span>.</p>
</div>
<p><span class="cmss-10x-x-109">(For consistency, we define walks for weighted and directed graphs, but the definition holds for simple graphs – i.e., graphs without edges and directional edges – as well. The same goes for most of the upcoming concepts.)</span></p>
<p><span class="cmss-10x-x-109">In general, we say that the walk </span><span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmr-6">1</span></sub></sub><span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmr-6">2</span></sub></sub><span class="cmmi-10x-x-109">…v</span><sub><span class="cmmi-8">k</span><sub><span class="cmmi-6">l</span></sub></sub> <span class="cmss-10x-x-109">starts at </span><span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmr-6">1</span></sub></sub> <span class="cmss-10x-x-109">and ends at </span><span class="cmmi-10x-x-109">v</span><sub><span class="cmmi-8">k</span><sub><span class="cmmi-6">l</span></sub></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">The term </span><span class="cmssi-10x-x-109">walk </span><span class="cmss-10x-x-109">is surprisingly descriptive, as it truly describes a walk on the directed edges, going from node to node. However, a graph-theoretic walk is a properly defined mathematical object, not just a vague intuition. Pick up a pen and a paper once again and sketch up a graph, then a couple of its walks, to understand the concept better.</span></p>
<p><span class="cmss-10x-x-109">What do walks have to do with connectivity? Simple: If you can reach every node from every other node, the graph is said to be connected. Since we are talking about </span><span class="cmssi-10x-x-109">directed </span><span class="cmss-10x-x-109">graphs, let’s add a bit of nuance to the discussion and conjure up a formal definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-136003r32"></span> <span class="cmbx-10x-x-109">Definition 32.</span> </span></p>
<p><span class="cmbx-10x-x-109">(Strong connectivity)</span></p>
<p>Let <span class="cmmi-10x-x-109">G </span>= (<span class="cmmi-10x-x-109">V,E,w</span>) be a weighted and directed graph. We say that <span class="cmmi-10x-x-109">G </span>is <span class="cmti-10x-x-109">strongly connected </span>if for every <span class="cmmi-10x-x-109">u,v </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span>, there exists a walk that starts at <span class="cmmi-10x-x-109">u</span> and ends at <span class="cmmi-10x-x-109">v</span>.</p>
</div>
<p><span class="cmss-10x-x-109">In other words, a directed graph is strongly connected if every node can be reached from every other node. If this is not true, the graph is not strongly connected. </span><span class="cmssi-10x-x-109">Figure 8.6 </span><span class="cmss-10x-x-109">shows you an example of both.</span></p>
<p><span class="cmssi-10x-x-109">Figure 8.6 </span><span class="cmss-10x-x-109">also illustrates that</span> <span id="dx1-136004"></span><span class="cmss-10x-x-109">strong connectivity does not match the connectivity concept for simple graphs.</span></p>
<p><span class="cmss-10x-x-109">It’s not enough to reach </span><span class="cmmi-10x-x-109">u </span><span class="cmss-10x-x-109">from </span><span class="cmmi-10x-x-109">v</span><span class="cmss-10x-x-109">; you have to be able to go back as well.</span></p>
<div class="minipage">
<p><img src="../media/file818.png" width="284" alt="PIC"/> <span id="x1-136005r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.6: Connected vs. strongly connected</span> </span>
</div>
<p><span class="cmss-10x-x-109">Now, let’s translate what we’ve learned to the language of matrices. Matrices that correspond to strongly connected graphs are called irreducible. All other nonnegative matrices are called reducible. Soon, we’ll see why, but first, here’s the formal definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-136006r33"></span> <span class="cmbx-10x-x-109">Definition 33.</span> </span></p>
<p><span class="cmbx-10x-x-109">(Irreducible and reducible matrices)</span></p>
<p>Let <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> be a nonnegative matrix.</p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">A </span>is called <span class="cmti-10x-x-109">irreducible </span>if its digraph is strongly connected. <span class="cmti-10x-x-109">(b) </span><span class="cmmi-10x-x-109">A </span>is called <span class="cmti-10x-x-109">reducible </span>if it is not irreducible.</p>
</div>
<p><span class="cmss-10x-x-109">Let’s see an example! </span><span class="cmssi-10x-x-109">Figure 8.7 </span><span class="cmss-10x-x-109">shows an irreducible matrix.</span></p>
<div class="minipage">
<p><img src="../media/file819.png" width="227" alt="PIC"/> <span id="x1-136007r7"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.7: Strongly connected digraphs and their matrices</span> </span>
</div>
<p><span class="cmss-10x-x-109">Back to the general case! Even though not all digraphs are strongly connected, we</span> <span id="dx1-136008"></span><span class="cmss-10x-x-109">can partition the nodes into strongly connected components (as </span><span class="cmssi-10x-x-109">Figure 8.8 </span><span class="cmss-10x-x-109">illustrates).</span></p>
<div class="minipage">
<p><img src="../media/file820.png" width="284" alt="PIC"/> <span id="x1-136009r8"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.8: Strongly connected components</span> </span>
</div>
<p><span class="cmss-10x-x-109">Let’s label the nodes of this graph and construct the corresponding matrix! If you assume that the weights are simply equal to 1 and translate all the edges into rows and columns as we have learned, you’ll get</span></p>
<div class="math-display">
<img src="../media/file821.png" class="math-display" alt=" ⌊ ⌋ |0 1 1 0 0 0| ||0 0 1 0 0 0|| | | ||1 0 0 1 0 0|| A = ||0 0 0 0 1 1||. || || |⌈0 0 0 0 0 1|⌉ 0 0 0 1 0 0 "/>
</div>
<p><span class="cmss-10x-x-109">That’s just a big</span> <span id="dx1-136010"></span><span class="cmss-10x-x-109">block of ones and zeroes, but you shouldn’t be disappointed: there is a pattern! By dividing </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">into blocks, the matrix of our example graph can be reduced to a simpler form:</span></p>
<div class="math-display">
<img src="../media/file822.png" class="math-display" alt=" ⌊ | ⌋ 0 1 1 |0 0 0 || | || || 0 0 1 |0 0 0 || ⌊ | ⌋ || 1 0 0 |1 0 0 || A1,1|A1,2 A = ||---------|--------|| = ⌈ -----|-----⌉ , | 0 0 0 |0 1 1 | A2,1|A2,2 || 0 0 0 |0 0 1 || ⌈ | ⌉ 0 0 0 |1 0 0 "/>
</div>
<p><span class="cmss-10x-x-109">where</span></p>
<div class="math-display">
<img src="../media/file823.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ 0 1 1 0 0 0 || || || || A1,1 = ⌈ 0 0 1⌉, A1,2 = ⌈0 0 0⌉ , 1 0 0 1 0 0 ⌊ ⌋ ⌊ ⌋ 0 0 0 0 1 1 A = || ||, A = || || . 2,1 ⌈ 0 0 0⌉ 2,2 ⌈0 0 1⌉ 0 0 0 1 0 0 "/>
</div>
<p><span class="cmss-10x-x-109">The diagonal blocks </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span><span class="cmmi-8">,</span><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span><span class="cmmi-8">,</span><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">represent graphs that are strongly</span> <span id="dx1-136011"></span><span class="cmss-10x-x-109">connected (that is, the blocks are irreducible). Furthermore, the block below the diagonal is 0. Is this true for all nonnegative matrices?</span></p>
<p><span class="cmss-10x-x-109">You bet. Let’s see!</span></p>
</section>
</section>
<section id="the-frobenius-normal-form" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_117"><span class="titlemark"><span class="cmss-10x-x-109">8.3 </span></span> <span id="x1-1370009.3"></span><span class="cmss-10x-x-109">The Frobenius normal form</span></h2>
<p><span class="cmss-10x-x-109">In general, the block-matrix structure that we have just seen is called</span><span id="dx1-137001"></span> <span class="cmss-10x-x-109">the Frobenius normal form. Here’s the precise definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-137002r34"></span> <span class="cmbx-10x-x-109">Definition 34.</span> </span></p>
<p><span class="cmbx-10x-x-109">(Frobenius normal form)</span></p>
<p>Let <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> be a nonnegative matrix. <span class="cmmi-10x-x-109">A </span>is said to be in <span class="cmti-10x-x-109">Frobenius normal form </span>if it can be written in the block matrix form</p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(15).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  
</div>
<p>where <span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,A</span><sub><span class="cmmi-8">k</span></sub> are irreducible matrices.</p>
</div>
<p><span class="cmss-10x-x-109">Let’s reverse the question: can we transform an arbitrary nonnegative matrix into the Frobenius normal form? Yes, and with the help of directed graphs, this is much easier to show than purely using algebra. Here is the famous theorem in full form.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-137003r54"></span> <span class="cmbx-10x-x-109">Theorem 54.</span> </span><span class="cmbxti-10x-x-109">(The existence of the Frobenius normal form)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">be a nonnegative matrix. There exists a permutation matrix </span><span class="cmmi-10x-x-109">P </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">such that </span><span class="cmmi-10x-x-109">P</span><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">AP </span><span class="cmti-10x-x-109">is in Frobenius normal form.</span></p>
</div>
<p><span class="cmss-10x-x-109">Rigorously spelling out the proof of </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch014.xhtml#x1-137003r54"><span class="cmssi-10x-x-109">54</span></a> <span class="cmss-10x-x-109">is quite complicated. However, the ideas behind the proof are simple to show. Thus, we’ll take the less rigorous, more fun route.</span></p>
<p><span class="cmss-10x-x-109">So, why is the</span> <span id="dx1-137004"></span><span class="cmss-10x-x-109">Frobenius normal form a big deal and what on Earth is a permutation matrix? Let’s dive into it.</span></p>
<section id="permutation-matrices" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_118"><span class="titlemark"><span class="cmss-10x-x-109">8.3.1 </span></span> <span id="x1-1380009.3.1"></span><span class="cmss-10x-x-109">Permutation matrices</span></h3>
<p><span class="cmss-10x-x-109">Mathematics is</span> <span id="dx1-138001"></span><span class="cmss-10x-x-109">often done</span> <span id="dx1-138002"></span><span class="cmss-10x-x-109">from concrete to abstract. That’s why we are often start with special cases: what happens if we multiply a 2 x 2 matrix by</span></p>
<div class="math-display">
<img src="../media/file825.png" class="math-display" alt=" ⌊ ⌋ P1,2 = ⌈0 1⌉ , 1 0 "/>
</div>
<p><span class="cmss-10x-x-109">a simple zero-one matrix? With a quick calculation, we can verify that</span></p>
<div class="math-display">
<img src="../media/file826.png" class="math-display" alt="⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌈0 1⌉ ⌈a b⌉ = ⌈c d⌉ , 1 0 c d a b ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ a b 0 1 b a ⌈ ⌉ ⌈ ⌉ = ⌈ ⌉ , c d 1 0 d c "/>
</div>
<p><span class="cmss-10x-x-109">that is,</span></p>
<ol>
<li><span id="x1-138004x1"><span class="cmss-10x-x-109">it switches the rows when multiplied from the left,</span></span></li>
<li><span id="x1-138006x2"><span class="cmss-10x-x-109">and it switches the columns when multiplied from the right.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Multiplying by P from both the left and right compounds the effects: it switches rows and columns, as</span></p>
<div class="math-display">
<img src="../media/file827.png" class="math-display" alt="⌊ ⌋ ⌊ ⌋ ⌊ ⌋ ⌊ ⌋ 0 1 a b 0 1 d c ⌈ ⌉ ⌈ ⌉ ⌈ ⌉ = ⌈ ⌉ 1 0 c d 1 0 b a "/>
</div>
<p><span class="cmss-10x-x-109">shows. (By the way, this is a similarity transformation, as our special zero-one matrix is its own inverse. This is not an accident; more about it later.)</span></p>
<p><span class="cmss-10x-x-109">Why are we looking at this? Because behind the scenes, this transformation doesn’t change the underlying graph structure, just relabels its nodes!</span></p>
<p><span class="cmss-10x-x-109">You can easily verify this by hand, but </span><span class="cmssi-10x-x-109">Figure 8.9 </span><span class="cmss-10x-x-109">illustrates this as well.</span></p>
<div class="minipage">
<p><img src="../media/file828.png" width="284" alt="PIC"/> <span id="x1-138007r9"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.9: Relabeling the nodes</span> </span>
</div>
<p><span class="cmss-10x-x-109">A similar</span> <span id="dx1-138008"></span><span class="cmss-10x-x-109">phenomenon is true in the general </span><span class="cmmi-10x-x-109">n</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">case. Here, we define</span><span id="dx1-138009"></span> <span class="cmss-10x-x-109">the so-called transposition matrices by switching the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th rows of the identity matrix, for example:</span></p>
<div class="math-display">
<img src="../media/file829.png" class="math-display" alt=" ⌊ ⌋ ⌊ ⌋ |0 1 0 0 0| |1 0 0 0 0| ||1 0 0 0 0|| ||0 1 0 0 0|| || || 5×5 || || 5×5 P1,2 = |0 0 1 0 0| ∈ ℝ , P3,5 = |0 0 0 0 1| ∈ ℝ . ||0 0 0 1 0|| ||0 0 0 1 0|| ⌈ ⌉ ⌈ ⌉ 0 0 0 0 1 0 0 1 0 0 "/>
</div>
<p><span class="cmss-10x-x-109">The two most important properties of the permutation matrices are </span><span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i,j</span></sub><sup><span class="cmmi-8">T</span></sup> = <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i,j</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i,j</span></sub><sup><span class="cmmi-8">T</span></sup> <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i,j</span></sub> = <span class="cmmi-10x-x-109">I</span><span class="cmss-10x-x-109">. That is, their inverse is their transpose.</span></p>
<p><span class="cmss-10x-x-109">Multiplication with a transposition matrix has the same effect: it switches rows from the left and columns from the right. To be precise,</span></p>
<ol>
<li><span id="x1-138011x1"><span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i,j</span></sub><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">switches the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th </span><span class="cmssi-10x-x-109">rows </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-138013x2"><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">AP</span><sub><span class="cmmi-8">i,j</span></sub> <span class="cmss-10x-x-109">switches the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th </span><span class="cmssi-10x-x-109">column</span><span class="cmss-10x-x-109">s of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Most importantly, the similarity transformation</span></p>
<img src="../media/file830.png" class="math-display" alt="Pi,jAPi,j " width="75"/>
<p><span class="cmss-10x-x-109">relabels the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th nodes of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">’s digraph, leaving the graph structure invariant.</span></p>
<p><span class="cmss-10x-x-109">Now, about the aforementioned permutation matrices. A permutation matrix is simply a product of transposition matrices:</span></p>
<div class="math-display">
<img src="../media/file831.png" class="math-display" alt="P = Pi1,i2Pi3,i4 ...Pi2k−1,i2k. "/>
</div>
<p><span class="cmss-10x-x-109">Permutation matrices</span> <span id="dx1-138014"></span><span class="cmss-10x-x-109">inherit some properties from their building blocks. Most importantly,</span></p>
<ol>
<li><span id="x1-138016x1"><span class="cmss-10x-x-109">their inverse is their transpose,</span></span></li>
<li><span id="x1-138018x2"><span class="cmss-10x-x-109">and a similarity transformation with them is just a relabeling of nodes that leave the graph structure invariant.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">To see this latter one, consider that</span></p>
<img src="../media/file832.png" class="math-display" alt="P TAP = (Pi1,i2Pi3,i4 ...Pi2k−1,i2k)TA (Pi1,i2Pi3,i4 ...Pi2k− 1,i2k) = (Pi2k− 1,i2k ...(Pi1,i2APi1,i2)...Pi2k−1,i2k), " width="450"/>=
<p><span class="cmss-10x-x-109">succesively relabeling the nodes. (Recall that transposing a matrix product switches up</span> <span id="dx1-138019"></span><span class="cmss-10x-x-109">the order, and transposition matrices are their own transposes.) Conversely, every node relabeling is equivalent to a similarity transformation with a well-constructed permutation matrix.</span></p>
<p><span class="cmss-10x-x-109">Why are we talking about this? Because the proper labeling of nodes is key to the Frobenius normal form.</span></p>
</section>
<section id="directed-graphs-and-their-strongly-connected-components" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_119"><span class="titlemark"><span class="cmss-10x-x-109">8.3.2 </span></span> <span id="x1-1390009.3.2"></span><span class="cmss-10x-x-109">Directed graphs and their strongly connected components</span></h3>
<p><span class="cmss-10x-x-109">Now, let’s talk</span> <span id="dx1-139001"></span><span class="cmss-10x-x-109">about graphs. We’ll see how</span> <span id="dx1-139002"></span><span class="cmss-10x-x-109">every digraph decomposes into strongly connected components. Let’s see a concrete example:</span></p>
<div class="minipage">
<p><img src="../media/file833.png" width="170" alt="PIC"/> <span id="x1-139003r10"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.10: A directed graph (that is complex enough for us to study)</span> </span>
</div>
<p><span class="cmss-10x-x-109">This’ll be our textbook example. How many nodes can be reached from a given node? Not necessarily all. Say, for the point highlighted in </span><span class="cmssi-10x-x-109">Figure 8.11</span><span class="cmss-10x-x-109">, only a portion of the graph is accessible.</span></p>
<div class="minipage">
<p><img src="../media/file834.png" width="284" alt="PIC"/> <span id="x1-139004r11"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.11: Downstream nodes for a single starting point</span> </span>
</div>
<p><span class="cmss-10x-x-109">However, the set</span><span id="dx1-139005"></span> <span class="cmss-10x-x-109">of mutually reachable</span> <span id="dx1-139006"></span><span class="cmss-10x-x-109">nodes is much smaller: </span><span class="cmssi-10x-x-109">Figure 8.12 </span><span class="cmss-10x-x-109">shows that in our example, it consists of only two points.</span></p>
<div class="minipage">
<p><img src="../media/file835.png" width="284" alt="PIC"/> <span id="x1-139007r12"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.12: Mutually reachable nodes</span> </span>
</div>
<p><span class="cmss-10x-x-109">Algebraically speaking, “</span><img src="../media/file836.png" class="math" alt="a "/> <span class="cmss-10x-x-109">and </span><img src="../media/file837.png" class="math" alt="b "/> <span class="cmss-10x-x-109">are mutually reachable from each other“ is a special relation that partitions the set of nodes into disjoint subsets such that</span></p>
<ol>
<li><span id="x1-139009x1"><span class="cmss-10x-x-109">two nodes from the same subset are mutually reachable from each other,</span></span></li>
<li><span id="x1-139011x2"><span class="cmss-10x-x-109">and two nodes from different subsets are not mutually reachable.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">The subsets of this partition are called the strongly connected components, and we can always decompose a directed graph in this way.</span></p>
<div class="minipage">
<p><img src="../media/file838.png" width="398" alt="PIC"/> <span id="x1-139012r13"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.13: Strongly connected components of our example graph</span> </span>
</div>
<p><span class="cmss-10x-x-109">Now, let’s connect</span> <span id="dx1-139013"></span><span class="cmss-10x-x-109">everything together (not in a graph way but, you</span><span id="dx1-139014"></span> <span class="cmss-10x-x-109">know, in a wholesome mathematical one)!</span></p>
</section>
<section id="putting-graphs-and-permutation-matrices-together" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_120"><span class="titlemark"><span class="cmss-10x-x-109">8.3.3 </span></span> <span id="x1-1400009.3.3"></span><span class="cmss-10x-x-109">Putting graphs and permutation matrices together</span></h3>
<p><span class="cmss-10x-x-109">We are two steps</span> <span id="dx1-140001"></span><span class="cmss-10x-x-109">away from proving that every nonnegative square matrix can be transformed into the Frobenius normal form with a permutation matrix. Here is the plan.</span></p>
<ol>
<li><span id="x1-140003x1"><span class="cmss-10x-x-109">Construct the graph for our nonnegative matrix.</span></span></li>
<li><span id="x1-140005x2"><span class="cmss-10x-x-109">Find the strongly connected components.</span></span></li>
<li><span id="x1-140007x3"><span class="cmss-10x-x-109">Relabel the nodes in a clever way.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">And that’s it! Why? Because, as we have seen, relabeling is the same as a similarity transform with a permutation matrix. There’s just one tiny snag: what is the clever way? I’ll show you.</span></p>
<p><span class="cmss-10x-x-109">First, we “skeletonize” the graph: merge the components together, as well as any edges between them.</span></p>
<p><span class="cmss-10x-x-109">Consider each component as a black box: we don’t care what’s inside, only about their external connections.</span></p>
<div class="minipage">
<p><img src="../media/file839.png" width="541" alt="PIC"/> <span id="x1-140008r14"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.14: Strongly connected components</span> </span>
</div>
<p><span class="cmss-10x-x-109">In this skeleton, we</span> <span id="dx1-140009"></span><span class="cmss-10x-x-109">can find components that cannot be entered from other components. These will be our starting points, the zeroth-class components. In our example, we only have one.</span></p>
<div class="minipage">
<p><img src="../media/file840.png" width="184" alt="PIC"/> <span id="x1-140010r15"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.15: Finding the ”zeroth” component</span> </span>
</div>
<p><span class="cmss-10x-x-109">Now, things get a bit tricky. We number each component by the longest path from the farthest zero-class component from which it can be reached.</span></p>
<p><span class="cmss-10x-x-109">This is hard to even read, let alone understand. </span><span class="cmssi-10x-x-109">Figure 8.16 </span><span class="cmss-10x-x-109">illustrates the process.</span></p>
<div class="minipage">
<p><img src="../media/file841.png" width="600" alt="PIC"/> <span id="x1-140011r16"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.16: Numbering the components</span> </span>
</div>
<p><span class="cmss-10x-x-109">The gist is</span> <span id="dx1-140012"></span><span class="cmss-10x-x-109">that if you can reach an </span><span class="cmmi-10x-x-109">m</span><span class="cmss-10x-x-109">-th class from an </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-th class, then </span><span class="cmmi-10x-x-109">n &lt;m</span><span class="cmss-10x-x-109">. In the end, we have something like </span><span class="cmssi-10x-x-109">Figure 8.17</span><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file842.png" width="342" alt="PIC"/> <span id="x1-140013r17"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.17: Numbered components</span> </span>
</div>
<p><span class="cmss-10x-x-109">This defines an ordering on the components (a partial ordering, if you would like to be precise).</span></p>
<p><span class="cmss-10x-x-109">Now, we label the nodes inside such that</span></p>
<ol>
<li><span id="x1-140015x1"><span class="cmss-10x-x-109">higher-order classes come first,</span></span></li>
<li><span id="x1-140017x2"><span class="cmss-10x-x-109">and consecutive indices are labeling nodes from the same component if possible.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">This is how it goes.</span></p>
<div class="minipage">
<p><img src="../media/file843.png" width="527" alt="PIC"/> <span id="x1-140018r18"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 8.18: Labeling the nodes</span> </span>
</div>
<p><span class="cmss-10x-x-109">Here is the matrix in</span> <span id="dx1-140019"></span><span class="cmss-10x-x-109">this particular example, with zeros and ones for simplicity:</span></p>
<div class="math-display">
<img src="../media/file844.png" class="math-display" alt="⌊ | | | | ⌋ 0 1 1 |1 0 0 0 |0 0 | 0 |0 0 0 || | | | | || || 0 0 1 |0 0 1 0 |0 0 | 1 |0 0 0 || || 1 0 0 |0 0 0 0 |1 0 | 0 |0 0 0 || | ---------|-----------|----|---|-------| || 0 0 0 |0 1 0 1 |0 0 | 0 |0 0 0 || || 0 0 0 |0 0 1 0 |0 0 | 0 |0 0 0 || || | | | | || || 0 0 0 |1 0 0 0 |0 0 | 0 |0 0 0 || || 0 0 0 |0 1 1 0 |0 0 | 0 |0 0 0 || | ---------|-----------|----|---|-------| || 0 0 0 |0 0 0 0 |0 1 | 1 |0 0 0 || || 0 0 0 |0 0 0 0 |1 0 | 0 |0 0 0 || || ---------|-----------|----|---|-------|| || -0--0--0-|0--0--0--0-|0-0-|-0-|1-0--0-|| || 0 0 0 |0 0 0 0 |0 0 | 0 |0 1 0 || | | | | | | || 0 0 0 |0 0 0 0 |0 0 | 0 |0 0 1 || || 0 0 0 |0 0 0 0 |0 0 | 0 |1 0 0 || ⌈ ⌉ "/>
</div>
<p><span class="cmss-10x-x-109">With that, the ideas behind the proof of </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch014.xhtml#x1-137003r54"><span class="cmssi-10x-x-109">54</span></a> <span class="cmss-10x-x-109">are clear! Now, we also finally understand why irreducible matrices are called </span><span class="cmssi-10x-x-109">irreducible</span><span class="cmss-10x-x-109">: as they describe strongly connected graphs, they cannot be further decomposed into smaller blocks in a meaningful way.</span></p>
</section>
</section>
<section id="summary7" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_121"><span class="titlemark"><span class="cmss-10x-x-109">8.4 </span></span> <span id="x1-1410009.4"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">With the study of the connections between linear algebra and graph theory, our journey through linear algebra is over.</span></p>
<p><span class="cmss-10x-x-109">In this and the previous seven chapters, we have learned that vectors and matrices are not merely data structures that store observations and measurements. Vectors and matrices possess a rich and beautiful geometric structure, describing data </span><span class="cmssi-10x-x-109">and </span><span class="cmss-10x-x-109">their transformations at the same time!</span></p>
<p><span class="cmss-10x-x-109">First, we learned that vectors live in so-called </span><span class="cmssi-10x-x-109">vector spaces</span><span class="cmss-10x-x-109">, the high-dimensional generalizations of the three-dimensional space we are living in (which might be 26-dimensional, according to some string theorists, but let’s stick to the Earth for now). We can measure lengths and distances via norms, most often defined by</span></p>
<div class="math-display">
<img src="../media/file845.png" class="math-display" alt=" ∑n ∥x ∥ = x2, x ∈ ℝn, i=1 i "/>
</div>
<p><span class="cmss-10x-x-109">or measure angles (among others) via inner products, most often defined by</span></p>
<div class="math-display">
<img src="../media/file846.png" class="math-display" alt=" ∑n ⟨x, y⟩ = xiyi i=1 = cos(α)∥x ∥∥y∥. "/>
</div>
<p><span class="cmss-10x-x-109">From a mathematical perspective, matrices originate from the linear transformation of vector spaces, i.e., functions of the form </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">U </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">, satisfying the linearity relation </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">a</span><span class="cmbx-10x-x-109">x </span>+ <span class="cmmi-10x-x-109">b</span><span class="cmbx-10x-x-109">y</span>) = <span class="cmmi-10x-x-109">af</span>(<span class="cmbx-10x-x-109">x</span>) + <span class="cmmi-10x-x-109">bf</span>(<span class="cmbx-10x-x-109">y</span>)<span class="cmss-10x-x-109">. Matrices arise from the algebraic representation of linear transformations by expressing them in the form</span></p>
<div class="math-display">
<img src="../media/file847.png" class="math-display" alt=" ⌊ ⌋ ⌊ ∑n ⌋ ⌊a a ... a ⌋| x1| | i=1 a1,ixi| | 1,1 1,2 1,m ||| x2|| || ∑n a2,ixi|| ||a2,1 a2,2 ... a2,m ||| .| | i=1. | f(x) = Af x = || .. .. .. .. |||| ..|| = || .. || , ⌈ . . . . ⌉|| x || || ∑n a x || an,1 an,2 ... an,m, ⌈ n⌉ ⌈ i=1 n,i i⌉ "/>
</div>
<p><span class="cmss-10x-x-109">allowing us to reason about data transformations from a geometric perspective. This is an extremely powerful tool in machine learning. Think about it: </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can be a regression model, a layer in a neural network, or various other machine learning building blocks. Ultimately, this is why we want to study vector spaces: the data lives there, and data transformations are described by matrices.</span></p>
<p><span class="cmss-10x-x-109">However, building a model doesn’t stop at linear algebra. To capture more complex patterns, we need </span><span class="cmssi-10x-x-109">nonlinearities</span><span class="cmss-10x-x-109">. For instance, consider the famous Sigmoid function, defined by</span></p>
<div class="math-display">
<img src="../media/file848.png" class="math-display" alt="σ(x) = ---1---. 1+ e− x "/>
</div>
<p><span class="cmss-10x-x-109">The transformation defined by </span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">(where </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">is applied elementwise) is a simple logistic regression model, allowing us to perform binary classification on our multidimensional feature space. Iterating on this idea, the expression</span></p>
<div class="math-display">
<img src="../media/file849.png" class="math-display" alt="N (x) = σ(B σ(Ax)) "/>
</div>
<p><span class="cmss-10x-x-109">defines a two-layer neural network.</span></p>
<p><span class="cmss-10x-x-109">So, the next part of our journey is into the domain of </span><span class="cmssi-10x-x-109">calculus</span><span class="cmss-10x-x-109">, where we’ll learn what functions really are, how we build predictive models from them, and how we fit these models by tuning the parameters with gradient descent.</span></p>
<p><span class="cmss-10x-x-109">Let’s go!</span></p>
</section>
<section id="problems7" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_122"><span class="titlemark"><span class="cmss-10x-x-109">8.5 </span></span> <span id="x1-1420009.5"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">G </span>= (<span class="cmmi-10x-x-109">V,E</span>) <span class="cmss-10x-x-109">be a directed graph and let </span><span class="cmmi-10x-x-109">u,v </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">V </span><span class="cmss-10x-x-109">be two of its nodes. Show that if there exists a walk from </span><span class="cmmi-10x-x-109">u </span><span class="cmss-10x-x-109">to </span><span class="cmmi-10x-x-109">v</span><span class="cmss-10x-x-109">, then there exists a walk without repeated edges and repeated vertices.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">G </span>= (<span class="cmmi-10x-x-109">V,E</span>) <span class="cmss-10x-x-109">be a strongly connected directed graph. Show that </span><span class="cmmi-10x-x-109">jEj </span><span class="cmsy-10x-x-109">≥</span><span class="cmmi-10x-x-109">jV j</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">jSj </span><span class="cmss-10x-x-109">denotes the number of elements in the set </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">. (In other words, show that in order to be strongly connected, </span><span class="cmmi-10x-x-109">G </span><span class="cmss-10x-x-109">must have at least as many edges as nodes.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">be an irreducible matrix. Is </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">also reducible? (If yes, prove it. If no, show a counterexample.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">4</span><span class="cmsy-8">×</span><span class="cmr-8">4</span></sup> <span class="cmss-10x-x-109">be the matrix defined by</span></p>
<div class="math-display">
<img src="../media/file850.png" class="math-display" alt=" ⌊ ⌋ | 0 0 1 0| | 0 0 0 1| A = || || . |⌈ 1 0 0 0|⌉ 0 1 0 0 "/>
</div>
<p><span class="cmss-10x-x-109">Find the permutation matrix </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">that transforms </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">to a Frobenius normal form!</span></p>
</section>
<section id="join-our-community-on-discord8" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_123"><span id="x1-143000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>