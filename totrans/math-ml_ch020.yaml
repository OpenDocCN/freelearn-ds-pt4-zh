- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differentiation
  prefs: []
  type: TYPE_NORMAL
- en: I turn with terror and horror from this lamentable scourge of continuous functions
    with no derivatives.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Charles Hermite
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the history of science, a few milestones are as significant as inventing
    the wheel. Even among these, differentiation is a highlight. With the invention
    of calculus, Newton created mechanics as we know it. Differentiation is all over
    science and engineering, and as it turns out, it’s a key component of machine
    learning as well.
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because of optimization! As it turns out, the extremal points of a function
    can be characterized in terms of their derivative, and these extremal points can
    be iteratively found via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll learn what differentiation is, what its origins are,
    and most importantly, how to use it in practice. Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Differentiation in theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of jumping straight into the mathematical definition, let’s start our
    discussion with a straightforward example: a point-like object moving along a
    straight line. Its movement is fully described by the time-distance plot (Figure [12.1](#)),
    which shows its distance from the starting point at a given time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1145.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Time-distance plot of our moving object'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to calculate the object’s velocity at a given time. In high school,
    we learned that
  prefs: []
  type: TYPE_NORMAL
- en: '![ distance average velocity = -time---. ](img/file1146.png)'
  prefs: []
  type: TYPE_IMG
- en: To put this into a quantitative form, if f(t) denotes the time-distance function,
    and t[1]/span>t[2] are two arbitrary points in time, then
  prefs: []
  type: TYPE_NORMAL
- en: '![average velocity between t and t = f-(t2)−--f(t1). 1 2 t2 − t1 ](img/file1147.png)'
  prefs: []
  type: TYPE_IMG
- en: Expressions like ![f(t2t2)−−ft(1t1)-](img/file1148.png) are called differential
    quotients. Note that if the object moves backwards, the average velocity is negative.
    (As opposed to speed, which is always positive. Velocity is speed and direction.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The average velocity has a simple geometric interpretation: if you replace
    the object’s motion with a constant velocity motion moving with that average,
    you’ll end up at exactly the same place. In graphical terms, this is equivalent
    of connecting (t[1],f(t[1])) and (t[2],f(t[2])) with a single line.'
  prefs: []
  type: TYPE_NORMAL
- en: The average velocity is just the slope of this line. This is visualized by Figure [12.2](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1149.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Average velocity between t[1] and t[2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this, we can calculate the exact velocity at a single time point t, which
    we’ll denote with v(t). The idea is simple: the average speed in the small time-interval
    between t and t + Δt should get closer and closer to v(t) if Δt is small enough.
    (Δt can be negative as well.)'
  prefs: []
  type: TYPE_NORMAL
- en: So,
  prefs: []
  type: TYPE_NORMAL
- en: v(t) = lim[Δt → 0] (f(t + Δt) - f(t)) / Δt, (12.1)
  prefs: []
  type: TYPE_NORMAL
- en: if the above limit exists. Figure [12.3](#) illustrates the limit defined by
    ([12.1](#)). There, we can see that as Δt gets closer and closer to 0, the slope
    of the line connecting (t,f(t)) to (t + Δt,f(t + Δt)) gets closer and closer to
    the slope of the tangent.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1151.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Approximating the speed at t'
  prefs: []
  type: TYPE_NORMAL
- en: Following our geometric intuition, we see that v(t) is simply the slope of the
    tangent line of f at t. Keeping this in mind, we are ready to introduce the formal
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 54\. (Differentiability)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ →ℝ be an arbitrary function. We say that f is differentiable at x[0]
    ∈ℝ if the limit'
  prefs: []
  type: TYPE_NORMAL
- en: '![-df-(x0) = lim f(x)-−-f(x0) dx x→x0 x − x0 ](img/file1152.png)'
  prefs: []
  type: TYPE_IMG
- en: exists. If so, this is called the derivative of f at x[0].
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if f describes a time-distance function of a moving object,
    then the derivative is simply its velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to continuity, differentiability is a local property. However, we’ll
    be more interested in functions that are differentiable (almost) everywhere. In
    those cases, the derivative is a function, often denoted with f^′(x).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it is confusing that x can denote the variable of f and the exact
    point where the derivative is taken. Here is a quick glossary of terms to clarify
    the difference between derivative and derivative function.
  prefs: []
  type: TYPE_NORMAL
- en: '![df- dx](img/file1153.png)(x[0]): derivative of f with respect to the variable
    x at the point x[0]. This is a scalar, also denoted with f^′(x[0]).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![df- dx](img/file1154.png): derivative function of f with respect to the variable
    x. This is a function, also denoted with f^′.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remark 9\. (Variables in the limit)
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t let the change in notation from t and t + Δt to x[0] and x confuse you;
    the limit that defines the derivative means exactly the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ f(x0 + h)− f(x0) f(x)− f(x0) lhim→0 -------h---------= xli→mx0 ---x−--x----.
    0 ](img/file1155.png)'
  prefs: []
  type: TYPE_IMG
- en: Also note that
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f(x)−-f(x0)-= lim f(x0)−--f(x). x→x0 x− x0 x→x0 x0 − x ](img/file1156.png)'
  prefs: []
  type: TYPE_IMG
- en: On occasion, we might even use x and y instead of x[0] and x, writing
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ f-(x-)−-f(y) f (x) = yli→mx x − y ](img/file1157.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ll use whichever is more convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some examples!
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. f(x) = x. For any x, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f(x)−-f(y)-= lim x-−-y = 1\. y→x x − y y→x x − y ](img/file1158.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, f(x) = x is differentiable everywhere and its derivative is the constant
    function f^′(x) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. f(x) = x². Here, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ f(x)− f (y ) x2 − y2 lim -----------= lim ------- y→x x− y y→x x − y (x-−-y)(x+-y)
    = yli→mx x − y = yli→mx x + y = 2x. ](img/file1159.png)'
  prefs: []
  type: TYPE_IMG
- en: So, f(x) = x² is differentiable everywhere and f^′(x) = 2x. Later, when talking
    about elementary functions, we’ll see the general case f(x) = x^k.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. f(x) = jxj at x = 0\. For this, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f(0)−-f-(y-)= lim |y|. y→0 0− y y→0 y ](img/file1160.png)'
  prefs: []
  type: TYPE_IMG
- en: Since
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 1 if y >0, |y|-= y |( − 1 if y <0, ](img/file1161.png)'
  prefs: []
  type: TYPE_IMG
- en: this limit does not exist, as it is illustrated on Figure [12.4](#). This is
    our first example of a non-differentiable function. However, jxj is differentiable
    everywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth drawing a picture here to enhance our understanding of differentiability.
    Recall that the value of the derivative at a given point equals the slope of the
    tangent line to the function’s graph.
  prefs: []
  type: TYPE_NORMAL
- en: Since jxj has a sharp corner at 0, the tangent line is not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1162.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Tangent planes of f(x) = jxj at 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Differentiability means no sharp corners in the graph, so differentiable functions
    are often called smooth. This is one reason we prefer differentiable functions:
    the rate of change is tractable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll see an equivalent definition of differentiability, involving local
    approximation with a linear function. From this perspective, differentiability
    means manageable behavior: no wrinkles, corners, or sharp changes in value.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 Equivalent forms of differentiation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To really understand derivatives and differentiation, we are going to take
    a look at it from another point of view: local linear approximations.'
  prefs: []
  type: TYPE_NORMAL
- en: Approximation is a very natural idea in mathematics. Say, have you ever thought
    about what happens when you punch sin(2.18) into a calculator? We cannot express
    the function sin with finitely many additions and multiplications, so we have
    to approximate it. In practice, we use functions of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![p(x) = p0 + p1x+ ⋅⋅⋅+ pnxn, ](img/file1163.png)'
  prefs: []
  type: TYPE_IMG
- en: which, can be evaluated easily. These are called polynomials, and they are just
    a finite combination of additions and multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: Can we just replace functions with polynomials to make computations easier?
    (Even at the cost of perfect precision.)
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that we can, and differentiation is one way to do so. In essence,
    the derivative desribes the best local approximation with a linear function.
  prefs: []
  type: TYPE_NORMAL
- en: The following theorem makes this clear.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 77\. (Differentiation as a local linear approximation)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ →ℝ be an arbitrary function. The following are equivalent.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) f is differentiable at x[0].
  prefs: []
  type: TYPE_NORMAL
- en: (b) there is an α ∈ℝ such that
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = f(x₀) + α(x − x₀) + o(|x − x₀|) *as* x → x₀. (12.2)
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the small O notation (see Definition [52](ch019.xhtml#x1-190013r52))
    means that the function is an order of magnitude smaller around x[0] than the
    function |x −x[0]|.
  prefs: []
  type: TYPE_NORMAL
- en: If exists, the α in the above theorem is the derivative f^′(x[0]). In other
    words, f(x) can be locally written as
  prefs: []
  type: TYPE_NORMAL
- en: f(x[0]) + f′(x[0])(x − x[0]) + o(|x − x[0]|). (12.3)
  prefs: []
  type: TYPE_NORMAL
- en: Proof. To show the equivalence of two statements, we have to prove that differentiation
    implies the desired property and vice versa. Although this might seem complicated,
    it is straightforward and entirely depends on how functions can be written as
    their limit plus an error term (Theorem [71](ch019.xhtml#x1-191002r71)).
  prefs: []
  type: TYPE_NORMAL
- en: (a) ⇒ (b). The existence of the limit
  prefs: []
  type: TYPE_NORMAL
- en: '![ f-(x)−-f(x0) ′ lxi→mx0 x − x0 = f (x0) ](img/file1165.png)'
  prefs: []
  type: TYPE_IMG
- en: implies that we can write the slope of the approximating tangent in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) − f(x0) ′ ---x-−-x----= f(x0) + error(x), 0 ](img/file1166.png)'
  prefs: []
  type: TYPE_IMG
- en: where lim[x→x[0]]error(x) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: With some simple algebra, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = f (x0 )+ f′(x0)(x − x0)+ error(x)(x − x0). ](img/file1167.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the error term tends to zero as x goes to x[0], error(x)(x −x[0]) = o(|x
    −x[0]|), which is what we wanted to show.
  prefs: []
  type: TYPE_NORMAL
- en: (b) ⇒ (a). Now, repeat what we did in the previous part, just in reverse order.
    We can rewrite
  prefs: []
  type: TYPE_NORMAL
- en: '![f (x) = f (x0) + α(x − x0)+ o(|x− x0|) ](img/file1169.png)'
  prefs: []
  type: TYPE_IMG
- en: in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x)−-f-(x0) x− x = α + o(1), 0 ](img/file1170.png)'
  prefs: []
  type: TYPE_IMG
- en: which, according to what we have used before, implies that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](img/file1171.png)'
  prefs: []
  type: TYPE_IMG
- en: So, f is differentiable at x[0] and its derivative is f^′(x[0]) = α.
  prefs: []
  type: TYPE_NORMAL
- en: One huge advantage of this form is that it will be easily generalized to multivariate
    functions. Even though we are far from it, we can get a glimpse. Multivariate
    functions map vectors to scalars, so the ratio
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x)-−-f(x0), x,x0 ∈ ℝn x − x0 ](img/file1172.png)'
  prefs: []
  type: TYPE_IMG
- en: is not even defined. (Since we can’t divide with a vector.) However, the expression
  prefs: []
  type: TYPE_NORMAL
- en: '![f (x ) = f (x0 )+ ∇f (x0)T(x− x0)+ o(∥x − x0∥) ](img/file1173.png)'
  prefs: []
  type: TYPE_IMG
- en: makes perfect sense, since ∇f(x[0])^T (x −x[0]) is a scalar. Here, ∇f(x[0])
    denotes the gradient of f, that is, the multivariable version of derivatives.
    ∇f(x[0]) is an n-dimensional vector. Don’t worry if you are not familiar with
    this notation, we’ll cover everything in due time. The take-home message is that
    this alternative definition will be more convenient for us in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of the locally-best-approximation mindset is that Theorem [77](ch020.xhtml#x1-199002r77)
    can be generalized to higher derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 78\. (Taylor’s theorem)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ →ℝ function that is n times differentiable at x[0]. Then'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n f(k)(x0) f (x ) = -------(x − x0)k + o(|x− x0|n) k=0 k! ](img/file1174.png)'
  prefs: []
  type: TYPE_IMG
- en: holds, where f^((k))(x[0]) denotes the k-th derivative of f at x[0].
  prefs: []
  type: TYPE_NORMAL
- en: (Note that the zeroth derivative f^((0)) equals to f.)
  prefs: []
  type: TYPE_NORMAL
- en: In other words, Theorem [78](ch020.xhtml#x1-199005r78) says that if f is differentiable
    enough times, it can be written as a polynomial plus a small error term. For infinitely
    differentiable functions, Taylor’s theorem gives rise to the famous Taylor expansion.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 55\. (Taylor expansion)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ → ℝ be a function that is differentiable at x[0] infinitely many
    times. The series defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ (k) f(x) ∼ ∑ f---(x0)(x− x )k k! 0 k=0 ](img/file1175.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the Taylor expansion of f around x[0].
  prefs: []
  type: TYPE_NORMAL
- en: To give you an example, as (e^x)^′ = e^x, the Taylor expansion of e^x around
    0 is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ ex = ∑ 1-xk. k! k=0 ](img/file1176.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the equality sign is not an accident: the Taylor expansion of e^x
    equals e^x! (This is not always the case.) Now we see why e = ∑ [k=0]^∞![1k!](img/file1177.png),
    as we hinted earlier when discussing sequences and series.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words,
  prefs: []
  type: TYPE_NORMAL
- en: '![ x ∑n 1 k e ≈ k!x , k=0 ](img/file1178.png)'
  prefs: []
  type: TYPE_IMG
- en: meaning that on any interval [ −α,α] and for any arbitrarily small 𝜀/span>0,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n |ex − -1xk| <𝜀 k=0k! ](img/file1179.png)'
  prefs: []
  type: TYPE_IMG
- en: holds if n is large enough. In practice, this polynomial is evaluated to approximate
    the value of e^x.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Differentiation and continuity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the following theorem states, differentiation is a more strict condition
    than continuity.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 79\.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiable functions are continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'If f : ℝ →ℝ is differentiable at a, it is also continuous there.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof. We’ll use Theorem [77](ch020.xhtml#x1-199002r77) to prove the result.
    With the general form ([12.2](ch020.xhtml#x1-199002r77)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ lim f (x) = lim (f(x )+ f′(x )(x− x ) + o(|x − x |)) = f(x ), x→x0 x→x0
    0 0 0 0 0 ](img/file1180.png)'
  prefs: []
  type: TYPE_IMG
- en: which shows the continuity of f at x[0].
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the previous theorem is not true the other way around: a function
    can be continuous, but not differentiable. (As the example f(x) = jxj at x = 0
    shows.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be taken to the extremes: there are functions that are continuous
    everywhere but differentiable nowhere. One of the first examples was provided
    by Weierstrass (from the [Bolzano-Weierstrass theorem](#)). The function itself
    is defined by the infinite sum'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑∞ W (x) = ancos(bnπx), n=0 ](img/file1181.png)'
  prefs: []
  type: TYPE_IMG
- en: where a ∈ (0,1), b is a positive odd integer, and ab/span>1 + 3π∕2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'I agree, this definition feels totally random, and you are probably wondering:
    how did the author come up with it? To get a grip on this function, imagine this
    as the superposition of cosine waves with smaller and smaller amplitude but higher
    and higher frequency. Remember that differentiation implies “no sharp corners”?
    This definition puts a sharp corner at every point on the real line.'
  prefs: []
  type: TYPE_NORMAL
- en: Its graph is a fractal curve with self-similarity, as illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Graph of the Weierstrass function. Source: [https://en.wikipedia.org/wiki/Weierstrass_function](https://en.wikipedia.org/wiki/Weierstrass_function)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples such as this inspired the opening quote of the section:'
  prefs: []
  type: TYPE_NORMAL
- en: I turn with terror and horror from this lamentable scourge of continuous functions
    with no derivatives. — Charles Hermite
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 19th-century mathematicians certainly did not think much about nondifferentiable
    functions. However, there are much more of them than differentiable ones. We won’t
    go into the details, but amongst all continuous functions, the set of ones that
    are differentiable at at least one point is meagre. Meagre is a proper technical
    term for sets, and although we don’t need to know what it means exactly, its name
    implies that it is extremely small.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what the derivative is, it’s time to put theory into
    practice. How do we compute derivatives, and how do we work with them in machine
    learning? We’ll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Differentiation in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During our first encounter with differentiation, we saw that computing derivatives
    by the definition
  prefs: []
  type: TYPE_NORMAL
- en: '![f′(x0) = lim f(x0)−-f-(x-) x→x0 x0 − x ](img/file1183.png)'
  prefs: []
  type: TYPE_IMG
- en: can be really hard in practice if we encounter convoluted functions such as
    f(x) = cos(x)sin(e^x). Similar to convergent sequences and limits, using the definition
    of differentiation won’t get us far—the complexity piles on fast. So, we have
    to find ways to decompose the complexity into its fundamental building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Rules of differentiation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll look at the simplest of operations: scalar multiplication, addition,
    multiplication, and division.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 80\. (Rules of differentiation)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ → ℝ and g : ℝ → ℝ be two arbitrary functions and let x ∈ ℝ. Suppose
    that both f and g is differentiable at x. Then'
  prefs: []
  type: TYPE_NORMAL
- en: (a) (cf)^′(x) = cf^′(x) for all c ∈ℝ,
  prefs: []
  type: TYPE_NORMAL
- en: (b) (f + g)^′(x) = f^′(x) + g^′(x),
  prefs: []
  type: TYPE_NORMAL
- en: (c) (fg)^′(x) = f^′(x)g(x) + f(x)g^′(x) (the product rule),
  prefs: []
  type: TYPE_NORMAL
- en: (d) (![f g](img/file1185.png))^′(x) = ![f′(x)g(x)−-f(x)g′(x) g(x)2](img/file1187.png)
    if g(x)≠0 (the quotient rule).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (a) and (b) is a direct consequence of the Theorem [70](ch019.xhtml#x1-190011r70).
  prefs: []
  type: TYPE_NORMAL
- en: 'To show (c), we have to do a bit of algebra:'
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f(x)g(x)−-f-(y)g(y)-= lim f(x)g(x)−--f(y)g-(x-)+-f(y)g(x)−-f-(y)g(y)-
    y→x x − y x→y x − y f(x)g(x)− f(y)g (x ) f(y)g(x)− f (y)g (y) = lyim→x -------x−--y-------+
    lxi→my -------x−-y-------- = lim [f(x)−-f(y)g(x)]+ f (y ) lim g(x)−-g(y)- y→x
    x − y x→y x − y = f′(x)g(x)+ f(x)g′(x), ](img/file1188.png)'
  prefs: []
  type: TYPE_IMG
- en: from which (c) follows.
  prefs: []
  type: TYPE_NORMAL
- en: For (d), we are going to start with the special case of (1∕g)^′. We have
  prefs: []
  type: TYPE_NORMAL
- en: '![ g1(x) − g1(y) 1 g(y)− g (x ) lyim→x --x-−-y---= lyim→x g(x)g(y)---x−--y---
    ′ = − g-(x-), g(x)2 ](img/file1189.png)'
  prefs: []
  type: TYPE_IMG
- en: from which the general case follows by applying (c) to f and 1∕g.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one operation which we haven’t covered in the previous theorem: function
    composition. In the study of neural networks, composition plays an essential role.
    Each layer can be thought of as a function, which are composed together to form
    the entire network.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 81\. (Chain rule/Leibniz rule)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ→ℝ and g : ℝ→ℝ be two arbitrary functions and let x ∈ ℝ. Suppose that
    g is differentiable at x and f is differentiable at g(x). Then'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ ′ ′ (f ∘g) (x) = f (g(x))g(x) ](img/file1192.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof. First, we rewrite the differential quotient into the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f(g(x))−-f(g(y)) = lim f(g(x))−-f(g(y))g(x)-−-g(y) y→x x − y y→x g(x)−
    g(y) x − y f(g(x))− f(g(y)) g(x) − g(y) = lyim→x ---g(x)−-g(y)---xli→my ---x-−-y---.
    ](img/file1193.png)'
  prefs: []
  type: TYPE_IMG
- en: Because g is differentiable at x, it is also continuous there, so lim[y→x]g(y)
    = g(x). So, the first term can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f(g(x-))-−-f(g(y))= lim f(y)−-f(g(x))-= f′(g(x)). y→x g(x) − g(y) y→g
    (x) y − g(x) ](img/file1194.png)'
  prefs: []
  type: TYPE_IMG
- en: Since g is differentiable at x, the second term is g^′(x). Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![lim f-(g(x-))-−-f(g(y))= f ′(g(x))g′(x), y→x x − y ](img/file1195.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: As neural networks are just huge composed functions, their derivative is calculated
    with the repeated application of the chain rule. (Although the derivatives of
    its layers are vectors and matrices since they are multivariable functions.)
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Derivatives of elementary functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following the already familiar pattern, now we calculate the derivatives for
    the most important class: elementary functions. There are a few that we will encounter
    all the time, like in the mean squared error, cross-entropy, Kullback-Leibler
    divergence, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 82\. (Derivatives of elementary functions)
  prefs: []
  type: TYPE_NORMAL
- en: (a) (x⁰)^′ = 0 and (x^n)^′ = nx^(n−1), where n ∈ℤ ∖ 0,
  prefs: []
  type: TYPE_NORMAL
- en: (b) (sinx)^′ = cosx and (cosx)^′ = −sinx,
  prefs: []
  type: TYPE_NORMAL
- en: (c) (e^x)^′ = e^x,
  prefs: []
  type: TYPE_NORMAL
- en: (d) (log x)^′ = ![1 x](img/file1196.png).
  prefs: []
  type: TYPE_NORMAL
- en: You don’t necessarily have to know how to prove these. I’ll include the proof
    of (a), but feel free to skip it, especially if this is your first encounter with
    calculus. What you have to remember, though, are the derivatives themselves. (However,
    I’ll refer back to this part when necessary.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof. (a) It is easy to see that for n = 0, the derivative (x⁰)^′ = 0\. The
    case n = 1 is also simple: calculating the differential quotient shows that (x)^′
    = 1\. For the case n ≥ 2, we are going to employ a small trick. Writing out the
    differential quotient for f(x) = x^n, we obtain'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n n x--−-y- , x − y ](img/file1197.png)'
  prefs: []
  type: TYPE_IMG
- en: which we want to simplify. If you don’t have a lot of experience in math, it
    might seem like magic, but x^n −y^n can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![xn − yn = (x− y )(xn− 1 + xn− 2y + ⋅⋅⋅+ xyn− 2 + yn−1) n−1 ∑ n− 1− k k =
    (x− y ) x y . k=0 ](img/file1198.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be seen easily by calculating the product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n∑−1 n−1−k k n n−1 n−1 (x − y) x y .= x + [x y + ⋅⋅⋅+ xy ] k=0 − [xn− 1y
    + ⋅⋅⋅+ xyn− 1]− yn = xn − yn. ](img/file1199.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ xn-−-yn (x-−-y)--nk−=01xn−1−kyk- liy→mx x− y = xli→my x− y n∑−1 = lim
    xn−1−kyk y→x k=0 = nxn −1\. ](img/file1200.png)'
  prefs: []
  type: TYPE_IMG
- en: So, (x^n)^′ = nx^(n−1). With this and the rules of differentiation, we can calculate
    the derivative of any polynomial p(x) = ∑ [k=0]^np[k]x^k as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ ∑n k−1 p(x) = kpkx . k=1 ](img/file1201.png)'
  prefs: []
  type: TYPE_IMG
- en: The case n/span>0 follows from x^(−n) = 1∕x^n using the rules of differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: With these rules under our belt, we can calculate the derivatives for some of
    the most famous activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: The most classical one, the sigmoid function is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ---1--- σ(x) = 1+ e− x. ](img/file1202.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since it is an elementary function, it is differentiable everywhere. To calculate
    its derivative, we can use the quotient rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/file1203.png)(12.4)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have the sigmoid and its derivative, let’s plot them together!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1204.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Sigmoid and its derivative'
  prefs: []
  type: TYPE_NORMAL
- en: Another popular activation function is the ReLU, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{x if x >0, ReLU (x) = |(0 otherwise. ](img/file1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s plot its graph first!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1206.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Graph of the ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: By looking at it, we can suspect that it is not differentiable at 0\. Indeed,
    since
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ReLU (x) − ReLU (0) { 1 if x >0, ---------x--------- = | ( 0 if x <0,
    ](img/file1207.png)'
  prefs: []
  type: TYPE_IMG
- en: the limit of the differential quotient doesn’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: However, besides 0, it is differentiable and
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ′ {1 if x >0, ReLU (x) = | (0 if x <0\. ](img/file1208.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though ReLU is not differentiable at 0, this is not a problem in practice.
    When performing backpropagation, it is extremely unlikely that ReLU^′(x) will
    receive 0 as its input. Even if this is the case, the derivative can be artificially
    extended to zero by defining it as 0.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Higher-order derivatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One last thing to do before we move on is to talk about higher-order derivatives.
    Because derivatives are functions, it is a completely natural idea to calculate
    the derivative of derivatives. As we will see when studying the basics of optimization
    in Chapter 14, the second derivatives contain quite a lot of essential information
    regarding minima and maxima.
  prefs: []
  type: TYPE_NORMAL
- en: The n-th derivative of f is denoted with f^((n)), where f^((0)) = f. There are
    a few rules regarding them that are worth keeping in mind. Although, we have to
    note that a derivative function is not always differentiable, as the example
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( | { 0 if x <0, f (x) = |( 2 x otherwise ](img/file1209.png)'
  prefs: []
  type: TYPE_IMG
- en: shows. Now, about those rules regarding higher-order derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 83\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ →ℝ and g : ℝ →ℝ be two arbitrary functions.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) (f + g)^((n)) = f^((n)) + g^((n))
  prefs: []
  type: TYPE_NORMAL
- en: (b) (fg)^((n)) = ∑ [k=0]^n![(n) k](img/file1210.png)f^((n−k))g^((k))
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (a) trivially follows from the linearity of differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding (b), we are going to use proof by induction. For n = 1, the statement
    simply says that (fg)^′ = f^′g + fg^′, as we have seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we assume that it is true for n and deduce the n + 1 case. For this, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ( ) (fg)(n+1) = ((fg)(n))′ = n (f(n−k)g(k))′ k=0 k ∑n ( ) = n [f(n−k+1)g(k)
    + f(n−k)g(k+1)] k=0 k ∑n ( ) ∑n ( ) = n f(n− k+1)g (k) + n f(n−k)g(k+1) k=0 k
    k=0 k ( ) n ( ) n− 1( ) ( ) = n f (n+1)g + [∑ n f(n+1−k)g(k)]+ [∑ n f(n−k)g(k+1)]+
    n fg(n+1). 0 k k n k=1 k=0 ](img/file1211.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we note that ![( ) n 0](img/file1212.png) = ![( ) n+1 0](img/file1213.png)
    = 1 and ![( ) n n](img/file1214.png) = ![( ) n+1 n+1](img/file1215.png) = 1\.
    Second, the recursive relation for binomial coefficients says that
  prefs: []
  type: TYPE_NORMAL
- en: '![( ) ( ) ( ) n+ 1 = n + n . k k k − 1 ](img/file1216.png)'
  prefs: []
  type: TYPE_IMG
- en: With a simple reindexing, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![n−1( ) n ( ) ∑ n f(n− k)g(k+1) = ∑ n f(n+1−k)g (k), k k − 1 k=0 k=1 ](img/file1217.png)'
  prefs: []
  type: TYPE_IMG
- en: so we can join the two sums together and obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) n ( ) ( ) ( ) (fg)(n+1) = n + 1 f(n+1)g + ∑ [ n + n f(n+1−k)g(k)]+ n+
    1 fg(n+1) 0 k k − 1 n+ 1 ( ) k=1 n∑+1 n + 1 (n+1−k) (k) = k f g , k=0 ](img/file1218.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 Extending the Function base class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have several tools under our belt to calculate derivatives, it’s
    time to think about implementations. Since we have our own Function base class
    (Section [9.2.3](ch017.xhtml#function-base-class)), a natural idea is to implement
    the derivative as a method. This is a simple solution that is in line with object-oriented
    principles as well, so we should go for it!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To see a concrete example, let’s revisit the sigmoid function, whose derivative
    is given by ([12.4](ch020.xhtml)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ σ (x) = σ(x)(1− σ (x )). ](img/file1219.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Simple implementation, powerful functionality. Now that we have the derivatives
    covered, let’s move towards calculating the derivative of more complex functions!
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.5 The derivative of compositions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, I have probably emphasized the importance of function compositions
    and the chain rule (Theorem [81](ch020.xhtml#x1-202003r81)) dozens of times. We
    have finally reached a point when we are ready to implement a simple neural network
    and compute its derivative! (Of course, our methods will be far more refined in
    the end, but still, this is a milestone.)
  prefs: []
  type: TYPE_NORMAL
- en: How can we calculate the derivative for a composition of n functions?
  prefs: []
  type: TYPE_NORMAL
- en: To see the pattern, let’s map out the first few cases. For n = 2, we have the
    good old chain rule
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ ′ ′ (f2(f1(x))) = f2(f1(x))⋅f1(x). ](img/file1220.png)'
  prefs: []
  type: TYPE_IMG
- en: For n = 3, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![(f (f (f (x))))′ = f ′(f (f (x ))) ⋅f′(f (x))⋅f ′(x). 3 2 1 3 2 1 2 1 1 ](img/file1221.png)'
  prefs: []
  type: TYPE_IMG
- en: Among the multitude of parentheses, we can notice a pattern. First, we should
    calculate the value of the composed function f[3] ∘f[2] ∘f[1] at x while storing
    the intermediate results, then pass these to the appropriate derivatives and take
    the product of the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To see if our implementation works, we should test it on a simple test case,
    say for
  prefs: []
  type: TYPE_NORMAL
- en: '![f1(x) = 2x, f2(x) = 3x, f3(x) = 4x. ](img/file1222.png)'
  prefs: []
  type: TYPE_IMG
- en: The derivative of the composition (f[3] ∘f[2] ∘f[1])(x) = 24x should be constant
    24.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1223.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: The derivative of f(x) = 24x'
  prefs: []
  type: TYPE_NORMAL
- en: Success! Even though we’re only dealing with single-variable functions for now,
    our Composition is going to be the skeleton for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 Numerical differentiation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have seen that in the cases when at least some formula is available
    for the function in question, we can apply the rules of differentiation (see Theorem [80](ch020.xhtml#x1-202002r80))
    to obtain the derivative.
  prefs: []
  type: TYPE_NORMAL
- en: However, in practice, this is often not the case. For instance, think about
    the case when the function represents a recorded audio signal. If we can’t compute
    the derivative exactly, a natural idea is to approximate it, that is, provide
    an estimate that is sufficiently close to the real value.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of example, suppose that we don’t know the exact formula of our
    function to be differentiated, which is secretly the good old sine function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Recall that, by definition, the derivative is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![f′(x ) = lim f(x-+-h)−-f(x). h→0 h ](img/file1226.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we can’t take limits inside a computer (as computers can’t deal with infinity),
    the second best thing to do is to approximate this by
  prefs: []
  type: TYPE_NORMAL
- en: '![ f (x + h) − f(x) Δhf (x ) =-------h-------, ](img/file1227.png)'
  prefs: []
  type: TYPE_IMG
- en: where h/span>0 is an arbitrarily small but fixed quantity. Δ[h]f(x) is called
    the forward difference quotient. In theory, Δ[h]f(x) ≈f^′(x) holds when h is sufficiently
    small. Let’s see how they perform!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1228.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Approximating the derivative with finite differences'
  prefs: []
  type: TYPE_NORMAL
- en: Although the Δ[h]f(x) functions seem to be close f^′(x), when h is small, there
    is a plethora of potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: For one, Δ[h]f(x) = ![f(x+h)−-f(x) h](img/file1229.png) only approximates the
    derivative from the right of x, as h/span>0\. To solve this, one might use the
    backward difference quotient
  prefs: []
  type: TYPE_NORMAL
- en: '![∇hf (x ) = f-(x-)−-f(x-−-h), h ](img/file1230.png)'
  prefs: []
  type: TYPE_IMG
- en: but that seem to have the same problems. The crux of the issue is that if f
    is differentiable at some x, then
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x+--h)−-f(x)-≈ f-(x)−-f(x-−-h), h h ](img/file1231.png)'
  prefs: []
  type: TYPE_IMG
- en: but only if h is very small, and the “good enough” choice for h can vary from
    point to point.
  prefs: []
  type: TYPE_NORMAL
- en: A middle ground is provided by the so-called symmetric difference quotients,
    defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ f(x + h)− f (x − h) δhf(x) = --------2h--------, h ∈ (0,∞ ), ](img/file1232.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which is the average of forward and backward differences: δ[h]f(x) = ![Δhf(x)+-∇hf(x)-
    2](img/file1233.png). These three approximators are called finite differences.
    With their repeated application, we can approximate higher-order derivatives as
    well.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though symmetric differences are provably better, the approximation errors
    can still be significantly amplified in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: All things considered, we are not going to use finite differences for machine
    learning in practice. However, as we’ll see, the gradient descent method is simply
    a forward difference approximation of a special differential equation.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter taught us about differentiation, the key component of optimizing
    functions. Yes, even functions with millions of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we focused on univariate functions (for now), we managed to build
    a deep understanding of differentiation. For instance, we’ve learned that the
    derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![f′(x) = lim f-(x-)−-f(y) y→x x − y ](img/file1234.png)'
  prefs: []
  type: TYPE_IMG
- en: describes the slope of the tangent line drawn to the graph of f at x, which
    describes the velocity if f is the trajectory of a one-dimensional motion. From
    the perspective of physics, the derivative describes the rate of change.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, from the perspective of mathematics, differentiation offers much more
    than the rate of change: we’ve seen that a differentiable function can be written
    in the form'
  prefs: []
  type: TYPE_NORMAL
- en: '![f (x ) = f(x0) + f′(x0)(x − x0)+ o(|x− x0|) ](img/file1235.png)'
  prefs: []
  type: TYPE_IMG
- en: around some x[0] ∈ℝ. In other words, locally speaking, a differentiable function
    is a linear part plus a small error term. Unlike the limit-of-quotients definition,
    this will generalize for multiple variables without an issue. Moreover, we can
    apply a similar idea to obtain the so-called Taylor expansion
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n f (k)(x0) f(x) = --------(x− x0 )k + o(|x − x0|n), k=0 k! ](img/file1236.png)'
  prefs: []
  type: TYPE_IMG
- en: allowing us to approximate transcendental functions like log x,sinx,cosx,e^x
    with polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the theory, we also learned about computing derivatives in practice.
  prefs: []
  type: TYPE_NORMAL
- en: This is done by either 1) decomposing complex functions into their building
    blocks, then calculating the derivative using the rules
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ ′ (cf) (x) = cf (x), (f + g)′(x) = f ′(x)+ g′(x), (fg)′(x) = f ′(x)g(x)+
    f (x )g ′(x), (f ∘g)′(x) = f ′(g(x))g′(x), ](img/file1237.png)'
  prefs: []
  type: TYPE_IMG
- en: or 2) approximating the derivative with finite differences like
  prefs: []
  type: TYPE_NORMAL
- en: '![ f (x + h) − f(x) Δhf (x ) =---------------, h ](img/file1238.png)'
  prefs: []
  type: TYPE_IMG
- en: where h/span>0 is a small constant. The former method is the foundation of backpropagation,
    while the latter lies at the heart of gradient descent. These are the methods
    that truly enable training huge neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand differentiation, it’s time to talk about its counterpart:
    integration. Let’s go!'
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Calculate the derivative of the tanh(x) function defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ex − e− x tanh(x) = -x---−-x. e + e ](img/file1239.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 2\. Define the function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 0 if x <0, f(x) = |( x2 otherwise. ](img/file1240.png)'
  prefs: []
  type: TYPE_IMG
- en: Find the derivative of f(x). Is f^′(x) differentiable everywhere?
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Define the function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( | { x2 if x ∈ ℚ, f (x ) = |( 2 − x otherwise. ](img/file1241.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that
  prefs: []
  type: TYPE_NORMAL
- en: (a) f is differentiable at 0, and f^′(0) = 0, (b) and f is nowhere else differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Calculate the derivatives of the following functions.
  prefs: []
  type: TYPE_NORMAL
- en: (a) f(x) = e^(−![ 2 x2-](img/file1242.png)) (b) f(x) = x²e^(sin x) (c) f(x)
    = sin(cosx²) (d) f(x) = ![x2+1 x2−-1](img/file1243.png)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 5\. Find the Taylor expansion of the following functions around 0.
  prefs: []
  type: TYPE_NORMAL
- en: (a) f(x) = sinx (b) f(x) = cosx (c) f(x) = log x (d) f(x) = e^({-x)2}
  prefs: []
  type: TYPE_NORMAL
- en: Problem 6\. Find the Taylor expansion of the function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ − 12 f(x) = e x if x ⁄= 0, |( 0 if x = 0\. ](img/file1244.png)'
  prefs: []
  type: TYPE_IMG
- en: around 0\. Is the Taylor expansion of f equal to f?
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
