["```py\nclass Function: \n    def __init__(self): \n        pass \n\n    def __call__(self, *args, **kwargs): \n        pass \n\n    def prime(self): \n        pass \n\n    def parameters(self): \n        return dict()\n```", "```py\ndef gradient_descent( \n    f: Function, \n    x_init: float,                  # the initial guess \n    learning_rate: float = 0.1,     # the learning rate \n    n_iter: int = 1000,             # number of steps \n    return_all: bool = False        # if true, returns all intermediate values \n): \n    xs = [x_init]    # we store the intermediate results for visualization \n\n    for n in range(n_iter): \n        x = xs[-1] \n        grad = f.prime(x) \n        x_next = x - learning_rate*grad \n        xs.append(x_next) \n\n    if return_all: \n        return xs \n    else: \n        return x\n```", "```py\nclass Square(Function): \n    def __call__(self, x): \n        return x**2 \n\n    def prime(self, x): \n        return 2*x \n\nf = Square() \n\ngradient_descent(f, x_init=5.0)\n```", "```py\n7.688949513507002e-97\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \n\ndef plot_gradient_descent(f, xs: list, x_min: float, x_max: float, label: str = /span>f(x): \n    ys = [f(x) for x in xs] \n\n    grid = np.linspace(x_min, x_max, 1000) \n    fs = [f(x) for x in grid] \n\n    with plt.style.context(\"/span>seaborn-v0_8-whitegrid: \n        plt.figure(figsize=(8, 8)) \n        plt.plot(grid, fs, label=label, c=\"/span>b lw=2.0) \n        plt.plot(xs, ys, label=\"/span>gradient descent c=\"/span>r lw=4.0) \n        plt.scatter(xs, ys, c=\"/span>r s=100.0) \n        plt.legend() \n        plt.show() \n\nxs = gradient_descent(f, x_init=5.0, n_iter=25, learning_rate=0.2, return_all=True) \nplot_gradient_descent(f, xs, x_min=-5, x_max=5, label=\"/span>x^2\"\n```", "```py\nclass CosPlusSquare(Function): \n    def __call__(self, x): \n        return np.sin(x) + 0.5*x \n\n    def prime(self, x): \n        return np.cos(x) + 0.5 \n\nf = CosPlusSquare() \nxs = gradient_descent(f, x_init=7.5, n_iter=20, learning_rate=0.2, return_all=True) \nplot_gradient_descent(f, xs, -10, 10, label=\"/span>sin(x) + 0.5x\n```", "```py\nf = Square() \n\nxs = gradient_descent(f, x_init=1.0, n_iter=20, learning_rate=1.05, return_all=True) \nplot_gradient_descent(f, xs, -8, 8, label=\"/span>x^2\"\n```"]