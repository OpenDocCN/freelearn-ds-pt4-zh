<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch025.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="multivariable-functions" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">15</span></span><br/>
<span id="x1-24700018"></span><span class="cmss-10x-x-109">Multivariable Functions</span></h1>
<p><span class="cmss-10x-x-109">How different</span><span id="dx1-247001"></span> <span class="cmss-10x-x-109">is multivariable calculus from its single-variable counterpart? When I was a student, I had a professor who used to say something like, “multivariable and single-variable functions behave the same, you just have to write more.”</span></p>
<p><span class="cmss-10x-x-109">Well, this couldn’t be further from the truth. Just think about what we are doing in machine learning: training models with gradient descent; that is, finding a configuration of parameters that minimize a parametric function. In one variable (which is not a realistic assumption), we can do this with the derivative, as we saw in </span><span class="cmssi-10x-x-109">Section </span><a href="ch021.xhtml#the-basics-of-gradient-descent"><span class="cmssi-10x-x-109">13.2</span></a><span class="cmss-10x-x-109">. How can we extend the derivative to multiple dimensions?</span></p>
<p><span class="cmss-10x-x-109">The inputs of multivariable functions are </span><span class="cmssi-10x-x-109">vectors</span><span class="cmss-10x-x-109">. Thus, given a function </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">, we can’t just define</span></p>
<div class="math-display">
<img src="../media/file1428.png" class="math-display" alt="df- f(x0)-−-f(x) n dx (x0 ) = xli→mx0 x0 − x , x0,x ∈ ℝ "/>
</div>
<p><span class="cmss-10x-x-109">to the analogue of </span><span class="cmssi-10x-x-109">Definition </span><a href="ch020.xhtml#x1-198007r54"><span class="cmssi-10x-x-109">54</span></a><span class="cmss-10x-x-109">. Why? Because the division with the vector </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> <span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is not defined.</span></p>
<p><span class="cmss-10x-x-109">As we’ll see, differentiation in multiple dimensions is much more complicated. Think about it: in one dimension, there are only two directions, left and right. This is not true even for two dimensions, with an infinite number of directions at each point.</span></p>
<p><span class="cmss-10x-x-109">So, what are multivariable functions anyway?</span></p>
<section id="what-is-a-multivariable-function" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_219"><span class="titlemark"><span class="cmss-10x-x-109">15.1 </span></span> <span id="x1-24800018.1"></span><span class="cmss-10x-x-109">What is a multivariable function?</span></h2>
<p><span class="cmss-10x-x-109">We introduced functions in </span><span class="cmssi-10x-x-109">Chapter 9</span><span class="cmss-10x-x-109">, as general mappings between two sets. However, we’ve</span> <span id="dx1-248001"></span><span class="cmss-10x-x-109">only discussed functions that map real numbers to real numbers. Simple scalar-scalar functions are great for conveying ideas, but the world around us is much more complex than what we could describe with them. At the other end of the spectrum, set-set functions are way too general to be useful.</span></p>
<p><span class="cmss-10x-x-109">In practice, three categories are special enough to be analyzed mathematically but general enough to describe the patterns in science and engineering: those that</span></p>
<ol>
<li><span id="x1-248003x1"><span class="cmss-10x-x-109">map scalars to vectors, that is, </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-248005x2"><span class="cmss-10x-x-109">map vectors to scalars, that is, </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-248007x3"><span class="cmss-10x-x-109">and those that map vectors to vectors, that is, </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">The scalar-vector</span> <span id="dx1-248008"></span><span class="cmss-10x-x-109">variants</span> <span id="dx1-248009"></span><span class="cmss-10x-x-109">are called </span><span class="cmssi-10x-x-109">curves</span><span class="cmss-10x-x-109">, the vector-scalar ones are </span><span class="cmssi-10x-x-109">scalar fields</span><span class="cmss-10x-x-109">, and the vector-vector functions are what we call </span><span class="cmssi-10x-x-109">vector fields</span><span class="cmss-10x-x-109">. This</span> <span id="dx1-248010"></span><span class="cmss-10x-x-109">nomenclature looks a bit abstract, so let’s see some examples.</span></p>
<p><span class="cmssbx-10x-x-109">Scalar-vector functions</span><span class="cmss-10x-x-109">, or curves to use their more user-friendly name, are the</span> <span id="dx1-248011"></span><span class="cmss-10x-x-109">mathematical representations of movement. A space station orbiting around Earth describes a curve. So does the trajectory of a stock in the market.</span></p>
<p><span class="cmss-10x-x-109">To give you a concrete example, the scalar-vector function</span></p>
<img src="../media/file1429.png" class="math-display" width="150" alt=" ⌊ ⌋ cos(t) f(t) = ⌈ ⌉ sin(t) "/>

<p><span class="cmss-10x-x-109">describes the unit circle. This is illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">15.1</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1430.png" width="456" alt="PIC"/> <span id="x1-248012r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 15.1: A scalar-vector function, that is, a curve</span> </span>
</div>
<p><span class="cmss-10x-x-109">Not all curves are closed. For example, the curve</span></p>

<img src="../media/file1431.png" class="math-display" alt=" ⌊ ⌋ | cos(t)| g(t) = | sin(t)| ⌈ ⌉ t " width="150"/>
<p><span class="cmss-10x-x-109">represents a motion that spirals upward, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">15.2</span></a><span class="cmss-10x-x-109">. These curves are called </span><span class="cmssi-10x-x-109">open</span><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1432.png" width="284" alt="PIC"/> <span id="x1-248013r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 15.2: An open curve</span> </span>
</div>
<p><span class="cmss-10x-x-109">Because</span> <span id="dx1-248014"></span><span class="cmss-10x-x-109">of their inherent ability to describe trajectories, scalar-vector functions are essential in mathematics and science. Are you familiar with Newton’s second law of motion, stating that force equals mass times acceleration? This is described by the equation, </span><span class="cmmi-10x-x-109">F </span>= <span class="cmmi-10x-x-109">ma</span><span class="cmss-10x-x-109">, which is an instance of an </span><span class="cmssi-10x-x-109">ordinary differential equation</span><span class="cmss-10x-x-109">. All of its solutions are curves.</span></p>
<p><span class="cmss-10x-x-109">On the surface, scalar-vector functions have little to do with machine learning, but that’s not the case. Even though we won’t deal with them extensively, they have a serious presence behind the scenes. For instance, gradient descent is a discretized curve, as we saw in </span><span class="cmssi-10x-x-109">Section </span><a href="ch021.xhtml#why-does-gradient-descent-work"><span class="cmssi-10x-x-109">13.3</span></a><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Vector-scalar </span><span class="cmss-10x-x-109">functions</span> <span id="dx1-248015"></span><span class="cmss-10x-x-109">will be our focus for the next few chapters. When I write “multivariable function,” I’ll most often refer to a </span><span class="cmssi-10x-x-109">vector-scalar </span><span class="cmss-10x-x-109">function.</span></p>
<p><span class="cmss-10x-x-109">Think about a map of a mountain landscape. This </span><span class="cmssi-10x-x-109">maps </span><span class="cmss-10x-x-109">the height – a scalar – to each coordinate, thereby defining the surface. This is just a function </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">in mathematical terms.</span></p>
<p><span class="cmss-10x-x-109">Thinking about scalar fields as surfaces is useful for building geometric intuition, giving us a way to visualize them as you can see in </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">15.3</span></a><span class="cmss-10x-x-109">. (Note that the surface analog breaks down for dimensions larger than two.)</span></p>
<div class="minipage">
<p><img src="../media/file1433.png" width="484" alt="PIC"/> <span id="x1-248016r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 15.3: A surface given by a vector-scalar function</span> </span>
</div>
<p><span class="cmss-10x-x-109">Let’s clear up the notation first. If </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">is a function of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">variables, we might write </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">for an </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">or </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">for </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">if we want to emphasize the dependence on its variables. A function of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">variables is the same as a function of a single vector variable. I know</span><span id="dx1-248017"></span> <span class="cmss-10x-x-109">this seems confusing, but trust me, you’ll get used to it in no time.</span></p>
<p><span class="cmss-10x-x-109">To give a concrete example for a vector-scalar function, let’s consider </span><span class="cmssi-10x-x-109">pressure</span><span class="cmss-10x-x-109">. Pressure is the ratio of the magnitude of the force and the area of the surface of contact:</span></p>

<img src="../media/file1434.png" class="math-display" width="75" alt="p = F-. A "/>

<p><span class="cmss-10x-x-109">This can be thought of as a function of two variables: </span><span class="cmmi-10x-x-109">p</span>(<span class="cmmi-10x-x-109">x,y</span>) = <span class="cmmi-10x-x-109">x∕y</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To illustrate how problematic things can become in multiple dimensions, consider the pressure around </span>(0<span class="cmmi-10x-x-109">,</span>0)<span class="cmss-10x-x-109">. Although we haven’t talked about the limits of multivariable functions yet, what do you think</span></p>
<img src="../media/file1435.png" width="75" class="math-display" alt=" x- (x,yli)→m(0,0)y "/>
<p><span class="cmss-10x-x-109">should be?</span></p>
<p><span class="cmss-10x-x-109">Based on how we defined limits for single-variable functions (see </span><span class="cmssi-10x-x-109">Definition </span><a href="ch019.xhtml#x1-190002r51"><span class="cmssi-10x-x-109">51</span></a><span class="cmss-10x-x-109">),</span></p>
<img src="../media/file1436.png" width="75" class="math-display" alt=" xn- nl→im∞ yn "/>

<p><span class="cmss-10x-x-109">must match for all possible choices for </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">y</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">. This is not the case. Consider </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub> = <span class="cmmi-10x-x-109">α</span><sup><span class="cmr-8">2</span></sup><span class="cmmi-10x-x-109">∕n </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">y</span><sub><span class="cmmi-8">n</span></sub> = <span class="cmmi-10x-x-109">α∕n </span><span class="cmss-10x-x-109">for any </span><span class="cmmi-10x-x-109">α </span><span class="cmss-10x-x-109">real number. With this choice, we have</span></p>
<div class="math-display">
<img src="../media/file1437.png" class="math-display" alt=" 2 lim xn-= α-∕n-= α. n→ ∞ yn α∕n "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the above limit is not defined. All we did here is approach zero along slightly different trajectories, yet the result is a total mess. In one</span> <span id="dx1-248018"></span><span class="cmss-10x-x-109">variable, we have to flex our intellectual muscles to produce such examples; in multiple variables, a simple </span><span class="cmmi-10x-x-109">x∕y </span><span class="cmss-10x-x-109">will do the trick.</span></p>
<p><span class="cmssbx-10x-x-109">Vector-vector </span><span class="cmss-10x-x-109">functions</span> <span id="dx1-248019"></span><span class="cmss-10x-x-109">are called </span><span class="cmssi-10x-x-109">vector fields</span><span class="cmss-10x-x-109">. For example, consider</span> <span id="dx1-248020"></span><span class="cmss-10x-x-109">our solar system, modeled by </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">3</span></sup><span class="cmss-10x-x-109">. Each point is affected by a gravitational force, which is a vector. Thus, the gravitational pull can be described by a </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">3</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">3</span></sup> <span class="cmss-10x-x-109">function, hence the name vector field.</span></p>
<p><span class="cmss-10x-x-109">Although they are often hidden in the background, vector fields play an essential role in machine learning. Remember when we discussed why gradient descent works in </span><span class="cmssi-10x-x-109">Section </span><a href="ch021.xhtml#why-does-gradient-descent-work"><span class="cmssi-10x-x-109">13.3</span></a><span class="cmss-10x-x-109">? (At least in one variable.) All the differential equations we have encountered there are equivalent to vector fields.</span></p>
<p><span class="cmss-10x-x-109">Why? Consider the differential equation </span><span class="cmmi-10x-x-109">x</span><sup><span class="cmsy-8">′</span></sup> = <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">. If </span><span class="cmmi-10x-x-109">x</span>(<span class="cmmi-10x-x-109">t</span>) <span class="cmss-10x-x-109">describes the trajectory of a moving object, then its derivative </span><span class="cmmi-10x-x-109">x</span><sup><span class="cmsy-8">′</span></sup>(<span class="cmmi-10x-x-109">t</span>) <span class="cmss-10x-x-109">is its speed. Thus, we can interpret the equation </span><span class="cmmi-10x-x-109">x</span><sup><span class="cmsy-8">′</span></sup>(<span class="cmmi-10x-x-109">t</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>(<span class="cmmi-10x-x-109">t</span>)) <span class="cmss-10x-x-109">as prescribing the speed of our object at every position. It’s not that spectacular when our object is moving in one dimension, but if the trajectory </span><span class="cmmi-10x-x-109">x </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">describes a motion on the plane, the function </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">can be visualized neatly.</span></p>
<p><span class="cmss-10x-x-109">For example, consider the population dynamics of a simple predator-prey system. Predators feed on the prey, thus, their numbers can grow in the abundance of food. In turn, over-consumption decreases the prey population, causing a famine among the predators and decreasing their numbers. This leads to a growth in the prey, and the cycle starts over again.</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub>(<span class="cmmi-10x-x-109">t</span>) <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub>(<span class="cmmi-10x-x-109">t</span>) <span class="cmss-10x-x-109">are the size of the prey and predator populations, respectively, then their dynamics are described by the famous</span> <span id="dx1-248021"></span><span class="cmss-10x-x-109">Lotka-Volterra equations:</span></p>
<div class="math-display">
<img src="../media/file1438.png" class="math-display" alt="x′1 = x1 − x1x2 ′ x2 = x1x2 − x2. "/>
</div>
<p><span class="cmss-10x-x-109">If we represent the trajectory as the scalar-vector function</span></p>
<div class="math-display">
<img src="../media/file1439.png" class="math-display" alt=" ⌊ ⌋ x : ℝ → ℝ2, x (t) = ⌈x1(t)⌉, x2(t) "/>
</div>
<p><span class="cmss-10x-x-109">then the derivative</span></p>
<img src="../media/file1440.png" class="math-display" alt=" ⌊ ⌋ ′ ⌈x ′1(t)⌉ x (t) = x ′(t) 2 " width="150"/>
<p><span class="cmss-10x-x-109">is given by the vector-vector function</span></p>
<div class="math-display">
<img src="../media/file1441.png" class="math-display" alt=" ⌊ ⌋ 2 2 x1 − x1x2 f : ℝ → ℝ , f (x1,x2) = ⌈ ⌉ . x1x2 − x2 "/>
</div>
<p><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">can be visualized by drawing a vector onto each point of the plane, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">13.4</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1442.png" width="569" alt="PIC"/> <span id="x1-248022r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 15.4: The vector field given by the Lotka-Volterra equations</span> </span>
</div>
<p><span class="cmss-10x-x-109">Vector fields have serious applications in machine learning. As we shall see soon, the</span> <span id="dx1-248023"></span><span class="cmss-10x-x-109">multivariable derivative (called </span><span class="cmssi-10x-x-109">gradient</span><span class="cmss-10x-x-109">) defines</span><span id="dx1-248024"></span> <span class="cmss-10x-x-109">a vector field.</span></p>
<p><span class="cmss-10x-x-109">Moreover, as indicated by the single-variable case (see </span><span class="cmssi-10x-x-109">Section </span><a href="ch021.xhtml#why-does-gradient-descent-work"><span class="cmssi-10x-x-109">13.3</span></a><span class="cmss-10x-x-109">), the gradient descent algorithm will be the discretized trajectory determined by the vector field of the gradient.</span></p>
<p><span class="cmss-10x-x-109">Now that we understand what multivariable functions are, let’s see a special case. You know how we roll: examples are essential, and we always start with them whenever possible. This time, we’ll put </span><span class="cmssi-10x-x-109">linear functions </span><span class="cmss-10x-x-109">under the microscope.</span></p>
</section>
<section id="linear-functions-in-multiple-variables" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_220"><span class="titlemark"><span class="cmss-10x-x-109">15.2 </span></span> <span id="x1-24900018.2"></span><span class="cmss-10x-x-109">Linear functions in multiple variables</span></h2>
<p><span class="cmss-10x-x-109">One of the most</span> <span id="dx1-249001"></span><span class="cmss-10x-x-109">important functions in mathematics is the linear function. In one variable, it takes the form </span><span class="cmmi-10x-x-109">l</span>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">ax </span>+ <span class="cmmi-10x-x-109">b</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">b </span><span class="cmss-10x-x-109">are arbitrary real numbers.</span></p>
<p><span class="cmss-10x-x-109">We’ve seen linear functions several times already. For instance, </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch020.xhtml#x1-199002r77"><span class="cmssi-10x-x-109">77</span></a> <span class="cmss-10x-x-109">gives that differentiation is equivalent to finding the best linear approximation.</span></p>
<p><span class="cmss-10x-x-109">Linear functions, that is, functions of the form</span></p>
<div class="math-display">
<img src="../media/file1443.png" class="math-display" alt=" ∑n f (x1,...,xn) = b+ aixi, b,ai ∈ ℝ i=1 "/>
</div>
<p><span class="cmss-10x-x-109">are as important in multiple variables as in one.</span></p>
<p><span class="cmss-10x-x-109">To build up a deep understanding, we’ll take a look at the simplest case: a line on the two-dimensional plane.</span></p>
<div class="minipage">
<p><img src="../media/file1444.png" width="227" alt="PIC"/> <span id="x1-249002r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 15.5: A line on the plane</span> </span>
</div>
<p><span class="cmss-10x-x-109">Given its normal vector </span><span class="cmbx-10x-x-109">m </span>= (<span class="cmmi-10x-x-109">m</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,m</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">and its arbitrary point </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">0</span></sub><span class="cmss-10x-x-109">, the vector </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is on the line if and only if </span><span class="cmbx-10x-x-109">m </span><span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">0</span></sub> <span class="cmss-10x-x-109">is orthogonal, that is, if</span></p>
<div class="math-display">
  <span>
    ⟨m, x − v<sub>0</sub>⟩ = 0
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(15.1)</span>
</div>

<p><span class="cmss-10x-x-109">holds. (</span><a href="ch025.xhtml"><span class="cmss-10x-x-109">15.1</span></a><span class="cmss-10x-x-109">) is called the </span><span class="cmssi-10x-x-109">normal vector equation of the line</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">By using the bilinearity of the inner product and writing out </span><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">m</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">in terms of their coordinates, we can simplify (</span><a href="ch025.xhtml"><span class="cmss-10x-x-109">15.1</span></a><span class="cmss-10x-x-109">). Assuming that </span><span class="cmmi-10x-x-109">m</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">≠</span>0<span class="cmss-10x-x-109">, that is, the line is not parallel to the </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">axis, a quick calculation yields</span></p>
<div class="math-display">
<img src="../media/file1445.png" class="math-display" alt=" m1- -1- x2 = −m2 x1 + m2 ⟨m, v0⟩. "/>
</div>
<p><span class="cmss-10x-x-109">This is a</span> <span id="dx1-249003"></span><span class="cmss-10x-x-109">linear function of the single variable </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">in its full glory. The coefficient </span><span class="cmsy-10x-x-109">−</span><img src="../media/file1446.png" width="15" data-align="middle" alt="m1- m2"/> <span class="cmss-10x-x-109">describes the slope, while</span> <img src="../media/file1447.png" width="15" data-align="middle" alt=" 1 m2-"/><span class="cmsy-10x-x-109">⟨</span><span class="cmbx-10x-x-109">m</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">0</span></sub><span class="cmsy-10x-x-109">⟩ </span><span class="cmss-10x-x-109">describes the intercept.</span></p>
<p><span class="cmss-10x-x-109">In other words, linear functions are equivalent to vector equations of the form (</span><a href="ch025.xhtml"><span class="cmss-10x-x-109">15.1</span></a><span class="cmss-10x-x-109">), at least in one variable.</span></p>
<p><span class="cmss-10x-x-109">What happens if we apply the same argument in higher dimensional spaces? In </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmr-8">+1</span></sup><span class="cmss-10x-x-109">, the normal vector equation</span></p>
<div class="math-display">
  <span>
    ⟨m, x − v<sub>0</sub>⟩ = 0, m, x, v<sub>0</sub> ∈ ℝ<sup>n+1</sup>
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(15.2)</span>
</div>

<p><span class="cmss-10x-x-109">defines a </span><span class="cmssi-10x-x-109">hyperplane</span><span class="cmss-10x-x-109">, that is, an </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional plane. (One dimension less than the embedding plane, which is </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmr-8">+1</span></sup> <span class="cmss-10x-x-109">in our case.) Unraveling (</span><a href="ch025.xhtml"><span class="cmss-10x-x-109">15.2</span></a><span class="cmss-10x-x-109">), we obtain</span></p>
<div class="math-display">
<img src="../media/file1448.png" class="math-display" alt=" 1 ∑n mi xn+1 = m----⟨m, v0⟩− m----xi. n+1 i=1 n+1 "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the general form of a linear function in </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">variables</span></p>
<div class="math-display">
<img src="../media/file1449.png" class="math-display" alt=" n f (x ,...,x ) = b+ ∑ ax , b,a ∈ ℝ 1 n i=1 i i i "/>
</div>
<p><span class="cmss-10x-x-109">originates from the normal vector equation of the </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional plane, embedded in the </span>(<span class="cmmi-10x-x-109">n </span>+ 1)<span class="cmss-10x-x-109">-dimensional space.</span></p>
<p><span class="cmss-10x-x-109">This can also be written in the vectorized form</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/file1450.png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(15.3)
  </div>
</div>

<p><span class="cmss-10x-x-109">which is how we’ll mostly use it in the future. (Note that when looking at the matrix representation of a vector </span><span class="cmbx-10x-x-109">u </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, we always use the column form </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">. Moreover, </span><span class="cmbx-10x-x-109">a </span><span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">not </span><span class="cmss-10x-x-109">the normal vector of the plane.)</span></p>
<p><span class="cmss-10x-x-109">Before we move on</span> <span id="dx1-249004"></span><span class="cmss-10x-x-109">to study the inner workings of multivariable calculus, I want to emphasize how seriously multiple dimensions complicate things in machine learning.</span></p>
</section>
<section id="the-curse-of-dimensionality" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_221"><span class="titlemark"><span class="cmss-10x-x-109">15.3 </span></span> <span id="x1-25000018.3"></span><span class="cmss-10x-x-109">The curse of dimensionality</span></h2>
<p><span class="cmss-10x-x-109">First, let’s talk</span><span id="dx1-250001"></span> <span class="cmss-10x-x-109">about optimization. If all else fails, optimizing a single-variable function </span><span class="cmmi-10x-x-109">f </span>: [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">can be as simple as partitioning</span> [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">into a grid of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">points, evaluating the function at each point, then finding the minima/maxima.</span></p>
<p><span class="cmss-10x-x-109">We cannot do this in higher dimensions. To see why, consider ResNet18, the famous convolutional network architecture. It has precisely 11,689,512 parameters. Thus, training is equivalent to optimizing a function of a whopping 11,689,512-variable function. If we were to construct a grid with just two points along every dimension, we would have </span>2<sup><span class="cmr-8">11689512</span></sup> <span class="cmss-10x-x-109">points to evaluate the function at. For comparison, the number of atoms in our observable universe is around</span> 10<sup><span class="cmr-8">82</span></sup><span class="cmss-10x-x-109">. A number that is dwarfed by the size of our grid. Thus, grid search is currently impossible on such an enormous grid. We are forced to devise clever algorithms that can tackle the size and complexity of large dimensional spaces.</span></p>
<p><span class="cmss-10x-x-109">Another issue is that, in high dimensions, a strange thing starts to happen with balls. Recall that, by definition, the n-dimensional ball of radius </span><span class="cmmi-10x-x-109">r </span><span class="cmss-10x-x-109">around the point </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is defined by</span></p>
<div class="math-display">
<img src="../media/file1451.png" class="math-display" alt="Bn (r,x0) := {x ∈ ℝn : ∥x − x0∥ &lt;r}, "/>
</div>
<p><span class="cmss-10x-x-109">and we denote its volume by </span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">n</span></sub>(<span class="cmmi-10x-x-109">r</span>)<span class="cmss-10x-x-109">. (The volume depends only on the radius and the dimension, not the center.)</span></p>
<p><span class="cmss-10x-x-109">It turns out that</span></p>
<div class="math-display">
<img src="../media/file1452.png" class="math-display" alt=" π n2 n Vn(r) = Γ (1+-n)r , 2 "/>
</div>
<p><span class="cmss-10x-x-109">where </span>Γ(<span class="cmmi-10x-x-109">z</span>) <span class="cmss-10x-x-109">is the famous Gamma function (</span><a href="https://en.wikipedia.org/wiki/Gamma_function" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Gamma_function</span></a><span class="cmss-10x-x-109">), the generalization of the factorial.</span></p>
<p><span class="cmss-10x-x-109">The volume formula might seem complicated because of the Gamma function, the </span><span class="cmmi-10x-x-109">π</span><span class="cmss-10x-x-109">, and all the other terms, but let’s focus on the core of the issue. What happens if we slice off an </span><span class="cmmi-10x-x-109">𝜀</span><span class="cmss-10x-x-109">-wide shell from the unit ball?</span></p>
<p><span class="cmss-10x-x-109">It turns out that the volume of the unit ball is concentrated around its outer shell, as shown by the volume formula:</span></p>
<div class="math-display">
<img src="../media/file1453.png" class="math-display" alt=" lim Vn(1−--𝜀)-= lim (1 − 𝜀)n = 0. n→ ∞ Vn(1) n→ ∞ "/>
</div>
<p><span class="cmss-10x-x-109">Heuristically, this means that if you randomly select a point from the unit ball, its distance from the center will be close to </span>1 <span class="cmss-10x-x-109">in high dimensions.</span></p>
<p><span class="cmss-10x-x-109">In other words, distance doesn’t behave as you would intuitively expect. Another way</span> <span id="dx1-250002"></span><span class="cmss-10x-x-109">of looking at the issue would be to study the effects of taking one step in each possible direction, starting from the origin and arriving at the point</span></p>
<div class="math-display">
<img src="../media/file1454.png" class="math-display" alt="1 = (1,1,...,1) ∈ ℝn, "/>
</div>
<p><span class="cmss-10x-x-109">something like what </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">15.6</span></a> <span class="cmss-10x-x-109">illustrates in the three-dimensional case.</span></p>
<div class="minipage">
<p><img src="../media/file1455.png" width="284" alt="PIC"/> <span id="x1-250003r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 15.6: Taking a step in each direction in three dimensions</span> </span>
</div>
<p><span class="cmss-10x-x-109">The Euclidean distance we have traveled is</span></p>
<div class="math-display">
<img src="../media/file1456.png" class="math-display" alt=" ┌ ----- ││ ∑n √ -- ∥1∥ = ∘ 1 = n, i=1 "/>
</div>
<p><span class="cmss-10x-x-109">which goes to infinity as the number of dimensions grows. That is, the diagonal of the unit cube is </span><span class="cmssi-10x-x-109">really big</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">These two</span> <span id="dx1-250004"></span><span class="cmss-10x-x-109">phenomena can cause significant headaches in practice. More parameters result in more expressive models but also make training much more difficult. This is called the </span><span class="cmssi-10x-x-109">curse of dimensionality</span><span class="cmss-10x-x-109">.</span></p>
</section>
<section id="summary14" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_222"><span class="titlemark"><span class="cmss-10x-x-109">15.4 </span></span> <span id="x1-25100018.4"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">In this chapter, we have dipped our toe into the ocean of multivariable functions. The very moment we add more dimensions, the complexity shoots up.</span></p>
<p><span class="cmss-10x-x-109">For instance, we have three classes:</span></p>
<ol>
<li><span id="x1-251002x1"><span class="cmss-10x-x-109">scalar-vector </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-251004x2"><span class="cmss-10x-x-109">vector-scalar </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-251006x3"><span class="cmss-10x-x-109">and vector-vector functions </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">All of them are essential in machine learning. Feature transformations, like layers in neural networks, are vector-vector functions. Loss landscapes are given by vector-scalar functions, but training is done by following along a (discretized) scalar-vector function, also known as a curve.</span></p>
<p><span class="cmss-10x-x-109">Besides more complicated notations, we also have the curse of dimensionality to deal with. This is why optimizing functions of millions of variables is hard: not only does the parameter space get insanely large, but the concept of distance also begins to break down.</span></p>
<p><span class="cmss-10x-x-109">Now that we’ve built some intuition about multivariable functions and familiarity with the notation, it’s time to dive deep. How can we do calculus in higher dimensions? Let’s see in the next chapter!</span></p>
</section>
<section id="join-our-community-on-discord15" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_223"><span id="x1-252000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>