- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multivariable Functions
  prefs: []
  type: TYPE_NORMAL
- en: How different is multivariable calculus from its single-variable counterpart?
    When I was a student, I had a professor who used to say something like, “multivariable
    and single-variable functions behave the same, you just have to write more.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, this couldn’t be further from the truth. Just think about what we are
    doing in machine learning: training models with gradient descent; that is, finding
    a configuration of parameters that minimize a parametric function. In one variable
    (which is not a realistic assumption), we can do this with the derivative, as
    we saw in Section [13.2](ch021.xhtml#the-basics-of-gradient-descent). How can
    we extend the derivative to multiple dimensions?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The inputs of multivariable functions are vectors. Thus, given a function f
    : ℝ^n →ℝ, we can’t just define'
  prefs: []
  type: TYPE_NORMAL
- en: '![df- f(x0)-−-f(x) n dx (x0 ) = xli→mx0 x0 − x , x0,x ∈ ℝ ](img/file1428.png)'
  prefs: []
  type: TYPE_IMG
- en: to the analogue of Definition [54](ch020.xhtml#x1-198007r54). Why? Because the
    division with the vector x[0] −x is not defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ll see, differentiation in multiple dimensions is much more complicated.
    Think about it: in one dimension, there are only two directions, left and right.
    This is not true even for two dimensions, with an infinite number of directions
    at each point.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what are multivariable functions anyway?
  prefs: []
  type: TYPE_NORMAL
- en: 15.1 What is a multivariable function?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced functions in Chapter 9, as general mappings between two sets.
    However, we’ve only discussed functions that map real numbers to real numbers.
    Simple scalar-scalar functions are great for conveying ideas, but the world around
    us is much more complex than what we could describe with them. At the other end
    of the spectrum, set-set functions are way too general to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, three categories are special enough to be analyzed mathematically
    but general enough to describe the patterns in science and engineering: those
    that'
  prefs: []
  type: TYPE_NORMAL
- en: 'map scalars to vectors, that is, f : ℝ →ℝ^n,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'map vectors to scalars, that is, f : ℝ^n →ℝ,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'and those that map vectors to vectors, that is, f : ℝ^n →ℝ^m.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scalar-vector variants are called curves, the vector-scalar ones are scalar
    fields, and the vector-vector functions are what we call vector fields. This nomenclature
    looks a bit abstract, so let’s see some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar-vector functions, or curves to use their more user-friendly name, are
    the mathematical representations of movement. A space station orbiting around
    Earth describes a curve. So does the trajectory of a stock in the market.
  prefs: []
  type: TYPE_NORMAL
- en: To give you a concrete example, the scalar-vector function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ cos(t) f(t) = ⌈ ⌉ sin(t) ](img/file1429.png)'
  prefs: []
  type: TYPE_IMG
- en: describes the unit circle. This is illustrated by Figure [15.1](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1430.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: A scalar-vector function, that is, a curve'
  prefs: []
  type: TYPE_NORMAL
- en: Not all curves are closed. For example, the curve
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | cos(t)| g(t) = | sin(t)| ⌈ ⌉ t ](img/file1431.png)'
  prefs: []
  type: TYPE_IMG
- en: represents a motion that spirals upward, as illustrated by Figure [15.2](#).
    These curves are called open.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1432.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: An open curve'
  prefs: []
  type: TYPE_NORMAL
- en: Because of their inherent ability to describe trajectories, scalar-vector functions
    are essential in mathematics and science. Are you familiar with Newton’s second
    law of motion, stating that force equals mass times acceleration? This is described
    by the equation, F = ma, which is an instance of an ordinary differential equation.
    All of its solutions are curves.
  prefs: []
  type: TYPE_NORMAL
- en: On the surface, scalar-vector functions have little to do with machine learning,
    but that’s not the case. Even though we won’t deal with them extensively, they
    have a serious presence behind the scenes. For instance, gradient descent is a
    discretized curve, as we saw in Section [13.3](ch021.xhtml#why-does-gradient-descent-work).
  prefs: []
  type: TYPE_NORMAL
- en: Vector-scalar functions will be our focus for the next few chapters. When I
    write “multivariable function,” I’ll most often refer to a vector-scalar function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about a map of a mountain landscape. This maps the height – a scalar
    – to each coordinate, thereby defining the surface. This is just a function f
    : ℝ² →ℝ in mathematical terms.'
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about scalar fields as surfaces is useful for building geometric intuition,
    giving us a way to visualize them as you can see in Figure [15.3](#). (Note that
    the surface analog breaks down for dimensions larger than two.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1433.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: A surface given by a vector-scalar function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s clear up the notation first. If f : ℝ^n →ℝ is a function of n variables,
    we might write f(x) for an x ∈ℝ^n or f(x[1],…,x[n]) for x[i] ∈ℝ if we want to
    emphasize the dependence on its variables. A function of n variables is the same
    as a function of a single vector variable. I know this seems confusing, but trust
    me, you’ll get used to it in no time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To give a concrete example for a vector-scalar function, let’s consider pressure.
    Pressure is the ratio of the magnitude of the force and the area of the surface
    of contact:'
  prefs: []
  type: TYPE_NORMAL
- en: '![p = F-. A ](img/file1434.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be thought of as a function of two variables: p(x,y) = x∕y.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how problematic things can become in multiple dimensions, consider
    the pressure around (0,0). Although we haven’t talked about the limits of multivariable
    functions yet, what do you think
  prefs: []
  type: TYPE_NORMAL
- en: '![ x- (x,yli)→m(0,0)y ](img/file1435.png)'
  prefs: []
  type: TYPE_IMG
- en: should be?
  prefs: []
  type: TYPE_NORMAL
- en: Based on how we defined limits for single-variable functions (see Definition [51](ch019.xhtml#x1-190002r51)),
  prefs: []
  type: TYPE_NORMAL
- en: '![ xn- nl→im∞ yn ](img/file1436.png)'
  prefs: []
  type: TYPE_IMG
- en: must match for all possible choices for x[n] and y[n]. This is not the case.
    Consider x[n] = α²∕n and y[n] = α∕n for any α real number. With this choice, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![ 2 lim xn-= α-∕n-= α. n→ ∞ yn α∕n ](img/file1437.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the above limit is not defined. All we did here is approach zero along
    slightly different trajectories, yet the result is a total mess. In one variable,
    we have to flex our intellectual muscles to produce such examples; in multiple
    variables, a simple x∕y will do the trick.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector-vector functions are called vector fields. For example, consider our
    solar system, modeled by ℝ³. Each point is affected by a gravitational force,
    which is a vector. Thus, the gravitational pull can be described by a f : ℝ³ →ℝ³
    function, hence the name vector field.'
  prefs: []
  type: TYPE_NORMAL
- en: Although they are often hidden in the background, vector fields play an essential
    role in machine learning. Remember when we discussed why gradient descent works
    in Section [13.3](ch021.xhtml#why-does-gradient-descent-work)? (At least in one
    variable.) All the differential equations we have encountered there are equivalent
    to vector fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why? Consider the differential equation x^′ = f(x). If x(t) describes the trajectory
    of a moving object, then its derivative x^′(t) is its speed. Thus, we can interpret
    the equation x^′(t) = f(x(t)) as prescribing the speed of our object at every
    position. It’s not that spectacular when our object is moving in one dimension,
    but if the trajectory x : ℝ →ℝ² describes a motion on the plane, the function
    f : ℝ² →ℝ² can be visualized neatly.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the population dynamics of a simple predator-prey system.
    Predators feed on the prey, thus, their numbers can grow in the abundance of food.
    In turn, over-consumption decreases the prey population, causing a famine among
    the predators and decreasing their numbers. This leads to a growth in the prey,
    and the cycle starts over again.
  prefs: []
  type: TYPE_NORMAL
- en: 'If x[1](t) and x[2](t) are the size of the prey and predator populations, respectively,
    then their dynamics are described by the famous Lotka-Volterra equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![x′1 = x1 − x1x2 ′ x2 = x1x2 − x2\. ](img/file1438.png)'
  prefs: []
  type: TYPE_IMG
- en: If we represent the trajectory as the scalar-vector function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ x : ℝ → ℝ2, x (t) = ⌈x1(t)⌉, x2(t) ](img/file1439.png)'
  prefs: []
  type: TYPE_IMG
- en: then the derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ′ ⌈x ′1(t)⌉ x (t) = x ′(t) 2 ](img/file1440.png)'
  prefs: []
  type: TYPE_IMG
- en: is given by the vector-vector function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 2 2 x1 − x1x2 f : ℝ → ℝ , f (x1,x2) = ⌈ ⌉ . x1x2 − x2 ](img/file1441.png)'
  prefs: []
  type: TYPE_IMG
- en: f can be visualized by drawing a vector onto each point of the plane, as illustrated
    by Figure [13.4](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1442.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: The vector field given by the Lotka-Volterra equations'
  prefs: []
  type: TYPE_NORMAL
- en: Vector fields have serious applications in machine learning. As we shall see
    soon, the multivariable derivative (called gradient) defines a vector field.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as indicated by the single-variable case (see Section [13.3](ch021.xhtml#why-does-gradient-descent-work)),
    the gradient descent algorithm will be the discretized trajectory determined by
    the vector field of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand what multivariable functions are, let’s see a special
    case. You know how we roll: examples are essential, and we always start with them
    whenever possible. This time, we’ll put linear functions under the microscope.'
  prefs: []
  type: TYPE_NORMAL
- en: 15.2 Linear functions in multiple variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important functions in mathematics is the linear function. In
    one variable, it takes the form l(x) = ax + b, where a and b are arbitrary real
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen linear functions several times already. For instance, Theorem [77](ch020.xhtml#x1-199002r77)
    gives that differentiation is equivalent to finding the best linear approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Linear functions, that is, functions of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n f (x1,...,xn) = b+ aixi, b,ai ∈ ℝ i=1 ](img/file1443.png)'
  prefs: []
  type: TYPE_IMG
- en: are as important in multiple variables as in one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build up a deep understanding, we’ll take a look at the simplest case: a
    line on the two-dimensional plane.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1444.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: A line on the plane'
  prefs: []
  type: TYPE_NORMAL
- en: Given its normal vector m = (m[1],m[2]) and its arbitrary point v[0], the vector
    x is on the line if and only if m and x −v[0] is orthogonal, that is, if
  prefs: []
  type: TYPE_NORMAL
- en: ⟨m, x − v[0]⟩ = 0 (15.1)
  prefs: []
  type: TYPE_NORMAL
- en: holds. ([15.1](ch025.xhtml)) is called the normal vector equation of the line.
  prefs: []
  type: TYPE_NORMAL
- en: By using the bilinearity of the inner product and writing out ⟨m,x⟩ in terms
    of their coordinates, we can simplify ([15.1](ch025.xhtml)). Assuming that m[2]≠0,
    that is, the line is not parallel to the x[2] axis, a quick calculation yields
  prefs: []
  type: TYPE_NORMAL
- en: '![ m1- -1- x2 = −m2 x1 + m2 ⟨m, v0⟩. ](img/file1445.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a linear function of the single variable x[1] in its full glory. The
    coefficient −![m1- m2](img/file1446.png) describes the slope, while ![ 1 m2-](img/file1447.png)⟨m,v[0]⟩
    describes the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, linear functions are equivalent to vector equations of the form
    ([15.1](ch025.xhtml)), at least in one variable.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we apply the same argument in higher dimensional spaces? In
    ℝ^(n+1), the normal vector equation
  prefs: []
  type: TYPE_NORMAL
- en: ⟨m, x − v[0]⟩ = 0, m, x, v[0] ∈ ℝ^(n+1) (15.2)
  prefs: []
  type: TYPE_NORMAL
- en: defines a hyperplane, that is, an n-dimensional plane. (One dimension less than
    the embedding plane, which is ℝ^(n+1) in our case.) Unraveling ([15.2](ch025.xhtml)),
    we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 ∑n mi xn+1 = m----⟨m, v0⟩− m----xi. n+1 i=1 n+1 ](img/file1448.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the general form of a linear function in n variables
  prefs: []
  type: TYPE_NORMAL
- en: '![ n f (x ,...,x ) = b+ ∑ ax , b,a ∈ ℝ 1 n i=1 i i i ](img/file1449.png)'
  prefs: []
  type: TYPE_IMG
- en: originates from the normal vector equation of the n-dimensional plane, embedded
    in the (n + 1)-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: This can also be written in the vectorized form
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/file1450.png)(15.3)'
  prefs: []
  type: TYPE_IMG
- en: which is how we’ll mostly use it in the future. (Note that when looking at the
    matrix representation of a vector u ∈ℝ^n, we always use the column form ℝ^(n×1).
    Moreover, a is not the normal vector of the plane.)
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to study the inner workings of multivariable calculus, I want
    to emphasize how seriously multiple dimensions complicate things in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3 The curse of dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s talk about optimization. If all else fails, optimizing a single-variable
    function f : [a,b] →ℝ can be as simple as partitioning [a,b] into a grid of n
    points, evaluating the function at each point, then finding the minima/maxima.'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot do this in higher dimensions. To see why, consider ResNet18, the famous
    convolutional network architecture. It has precisely 11,689,512 parameters. Thus,
    training is equivalent to optimizing a function of a whopping 11,689,512-variable
    function. If we were to construct a grid with just two points along every dimension,
    we would have 2^(11689512) points to evaluate the function at. For comparison,
    the number of atoms in our observable universe is around 10^(82). A number that
    is dwarfed by the size of our grid. Thus, grid search is currently impossible
    on such an enormous grid. We are forced to devise clever algorithms that can tackle
    the size and complexity of large dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is that, in high dimensions, a strange thing starts to happen
    with balls. Recall that, by definition, the n-dimensional ball of radius r around
    the point x[0] ∈ℝ^n is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![Bn (r,x0) := {x ∈ ℝn : ∥x − x0∥ <r}, ](img/file1451.png)'
  prefs: []
  type: TYPE_IMG
- en: and we denote its volume by V [n](r). (The volume depends only on the radius
    and the dimension, not the center.)
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that
  prefs: []
  type: TYPE_NORMAL
- en: '![ π n2 n Vn(r) = Γ (1+-n)r , 2 ](img/file1452.png)'
  prefs: []
  type: TYPE_IMG
- en: where Γ(z) is the famous Gamma function ([https://en.wikipedia.org/wiki/Gamma_function](https://en.wikipedia.org/wiki/Gamma_function)),
    the generalization of the factorial.
  prefs: []
  type: TYPE_NORMAL
- en: The volume formula might seem complicated because of the Gamma function, the
    π, and all the other terms, but let’s focus on the core of the issue. What happens
    if we slice off an 𝜀-wide shell from the unit ball?
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that the volume of the unit ball is concentrated around its outer
    shell, as shown by the volume formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ lim Vn(1−--𝜀)-= lim (1 − 𝜀)n = 0\. n→ ∞ Vn(1) n→ ∞ ](img/file1453.png)'
  prefs: []
  type: TYPE_IMG
- en: Heuristically, this means that if you randomly select a point from the unit
    ball, its distance from the center will be close to 1 in high dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, distance doesn’t behave as you would intuitively expect. Another
    way of looking at the issue would be to study the effects of taking one step in
    each possible direction, starting from the origin and arriving at the point
  prefs: []
  type: TYPE_NORMAL
- en: '![1 = (1,1,...,1) ∈ ℝn, ](img/file1454.png)'
  prefs: []
  type: TYPE_IMG
- en: something like what Figure [15.6](#) illustrates in the three-dimensional case.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1455.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Taking a step in each direction in three dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean distance we have traveled is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ┌ ----- ││ ∑n √ -- ∥1∥ = ∘ 1 = n, i=1 ](img/file1456.png)'
  prefs: []
  type: TYPE_IMG
- en: which goes to infinity as the number of dimensions grows. That is, the diagonal
    of the unit cube is really big.
  prefs: []
  type: TYPE_NORMAL
- en: These two phenomena can cause significant headaches in practice. More parameters
    result in more expressive models but also make training much more difficult. This
    is called the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 15.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have dipped our toe into the ocean of multivariable functions.
    The very moment we add more dimensions, the complexity shoots up.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we have three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'scalar-vector f : ℝ →ℝ^n,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'vector-scalar f : ℝ^n →ℝ,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'and vector-vector functions f : ℝ^n →ℝ^m.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of them are essential in machine learning. Feature transformations, like
    layers in neural networks, are vector-vector functions. Loss landscapes are given
    by vector-scalar functions, but training is done by following along a (discretized)
    scalar-vector function, also known as a curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides more complicated notations, we also have the curse of dimensionality
    to deal with. This is why optimizing functions of millions of variables is hard:
    not only does the parameter space get insanely large, but the concept of distance
    also begins to break down.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve built some intuition about multivariable functions and familiarity
    with the notation, it’s time to dive deep. How can we do calculus in higher dimensions?
    Let’s see in the next chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
