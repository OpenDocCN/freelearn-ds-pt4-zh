<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch026.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="derivatives-and-gradients" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">16</span></span><br/>
<span id="x1-25300019"></span><span class="cmss-10x-x-109">Derivatives and Gradients</span></h1>
<p><span class="cmss-10x-x-109">Now that we understand why multivariate functions and high-dimensional spaces are more complex than the single-variable case we studied earlier, it’s time to see how to do things in the general case.</span></p>
<p><span class="cmss-10x-x-109">To recap quickly, our goal in machine learning is to </span><span class="cmssi-10x-x-109">optimize functions with millions of variables</span><span class="cmss-10x-x-109">. For instance, think about a neural network </span><span class="cmmi-10x-x-109">N</span>(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">w</span>) <span class="cmss-10x-x-109">trained for binary classification, where</span></p>
<ul>
<li><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the input data,</span></li>
<li><span class="cmbx-10x-x-109">w </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">is the vector compressing all of the weight parameters,</span></li>
<li><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">N</span>(<span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">w</span>) <span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">is the prediction, representing the probability of belonging to the positive class.</span></li>
</ul>
<p><span class="cmss-10x-x-109">In the case of, say, binary cross-entropy loss, we have the loss function</span></p>
<div class="math-display">
<img src="../media/file1457.png" class="math-display" alt=" d L(w ) = − ∑ y log N (x ,w ), i i k=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th data point with ground truth </span><span class="cmmi-10x-x-109">y</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∈{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. See, I told you that we have to write </span><span class="cmssi-10x-x-109">much </span><span class="cmss-10x-x-109">more in multivariable calculus. (We’ll talk about binary cross-entropy loss in </span><span class="cmssi-10x-x-109">Chapter </span><a href="ch032.xhtml#the-expected-value"><span class="cmssi-10x-x-109">20</span></a><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">Training the neural network is the same as finding a global minimum of </span><span class="cmmi-10x-x-109">L</span>(<span class="cmbx-10x-x-109">w</span>)<span class="cmss-10x-x-109">, if it exists. We have already seen how we can do optimization in a single variable:</span></p>
<ul>
<li><span class="cmss-10x-x-109">figure out the direction of increase by calculating the derivative,</span></li>
<li><span class="cmss-10x-x-109">take a small step,</span></li>
<li><span class="cmss-10x-x-109">then iterate.</span></li>
</ul>
<p><span class="cmss-10x-x-109">For this to work in multiple variables, we need to generalize the concept of the derivative. We can quickly discover the issue: since division with a vector is not defined, the difference quotient</span></p>
<img src="../media/file1458.png" width="150" class="math-display" alt="f(x)−-f-(y-) x− y "/>
<p><span class="cmss-10x-x-109">makes no sense when </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">is a function of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">variables and </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">are </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional vectors.</span></p>
<p><span class="cmss-10x-x-109">How can we make sense of it, then? This is what we’ll learn in the following chapter.</span></p>
<section id="partial-and-total-derivatives" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_224"><span class="titlemark"><span class="cmss-10x-x-109">16.1 </span></span> <span id="x1-25400019.1"></span><span class="cmss-10x-x-109">Partial and total derivatives</span></h2>
<p><span class="cmss-10x-x-109">Let’s take a</span><span id="dx1-254001"></span> <span class="cmss-10x-x-109">look at multivariable</span> <span id="dx1-254002"></span><span class="cmss-10x-x-109">functions more closely! For the sake of</span> <span id="dx1-254003"></span><span class="cmss-10x-x-109">simplicity, let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">be our function of two variables. To emphasize the dependence on the individual variables, we often write</span></p>
<div class="math-display">
<img src="../media/file1459.png" class="math-display" alt="f(x1,x2), x1,x2 ∈ ℝ. "/>
</div>
<p><span class="cmss-10x-x-109">Here’s the trick: by fixing one of the variables, we obtain the two single-variable functions! That is, if </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">is fixed, we have </span><span class="cmmi-10x-x-109">x</span>→<span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span>)<span class="cmss-10x-x-109">, and if </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">is fixed, we have </span><span class="cmmi-10x-x-109">x</span>→<span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x,x</span><sub><span class="cmr-8">2</span></sub>)<span class="cmmi-10x-x-109">, </span><span class="cmss-10x-x-109">both of which are well-defined univariate functions. Think about this as slicing the function graph with a plane parallel to the </span><span class="cmmi-10x-x-109">x</span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">z </span><span class="cmss-10x-x-109">or the </span><span class="cmmi-10x-x-109">y </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">z </span><span class="cmss-10x-x-109">axes, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">16.1</span></a><span class="cmss-10x-x-109">. The part cut out by the plane is a single-variable function.</span></p>
<div class="minipage">
<p><img src="../media/file1462.png" width="456" alt="PIC"/> <span id="x1-254004r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 16.1: Slicing the surface with the </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">z </span><span class="cmss-10x-x-109">plane</span> </span>
</div>
<p><span class="cmss-10x-x-109">We can define the derivative of these functions by the limit of difference quotients. These are called the </span><span class="cmssi-10x-x-109">partial derivatives</span><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file1463.png" class="math-display" alt="∂f-- f(x,x2)-−-f(x1,x2) ∂x1 (x1,x2 ) = xli→mx1 x − x1 , ∂f f(x ,x) − f(x ,x ) ----(x1,x2 ) = lim ---1---------1--2-. ∂x2 x→x2 x − x2 "/>
</div>
<p><span class="cmss-10x-x-109">(Keep in mind that </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">signifies the variable in</span> <img src="../media/file1464.png" width="15" data-align="middle" alt="∂f- ∂x1"/><span class="cmss-10x-x-109">, but an actual scalar value in the argument of</span> <img src="../media/file1465.png" width="15" data-align="middle" alt="∂f- ∂x1"/>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>)<span class="cmss-10x-x-109">. This can be quite confusing, but you’ll soon learn to make sense of it.)</span></p>
<p><span class="cmss-10x-x-109">The</span> <span id="dx1-254005"></span><span class="cmss-10x-x-109">definition is similar for</span> <span id="dx1-254006"></span><span class="cmss-10x-x-109">general multivariable functions; we just have</span> <span id="dx1-254007"></span><span class="cmss-10x-x-109">to write much more. There, the partial derivative of </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">at the point </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">with respect to the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th variable is defined by</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(21).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(16.1)
  </div>
</div>
<p><span class="cmss-10x-x-109">One of the biggest challenges in multivariable calculus is to manage the ever-growing notational complexity. Just take a look at the difference quotient above:</span></p>
<div class="math-display">
<img src="../media/file1468.png" class="math-display" alt="f(x1,...,x,...,xn)-−-f(x1,...,xi,...,xn). x − xi "/>
</div>
<p><span class="cmss-10x-x-109">This is</span> <span id="dx1-254008"></span><span class="cmss-10x-x-109">not the prettiest to look at, and this kind of notational complexity can pile up fast. Fortunately, linear algebra comes to the rescue! Not only can we compact the variables into the vector </span><span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">, we can use the standard basis</span></p>
<div class="math-display">
<img src="../media/file1469.png" class="math-display" alt="ei = (0,...,0, 1 ,0,...,0) ◟◝◜◞ i- th component "/>
</div>
<p><span class="cmss-10x-x-109">to write the difference quotients as</span></p>
<div class="math-display">
<img src="../media/file1470.png" class="math-display" alt="f(x+-hei)-−-f(x), h ∈ ℝ. h "/>
</div>
<p><span class="cmss-10x-x-109">Thus, (</span><a href="#"><span class="cmss-10x-x-109">19.1</span></a><span class="cmss-10x-x-109">) can be compacted. With this newly found form, we are ready to make a concise and formal definition for partial derivatives.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-254009r66"></span> <span class="cmbx-10x-x-109">Definition 66.</span> </span><span class="cmbx-10x-x-109">(Partial derivatives)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be a function of <span class="cmmi-10x-x-109">n </span>variables. The partial derivative of <span class="cmmi-10x-x-109">f </span>at the point <span class="cmbx-10x-x-109">x </span>= (<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>) with respect to the <span class="cmmi-10x-x-109">i</span>-th variable is defined by</p>
<div class="math-display">
<img src="../media/file1471.png" class="math-display" alt="-∂f-(x ) = lim f(x-+-hei)−-f-(x-). ∂xi h→0 h "/>
</div>
<p>If the above limit exists, we say that <span class="cmmi-10x-x-109">f </span>is <span class="cmti-10x-x-109">partially differentiable </span>with respect to the <span class="cmmi-10x-x-109">i</span>-th variable <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub>.</p>
</div>
<p><span class="cmss-10x-x-109">The</span> <span id="dx1-254010"></span><span class="cmss-10x-x-109">partial derivative is again a</span><span id="dx1-254011"></span> <span class="cmss-10x-x-109">vector-scalar function. Because of this, it is often written as</span> <img src="../media/file1472.png" width="15" data-align="middle" alt="-∂- ∂xi"/><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">, reflecting on the fact that the symbol</span> <img src="../media/file1473.png" width="15" data-align="middle" alt="∂-- ∂xi"/> <span class="cmss-10x-x-109">can be thought of as a </span><span class="cmssi-10x-x-109">function </span><span class="cmss-10x-x-109">that maps functions to functions. I know, this is a bit abstract, but you’ll get used to it quickly.</span></p>
<p><span class="cmss-10x-x-109">As usual, there are several alternative notations for the partial derivatives. Among others, the symbols</span></p>
<ul>
<li><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">x</span><sub><span class="cmmi-6">i</span></sub></sub>(<span class="cmbx-10x-x-109">x</span>)<span class="cmss-10x-x-109">,</span></li>
<li><span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmss-10x-x-109">,</span></li>
<li><span class="cmmi-10x-x-109">∂</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)</li>
</ul>
<p><span class="cmss-10x-x-109">denote the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th partial</span> <span id="dx1-254012"></span><span class="cmss-10x-x-109">derivative of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">at </span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">. For simplicity, we’ll use the old-school</span> <img src="../media/file1474.png" width="15" data-align="middle" alt="-∂f ∂xi"/>(<span class="cmbx-10x-x-109">x</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">It’s best to start with a few examples to illustrate the concept of partial derivatives.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">Let</span></p>
<div class="math-display">
<img src="../media/file1475.png" class="math-display" alt="f(x1,x2) = x21 + x22. "/>
</div>
<p><span class="cmss-10x-x-109">To calculate, say, </span><span class="cmmi-10x-x-109">∂f∕∂x</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, we fix the second variable and treat </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">as a constant. Formally, we obtain the single-variable function</span></p>
<div class="math-display">
<img src="../media/file1476.png" class="math-display" alt=" 1 2 2 f (x) := x + x2, x2 ∈ ℝ, "/>
</div>
<p><span class="cmss-10x-x-109">whose derivative gives the first partial derivative:</span></p>
<div class="math-display">
<img src="../media/file1477.png" class="math-display" alt=" 1 ∂f-(x ,x ) = df-(x ) = 2x . ∂x1 1 2 dx 1 1 "/>
</div>
<p><span class="cmss-10x-x-109">Similarly, we get that</span></p>
<div class="math-display">
<img src="../media/file1478.png" class="math-display" alt="∂f ∂x-(x1,x2) = 2x2. 2 "/>
</div>
<p><span class="cmss-10x-x-109">Once you are comfortable with the mental gymnastics of fixing variables, you’ll be able to perform partial differentiation without writing out all the intermediate steps.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">Let’s see a more complicated example. Define</span></p>
<div class="math-display">
<img src="../media/file1479.png" class="math-display" alt="f(x1,x2) = sin(x21 + x2). "/>
</div>
<p><span class="cmss-10x-x-109">By fixing </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, we obtain a composite function. Thus the chain rule is used to calculate the first partial derivative:</span></p>
<div class="math-display">
<img src="../media/file1480.png" class="math-display" alt=" ∂f ----(x1,x2 ) = 2x1 cos(x21 + x2). ∂x1 "/>
</div>
<p><span class="cmss-10x-x-109">Similarly, we obtain that</span></p>
<div class="math-display">
<img src="../media/file1481.png" class="math-display" alt="∂f-- 2 ∂x2(x1,x2) = cos(x1 + x2). "/>
</div>
<p><span class="cmss-10x-x-109">(I highly advise you to carry out the above calculations step by step as an exercise, even if you understand all the intermediate steps.)</span></p>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmss-10x-x-109">Finally, let’s see a function that is partially differentiable in</span><span id="dx1-254013"></span> <span class="cmss-10x-x-109">one variable but not in the other. Define the function</span></p>
<div class="math-display">
<img src="../media/file1482.png" class="math-display" alt=" ( |{ − 1 if x2 &lt;0, f(x1,x2) = |( 1 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">As </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">does</span> <span id="dx1-254014"></span><span class="cmss-10x-x-109">not depend on </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmss-10x-x-109">, we</span> <span id="dx1-254015"></span><span class="cmss-10x-x-109">can see that by fixing </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, the resulting function is constant. Thus,</span></p>

<img src="../media/file1483.png" width="150" class="math-display" alt="-∂f-(x ,x ) = 0 ∂x1 1 2 "/>

<p><span class="cmss-10x-x-109">holds everywhere. However, in </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, there is a discontinuity at </span>0<span class="cmss-10x-x-109">; thus,</span> <img src="../media/file1484.png" width="15" data-align="middle" alt="∂f- ∂x2"/> <span class="cmss-10x-x-109">is undefined there.</span></p>
<section id="the-gradient" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_225"><span class="titlemark"><span class="cmss-10x-x-109">16.1.1 </span></span> <span id="x1-25500019.1.1"></span><span class="cmss-10x-x-109">The gradient</span></h3>
<p><span class="cmss-10x-x-109">If a</span><span id="dx1-255001"></span> <span class="cmss-10x-x-109">function is partially differentiable in every variable, we can compact the derivatives together in a single vector to form the </span><span class="cmssi-10x-x-109">gradient</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-255002r67"></span> <span class="cmbx-10x-x-109">Definition 67.</span> </span><span class="cmbx-10x-x-109">(The gradient)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℝ </span>be a function that is partially differentiable in all of its variables. Then, its <span class="cmti-10x-x-109">gradient </span>is defined by the (column) vector</p>
<img src="../media/file1485.png" width="150" class="math-display" alt=" ⌊ ⌋ ∂∂x1f (x ) ||-∂-f (x )|| ∇f(x ) := ||∂x2 . || ∈ ℝn ×1. |⌈ .. |⌉ -∂- ∂xnf (x ) "/>

</div>
<p><span class="cmss-10x-x-109">A few remarks are in order. First, the symbol </span><span class="cmsy-10x-x-109">∇ </span><span class="cmss-10x-x-109">is called </span><span class="cmssi-10x-x-109">nabla</span><span class="cmss-10x-x-109">, a symbol that was conceived to denote gradients.</span></p>
<p><span class="cmss-10x-x-109">Second, the</span> <span id="dx1-255003"></span><span class="cmss-10x-x-109">gradient can be thought of as a vector-vector function. To see that, consider the already familiar function </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">. The gradient of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is</span></p>
<img src="../media/file1486.png" width="150" class="math-display" alt=" ⌊ ⌋ 2x1 ∇f (x1,x2) = ⌈ ⌉ , 2x2 "/>

<p><span class="cmss-10x-x-109">or</span></p>

<img src="../media/file1487.png" width="105" class="math-display" alt="∇f (x ) = 2x "/>

<p><span class="cmss-10x-x-109">in vectorized form. We can visualize this by drawing the vector </span><span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmss-10x-x-109">at each point </span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1488.png" width="441" alt="PIC"/> <span id="x1-255004r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 16.2: The vector field given by the gradient of </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><sup><span class="cmr-8">2</span></sup> </span>
</div>
<p><span class="cmss-10x-x-109">Thus, you can think about </span><span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">as a vector-vector function </span><span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. The gradient at a given point </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is obtained by evaluating this function, yielding</span> (<span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>)(<span class="cmbx-10x-x-109">x</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">For clarity, the parentheses are omitted, arriving at the all familiar notation </span><span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>)<span class="cmss-10x-x-109">.</span></p>
</section>
<section id="higher-order-partial-derivatives" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_226"><span class="titlemark"><span class="cmss-10x-x-109">16.1.2 </span></span> <span id="x1-45600019.1.2"></span><span class="cmss-10x-x-109">Higher order partial derivatives</span></h3>
<p><span class="cmss-10x-x-109">The</span> <span id="dx1-456001"></span><span class="cmss-10x-x-109">partial derivatives of a vector-scalar function </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">are vector-scalar functions themselves. Thus, we can perform partial differentiation one more time!</span></p>
<p><span class="cmss-10x-x-109">If they exist, the second order partial derivatives are defined by</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(22).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(16.2)
  </div>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is an arbitrary vector. (When the second partial differentiation takes place with respect to the same variable, (</span><a href="ch026.xhtml#higher-order-partial-derivatives"><span class="cmss-10x-x-109">16.2</span></a><span class="cmss-10x-x-109">) is abbreviated by</span> <img src="../media/file1494.png" width="15" data-align="middle" alt="∂2f ∂x2i"/>(<span class="cmbx-10x-x-109">a</span>)<span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">The definition begs the question: is the order of differentiation interchangeable? That is, does</span></p>
<div class="math-display">
<img src="../media/file1495.png" class="math-display" alt=" 2 2 --∂-f--(a) = -∂-f--(a) ∂xi∂xj ∂xj∂xi "/>
</div>
<p><span class="cmss-10x-x-109">hold? The answer is quite surprising: the order is interchangeable under some mild assumptions, but not in the general case. There is a famous theorem about it which we won’t prove, but it’s essential to know.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-456002r98"></span> <span class="cmbx-10x-x-109">Theorem 98.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be an arbitrary vector-scalar function and let </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">. If there is a small ball </span><span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>) <span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">centered at </span><span class="cmbx-10x-x-109">a </span><span class="cmti-10x-x-109">such that </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">has continuous second-order partial derivatives at all points of </span><span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>)<span class="cmti-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file1496.png" class="math-display" alt="--∂2f-- -∂2f--- ∂xi ∂xj(a) = ∂xj∂xi(a) "/>
</div>
<p><span class="cmti-10x-x-109">holds for all </span><span class="cmmi-10x-x-109">i </span>= 1<span class="cmmi-10x-x-109">,…,n</span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-456002r98"><span class="cmssi-10x-x-109">98</span></a> <span class="cmss-10x-x-109">is known as either Schwarz’s theorem, Clairaut’s theorem, or Young’s theorem.</span></p>
</section>
<section id="the-total-derivative" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_227"><span class="titlemark"><span class="cmss-10x-x-109">16.1.3 </span></span> <span id="x1-25700019.1.3"></span><span class="cmss-10x-x-109">The total derivative</span></h3>
<p><span class="cmss-10x-x-109">Partial</span> <span id="dx1-257001"></span><span class="cmss-10x-x-109">derivatives seem to generalize the notion of differentiability for multivariable functions. However, something is missing. Let’s revisit the single-variable case for a moment.</span></p>
<p><span class="cmss-10x-x-109">Recall that according to </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch020.xhtml#x1-199002r77"><span class="cmssi-10x-x-109">77</span></a><span class="cmss-10x-x-109">, the differentiability of a single-variable function </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">at a given point </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">is equivalent to a local approximation of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">by the linear function</span></p>
<div class="math-display">
<img src="../media/file1497.png" class="math-display" alt="l(x ) = f(a) + f′(a)(x − a). "/>
</div>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">is close to </span><span class="cmmi-10x-x-109">a</span><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">l</span>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">is also close to </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">. Moreover, this is the best linear approximation we can do around </span><span class="cmmi-10x-x-109">a</span><span class="cmss-10x-x-109">. In a single variable, this is </span><span class="cmssi-10x-x-109">equivalent </span><span class="cmss-10x-x-109">to differentiation.</span></p>
<p><span class="cmss-10x-x-109">This</span> <span id="dx1-257002"></span><span class="cmss-10x-x-109">gives us an idea: even though difference quotients like</span> <img src="../media/file1498.png" class="frac" data-align="middle" alt="f(x)−f(y) x−y"/> <span class="cmss-10x-x-109">do not exist in multiple variables, the best local approximation with a multivariable linear function does!</span></p>
<p><span class="cmss-10x-x-109">Thus, the notion of total differentiability is born.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-257003r68"></span> <span class="cmbx-10x-x-109">Definition 68.</span> </span><span class="cmbx-10x-x-109">(Total differentiability)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be a function of <span class="cmmi-10x-x-109">n </span>variables. We say that <span class="cmmi-10x-x-109">f </span>is <span class="cmti-10x-x-109">totally differentiable</span> (or sometimes just <span class="cmti-10x-x-109">differentiable </span>for short) at <span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> if there exists a row vector <span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">1</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> such that</p>
<div class="math-display">
  <span>
    f(x) = f(a) + D<sub>f</sub>(a)(x − a) + o(∥x − a∥)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(16.3)</span>
</div>

<p>holds for all <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>), where <span class="cmmi-10x-x-109">𝜀/span&gt;0 and <span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>) is defined by </span></p>
<div class="math-display">
<img src="../media/file1499.png" class="math-display" alt="B(𝜀,a) = {x ∈ ℝn : ∥x − a∥} &lt;𝜀. "/>
</div>
<p>(In other words, <span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>) is a ball of radius <span class="cmmi-10x-x-109">𝜀/span&gt;0 around <span class="cmbx-10x-x-109">a</span>.) When exists, the vector <span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) is called the <span class="cmti-10x-x-109">total derivative </span>of <span class="cmmi-10x-x-109">f </span>at <span class="cmbx-10x-x-109">a</span>. </span></p>
</div>
<p><span class="cmss-10x-x-109">Recall that when it is not stated explicitly, we use </span><span class="cmssi-10x-x-109">column vectors</span><span class="cmss-10x-x-109">, because we want to write our linear transformations in the form </span><span class="cmmi-10x-x-109">A</span><span class="cmbx-10x-x-109">x</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">and </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">×</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">. Thus, the “dimensionology” of the formula</span></p>
<div class="math-display">
<img src="../media/file1500.png" class="math-display" alt=" f(x) = f(a) + Df (a )(x− a )+o(∥x − a∥) ∈ ℝ1×1 ◟◝1◜×◞1 ◟◝◜1◞×1 ◟-◝1◜× ◞n ◟-◝◜n×◞1 ∈ℝ ∈ℝ ∈ℝ ∈ℝ "/>
</div>
<p><span class="cmss-10x-x-109">works out. (Don’t be fooled, </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">1</span><span class="cmsy-8">×</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is a scalar.)</span></p>
<p><span class="cmss-10x-x-109">Let’s unravel the notion of total differentiability. The form (</span><a href="ch026.xhtml#x1-257003r68"><span class="cmss-10x-x-109">16.3</span></a><span class="cmss-10x-x-109">) implies that a totally differentiable function </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">equals to the linear part </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>) + <span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>)(<span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">plus a small error.</span></p>
<p><span class="cmss-10x-x-109">The surface given by the linear part is called the </span><span class="cmssi-10x-x-109">tangent plane</span><span class="cmss-10x-x-109">. We can visualize it for functions of two variables.</span></p>
<div class="minipage">
<p><img src="../media/file1501.png" width="484" alt="PIC"/> <span id="x1-257004r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 16.3: The tangent plane</span> </span>
</div>
<p><span class="cmss-10x-x-109">Unsurprisingly, the</span> <span id="dx1-257005"></span><span class="cmss-10x-x-109">partial and total derivatives share an intimate connection.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-257006r99"></span> <span class="cmbx-10x-x-109">Theorem 99.</span> </span><span class="cmbxti-10x-x-109">(Total derivative and the partial derivatives)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be a function that is totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">. Then, all of its partial derivatives exist at </span><span class="cmbx-10x-x-109">a </span><span class="cmti-10x-x-109">and</span></p>
<div class="math-display">
  <span>
    f(x) = f(a) + ∇f(a)<sup>T</sup> (x − a) + o(∥x − a∥)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(16.4)</span>
</div>

<p><span class="cmti-10x-x-109">holds for all </span><span class="cmbx-10x-x-109">a </span><span class="cmti-10x-x-109">in some </span><span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>)<span class="cmti-10x-x-109">, </span><span class="cmmi-10x-x-109">𝜀/span&gt;0<span class="cmti-10x-x-109">. (That is, </span><span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) = <span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>)<sup><span class="cmmi-8">T</span></sup> <span class="cmti-10x-x-109">.)</span> </span></p>
</div>
<p><span class="cmss-10x-x-109">In other words, the equation (</span><a href="ch026.xhtml#x1-257006r99"><span class="cmss-10x-x-109">16.4</span></a><span class="cmss-10x-x-109">) gives that the coefficients of the best linear approximation are equal to the partial derivatives.</span></p>
<div id="tcolobox-278" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Because </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">a</span><span class="cmss-10x-x-109">, the definition gives that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">can be written in the form</span></p>
<div class="math-display">
<img src="../media/file1502.png" class="math-display" alt="f(x) = f(a)+ Df (a)(x− a)+ o(∥x − a∥), "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) = (<span class="cmmi-10x-x-109">d</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,d</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">is the vector that describes the coefficients of the linear part.</span></p>
<p><span class="cmss-10x-x-109">Our goal is to show that</span></p>
<div class="math-display">
<img src="../media/file1503.png" class="math-display" alt=" f-(a-+-hei)−-f-(a) hli→m0 h = di, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the unit (column) vector whose </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th component is </span>1<span class="cmss-10x-x-109">, while the others are </span>0<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Let’s do a quick calculation! Based on what we know, we have</span></p>
<div class="math-display">
<img src="../media/file1504.png" class="math-display" alt="f(a-+-hei)−-f(a)-= Df-(a)hei +-o(∥hei∥) h h = Df (a)ei + o(1) = di + o(1), "/>
</div>
<p><span class="cmss-10x-x-109">thus confirming that</span> lim<sub><span class="cmmi-8">h</span><span class="cmsy-8">→</span><span class="cmr-8">0</span></sub><img src="../media/file1505.png" class="frac" data-align="middle" alt="f(a+hei)−-f(a)- h"/> = <span class="cmmi-10x-x-109">d</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">, which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">What’s all the hassle with total differentiation, then? </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-257006r99"><span class="cmssi-10x-x-109">99</span></a> <span class="cmss-10x-x-109">tells us that</span> <span id="dx1-257007"></span><span class="cmss-10x-x-109">total differentiability is a stronger condition than partial differentiability.</span></p>
<p><span class="cmss-10x-x-109">Surprisingly, the other direction is not true: the existence of partial derivatives does not imply total differentiability, as the example</span></p>
<div class="math-display">
<img src="../media/file1506.png" class="math-display" alt=" ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise "/>
</div>
<p><span class="cmss-10x-x-109">illustrates. This function has all its partial derivatives at </span>0<span class="cmss-10x-x-109">, yet the total derivative does not exist. (You can convince yourself by either drawing a figure, or noting that the function </span>1 <span class="cmsy-10x-x-109">−</span><span class="cmbx-10x-x-109">d</span><sup><span class="cmmi-8">T</span></sup> <span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">can never be </span><span class="cmmi-10x-x-109">o</span>(<span class="cmsy-10x-x-109">∥</span><span class="cmbx-10x-x-109">x</span><span class="cmsy-10x-x-109">∥</span>)<span class="cmss-10x-x-109">, regardless</span> <span id="dx1-257008"></span><span class="cmss-10x-x-109">of the choice of </span><span class="cmbx-10x-x-109">d</span><span class="cmss-10x-x-109">.)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-257009r11"></span> <span class="cmbx-10x-x-109">Remark 11.</span> </span><span class="cmbx-10x-x-109">(The total derivative as an operator)</span></p>
<p>Just like for single-variable functions, the total derivative of <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>is a function <span class="cmmi-10x-x-109">D</span><sub><span class="cmmi-8">f</span></sub> : <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>.</p>
<p>At the highest level of abstraction, we can think about the total derivative as an <span class="cmti-10x-x-109">operator </span>that maps a vector-scalar function to a vector-vector function:</p>
<div class="math-display">
<img src="../media/file1507.png" class="math-display" alt=" n ℝ n ℝn D : (ℝ ) → (ℝ ) , D : f ↦→ D , f "/>
</div>
<p>where <span class="cmmi-10x-x-109">A</span><sup><span class="cmmi-8">B</span></sup> denotes the set of all functions mapping <span class="cmmi-10x-x-109">A </span>to <span class="cmmi-10x-x-109">B</span>.</p>
<p>You are not <span class="cmti-10x-x-109">required </span>to understand this at all, but trust me, the more abstract your thinking is, the more powerful you’ll be.</p>
</div>
</section>
<section id="directional-derivatives" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_228"><span class="titlemark"><span class="cmss-10x-x-109">16.1.4 </span></span> <span id="x1-25800019.1.4"></span><span class="cmss-10x-x-109">Directional derivatives</span></h3>
<p><span class="cmss-10x-x-109">So far, we</span> <span id="dx1-258001"></span><span class="cmss-10x-x-109">have talked about two kinds of derivatives: partial derivatives that describe the rate of change along a fixed axis, and total derivatives that give the best linear approximation of the function at a given point.</span></p>
<p><span class="cmss-10x-x-109">Partial derivatives are only concerned with a few particular directions. However, this is not the end of the story in multiple variables. With the standard orthonormal basis vectors </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">, the partial derivatives are defined by</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(23).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(16.5)
  </div>
</div>
<p><span class="cmss-10x-x-109">As we saw earlier, these describe the rate of change along the dimensions. However, the standard orthonormal vectors are just a few special directions.</span></p>
<p><span class="cmss-10x-x-109">What about an arbitrary direction </span><span class="cmbx-10x-x-109">v</span><span class="cmss-10x-x-109">? Can we define the derivative along these? Sure! There is nothing stopping us from replacing </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">with </span><span class="cmbx-10x-x-109">v </span><span class="cmss-10x-x-109">in (</span><a href="ch026.xhtml#directional-derivatives"><span class="cmss-10x-x-109">16.5</span></a><span class="cmss-10x-x-109">). Thus, directional derivatives are born.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-258002r69"></span> <span class="cmbx-10x-x-109">Definition 69.</span> </span><span class="cmbx-10x-x-109">(Directional derivatives)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be a function of <span class="cmmi-10x-x-109">n </span>variables and let <span class="cmbx-10x-x-109">v </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> be an arbitrary vector. The <span class="cmti-10x-x-109">directional derivative </span>of <span class="cmmi-10x-x-109">f </span>along <span class="cmbx-10x-x-109">v </span>is defined by the limit</p>
<div class="math-display">
<img src="../media/file1510.png" class="math-display" alt="∂f f(a + hv) − f(a) ∂v-:= lhim→0 -------h--------. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Good news: the directional derivatives can be described in terms of the gradient!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-258003r100"></span> <span class="cmbx-10x-x-109">Theorem 100.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be a function of </span><span class="cmmi-10x-x-109">n </span><span class="cmti-10x-x-109">variables. If </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">, then its directional derivatives exist in all directions, and</span></p>
<img src="../media/file1511.png" width="150" class="math-display" alt="∂f(a) = ∇f (a)Tv. ∂v "/>
</div>
<div id="tcolobox-279" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Because of the total differentiability, </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-263004r103"><span class="cmssi-10x-x-109">103</span></a> <span class="cmss-10x-x-109">gives that</span></p>
<div class="math-display">
<img src="../media/file1512.png" class="math-display" alt="f (x ) = f (a )+ ∇f (a)T(x − a)+ o(∥x − a∥) "/>
</div>
<p><span class="cmss-10x-x-109">around </span><span class="cmbx-10x-x-109">a</span><span class="cmss-10x-x-109">. Thus,</span></p>
<div class="math-display">
<img src="../media/file1513.png" class="math-display" alt="f(a+ hv )− f (a ) h∇f (a)Tv + o(h) ----------------= ---------------- h h = ∇f (a)Tv + o(1), "/>
</div>
<p><span class="cmss-10x-x-109">giving that</span></p>
<div class="math-display">
<img src="../media/file1514.png" class="math-display" alt="∂f-(a) = lim f-(a-+-hv-)−-f(a) ∂v h→0 h = lim ∇f (a)Tv + o(1) h→0 = ∇f (a)T v, "/>
</div>
<p><span class="cmss-10x-x-109">as we needed to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">In</span> <span id="dx1-258004"></span><span class="cmss-10x-x-109">other words, </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-258003r100"><span class="cmssi-10x-x-109">100</span></a> <span class="cmss-10x-x-109">gives that no matter the direction </span><span class="cmbx-10x-x-109">v</span><span class="cmss-10x-x-109">, the directional derivative can be written in terms of the gradient and </span><span class="cmbx-10x-x-109">v</span><span class="cmss-10x-x-109">. If you think about this for a minute, this is quite amazing: the rates of change along </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">special directions determine the rate of change in </span><span class="cmssi-10x-x-109">any </span><span class="cmss-10x-x-109">other direction.</span></p>
</section>
<section id="properties-of-the-gradient" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_229"><span class="titlemark"><span class="cmss-10x-x-109">16.1.5 </span></span> <span id="x1-25900019.1.5"></span><span class="cmss-10x-x-109">Properties of the gradient</span></h3>
<p><span class="cmss-10x-x-109">In one variable, we</span> <span id="dx1-259001"></span><span class="cmss-10x-x-109">have learned that if the derivative of </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is positive at some </span><span class="cmmi-10x-x-109">a</span><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is increasing around </span><span class="cmmi-10x-x-109">a</span><span class="cmss-10x-x-109">. (If the derivative is negative, </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is decreasing.) If we think about the derivative </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">′</span></sup>(<span class="cmmi-10x-x-109">a</span>) <span class="cmss-10x-x-109">as a one-dimensional vector, then the derivative points towards the direction of increase.</span></p>
<p><span class="cmss-10x-x-109">Is this true in higher dimensions? Yes, and this is what makes gradient descent work.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-259002r101"></span> <span class="cmbx-10x-x-109">Theorem 101.</span> </span><span class="cmbxti-10x-x-109">(The gradient determines the direction of the increase)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be a function of </span><span class="cmmi-10x-x-109">n </span><span class="cmti-10x-x-109">variables, and suppose that </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">Then</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(24).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(16.6)
  </div>
</div>
</div>
<p><span class="cmss-10x-x-109">I know, (</span><a href="ch026.xhtml#x1-259002r101"><span class="cmss-10x-x-109">16.6</span></a><span class="cmss-10x-x-109">) is pretty overloaded, so let’s unpack it. First, let’s start with the mysterious</span> argmax<span class="cmss-10x-x-109">. For a given function </span><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">,</span></p>

<img src="../media/file1517.png" width="150" class="math-display" alt="argmaxx ∈Sf(x) "/>

<p><span class="cmss-10x-x-109">denotes the values that maximize </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">on the set </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">. As the maximum may not be unique,</span> argmax <span class="cmss-10x-x-109">can yield a set. (The definition of</span> argmin <span class="cmss-10x-x-109">is the same, but with minimum instead of maximum.)</span></p>
<p><span class="cmss-10x-x-109">Thus, in English, (</span><a href="ch026.xhtml#x1-259002r101"><span class="cmss-10x-x-109">16.6</span></a><span class="cmss-10x-x-109">) states that the unit direction that maximizes the directional derivative at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the normalized gradient. Now we are</span> <span id="dx1-259003"></span><span class="cmss-10x-x-109">ready to see the proof!</span></p>
<div id="tcolobox-280" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Do you remember the Cauchy-Schwarz inequality (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch008.xhtml#x1-43003r8"><span class="cmssi-10x-x-109">8</span></a><span class="cmss-10x-x-109">)? It was a long time ago, so let’s recall! In the vector space </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, the Cauchy-Schwarz inequality tells us that for any </span><span class="cmbx-10x-x-109">x</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file1518.png" class="math-display" alt="xT y ≤ ∥x∥∥y∥. "/>
</div>
<p><span class="cmss-10x-x-109">Now, as </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-258003r100"><span class="cmssi-10x-x-109">100</span></a> <span class="cmss-10x-x-109">implies, the directional derivatives can be written as</span></p>
<div class="math-display">
<img src="../media/file1519.png" class="math-display" alt="∂f- T ∂v(a) = ∇f (a) v. "/>
</div>
<p><span class="cmss-10x-x-109">Combined with the Cauchy-Schwarz inequality, we get that</span></p>
<div class="math-display">
<img src="../media/file1520.png" class="math-display" alt="∂f ---(a) = ∇f (a)T v ∂v ≤ ∥ ∇f (a )∥∥v ∥. "/>
</div>
<p><span class="cmss-10x-x-109">By restricting the directions to unit vectors,</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(25).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(16.7)
  </div>
</div>
<p><span class="cmss-10x-x-109">follows. Thus, the directional derivatives must be less than or equal to the gradient’s norm. (At least, along a direction vector with unit length.)</span></p>
<p><span class="cmss-10x-x-109">However, by letting </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">0</span></sub> = <span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>)<span class="cmmi-10x-x-109">∕</span><span class="cmsy-10x-x-109">∥∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>)<span class="cmsy-10x-x-109">∥</span><span class="cmss-10x-x-109">, we obtain that</span></p>
<div class="math-display">
<img src="../media/file1522.png" class="math-display" alt="∂f--(a ) = ∇f (a)Tv0 ∂v0 ∇f (a)T∇f (a) = ---∥∇f-(a)∥-- 2 = ∥∇f-(a)∥- ∥∇f (a)∥ = ∥∇f (a)∥. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, with the choice </span><span class="cmbx-10x-x-109">v</span><sub><span class="cmr-8">0</span></sub> = <img src="../media/file1523.png" class="frac" data-align="middle" alt="-∇f(a)-"/><span class="cmss-10x-x-109">, equality can be attained in (</span><a href="#"><span class="cmss-10x-x-109">16.7</span></a><span class="cmss-10x-x-109">). This means that</span> <img src="../media/file1524.png" class="frac" data-align="middle" alt=""/> <span class="cmss-10x-x-109">maximizes the directional derivative at </span><span class="cmbx-10x-x-109">a</span><span class="cmss-10x-x-109">, which is what we had to prove.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">With that, we have the basics of differentiation in multiple variables under our belt. To sum up, we have learned that the difference quotient definition of</span><span id="dx1-259004"></span> <span class="cmss-10x-x-109">the derivative does not generalize directly for multiple variables, but we can fix all but one variables to make the difference quotient work, thus obtaining partial derivatives.</span></p>
<p><span class="cmss-10x-x-109">On the other hand, the linear approximation definition works in multiple dimensions, but instead of</span></p>
<div class="math-display">
<img src="../media/file1525.png" class="math-display" alt=" ′ f(a)+ f (a)(x− a), f : ℝ → ℝ, x,a ∈ ℝ, "/>
</div>
<p><span class="cmss-10x-x-109">like we had in one variable, we obtain</span></p>
<div class="math-display">
<img src="../media/file1526.png" class="math-display" alt="f (a )+ ∇f (a)T(x− a), f : ℝn → ℝ, x,a ∈ ℝn, "/>
</div>
<p><span class="cmss-10x-x-109">where the analogue of the derivative is the gradient vector </span><span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>) <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Even when we were studying differentiation in one variable for the first time, I told you that the local linear approximation definition would be useful someday. That time is now, and we are reaping the benefits. Soon, we’ll see gradient descent in its full glory.</span></p>
</section>
</section>
<section id="derivatives-of-vectorvalued-functions" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_230"><span class="titlemark"><span class="cmss-10x-x-109">16.2 </span></span> <span id="x1-26000019.2"></span><span class="cmss-10x-x-109">Derivatives of vector-valued functions</span></h2>
<p><span class="cmss-10x-x-109">In a single variable, defining higher-order derivatives is easy. We simply have</span> <span id="dx1-260001"></span><span class="cmss-10x-x-109">to keep repeating differentiation:</span></p>
<div class="math-display">
<img src="../media/file1527.png" class="math-display" alt=" ′′ ′ ′ f (x) = (f (x)), f′′′(x) = (f′′(x))′, "/>
</div>
<p><span class="cmss-10x-x-109">and so on. However, this is not that straightforward with multivariable functions.</span></p>
<p><span class="cmss-10x-x-109">So far, we have only talked about gradients, the generalization of the derivative for vector-scalar functions.</span></p>
<p><span class="cmss-10x-x-109">As </span><span class="cmsy-10x-x-109">∇</span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">is a column vector, the gradient is a vector-vector function </span><span class="cmsy-10x-x-109">∇ </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. We only know how to compute the derivative of vector-scalar functions. It’s time to change that!</span></p>
<section id="the-derivatives-of-curves" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_231"><span class="titlemark"><span class="cmss-10x-x-109">16.2.1 </span></span> <span id="x1-26100019.2.1"></span><span class="cmss-10x-x-109">The derivatives of curves</span></h3>
<p><span class="cmss-10x-x-109">Curves, often describing the solutions of dynamical systems, are one of the most</span> <span id="dx1-261001"></span><span class="cmss-10x-x-109">important objects in mathematics. We don’t explicitly use them in machine learning, but they are underneath algorithms such as gradient descent. (Where we traverse a discretized curve leading to a local minimum.)</span></p>
<p><span class="cmss-10x-x-109">Formally, a curve – that is, a scalar-vector function – is given by a function</span></p>
<div class="math-display">
<img src="../media/file1528.png" class="math-display" alt=" ⌊γ (t)⌋ | 1 | n ||γ2(t)|| n(×1) γ : ℝ → ℝ , γ (t) = || .. || ∈ ℝ , ⌈ . ⌉ γn(t) "/>
</div>
<p><span class="cmss-10x-x-109">where the </span><span class="cmmi-10x-x-109">γ</span><sub><span class="cmmi-8">i</span></sub> : <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">functions are good old single-variable scalar-scalar functions. As the independent variable</span><span id="dx1-261002"></span> <span class="cmss-10x-x-109">often represents time, it is customary to denote it with </span><span class="cmmi-10x-x-109">t</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">We can differentiate </span><span class="cmmi-10x-x-109">γ </span><span class="cmss-10x-x-109">componentwise:</span></p>
<div class="math-display">
<img src="../media/file1529.png" class="math-display" alt=" ⌊ ⌋ γ′1(t) ||γ′(t)|| γ′(t) := || 2. || ∈ ℝn(×1). |⌈ .. |⌉ ′ γn(t) "/>
</div>
<p><span class="cmss-10x-x-109">If we indeed imagine </span><span class="cmmi-10x-x-109">γ</span>(<span class="cmmi-10x-x-109">t</span>) <span class="cmss-10x-x-109">as a trajectory in space, </span><span class="cmmi-10x-x-109">γ</span><sup><span class="cmsy-8">′</span></sup>(<span class="cmmi-10x-x-109">t</span>) <span class="cmss-10x-x-109">is the tangent vector to </span><span class="cmmi-10x-x-109">γ </span><span class="cmss-10x-x-109">at </span><span class="cmmi-10x-x-109">t</span><span class="cmss-10x-x-109">. Since the differentiation is componentwise, </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch020.xhtml#x1-199002r77"><span class="cmssi-10x-x-109">77</span></a> <span class="cmss-10x-x-109">implies that if </span><span class="cmmi-10x-x-109">γ </span><span class="cmss-10x-x-109">is differentiable at some </span><span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">,</span></p>
<div class="math-display"> 
  <span>
    γ(t) = γ(a) + γ′(t)<sup>T</sup> (t − a) + o(|t − a|)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(16.8)</span>
</div>

<p><span class="cmss-10x-x-109">there. The equation (</span><a href="ch026.xhtml#the-derivatives-of-curves"><span class="cmss-10x-x-109">16.8</span></a><span class="cmss-10x-x-109">) is a true vectorized formula: some components are vectors, and some are scalars. Yet, this is simple and makes perfect sense to us. Hiding the complexities of vectors and matrices is the true power of linear algebra.</span></p>
<p><span class="cmss-10x-x-109">It is easy to see that for any two curves </span><span class="cmmi-10x-x-109">γ,η </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, differentiation is additive, as</span> (<span class="cmmi-10x-x-109">γ </span>+ <span class="cmmi-10x-x-109">η</span>)<sup><span class="cmsy-8">′</span></sup> = <span class="cmmi-10x-x-109">γ</span><sup><span class="cmsy-8">′</span></sup> + <span class="cmmi-10x-x-109">η</span><sup><span class="cmsy-8">′</span></sup><span class="cmss-10x-x-109">. What happens when we compose a scalar-vector function with a vector-scalar one?</span></p>
<p><span class="cmss-10x-x-109">This situation is commonplace in machine learning. If, say, </span><span class="cmmi-10x-x-109">L </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">describes the loss function and </span><span class="cmmi-10x-x-109">γ </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is our trajectory in the parameter space </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">, the composite function </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">γ</span>(<span class="cmmi-10x-x-109">t</span>)) <span class="cmss-10x-x-109">describes the model loss at time </span><span class="cmmi-10x-x-109">t</span><span class="cmss-10x-x-109">. Thus, to compute</span> (<span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmmi-10x-x-109">γ</span>)<sup><span class="cmsy-8">′</span></sup><span class="cmss-10x-x-109">, we have to generalize the chain rule.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-261003r102"></span> <span class="cmbx-10x-x-109">Theorem 102.</span> </span><span class="cmbxti-10x-x-109">(The chain rule for composing scalar-vector and vector-scalar functions)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">γ </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be arbitrary functions. If </span><span class="cmmi-10x-x-109">γ </span><span class="cmti-10x-x-109">is differentiable at some </span><span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">f </span><span class="cmti-10x-x-109">is differentiable at </span><span class="cmmi-10x-x-109">γ</span>(<span class="cmmi-10x-x-109">a</span>)<span class="cmti-10x-x-109">, then </span><span class="cmmi-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmmi-10x-x-109">γ </span>: <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">is also differentiable at </span><span class="cmmi-10x-x-109">a </span><span class="cmti-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1530.png" class="math-display" alt="(f ∘γ)′(a) = ∇f (γ(a))Tγ′(a ) "/>
</div>
<p><span class="cmti-10x-x-109">there.</span></p>
</div>
<div id="tcolobox-281" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is differentiable at </span><span class="cmmi-10x-x-109">γ</span>(<span class="cmmi-10x-x-109">a</span>)<span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-257006r99"><span class="cmssi-10x-x-109">99</span></a> <span class="cmss-10x-x-109">gives</span></p>
<div class="math-dispay">
<img src="../media/file1531.png" width="450" class="math-display" alt="f(γ(t)) = f (γ (a )) + ∇f (γ(a))T (γ (t)− γ (a )) + o(∥γ(t) − γ(a)∥). "/>
</div>
<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file1532.png" class="math-display" alt="(f ∘γ )′(a) = lim f(γ(t))−-f(γ(a)) t→a t− a T γ(t)−-γ(a)- = ∇f (γ(a)) lit→ma [ t− a + o(1)] T ′ = ∇f (γ(a)) γ (a), "/>
</div>
<p><span class="cmss-10x-x-109">which is what</span><span id="dx1-261004"></span> <span class="cmss-10x-x-109">we had to prove.</span></p>
</div>
</div>
</section>
<section id="the-jacobian-and-hessian-matrices" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_232"><span class="titlemark"><span class="cmss-10x-x-109">16.2.2 </span></span> <span id="x1-26200019.2.2"></span><span class="cmss-10x-x-109">The Jacobian and Hessian matrices</span></h3>
<p><span class="cmss-10x-x-109">Now, our task is</span> <span id="dx1-262001"></span><span class="cmss-10x-x-109">to extend</span><span id="dx1-262002"></span> <span class="cmss-10x-x-109">the derivative for vector-vector functions, so let </span><span class="cmbx-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">be one. By writing out the output of </span><span class="cmbx-10x-x-109">f </span><span class="cmss-10x-x-109">explicitly, we can decompose it into multiple components:</span></p>
<div class="math-display">
<img src="../media/file1533.png" class="math-display" alt=" ⌊ ⌋ | f1(x )| f(x) = |⌈ ... |⌉ ∈ ℝm (×1) fm(x ) "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">i</span></sub> : <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">are vector-scalar functions.</span></p>
<p><span class="cmss-10x-x-109">The natural idea is to compute the partial derivatives for </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">, compacting them into a matrix. And so we shall!</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-262003r70"></span> <span class="cmbx-10x-x-109">Definition 70.</span> </span><span class="cmbx-10x-x-109">(The Jacobian matrix)</span></p>
<p>Let <span class="cmbx-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> be an arbitrary vector-vector function, and suppose that</p>
<div class="math-display">
<img src="../media/file1534.png" class="math-display" alt="f(x) = (f (x ),...,f (x)), 1 m "/>
</div>
<p>where all <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">i</span></sub> : <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℝ </span>are (partially) differentiable at some <span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>. The matrix</p>
<div class="math-display">
<img src="../media/file1535.png" class="math-display" alt=" ⌊ ∂f1 ∂f1 ∂f1 ⌋ | ∂x1(a) ∂x2(a) ... ∂xn(a)| | ∂f2(a) ∂f2(a) ... ∂f2(a)| Jf(a) := || ∂x1. ∂x2. . ∂xn. || ∈ ℝm ×n |⌈ .. .. .. .. |⌉ ∂fm- ∂fm- ∂fm-- ∂x1 (a) ∂x2 (a) ... ∂xn(a) "/>
</div>
<p>is called the <span class="cmti-10x-x-109">Jacobian </span>of <span class="cmbx-10x-x-109">f </span>at <span class="cmbx-10x-x-109">a</span>.</p>
</div>
<p><span class="cmss-10x-x-109">In other</span> <span id="dx1-262004"></span><span class="cmss-10x-x-109">words, the rows of the Jacobian</span> <span id="dx1-262005"></span><span class="cmss-10x-x-109">are the gradients of </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file1536.png" class="math-display" alt=" ⌊ T ⌋ | ∇f1(a) | || ∇f2(a)T || || . || Jf(a) = | .. |. ||∇f (a )T || ⌈ m ⌉ "/>
</div>
<p><span class="cmss-10x-x-109">I have good news: the best local linear approximation of </span><span class="cmbx-10x-x-109">f </span><span class="cmss-10x-x-109">around </span><span class="cmbx-10x-x-109">a </span><span class="cmss-10x-x-109">is given by</span></p>
<div class="math-display">
<img src="../media/file1537.png" class="math-display" alt="f(x ) = f(a)+ J (a)(x − a)+ o(∥x − a∥), f "/>
</div>
<p><span class="cmss-10x-x-109">if the best local linear approximation exists. Thus, the Jacobian is a proper generalization of the gradient.</span></p>
<p><span class="cmss-10x-x-109">We can use the Jacobian to generalize the notion of second-order derivatives for vector-scalar functions: by computing the Jacobian of the gradient, we obtain a special matrix, the analogue of the second derivative.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-262006r71"></span> <span class="cmbx-10x-x-109">Definition 71.</span> </span><span class="cmbx-10x-x-109">(The Hessian matrix)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be an arbitrary vector-scalar function, and suppose that all of its second-order partial derivatives exist at <span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>.</p>
<p>The matrix</p>
<div class="math-display">
<img src="../media/file1538.png" class="math-display" alt=" ⌊ ⌋ ∂2f2(a) -∂2f-(a) ... -∂2f-(a) || ∂x21 ∂x1∂2x2 ∂x1∂2xn || || ∂∂x2f∂x1(a ) ∂∂xf2(a) ... ∂x∂2∂fxn(a)|| n×n Hf (a) := | .. 2.. .. .. | ∈ ℝ |⌈ . . . . |⌉ -∂2f--(a ) -∂2f-(a) ... ∂2f2(a) ∂xn∂x1 ∂xn∂x2 ∂xn "/>
</div>
<p>is called the <span class="cmti-10x-x-109">Hessian </span>of <span class="cmmi-10x-x-109">f </span>at <span class="cmbx-10x-x-109">a</span>.</p>
</div>
<p><span class="cmss-10x-x-109">In other words,</span></p>
<div class="math-display">
<img src="../media/file1539.png" class="math-display" alt="Hf (a) = J ∇f(a)T "/>
</div>
<p><span class="cmss-10x-x-109">holds</span><span id="dx1-262007"></span> <span class="cmss-10x-x-109">by</span><span id="dx1-262008"></span> <span class="cmss-10x-x-109">definition. Moreover, if </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">behaves nicely (for instance, all second-order partial derivatives exist and are continuous), </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-456002r98"><span class="cmssi-10x-x-109">98</span></a> <span class="cmss-10x-x-109">implies that the Hessian is symmetric; that is, </span><span class="cmmi-10x-x-109">H</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) = <span class="cmmi-10x-x-109">H</span><sub><span class="cmmi-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>)<sup><span class="cmmi-8">T</span></sup> <span class="cmss-10x-x-109">.</span></p>
</section>
<section id="the-total-derivative-for-vectorvector-functions" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_233"><span class="titlemark"><span class="cmss-10x-x-109">16.2.3 </span></span> <span id="x1-26300019.2.3"></span><span class="cmss-10x-x-109">The total derivative for vector-vector functions</span></h3>
<p><span class="cmss-10x-x-109">One last generalization. (I promise.) Recall that the existence of the</span><span id="dx1-263001"></span> <span class="cmss-10x-x-109">gradient (that is, partial differentiability) doesn’t imply total differentiability for vector-scalar functions, as the example</span></p>
<div class="math-display">
<img src="../media/file1540.png" class="math-display" alt=" ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise "/>
</div>
<p><span class="cmss-10x-x-109">shows at zero.</span></p>
<p><span class="cmss-10x-x-109">This is true for vector-vector functions as well, as the Jacobian is the generalization of the gradient, not the total derivative.</span></p>
<p><span class="cmss-10x-x-109">It is best to rip the band-aid off quickly and define the total derivative for vector-vector functions. The definition will be a bit abstract, but trust me, the investment will pay off when talking about the chain rule. (Which is the foundation of backpropagation, the algorithm that makes gradient descent computationally feasible.)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-263002r72"></span> <span class="cmbx-10x-x-109">Definition 72.</span> </span><span class="cmbx-10x-x-109">(Total differentiability of vector-vector functions)</span></p>
<p>Let <span class="cmbx-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> be an arbitrary vector-vector function. We say that <span class="cmmi-10x-x-109">f </span>is <span class="cmti-10x-x-109">totally differentiable </span>(or sometimes just <span class="cmti-10x-x-109">differentiable </span>in short) at <span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> if there exists a matrix<span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) <span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span><span class="cmsy-8">×</span><span class="cmmi-8">n</span></sup> such that</p>
<div class="math-display"> 
  <span>
    f(x) = f(a) + D<sub>f</sub>(a)(x − a) + o(∥x − a∥)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(16.9)</span>
</div>

<p>holds for all <span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>), where <span class="cmmi-10x-x-109">𝜀/span&gt;0 and <span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>) is defined by </span></p>
<div class="math-display">
<img src="../media/file1541.png" class="math-display" alt="B(𝜀,a) = {x ∈ ℝn : ∥x − a∥ &lt;𝜀}. "/>
</div>
<p>(In other words, <span class="cmmi-10x-x-109">B</span>(<span class="cmmi-10x-x-109">𝜀,</span><span class="cmbx-10x-x-109">a</span>) is a ball of radius <span class="cmmi-10x-x-109">𝜀/span&gt;0 around <span class="cmbx-10x-x-109">a</span>.) When exists, the matrix <span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) is called the <span class="cmti-10x-x-109">total derivative </span>of <span class="cmmi-10x-x-109">f </span>at <span class="cmbx-10x-x-109">a</span>. </span></p>
</div>
<p><span class="cmss-10x-x-109">Notice that </span><span class="cmssi-10x-x-109">Definition </span><a href="ch026.xhtml#x1-263002r72"><span class="cmssi-10x-x-109">72</span></a> <span class="cmss-10x-x-109">is almost verbatim to </span><span class="cmssi-10x-x-109">Definition </span><a href="ch026.xhtml#x1-257003r68"><span class="cmssi-10x-x-109">68</span></a><span class="cmss-10x-x-109">, except that the “derivative” is a matrix this time.</span></p>
<p><span class="cmss-10x-x-109">You are</span> <span id="dx1-263003"></span><span class="cmss-10x-x-109">probably not surprised to hear that its relation with the Jacobian is the same as the gradient and the total derivative in the vector-scalar case.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-263004r103"></span> <span class="cmbx-10x-x-109">Theorem 103.</span> </span><span class="cmbxti-10x-x-109">(Total derivative and the partial derivatives)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmbx-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmti-10x-x-109">be a function that is totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">. Then, all of its partial derivatives exist at </span><span class="cmbx-10x-x-109">a </span><span class="cmti-10x-x-109">and</span></p>

<img src="../media/file1542.png" width="150" class="math-display" alt="Df (a) = Jf(a ). "/>

</div>
<p><span class="cmss-10x-x-109">The proof is almost identical to the one of </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-257006r99"><span class="cmssi-10x-x-109">99</span></a><span class="cmss-10x-x-109">, with more complex notations. I strongly recommend you work it out line by line, as this kind of mental gymnastics helps significantly to get used to matrices in practice.</span></p>
<p><span class="cmss-10x-x-109">Componentwise, the total derivative can be written as</span></p>
<div class="math-display">
<img src="../media/file1543.png" class="math-display" alt=" ⌊ ∂f1 ∂f1 ∂f1- ⌋ | ∂x1(a) ∂x2(a) ... ∂xn (a )| | ∂f2(a) ∂f2(a) ... ∂f2-(a )| Df(a) = || ∂x1. ∂x2. . ∂xn. ||∈ ℝm ×n. |⌈ .. .. .. .. |⌉ ∂fm-(a) ∂fm(a) ... ∂fm-(a) ∂x1 ∂x2 ∂xn "/>
</div>
<p><span class="cmss-10x-x-109">By introducing the notation</span></p>
<div class="math-display">
<img src="../media/file1544.png" class="math-display" alt=" ⌊ ⌋ ∂∂fx1(a ) || ∂fi2 || || ∂xi(a )|| -∂f(a) = | ... | ∈ ℝm ×1, ∂xi || ∂f || |⌈ ∂mxi (a)|⌉ "/>
</div>
<p><span class="cmss-10x-x-109">the total</span> <span id="dx1-263005"></span><span class="cmss-10x-x-109">derivative </span><span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">f</span></sub>(<span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">can be written in the block-forms</span></p>
<div class="math-display">
<img src="../media/file1545.png" class="math-display" alt=" [-∂f -∂f -∂f ] Df (a ) = ∂x1(a) ∂x2(a) ... ∂xn(a) "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1546.png" class="math-display" alt=" ⌊ ⌋ ∇f1 (a )T || T || D (a ) = || ∇f2 (a ) || . f | ... | ⌈ ⌉ ∇fm (a)T "/>
</div>
</section>
<section id="derivatives-and-function-operations" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_234"><span class="titlemark"><span class="cmss-10x-x-109">16.2.4 </span></span> <span id="x1-26400019.2.4"></span><span class="cmss-10x-x-109">Derivatives and function operations</span></h3>
<p><span class="cmss-10x-x-109">We have generalized the notion</span> <span id="dx1-264001"></span><span class="cmss-10x-x-109">of derivatives as far as possible for us. Now it’s time to study their relations with the two essential function operations: addition and composition. (As there is no vector multiplication in higher dimensional spaces, the product and ratio of vector-vector functions are undefined.)</span></p>
<p><span class="cmss-10x-x-109">Let’s start with the simpler one: addition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-264002r104"></span> <span class="cmbx-10x-x-109">Theorem 104.</span> </span><span class="cmbxti-10x-x-109">(Linearity of the total derivative)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmbx-10x-x-109">f</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">g </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmti-10x-x-109">be two vector-vector functions that are differentiable at some </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">, and let </span><span class="cmmi-10x-x-109">α,β </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be two arbitrary scalars.</span></p>
<p><span class="cmti-10x-x-109">Then, </span><span class="cmmi-10x-x-109">α</span><span class="cmbx-10x-x-109">f </span>+ <span class="cmmi-10x-x-109">β</span><span class="cmbx-10x-x-109">g </span><span class="cmti-10x-x-109">is also differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmti-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1547.png" class="math-display" alt="D αf+ βg(a) = αDf (a)+ βDg (a ) "/>
</div>
<p><span class="cmti-10x-x-109">there.</span></p>
</div>
<div id="tcolobox-282" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Because of the total differentiability, (</span><a href="ch026.xhtml#x1-263002r72"><span class="cmss-10x-x-109">16.9</span></a><span class="cmss-10x-x-109">) implies that</span></p>
<div class="math-display">
<img src="../media/file1548.png" class="math-display" alt="αf(x) + βg(x) = αf(a) + βg(a) + (αD (a )+ βD (a))(x − a) f g + o(∥x− a∥), "/>
</div>
<p><span class="cmss-10x-x-109">which implies</span></p>
<div class="math-display">
<img src="../media/file1549.png" class="math-display" alt="Dαf+βg(a) = αDf (a)+ βDg (a). "/>
</div>
<p><span class="cmss-10x-x-109">This is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Linearity is always nice, but what we need is the ultimate generalization</span> <span id="dx1-264003"></span><span class="cmss-10x-x-109">of the chain rule. We previously saw the special case of composing a scalar-vector and a vector-vector function (see </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-261003r102"><span class="cmssi-10x-x-109">102</span></a><span class="cmss-10x-x-109">), but we need to go one step further.</span></p>
<p><span class="cmss-10x-x-109">The multivariable chain rule is extremely important in machine learning. A neural network is a composite function, with layers acting as components. During gradient descent, we use the chain rule to calculate the derivative of this composition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-264004r105"></span> <span class="cmbx-10x-x-109">Theorem 105.</span> </span><span class="cmbxti-10x-x-109">(Multivariable chain rule)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmbx-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">l</span></sup> <span class="cmti-10x-x-109">and </span><span class="cmbx-10x-x-109">g </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmti-10x-x-109">be two vector-vector functions. If </span><span class="cmbx-10x-x-109">g </span><span class="cmti-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmti-10x-x-109">and </span><span class="cmbx-10x-x-109">f </span><span class="cmti-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">g</span>(<span class="cmbx-10x-x-109">a</span>)<span class="cmti-10x-x-109">, then </span><span class="cmbx-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmbx-10x-x-109">g </span><span class="cmti-10x-x-109">is also totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmti-10x-x-109">and</span></p>
<div class="math-display">
  <span>
    D<sub>f∘g</sub>(a) = D<sub>f</sub>(g(a)) D<sub>g</sub>(a)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(16.10)</span>
</div>

<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<p><span class="cmss-10x-x-109">To our advantage, the derivative of a composed function (</span><a href="ch026.xhtml#x1-264004r105"><span class="cmss-10x-x-109">16.10</span></a><span class="cmss-10x-x-109">) is given by the product of two matrices. Since matrix multiplication can be done lightning fast, this is good news.</span></p>
<p><span class="cmss-10x-x-109">We will see two proofs for </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-264004r105"><span class="cmssi-10x-x-109">105</span></a><span class="cmss-10x-x-109">. One is done with a faster-than-light engine, while the other shows much more by reducing the general case to </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-261003r102"><span class="cmssi-10x-x-109">102</span></a><span class="cmss-10x-x-109">. Both provide a ton of insight. Let’s start with the heavy machinery.</span></p>
<div id="tcolobox-283" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">(First method.)</span></p>
<p><span class="cmss-10x-x-109">As </span><span class="cmbx-10x-x-109">f </span><span class="cmss-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">g</span>(<span class="cmbx-10x-x-109">a</span>)<span class="cmss-10x-x-109">, the equation (</span><a href="ch026.xhtml#x1-263002r72"><span class="cmss-10x-x-109">16.9</span></a><span class="cmss-10x-x-109">) implies</span></p>

<img src="../media/file1550.png" width="450" class="math-display" alt="f(g(x)) = f(g(a))+ D (g (a ))(g(x) − g(a))+ o(∥g(x)− g(a)∥). f "/>

<p><span class="cmss-10x-x-109">In turn, again because of the total differentiability of </span><span class="cmbx-10x-x-109">g </span><span class="cmss-10x-x-109">at </span><span class="cmbx-10x-x-109">a</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file1551.png" class="math-display" alt="g(x)− g(a) = Dg (a)(x− a)+ o(∥x − a∥). "/>
</div>
<p><span class="cmss-10x-x-109">Thus, we can continue our calculation by</span></p>

<img src="../media/file1552.png" width="450" class="math-display" alt="f(g (x )) = f(g(a))+ Df (g(a))(g(x )− g(a))+ o(∥g(x) − g(a)∥) = f(g(a))+ D (g(a))D (a)(x − a) f g + Df (g (a ))[o(∥x− a ∥)+ o(∥g(x)− g(a)∥)], ◟-----------------◝◜----------------◞ =o (∥x−a∥) "/>
<p><span class="cmss-10x-x-109">showing that </span><span class="cmbx-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmbx-10x-x-109">g </span><span class="cmss-10x-x-109">is totally differentiable at </span><span class="cmbx-10x-x-109">a </span><span class="cmss-10x-x-109">with total derivative</span></p>
<div class="math-display">
<img src="../media/file1553.png" class="math-display" alt="Df∘g(a) = Df(g (a ))Dg (a), "/>
</div>
<p><span class="cmss-10x-x-109">which is what we needed to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Now, about that</span> <span id="dx1-264005"></span><span class="cmss-10x-x-109">second proof.</span></p>
<div id="tcolobox-284" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">(Second method.)</span></p>
<p><span class="cmss-10x-x-109">Let’s unpack </span><span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">f</span><span class="cmsy-8">∘</span><span class="cmbx-8">g</span></sub>(<span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">a bit. Writing out the components of </span><span class="cmbx-10x-x-109">f </span><span class="cmsy-10x-x-109">∘</span><span class="cmbx-10x-x-109">g</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file1554.png" class="math-display" alt=" ⌊ ⌋ | (f ∘g )1(x )| | (f ∘g )2(x )| (f ∘ g)(x) = || . || ∈ ℝl, x ∈ ℝn. |⌈ .. |⌉ (f ∘ g)(x) l "/>
</div>
<p><span class="cmss-10x-x-109">By definition, the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column of </span><span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">f</span><span class="cmsy-8">∘</span><span class="cmbx-8">g</span></sub>(<span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">is</span></p>
<div class="math-display">
<img src="../media/file1555.png" class="math-display" alt=" ∂ (f ∘g )i (Df∘g(a))i,j = ---∂x---(a). j "/>
</div>
<p><span class="cmss-10x-x-109">If you look at it long enough, you’ll realize that</span> <img src="../media/file1556.png" class="frac" data-align="middle" alt="∂(f∘g) --∂xji"/>(<span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">is the derivative of a single variable function. Indeed, the function to be differentiated is the composition of the curve</span></p>
<div class="math-display">
<img src="../media/file1557.png" class="math-display" alt="γ : t ↦→ g(a1,...,aj−1,t,aj+1,...,an) "/>
</div>
<p><span class="cmss-10x-x-109">and the vector-scalar function </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">i</span></sub> : <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">. Thus, the chain rule for the composition of scalar-vector and vector-scalar functions (given by </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch026.xhtml#x1-261003r102"><span class="cmssi-10x-x-109">102</span></a><span class="cmss-10x-x-109">) can be applied:</span></p>
<div class="math-display">
<img src="../media/file1558.png" class="math-display" alt="∂(f ∘g)i T ∂ --∂x----(a) = ∇fi(g(a)) ∂x--g(a), j j "/>
</div>
<p><span class="cmss-10x-x-109">where</span> <img src="../media/file1559.png" width="15" data-align="middle" alt="∂∂xj"/><span class="cmbx-10x-x-109">g</span>(<span class="cmbx-10x-x-109">a</span>) <span class="cmss-10x-x-109">is the componentwise derivative</span></p>
<img src="../media/file1560.png" width="150" class="math-display" alt=" ⌊ ⌋ ∂g1(a) || ∂∂g2xj(a)|| -∂-- || -∂xj--|| ∂xj g(a) = | .. | . |⌈ . |⌉ ∂gm∂(xa) j "/>

<p><span class="cmss-10x-x-109">To sum up, we have</span></p>
<div class="math-display">
<img src="../media/file1561.png" class="math-display" alt=" "/>
</div>
<p><span class="cmss-10x-x-109">This is the element in the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th row and </span><span class="cmmi-10x-x-109">j</span><span class="cmss-10x-x-109">-th column of the matrix product </span><span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">f</span></sub>(<span class="cmbx-10x-x-109">g</span>(<span class="cmbx-10x-x-109">a</span>))<span class="cmmi-10x-x-109">D</span><sub><span class="cmbx-8">g</span></sub>(<span class="cmbx-10x-x-109">a</span>)<span class="cmss-10x-x-109">, hence</span></p>
<div class="math-display">
<img src="../media/file1562.png" class="math-display" alt=" "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">With the concept of total derivatives for vector-vector functions</span> <span id="dx1-264006"></span><span class="cmss-10x-x-109">and the general chain rule under our belt, we are ready to actually do things with multivariable functions. Thus, our next stop lays the foundations of optimization.</span></p>
</section>
</section>
<section id="summary15" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_235"><span class="titlemark"><span class="cmss-10x-x-109">16.3 </span></span> <span id="x1-26500019.3"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">You know by now: half the success in mathematics is picking the right representations and notations. Although multivariable calculus can seem insanely complex, it’s a cakewalk if we have a good understanding of linear algebra. This is why we started our entire journey with vectors and matrices! Going from </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,x</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">to </span><span class="cmmi-10x-x-109">f</span>(<span class="cmbx-10x-x-109">x</span>) <span class="cmss-10x-x-109">is a big deal.</span></p>
<p><span class="cmss-10x-x-109">In this chapter, we have learned that differentiation in multiple dimensions is slightly more complicated than in the single-variable case. First, we have the </span><span class="cmssi-10x-x-109">partial derivatives </span><span class="cmss-10x-x-109">defined by</span></p>
<div class="math-display">
<img src="../media/file1563.png" class="math-display" alt="∂f-(a) = lim f(a+-hei)-−-f(a), a ∈ ℝn, ∂xi h→0 h "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">e</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the vector whose </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th component is one, while the others are zero. We can think about</span> <img src="../media/file1564.png" width="15" data-align="middle" alt="∂∂fx- i"/> <span class="cmss-10x-x-109">as the derivative of the single-variable function obtained by fixing all but the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th variable of </span><span class="cmmi-10x-x-109">f</span><span class="cmss-10x-x-109">. Together, the partial derivatives form the gradient:</span></p>
<div class="math-display">
<img src="../media/file1565.png" class="math-display" alt=" ⌊-∂- ⌋ |∂x1f (a )| ||∂∂x2f (a )|| n×1 ∇f (a ) := || .. || ∈ ℝ . ⌈ . ⌉ -∂-f (a ) ∂xn "/>
</div>
<p><span class="cmss-10x-x-109">However, the partial derivatives are not exactly the perfect analogue of the univariate derivatives. There, we learned that the derivative is the best local linear approximation, and this is the version that can be generalized to multiple variables. Thus, we say that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">totally differentiable </span><span class="cmss-10x-x-109">at </span><span class="cmbx-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">if it can be written in the form</span></p>
<div class="math-display">
<img src="../media/file1566.png" class="math-display" alt="f(x) = f(a) + ∇f (a )T (x − a) + o(∥x− a ∥). "/>
</div>
<p><span class="cmss-10x-x-109">In machine learning, one of the most essential tools is the multivariable chain rule</span></p>
<div class="math-display">
<img src="../media/file1567.png" class="math-display" alt="Df∘g(a) = Df(g (a))Dg (a), "/>
</div>
<p><span class="cmss-10x-x-109">which is used to compute the derivatives in practice. Without the chain rule, we wouldn’t have any effective method to compute the gradient. In turn, as the name suggests, the gradient is the cornerstone of </span><span class="cmssi-10x-x-109">gradient descent</span><span class="cmss-10x-x-109">. We already understand the single-variable version, so it’s time to dive deep into the general one. See you in the next chapter!</span></p>
</section>
<section id="problems14" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_236"><span class="titlemark"><span class="cmss-10x-x-109">16.4 </span></span> <span id="x1-26600019.4"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Compute the partial derivatives and the Hessian matrix of the following functions.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmr-8">3</span><span class="cmmi-8">x</span><sub><span class="cmr-6">2</span></sub><sup><span class="cmr-6">2</span></sup></sup> + 2<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub> + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><sup><span class="cmr-8">3</span></sup> <span class="cmssi-10x-x-109">(b) </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = <span class="cmmi-10x-x-109">e</span><sup><span class="cmmi-8">x</span><sub><span class="cmr-6">1</span></sub><sup><span class="cmr-6">2</span></sup><span class="cmsy-8">−</span><span class="cmmi-8">x</span><sub><span class="cmr-6">2</span></sub></sup> + sin(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmssi-10x-x-109">(c) </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = ln(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub><sup><span class="cmr-8">2</span></sup>) + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">e</span><sup><span class="cmmi-8">x</span><sub><span class="cmr-6">2</span></sub></sup> <span class="cmssi-10x-x-109">(d) </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = cos(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub>) + <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmr-8">2</span></sup> sin(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">2</span></sub>) <span class="cmssi-10x-x-109">(e) </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = <img src="../media/file1568.png" class="frac" data-align="middle" alt="x2+x2 x11−x22"/></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Compute the Jacobian matrix of the following functions.</span></p>
<p><span class="cmssi-10x-x-109">(a)</span></p>
<div class="math-display">
<img src="../media/file1569.png" class="math-display" alt=" ⌊ ⌋ x21x2 + ex2 f(x1,x2) = ⌈ x2⌉ sin(x1x2)+ x1e "/>
</div>
<p><span class="cmssi-10x-x-109">(b)</span></p>
<div class="math-display">
<img src="../media/file1570.png" class="math-display" alt=" ⌊ ⌋ ln(x21 + x22) + x1x2 f(x1,x2) = ⌈ 2 x1 ⌉ cos(x1)+ x 2e "/>
</div>
<p><span class="cmssi-10x-x-109">(c)</span></p>
<div class="math-display">
<img src="../media/file1571.png" class="math-display" alt=" ⌊ ⌋ x3 − x2 f(x1,x2) = ⌈ 1 2 ⌉ ex1x2 + x1 cos(x2 ) "/>
</div>
<p><span class="cmssi-10x-x-109">(d)</span></p>
<div class="math-display">
<img src="../media/file1572.png" class="math-display" alt=" ⌊ ⌋ ⌈ tan (x1x2 )+ x32 ⌉ f(x1,x2 ) = ∘x2-+-x2-+ sin(x ) 1 2 1 "/>
</div>
<p><span class="cmssi-10x-x-109">(e)</span></p>
<div class="math-display">
<img src="../media/file1573.png" class="math-display" alt=" ⌊ ⌋ x ex2 − ln(1 + x2) f(x1,x2) = ⌈ 1 1 ⌉ x22cos(x1)+ x1x2 "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub>) = <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><img src="../media/file1574.png" class="sqrt" alt="∘ |x2|-"/><span class="cmss-10x-x-109">. Show that </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">is partially differentiable but not totally differentiable at </span>(0<span class="cmmi-10x-x-109">,</span>0)<span class="cmss-10x-x-109">.</span></p>
</section>
<section id="join-our-community-on-discord16" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_237"><span id="x1-267000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>