- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Derivatives and Gradients
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand why multivariate functions and high-dimensional spaces
    are more complex than the single-variable case we studied earlier, it’s time to
    see how to do things in the general case.
  prefs: []
  type: TYPE_NORMAL
- en: To recap quickly, our goal in machine learning is to optimize functions with
    millions of variables. For instance, think about a neural network N(x,w) trained
    for binary classification, where
  prefs: []
  type: TYPE_NORMAL
- en: x ∈ℝ^n is the input data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: w ∈ℝ^m is the vector compressing all of the weight parameters,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and N(x,w) ∈ [0,1] is the prediction, representing the probability of belonging
    to the positive class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of, say, binary cross-entropy loss, we have the loss function
  prefs: []
  type: TYPE_NORMAL
- en: '![ d L(w ) = − ∑ y log N (x ,w ), i i k=1 ](img/file1457.png)'
  prefs: []
  type: TYPE_IMG
- en: where x[i] is the i-th data point with ground truth y[i] ∈{0,1}. See, I told
    you that we have to write much more in multivariable calculus. (We’ll talk about
    binary cross-entropy loss in Chapter [20](ch032.xhtml#the-expected-value).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Training the neural network is the same as finding a global minimum of L(w),
    if it exists. We have already seen how we can do optimization in a single variable:'
  prefs: []
  type: TYPE_NORMAL
- en: figure out the direction of increase by calculating the derivative,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: take a small step,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then iterate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this to work in multiple variables, we need to generalize the concept of
    the derivative. We can quickly discover the issue: since division with a vector
    is not defined, the difference quotient'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x)−-f-(y-) x− y ](img/file1458.png)'
  prefs: []
  type: TYPE_IMG
- en: 'makes no sense when f : ℝ^n →ℝ is a function of n variables and x,y ∈ℝ^n are
    n-dimensional vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: How can we make sense of it, then? This is what we’ll learn in the following
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1 Partial and total derivatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a look at multivariable functions more closely! For the sake of
    simplicity, let f : ℝ² →ℝ be our function of two variables. To emphasize the dependence
    on the individual variables, we often write'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x1,x2), x1,x2 ∈ ℝ. ](img/file1459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s the trick: by fixing one of the variables, we obtain the two single-variable
    functions! That is, if x[1] ∈ℝ² is fixed, we have x→f(x[1],x), and if x[2] ∈ℝ²
    is fixed, we have x→f(x,x[2]), both of which are well-defined univariate functions.
    Think about this as slicing the function graph with a plane parallel to the x−z
    or the y −z axes, as illustrated by Figure [16.1](#). The part cut out by the
    plane is a single-variable function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1462.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Slicing the surface with the x −z plane'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the derivative of these functions by the limit of difference
    quotients. These are called the partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f-- f(x,x2)-−-f(x1,x2) ∂x1 (x1,x2 ) = xli→mx1 x − x1 , ∂f f(x ,x) − f(x
    ,x ) ----(x1,x2 ) = lim ---1---------1--2-. ∂x2 x→x2 x − x2 ](img/file1463.png)'
  prefs: []
  type: TYPE_IMG
- en: (Keep in mind that x[1] signifies the variable in ![∂f- ∂x1](img/file1464.png),
    but an actual scalar value in the argument of ![∂f- ∂x1](img/file1465.png)(x[1],x[2]).
    This can be quite confusing, but you’ll soon learn to make sense of it.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition is similar for general multivariable functions; we just have
    to write much more. There, the partial derivative of f : ℝ^n →ℝ at the point x
    = (x[1],…,x[n]) with respect to the i-th variable is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(21).png)(16.1)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the biggest challenges in multivariable calculus is to manage the ever-growing
    notational complexity. Just take a look at the difference quotient above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x1,...,x,...,xn)-−-f(x1,...,xi,...,xn). x − xi ](img/file1468.png)'
  prefs: []
  type: TYPE_IMG
- en: This is not the prettiest to look at, and this kind of notational complexity
    can pile up fast. Fortunately, linear algebra comes to the rescue! Not only can
    we compact the variables into the vector x = (x[1],…,x[n]), we can use the standard
    basis
  prefs: []
  type: TYPE_NORMAL
- en: '![ei = (0,...,0, 1 ,0,...,0) ◟◝◜◞ i- th component ](img/file1469.png)'
  prefs: []
  type: TYPE_IMG
- en: to write the difference quotients as
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x+-hei)-−-f(x), h ∈ ℝ. h ](img/file1470.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, ([19.1](#)) can be compacted. With this newly found form, we are ready
    to make a concise and formal definition for partial derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 66\. (Partial derivatives)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be a function of n variables. The partial derivative of f at
    the point x = (x[1],…,x[n]) with respect to the i-th variable is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![-∂f-(x ) = lim f(x-+-hei)−-f-(x-). ∂xi h→0 h ](img/file1471.png)'
  prefs: []
  type: TYPE_IMG
- en: If the above limit exists, we say that f is partially differentiable with respect
    to the i-th variable x[i].
  prefs: []
  type: TYPE_NORMAL
- en: The partial derivative is again a vector-scalar function. Because of this, it
    is often written as ![-∂- ∂xi](img/file1472.png)f, reflecting on the fact that
    the symbol ![∂-- ∂xi](img/file1473.png) can be thought of as a function that maps
    functions to functions. I know, this is a bit abstract, but you’ll get used to
    it quickly.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, there are several alternative notations for the partial derivatives.
    Among others, the symbols
  prefs: []
  type: TYPE_NORMAL
- en: f[x[i]](x),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D[i]f(x),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∂[i]f(x)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: denote the i-th partial derivative of f at x. For simplicity, we’ll use the
    old-school ![-∂f ∂xi](img/file1474.png)(x).
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to start with a few examples to illustrate the concept of partial
    derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. Let
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x1,x2) = x21 + x22\. ](img/file1475.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate, say, ∂f∕∂x[1], we fix the second variable and treat x[2] as a
    constant. Formally, we obtain the single-variable function
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 2 2 f (x) := x + x2, x2 ∈ ℝ, ](img/file1476.png)'
  prefs: []
  type: TYPE_IMG
- en: 'whose derivative gives the first partial derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 ∂f-(x ,x ) = df-(x ) = 2x . ∂x1 1 2 dx 1 1 ](img/file1477.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we get that
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f ∂x-(x1,x2) = 2x2\. 2 ](img/file1478.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you are comfortable with the mental gymnastics of fixing variables, you’ll
    be able to perform partial differentiation without writing out all the intermediate
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. Let’s see a more complicated example. Define
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x1,x2) = sin(x21 + x2). ](img/file1479.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By fixing x[2], we obtain a composite function. Thus the chain rule is used
    to calculate the first partial derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∂f ----(x1,x2 ) = 2x1 cos(x21 + x2). ∂x1 ](img/file1480.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we obtain that
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f-- 2 ∂x2(x1,x2) = cos(x1 + x2). ](img/file1481.png)'
  prefs: []
  type: TYPE_IMG
- en: (I highly advise you to carry out the above calculations step by step as an
    exercise, even if you understand all the intermediate steps.)
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. Finally, let’s see a function that is partially differentiable in
    one variable but not in the other. Define the function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ − 1 if x2 <0, f(x1,x2) = |( 1 otherwise. ](img/file1482.png)'
  prefs: []
  type: TYPE_IMG
- en: As f(x[1],x[2]) does not depend on x[1], we can see that by fixing x[2], the
    resulting function is constant. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![-∂f-(x ,x ) = 0 ∂x1 1 2 ](img/file1483.png)'
  prefs: []
  type: TYPE_IMG
- en: holds everywhere. However, in x[2], there is a discontinuity at 0; thus, ![∂f-
    ∂x2](img/file1484.png) is undefined there.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.1 The gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If a function is partially differentiable in every variable, we can compact
    the derivatives together in a single vector to form the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 67\. (The gradient)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n → ℝ be a function that is partially differentiable in all of its
    variables. Then, its gradient is defined by the (column) vector'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ∂∂x1f (x ) ||-∂-f (x )|| ∇f(x ) := ||∂x2 . || ∈ ℝn ×1\. |⌈ .. |⌉ -∂-
    ∂xnf (x ) ](img/file1485.png)'
  prefs: []
  type: TYPE_IMG
- en: A few remarks are in order. First, the symbol ∇ is called nabla, a symbol that
    was conceived to denote gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the gradient can be thought of as a vector-vector function. To see that,
    consider the already familiar function f(x[1],x[2]) = x[1]² + x[2]². The gradient
    of f is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ 2x1 ∇f (x1,x2) = ⌈ ⌉ , 2x2 ](img/file1486.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![∇f (x ) = 2x ](img/file1487.png)'
  prefs: []
  type: TYPE_IMG
- en: in vectorized form. We can visualize this by drawing the vector ∇f(x[1],x[2])
    at each point (x[1],x[2]) ∈ℝ².
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1488.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: The vector field given by the gradient of x[1]² + x[2]²'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, you can think about ∇f as a vector-vector function ∇f : ℝ^n →ℝ^n. The
    gradient at a given point x is obtained by evaluating this function, yielding
    (∇f)(x).'
  prefs: []
  type: TYPE_NORMAL
- en: For clarity, the parentheses are omitted, arriving at the all familiar notation
    ∇f(x).
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.2 Higher order partial derivatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The partial derivatives of a vector-scalar function f : ℝ^n →ℝ are vector-scalar
    functions themselves. Thus, we can perform partial differentiation one more time!'
  prefs: []
  type: TYPE_NORMAL
- en: If they exist, the second order partial derivatives are defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(22).png)(16.2)'
  prefs: []
  type: TYPE_IMG
- en: where a ∈ℝ^n is an arbitrary vector. (When the second partial differentiation
    takes place with respect to the same variable, ([16.2](ch026.xhtml#higher-order-partial-derivatives))
    is abbreviated by ![∂2f ∂x2i](img/file1494.png)(a).)
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition begs the question: is the order of differentiation interchangeable?
    That is, does'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 2 2 --∂-f--(a) = -∂-f--(a) ∂xi∂xj ∂xj∂xi ](img/file1495.png)'
  prefs: []
  type: TYPE_IMG
- en: 'hold? The answer is quite surprising: the order is interchangeable under some
    mild assumptions, but not in the general case. There is a famous theorem about
    it which we won’t prove, but it’s essential to know.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 98\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n → ℝ be an arbitrary vector-scalar function and let a ∈ ℝ^n. If
    there is a small ball B(𝜀,a) ⊆ℝ^n centered at a such that f has continuous second-order
    partial derivatives at all points of B(𝜀,a), then'
  prefs: []
  type: TYPE_NORMAL
- en: '![--∂2f-- -∂2f--- ∂xi ∂xj(a) = ∂xj∂xi(a) ](img/file1496.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all i = 1,…,n.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem [98](ch026.xhtml#x1-456002r98) is known as either Schwarz’s theorem,
    Clairaut’s theorem, or Young’s theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.3 The total derivative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Partial derivatives seem to generalize the notion of differentiability for multivariable
    functions. However, something is missing. Let’s revisit the single-variable case
    for a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that according to Theorem [77](ch020.xhtml#x1-199002r77), the differentiability
    of a single-variable function f : ℝ →ℝ at a given point a is equivalent to a local
    approximation of f by the linear function'
  prefs: []
  type: TYPE_NORMAL
- en: '![l(x ) = f(a) + f′(a)(x − a). ](img/file1497.png)'
  prefs: []
  type: TYPE_IMG
- en: If x is close to a, l(x) is also close to f(x). Moreover, this is the best linear
    approximation we can do around a. In a single variable, this is equivalent to
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us an idea: even though difference quotients like ![f(x)−f(y) x−y](img/file1498.png)
    do not exist in multiple variables, the best local approximation with a multivariable
    linear function does!'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the notion of total differentiability is born.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 68\. (Total differentiability)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be a function of n variables. We say that f is totally differentiable
    (or sometimes just differentiable for short) at a ∈ℝ^n if there exists a row vector
    D[f](a) ∈ℝ^(1×n) such that'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = f(a) + D[f](a)(x − a) + o(∥x − a∥) (16.3)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all x ∈B(𝜀,a), where 𝜀/span>0 and B(𝜀,a) is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![B(𝜀,a) = {x ∈ ℝn : ∥x − a∥} <𝜀. ](img/file1499.png)'
  prefs: []
  type: TYPE_IMG
- en: (In other words, B(𝜀,a) is a ball of radius 𝜀/span>0 around a.) When exists,
    the vector D[f](a) is called the total derivative of f at a.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that when it is not stated explicitly, we use column vectors, because
    we want to write our linear transformations in the form Ax, where A ∈ℝ^(m×n) and
    x ∈ℝ^(n×1). Thus, the “dimensionology” of the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ f(x) = f(a) + Df (a )(x− a )+o(∥x − a∥) ∈ ℝ1×1 ◟◝1◜×◞1 ◟◝◜1◞×1 ◟-◝1◜× ◞n
    ◟-◝◜n×◞1 ∈ℝ ∈ℝ ∈ℝ ∈ℝ ](img/file1500.png)'
  prefs: []
  type: TYPE_IMG
- en: works out. (Don’t be fooled, ℝ^(1×1) is a scalar.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s unravel the notion of total differentiability. The form ([16.3](ch026.xhtml#x1-257003r68))
    implies that a totally differentiable function f equals to the linear part f(a)
    + D[f](a)(x −a) plus a small error.
  prefs: []
  type: TYPE_NORMAL
- en: The surface given by the linear part is called the tangent plane. We can visualize
    it for functions of two variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: The tangent plane'
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, the partial and total derivatives share an intimate connection.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 99\. (Total derivative and the partial derivatives)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be a function that is totally differentiable at a ∈ℝ^n. Then,
    all of its partial derivatives exist at a and'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = f(a) + ∇f(a)^T (x − a) + o(∥x − a∥) (16.4)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all a in some B(𝜀,a), 𝜀/span>0\. (That is, D[f](a) = ∇f(a)^T .)
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the equation ([16.4](ch026.xhtml#x1-257006r99)) gives that the
    coefficients of the best linear approximation are equal to the partial derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Because f is totally differentiable at a, the definition gives that f
    can be written in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = f(a)+ Df (a)(x− a)+ o(∥x − a∥), ](img/file1502.png)'
  prefs: []
  type: TYPE_IMG
- en: where D[f](a) = (d[1],…,d[n]) is the vector that describes the coefficients
    of the linear part.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ f-(a-+-hei)−-f-(a) hli→m0 h = di, ](img/file1503.png)'
  prefs: []
  type: TYPE_IMG
- en: where e[i] is the unit (column) vector whose i-th component is 1, while the
    others are 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do a quick calculation! Based on what we know, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![f(a-+-hei)−-f(a)-= Df-(a)hei +-o(∥hei∥) h h = Df (a)ei + o(1) = di + o(1),
    ](img/file1504.png)'
  prefs: []
  type: TYPE_IMG
- en: thus confirming that lim[h→0]![f(a+hei)−-f(a)- h](img/file1505.png) = d[i],
    which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: What’s all the hassle with total differentiation, then? Theorem [99](ch026.xhtml#x1-257006r99)
    tells us that total differentiability is a stronger condition than partial differentiability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Surprisingly, the other direction is not true: the existence of partial derivatives
    does not imply total differentiability, as the example'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise ](img/file1506.png)'
  prefs: []
  type: TYPE_IMG
- en: illustrates. This function has all its partial derivatives at 0, yet the total
    derivative does not exist. (You can convince yourself by either drawing a figure,
    or noting that the function 1 −d^T x can never be o(∥x∥), regardless of the choice
    of d.)
  prefs: []
  type: TYPE_NORMAL
- en: Remark 11\. (The total derivative as an operator)
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like for single-variable functions, the total derivative of f : ℝ^n →ℝ
    is a function D[f] : ℝ^n →ℝ^n.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the highest level of abstraction, we can think about the total derivative
    as an operator that maps a vector-scalar function to a vector-vector function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ℝ n ℝn D : (ℝ ) → (ℝ ) , D : f ↦→ D , f ](img/file1507.png)'
  prefs: []
  type: TYPE_IMG
- en: where A^B denotes the set of all functions mapping A to B.
  prefs: []
  type: TYPE_NORMAL
- en: You are not required to understand this at all, but trust me, the more abstract
    your thinking is, the more powerful you’ll be.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.4 Directional derivatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we have talked about two kinds of derivatives: partial derivatives
    that describe the rate of change along a fixed axis, and total derivatives that
    give the best linear approximation of the function at a given point.'
  prefs: []
  type: TYPE_NORMAL
- en: Partial derivatives are only concerned with a few particular directions. However,
    this is not the end of the story in multiple variables. With the standard orthonormal
    basis vectors e[i], the partial derivatives are defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(23).png)(16.5)'
  prefs: []
  type: TYPE_IMG
- en: As we saw earlier, these describe the rate of change along the dimensions. However,
    the standard orthonormal vectors are just a few special directions.
  prefs: []
  type: TYPE_NORMAL
- en: What about an arbitrary direction v? Can we define the derivative along these?
    Sure! There is nothing stopping us from replacing e[i] with v in ([16.5](ch026.xhtml#directional-derivatives)).
    Thus, directional derivatives are born.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 69\. (Directional derivatives)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be a function of n variables and let v ∈ℝ^n be an arbitrary
    vector. The directional derivative of f along v is defined by the limit'
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f f(a + hv) − f(a) ∂v-:= lhim→0 -------h--------. ](img/file1510.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Good news: the directional derivatives can be described in terms of the gradient!'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 100\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be a function of n variables. If f is totally differentiable
    at a ∈ℝ^n, then its directional derivatives exist in all directions, and'
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f(a) = ∇f (a)Tv. ∂v ](img/file1511.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. Because of the total differentiability, Theorem [103](ch026.xhtml#x1-263004r103)
    gives that
  prefs: []
  type: TYPE_NORMAL
- en: '![f (x ) = f (a )+ ∇f (a)T(x − a)+ o(∥x − a∥) ](img/file1512.png)'
  prefs: []
  type: TYPE_IMG
- en: around a. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![f(a+ hv )− f (a ) h∇f (a)Tv + o(h) ----------------= ---------------- h h
    = ∇f (a)Tv + o(1), ](img/file1513.png)'
  prefs: []
  type: TYPE_IMG
- en: giving that
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f-(a) = lim f-(a-+-hv-)−-f(a) ∂v h→0 h = lim ∇f (a)Tv + o(1) h→0 = ∇f (a)T
    v, ](img/file1514.png)'
  prefs: []
  type: TYPE_IMG
- en: as we needed to show.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, Theorem [100](ch026.xhtml#x1-258003r100) gives that no matter
    the direction v, the directional derivative can be written in terms of the gradient
    and v. If you think about this for a minute, this is quite amazing: the rates
    of change along n special directions determine the rate of change in any other
    direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.5 Properties of the gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In one variable, we have learned that if the derivative of f is positive at
    some a, then f is increasing around a. (If the derivative is negative, f is decreasing.)
    If we think about the derivative f^′(a) as a one-dimensional vector, then the
    derivative points towards the direction of increase.
  prefs: []
  type: TYPE_NORMAL
- en: Is this true in higher dimensions? Yes, and this is what makes gradient descent
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 101\. (The gradient determines the direction of the increase)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be a function of n variables, and suppose that f is totally
    differentiable at a ∈ℝ^n.'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(24).png)(16.6)'
  prefs: []
  type: TYPE_IMG
- en: I know, ([16.6](ch026.xhtml#x1-259002r101)) is pretty overloaded, so let’s unpack
    it. First, let’s start with the mysterious argmax. For a given function f,
  prefs: []
  type: TYPE_NORMAL
- en: '![argmaxx ∈Sf(x) ](img/file1517.png)'
  prefs: []
  type: TYPE_IMG
- en: denotes the values that maximize f on the set S. As the maximum may not be unique,
    argmax can yield a set. (The definition of argmin is the same, but with minimum
    instead of maximum.)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in English, ([16.6](ch026.xhtml#x1-259002r101)) states that the unit direction
    that maximizes the directional derivative at a ∈ℝ^n is the normalized gradient.
    Now we are ready to see the proof!
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Do you remember the Cauchy-Schwarz inequality (Theorem [8](ch008.xhtml#x1-43003r8))?
    It was a long time ago, so let’s recall! In the vector space ℝ^n, the Cauchy-Schwarz
    inequality tells us that for any x,y ∈ℝ^n,
  prefs: []
  type: TYPE_NORMAL
- en: '![xT y ≤ ∥x∥∥y∥. ](img/file1518.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, as Theorem [100](ch026.xhtml#x1-258003r100) implies, the directional derivatives
    can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f- T ∂v(a) = ∇f (a) v. ](img/file1519.png)'
  prefs: []
  type: TYPE_IMG
- en: Combined with the Cauchy-Schwarz inequality, we get that
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f ---(a) = ∇f (a)T v ∂v ≤ ∥ ∇f (a )∥∥v ∥. ](img/file1520.png)'
  prefs: []
  type: TYPE_IMG
- en: By restricting the directions to unit vectors,
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(25).png)(16.7)'
  prefs: []
  type: TYPE_IMG
- en: follows. Thus, the directional derivatives must be less than or equal to the
    gradient’s norm. (At least, along a direction vector with unit length.)
  prefs: []
  type: TYPE_NORMAL
- en: However, by letting v[0] = ∇f(a)∕∥∇f(a)∥, we obtain that
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f--(a ) = ∇f (a)Tv0 ∂v0 ∇f (a)T∇f (a) = ---∥∇f-(a)∥-- 2 = ∥∇f-(a)∥- ∥∇f
    (a)∥ = ∥∇f (a)∥. ](img/file1522.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, with the choice v[0] = ![-∇f(a)-](img/file1523.png), equality can be attained
    in ([16.7](#)). This means that ![](img/file1524.png) maximizes the directional
    derivative at a, which is what we had to prove.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have the basics of differentiation in multiple variables under
    our belt. To sum up, we have learned that the difference quotient definition of
    the derivative does not generalize directly for multiple variables, but we can
    fix all but one variables to make the difference quotient work, thus obtaining
    partial derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the linear approximation definition works in multiple dimensions,
    but instead of
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′ f(a)+ f (a)(x− a), f : ℝ → ℝ, x,a ∈ ℝ, ](img/file1525.png)'
  prefs: []
  type: TYPE_IMG
- en: like we had in one variable, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![f (a )+ ∇f (a)T(x− a), f : ℝn → ℝ, x,a ∈ ℝn, ](img/file1526.png)'
  prefs: []
  type: TYPE_IMG
- en: where the analogue of the derivative is the gradient vector ∇f(a) ∈ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: Even when we were studying differentiation in one variable for the first time,
    I told you that the local linear approximation definition would be useful someday.
    That time is now, and we are reaping the benefits. Soon, we’ll see gradient descent
    in its full glory.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2 Derivatives of vector-valued functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a single variable, defining higher-order derivatives is easy. We simply
    have to keep repeating differentiation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ′′ ′ ′ f (x) = (f (x)), f′′′(x) = (f′′(x))′, ](img/file1527.png)'
  prefs: []
  type: TYPE_IMG
- en: and so on. However, this is not that straightforward with multivariable functions.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only talked about gradients, the generalization of the derivative
    for vector-scalar functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As ∇f(a) is a column vector, the gradient is a vector-vector function ∇ : ℝ^n
    →ℝ^n. We only know how to compute the derivative of vector-scalar functions. It’s
    time to change that!'
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.1 The derivatives of curves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Curves, often describing the solutions of dynamical systems, are one of the
    most important objects in mathematics. We don’t explicitly use them in machine
    learning, but they are underneath algorithms such as gradient descent. (Where
    we traverse a discretized curve leading to a local minimum.)
  prefs: []
  type: TYPE_NORMAL
- en: Formally, a curve – that is, a scalar-vector function – is given by a function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊γ (t)⌋ | 1 | n ||γ2(t)|| n(×1) γ : ℝ → ℝ , γ (t) = || .. || ∈ ℝ , ⌈ . ⌉
    γn(t) ](img/file1528.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where the γ[i] : ℝ →ℝ functions are good old single-variable scalar-scalar
    functions. As the independent variable often represents time, it is customary
    to denote it with t.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can differentiate γ componentwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ γ′1(t) ||γ′(t)|| γ′(t) := || 2\. || ∈ ℝn(×1). |⌈ .. |⌉ ′ γn(t) ](img/file1529.png)'
  prefs: []
  type: TYPE_IMG
- en: If we indeed imagine γ(t) as a trajectory in space, γ^′(t) is the tangent vector
    to γ at t. Since the differentiation is componentwise, Theorem [77](ch020.xhtml#x1-199002r77)
    implies that if γ is differentiable at some a ∈ℝ,
  prefs: []
  type: TYPE_NORMAL
- en: γ(t) = γ(a) + γ′(t)^T (t − a) + o(|t − a|) (16.8)
  prefs: []
  type: TYPE_NORMAL
- en: 'there. The equation ([16.8](ch026.xhtml#the-derivatives-of-curves)) is a true
    vectorized formula: some components are vectors, and some are scalars. Yet, this
    is simple and makes perfect sense to us. Hiding the complexities of vectors and
    matrices is the true power of linear algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to see that for any two curves γ,η : ℝ →ℝ^n, differentiation is
    additive, as (γ + η)^′ = γ^′ + η^′. What happens when we compose a scalar-vector
    function with a vector-scalar one?'
  prefs: []
  type: TYPE_NORMAL
- en: 'This situation is commonplace in machine learning. If, say, L : ℝ^n →ℝ describes
    the loss function and γ : ℝ →ℝ^n is our trajectory in the parameter space ℝ^n,
    the composite function f(γ(t)) describes the model loss at time t. Thus, to compute
    (f ∘γ)^′, we have to generalize the chain rule.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 102\. (The chain rule for composing scalar-vector and vector-scalar
    functions)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let γ : ℝ →ℝ^n and f : ℝ^n →ℝ be arbitrary functions. If γ is differentiable
    at some a ∈ℝ and f is differentiable at γ(a), then f ∘γ : ℝ →ℝ is also differentiable
    at a and'
  prefs: []
  type: TYPE_NORMAL
- en: '![(f ∘γ)′(a) = ∇f (γ(a))Tγ′(a ) ](img/file1530.png)'
  prefs: []
  type: TYPE_IMG
- en: there.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As f is differentiable at γ(a), Theorem [99](ch026.xhtml#x1-257006r99)
    gives
  prefs: []
  type: TYPE_NORMAL
- en: '![f(γ(t)) = f (γ (a )) + ∇f (γ(a))T (γ (t)− γ (a )) + o(∥γ(t) − γ(a)∥). ](img/file1531.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![(f ∘γ )′(a) = lim f(γ(t))−-f(γ(a)) t→a t− a T γ(t)−-γ(a)- = ∇f (γ(a)) lit→ma
    [ t− a + o(1)] T ′ = ∇f (γ(a)) γ (a), ](img/file1532.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to prove.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.2 The Jacobian and Hessian matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, our task is to extend the derivative for vector-vector functions, so let
    f : ℝ^n →ℝ^m be one. By writing out the output of f explicitly, we can decompose
    it into multiple components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | f1(x )| f(x) = |⌈ ... |⌉ ∈ ℝm (×1) fm(x ) ](img/file1533.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where f[i] : ℝ^n →ℝ are vector-scalar functions.'
  prefs: []
  type: TYPE_NORMAL
- en: The natural idea is to compute the partial derivatives for f[i], compacting
    them into a matrix. And so we shall!
  prefs: []
  type: TYPE_NORMAL
- en: Definition 70\. (The Jacobian matrix)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ^m be an arbitrary vector-vector function, and suppose that'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = (f (x ),...,f (x)), 1 m ](img/file1534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where all f[i] : ℝ^n → ℝ are (partially) differentiable at some a ∈ ℝ^n. The
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ∂f1 ∂f1 ∂f1 ⌋ | ∂x1(a) ∂x2(a) ... ∂xn(a)| | ∂f2(a) ∂f2(a) ... ∂f2(a)|
    Jf(a) := || ∂x1\. ∂x2\. . ∂xn. || ∈ ℝm ×n |⌈ .. .. .. .. |⌉ ∂fm- ∂fm- ∂fm-- ∂x1
    (a) ∂x2 (a) ... ∂xn(a) ](img/file1535.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the Jacobian of f at a.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the rows of the Jacobian are the gradients of f[i]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ T ⌋ | ∇f1(a) | || ∇f2(a)T || || . || Jf(a) = | .. |. ||∇f (a )T || ⌈ m
    ⌉ ](img/file1536.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I have good news: the best local linear approximation of f around a is given
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x ) = f(a)+ J (a)(x − a)+ o(∥x − a∥), f ](img/file1537.png)'
  prefs: []
  type: TYPE_IMG
- en: if the best local linear approximation exists. Thus, the Jacobian is a proper
    generalization of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the Jacobian to generalize the notion of second-order derivatives
    for vector-scalar functions: by computing the Jacobian of the gradient, we obtain
    a special matrix, the analogue of the second derivative.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 71\. (The Hessian matrix)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ be an arbitrary vector-scalar function, and suppose that all
    of its second-order partial derivatives exist at a ∈ℝ^n.'
  prefs: []
  type: TYPE_NORMAL
- en: The matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ∂2f2(a) -∂2f-(a) ... -∂2f-(a) || ∂x21 ∂x1∂2x2 ∂x1∂2xn || || ∂∂x2f∂x1(a
    ) ∂∂xf2(a) ... ∂x∂2∂fxn(a)|| n×n Hf (a) := | .. 2.. .. .. | ∈ ℝ |⌈ . . . . |⌉
    -∂2f--(a ) -∂2f-(a) ... ∂2f2(a) ∂xn∂x1 ∂xn∂x2 ∂xn ](img/file1538.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the Hessian of f at a.
  prefs: []
  type: TYPE_NORMAL
- en: In other words,
  prefs: []
  type: TYPE_NORMAL
- en: '![Hf (a) = J ∇f(a)T ](img/file1539.png)'
  prefs: []
  type: TYPE_IMG
- en: holds by definition. Moreover, if f behaves nicely (for instance, all second-order
    partial derivatives exist and are continuous), Theorem [98](ch026.xhtml#x1-456002r98)
    implies that the Hessian is symmetric; that is, H[f](a) = H[f](a)^T .
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.3 The total derivative for vector-vector functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One last generalization. (I promise.) Recall that the existence of the gradient
    (that is, partial differentiability) doesn’t imply total differentiability for
    vector-scalar functions, as the example
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ f (x, y) = 1 if x = 0 or y = 0, |( 0 otherwise ](img/file1540.png)'
  prefs: []
  type: TYPE_IMG
- en: shows at zero.
  prefs: []
  type: TYPE_NORMAL
- en: This is true for vector-vector functions as well, as the Jacobian is the generalization
    of the gradient, not the total derivative.
  prefs: []
  type: TYPE_NORMAL
- en: It is best to rip the band-aid off quickly and define the total derivative for
    vector-vector functions. The definition will be a bit abstract, but trust me,
    the investment will pay off when talking about the chain rule. (Which is the foundation
    of backpropagation, the algorithm that makes gradient descent computationally
    feasible.)
  prefs: []
  type: TYPE_NORMAL
- en: Definition 72\. (Total differentiability of vector-vector functions)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ^m be an arbitrary vector-vector function. We say that f is totally
    differentiable (or sometimes just differentiable in short) at a ∈ℝ^n if there
    exists a matrixD[f](a) ∈ℝ^(m×n) such that'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = f(a) + D[f](a)(x − a) + o(∥x − a∥) (16.9)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all x ∈B(𝜀,a), where 𝜀/span>0 and B(𝜀,a) is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![B(𝜀,a) = {x ∈ ℝn : ∥x − a∥ <𝜀}. ](img/file1541.png)'
  prefs: []
  type: TYPE_IMG
- en: (In other words, B(𝜀,a) is a ball of radius 𝜀/span>0 around a.) When exists,
    the matrix D[f](a) is called the total derivative of f at a.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that Definition [72](ch026.xhtml#x1-263002r72) is almost verbatim to
    Definition [68](ch026.xhtml#x1-257003r68), except that the “derivative” is a matrix
    this time.
  prefs: []
  type: TYPE_NORMAL
- en: You are probably not surprised to hear that its relation with the Jacobian is
    the same as the gradient and the total derivative in the vector-scalar case.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 103\. (Total derivative and the partial derivatives)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^n →ℝ^m be a function that is totally differentiable at a ∈ℝ^n. Then,
    all of its partial derivatives exist at a and'
  prefs: []
  type: TYPE_NORMAL
- en: '![Df (a) = Jf(a ). ](img/file1542.png)'
  prefs: []
  type: TYPE_IMG
- en: The proof is almost identical to the one of Theorem [99](ch026.xhtml#x1-257006r99),
    with more complex notations. I strongly recommend you work it out line by line,
    as this kind of mental gymnastics helps significantly to get used to matrices
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Componentwise, the total derivative can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ∂f1 ∂f1 ∂f1- ⌋ | ∂x1(a) ∂x2(a) ... ∂xn (a )| | ∂f2(a) ∂f2(a) ... ∂f2-(a
    )| Df(a) = || ∂x1\. ∂x2\. . ∂xn. ||∈ ℝm ×n. |⌈ .. .. .. .. |⌉ ∂fm-(a) ∂fm(a) ...
    ∂fm-(a) ∂x1 ∂x2 ∂xn ](img/file1543.png)'
  prefs: []
  type: TYPE_IMG
- en: By introducing the notation
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ∂∂fx1(a ) || ∂fi2 || || ∂xi(a )|| -∂f(a) = | ... | ∈ ℝm ×1, ∂xi || ∂f
    || |⌈ ∂mxi (a)|⌉ ](img/file1544.png)'
  prefs: []
  type: TYPE_IMG
- en: the total derivative D[f](a) can be written in the block-forms
  prefs: []
  type: TYPE_NORMAL
- en: '![ [-∂f -∂f -∂f ] Df (a ) = ∂x1(a) ∂x2(a) ... ∂xn(a) ](img/file1545.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ∇f1 (a )T || T || D (a ) = || ∇f2 (a ) || . f | ... | ⌈ ⌉ ∇fm (a)T ](img/file1546.png)'
  prefs: []
  type: TYPE_IMG
- en: 16.2.4 Derivatives and function operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have generalized the notion of derivatives as far as possible for us. Now
    it’s time to study their relations with the two essential function operations:
    addition and composition. (As there is no vector multiplication in higher dimensional
    spaces, the product and ratio of vector-vector functions are undefined.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the simpler one: addition.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 104\. (Linearity of the total derivative)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f,g : ℝ^n →ℝ^m be two vector-vector functions that are differentiable at
    some a ∈ℝ^n, and let α,β ∈ℝ be two arbitrary scalars.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, αf + βg is also differentiable at a and
  prefs: []
  type: TYPE_NORMAL
- en: '![D αf+ βg(a) = αDf (a)+ βDg (a ) ](img/file1547.png)'
  prefs: []
  type: TYPE_IMG
- en: there.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Because of the total differentiability, ([16.9](ch026.xhtml#x1-263002r72))
    implies that
  prefs: []
  type: TYPE_NORMAL
- en: '![αf(x) + βg(x) = αf(a) + βg(a) + (αD (a )+ βD (a))(x − a) f g + o(∥x− a∥),
    ](img/file1548.png)'
  prefs: []
  type: TYPE_IMG
- en: which implies
  prefs: []
  type: TYPE_NORMAL
- en: '![Dαf+βg(a) = αDf (a)+ βDg (a). ](img/file1549.png)'
  prefs: []
  type: TYPE_IMG
- en: This is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Linearity is always nice, but what we need is the ultimate generalization of
    the chain rule. We previously saw the special case of composing a scalar-vector
    and a vector-vector function (see Theorem [102](ch026.xhtml#x1-261003r102)), but
    we need to go one step further.
  prefs: []
  type: TYPE_NORMAL
- en: The multivariable chain rule is extremely important in machine learning. A neural
    network is a composite function, with layers acting as components. During gradient
    descent, we use the chain rule to calculate the derivative of this composition.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 105\. (Multivariable chain rule)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : ℝ^m →ℝ^l and g : ℝ^n →ℝ^m be two vector-vector functions. If g is totally
    differentiable at a ∈ℝ^n and f is totally differentiable at g(a), then f ∘g is
    also totally differentiable at a and'
  prefs: []
  type: TYPE_NORMAL
- en: D[f∘g](a) = D[f](g(a)) D[g](a) (16.10)
  prefs: []
  type: TYPE_NORMAL
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: To our advantage, the derivative of a composed function ([16.10](ch026.xhtml#x1-264004r105))
    is given by the product of two matrices. Since matrix multiplication can be done
    lightning fast, this is good news.
  prefs: []
  type: TYPE_NORMAL
- en: We will see two proofs for Theorem [105](ch026.xhtml#x1-264004r105). One is
    done with a faster-than-light engine, while the other shows much more by reducing
    the general case to Theorem [102](ch026.xhtml#x1-261003r102). Both provide a ton
    of insight. Let’s start with the heavy machinery.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (First method.)
  prefs: []
  type: TYPE_NORMAL
- en: As f is totally differentiable at g(a), the equation ([16.9](ch026.xhtml#x1-263002r72))
    implies
  prefs: []
  type: TYPE_NORMAL
- en: '![f(g(x)) = f(g(a))+ D (g (a ))(g(x) − g(a))+ o(∥g(x)− g(a)∥). f ](img/file1550.png)'
  prefs: []
  type: TYPE_IMG
- en: In turn, again because of the total differentiability of g at a, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![g(x)− g(a) = Dg (a)(x− a)+ o(∥x − a∥). ](img/file1551.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we can continue our calculation by
  prefs: []
  type: TYPE_NORMAL
- en: '![f(g (x )) = f(g(a))+ Df (g(a))(g(x )− g(a))+ o(∥g(x) − g(a)∥) = f(g(a))+
    D (g(a))D (a)(x − a) f g + Df (g (a ))[o(∥x− a ∥)+ o(∥g(x)− g(a)∥)], ◟-----------------◝◜----------------◞
    =o (∥x−a∥) ](img/file1552.png)'
  prefs: []
  type: TYPE_IMG
- en: showing that f ∘g is totally differentiable at a with total derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![Df∘g(a) = Df(g (a ))Dg (a), ](img/file1553.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we needed to show.
  prefs: []
  type: TYPE_NORMAL
- en: Now, about that second proof.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (Second method.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s unpack D[f∘g](a) a bit. Writing out the components of f ∘g, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ | (f ∘g )1(x )| | (f ∘g )2(x )| (f ∘ g)(x) = || . || ∈ ℝl, x ∈ ℝn. |⌈
    .. |⌉ (f ∘ g)(x) l ](img/file1554.png)'
  prefs: []
  type: TYPE_IMG
- en: By definition, the i-th row and j-th column of D[f∘g](a) is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∂ (f ∘g )i (Df∘g(a))i,j = ---∂x---(a). j ](img/file1555.png)'
  prefs: []
  type: TYPE_IMG
- en: If you look at it long enough, you’ll realize that ![∂(f∘g) --∂xji](img/file1556.png)(a)
    is the derivative of a single variable function. Indeed, the function to be differentiated
    is the composition of the curve
  prefs: []
  type: TYPE_NORMAL
- en: '![γ : t ↦→ g(a1,...,aj−1,t,aj+1,...,an) ](img/file1557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and the vector-scalar function f[i] : ℝ^m → ℝ. Thus, the chain rule for the
    composition of scalar-vector and vector-scalar functions (given by Theorem [102](ch026.xhtml#x1-261003r102))
    can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![∂(f ∘g)i T ∂ --∂x----(a) = ∇fi(g(a)) ∂x--g(a), j j ](img/file1558.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![∂∂xj](img/file1559.png)g(a) is the componentwise derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ∂g1(a) || ∂∂g2xj(a)|| -∂-- || -∂xj--|| ∂xj g(a) = | .. | . |⌈ . |⌉ ∂gm∂(xa)
    j ](img/file1560.png)'
  prefs: []
  type: TYPE_IMG
- en: To sum up, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](img/file1561.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the element in the i-th row and j-th column of the matrix product D[f](g(a))D[g](a),
    hence
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](img/file1562.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: With the concept of total derivatives for vector-vector functions and the general
    chain rule under our belt, we are ready to actually do things with multivariable
    functions. Thus, our next stop lays the foundations of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You know by now: half the success in mathematics is picking the right representations
    and notations. Although multivariable calculus can seem insanely complex, it’s
    a cakewalk if we have a good understanding of linear algebra. This is why we started
    our entire journey with vectors and matrices! Going from f(x[1],…,x[n]) to f(x)
    is a big deal.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have learned that differentiation in multiple dimensions
    is slightly more complicated than in the single-variable case. First, we have
    the partial derivatives defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![∂f-(a) = lim f(a+-hei)-−-f(a), a ∈ ℝn, ∂xi h→0 h ](img/file1563.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where e[i] is the vector whose i-th component is one, while the others are
    zero. We can think about ![∂∂fx- i](img/file1564.png) as the derivative of the
    single-variable function obtained by fixing all but the i-th variable of f. Together,
    the partial derivatives form the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊-∂- ⌋ |∂x1f (a )| ||∂∂x2f (a )|| n×1 ∇f (a ) := || .. || ∈ ℝ . ⌈ . ⌉ -∂-f
    (a ) ∂xn ](img/file1565.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the partial derivatives are not exactly the perfect analogue of the
    univariate derivatives. There, we learned that the derivative is the best local
    linear approximation, and this is the version that can be generalized to multiple
    variables. Thus, we say that f is totally differentiable at a ∈ℝ^n if it can be
    written in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) = f(a) + ∇f (a )T (x − a) + o(∥x− a ∥). ](img/file1566.png)'
  prefs: []
  type: TYPE_IMG
- en: In machine learning, one of the most essential tools is the multivariable chain
    rule
  prefs: []
  type: TYPE_NORMAL
- en: '![Df∘g(a) = Df(g (a))Dg (a), ](img/file1567.png)'
  prefs: []
  type: TYPE_IMG
- en: which is used to compute the derivatives in practice. Without the chain rule,
    we wouldn’t have any effective method to compute the gradient. In turn, as the
    name suggests, the gradient is the cornerstone of gradient descent. We already
    understand the single-variable version, so it’s time to dive deep into the general
    one. See you in the next chapter!
  prefs: []
  type: TYPE_NORMAL
- en: 16.4 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Compute the partial derivatives and the Hessian matrix of the following
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: (a) f(x[1],x[2]) = x[1]^(3x[2]²) + 2x[1]x[2] + x[2]³ (b) f(x[1],x[2]) = e^(x[1]²−x[2])
    + sin(x[1]x[2]) (c) f(x[1],x[2]) = ln(x[1]² + x[2]²) + x[1]e^(x[2]) (d) f(x[1],x[2])
    = cos(x[1]x[2]) + x[1]² sin(x[2]) (e) f(x[1],x[2]) = f(x[1],x[2]) = ![x2+x2 x11−x22](img/file1568.png)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Compute the Jacobian matrix of the following functions.
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ x21x2 + ex2 f(x1,x2) = ⌈ x2⌉ sin(x1x2)+ x1e ](img/file1569.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ln(x21 + x22) + x1x2 f(x1,x2) = ⌈ 2 x1 ⌉ cos(x1)+ x 2e ](img/file1570.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ x3 − x2 f(x1,x2) = ⌈ 1 2 ⌉ ex1x2 + x1 cos(x2 ) ](img/file1571.png)'
  prefs: []
  type: TYPE_IMG
- en: (d)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌈ tan (x1x2 )+ x32 ⌉ f(x1,x2 ) = ∘x2-+-x2-+ sin(x ) 1 2 1 ](img/file1572.png)'
  prefs: []
  type: TYPE_IMG
- en: (e)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ x ex2 − ln(1 + x2) f(x1,x2) = ⌈ 1 1 ⌉ x22cos(x1)+ x1x2 ](img/file1573.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 3\. Let f(x[1],x[2]) = x[1]![∘ |x2|-](img/file1574.png). Show that f
    is partially differentiable but not totally differentiable at (0,0).
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
