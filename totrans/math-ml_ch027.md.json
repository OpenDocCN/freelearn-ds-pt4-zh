["```py\nclass MultivariableFunction: \n    def __init__(self): \n        pass \n\n    def __call__(self, *args, **kwargs): \n        pass \n\n    def grad(self): \n        pass \n\n    def parameters(self): \n        return dict()\n```", "```py\nimport numpy as np\n\nclass SquaredNorm(MultivariableFunction):\n    def __call__(self, x: np.array):\n        return np.sum(x**2)\n\n    def grad(self, x: np.array):\n        return 2*x\n```", "```py\nclass Linear(MultivariableFunction): \n    def __init__(self, a: float, b: float): \n        self.a = a \n        self.b = b \n\n    def __call__(self, x: np.array): \n        #x0022;\"/span> \n        x: np.array of shape (2, ) \n        #x0022;\"/span> \n        x = x.reshape(2) \n        return self.a*x[0] + self.b*x[1] \n\n    def grad(self, x: np.array): \n        return np.array([self.a, self.b]).reshape(2, 1) \n\n    def parameters(self): \n        return {\"/span>a self.a, /span>b self.b}\n```", "```py\ng = Linear(a=1, b=-1) \n\ng(np.array([1, 0]))\n```", "```py\nnp.int64(1)\n```", "```py\ndef gradient_descent( \n    f: MultivariableFunction, \n    x_init: np.array,               # the initial guess \n    learning_rate: float = 0.1,     # the learning rate \n    n_iter: int = 1000,             # number of steps \n): \n    x = x_init \n\n    for n in range(n_iter): \n        grad = f.grad(x) \n        x = x - learning_rate*grad \n\n    return x\n```", "```py\nsquared_norm = SquaredNorm() \nlocal_minimum = gradient_descent( \n    f=squared_norm, \n    x_init=np.array([10.0, -15.0]) \n) \n\nlocal_minimum\n```", "```py\narray([ 1.23023192e-96, -1.84534788e-96])\n```"]