<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch030.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="what-is-probability" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">18</span></span><br/>
<span id="x1-27700022"></span><span class="cmss-10x-x-109">What is Probability?</span></h1>
<p><span class="cmss-10x-x-109">When going about our lives, we almost always think in binary terms. A statement is either true or false. An outcome has either occurred or not.</span></p>
<p><span class="cmss-10x-x-109">In practice, we rarely have the comfort of certainty. We have to operate with incomplete information. When a scientist observes the outcome of an experiment, can they verify their hypothesis with 100% certainty? No. Because they do not have complete control over all the variables (such as the weather or the alignment of stars), the observed effect might be unintentional. Each result will either strengthen or weaken our belief in the hypothesis, but none will provide ultimate proof.</span></p>
<p><span class="cmss-10x-x-109">In machine learning, our job is not simply to provide a prediction about some class label but to formulate a mathematical model that summarizes our knowledge about the data in a way that conveys information about the degree of our certainty in the prediction.</span></p>
<p><span class="cmss-10x-x-109">So, fitting a parametric function </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">m</span></sup> <span class="cmss-10x-x-109">to model the relation between the data and the variable to be predicted is not enough. We will need an entirely new vocabulary to formulate such models. We need to think in terms of </span><span class="cmssi-10x-x-109">probabilities</span><span class="cmss-10x-x-109">.</span></p>
<section id="the-language-of-thinking" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_245"><span class="titlemark"><span class="cmss-10x-x-109">18.1 </span></span> <span id="x1-27800022.1"></span><span class="cmss-10x-x-109">The language of thinking</span></h2>
<p><span class="cmss-10x-x-109">First, let’s talk about how we think. On the most basic level, our knowledge</span> <span id="dx1-278001"></span><span class="cmss-10x-x-109">about the world is stored</span> <span id="dx1-278002"></span><span class="cmss-10x-x-109">in propositions. In a mathematical sense, a proposition is a declaration that is either true or false. (In binary terms, true is denoted by 1 and false is denoted by 0.)</span></p>
<p><span class="cmssi-10x-x-109">“The sky is blue.”</span></p>
<p><span class="cmssi-10x-x-109">“There are infinitely many prime numbers.”</span></p>
<p><span class="cmssi-10x-x-109">“1 + 1 = 3.”</span></p>
<p><span class="cmssi-10x-x-109">“I got the flu.”</span></p>
<p><span class="cmss-10x-x-109">Propositions are often abbreviated as variables such as </span><span class="cmmi-10x-x-109">A </span>= <span class="cmss-10x-x-109">”it’s raining outside”.</span></p>
<p><span class="cmss-10x-x-109">Determining</span> <span id="dx1-278003"></span><span class="cmss-10x-x-109">the truth value of a given</span> <span id="dx1-278004"></span><span class="cmss-10x-x-109">proposition using evidence and reasoning is called </span><span class="cmssi-10x-x-109">inference</span><span class="cmss-10x-x-109">. To be able to formulate valid arguments</span><span id="dx1-278005"></span> <span class="cmss-10x-x-109">and understand how inference works, we’ll take a quick visit to the world of mathematical logic.</span></p>
<section id="thinking-in-absolutes" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_246"><span class="titlemark"><span class="cmss-10x-x-109">18.1.1 </span></span> <span id="x1-27900022.1.1"></span><span class="cmss-10x-x-109">Thinking in absolutes</span></h3>
<p><span class="cmss-10x-x-109">So, we have</span> <span id="dx1-279001"></span><span class="cmss-10x-x-109">propositions such as </span><span class="cmmi-10x-x-109">A </span>= <span class="cmss-10x-x-109">”it’s raining outside” or </span><span class="cmmi-10x-x-109">B </span>= <span class="cmss-10x-x-109">”the sidewalk is wet”. We need more expressive power: propositions are building blocks, and we want to combine them, yielding more complex propositions. (We’ll review the fundamentals of mathematical logic here, but check out </span><span class="cmssi-10x-x-109">Appendix </span><a href="ch035.xhtml#its-just-logic"><span class="cmssi-10x-x-109">A</span></a> <span class="cmss-10x-x-109">for more.)</span></p>
<p><span class="cmss-10x-x-109">We can formulate complex propositions from simpler ones with </span><span class="cmssi-10x-x-109">logical connectives</span><span class="cmss-10x-x-109">. Consider the proposition “if it is raining outside, then the sidewalk is wet”. This is the combination of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">, strung together by the implication connective.</span></p>
<p><span class="cmss-10x-x-109">There are four essential connectives:</span></p>
<ul>
<li><span class="cmss-10x-x-109">NOT (</span><span class="cmsy-10x-x-109">¬</span><span class="cmss-10x-x-109">), also known as negation,</span></li>
<li><span class="cmss-10x-x-109">AND ( </span><span class="cmsy-10x-x-109">∧</span><span class="cmss-10x-x-109">), also known as conjunction,</span></li>
<li><span class="cmss-10x-x-109">OR (</span><span class="cmsy-10x-x-109">∨</span><span class="cmss-10x-x-109">), also known as disjunction,</span></li>
<li><span class="cmss-10x-x-109">THEN (</span><span class="cmsy-10x-x-109">→</span><span class="cmss-10x-x-109">), also known as implication.</span></li>
</ul>
<p><span class="cmss-10x-x-109">Connectives are defined by the truth values of the resulting propositions. For instance, if </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is true, then </span><span class="cmsy-10x-x-109">¬</span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is false; if </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is false, then </span><span class="cmsy-10x-x-109">¬</span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is true. Denoting true by </span>1 <span class="cmss-10x-x-109">and false by </span>0<span class="cmss-10x-x-109">, we can describe connectives with </span><span class="cmssi-10x-x-109">truth tables</span><span class="cmss-10x-x-109">. Here is the one for negation:</span></p>

<img src="../media/file1606.png" width="75" class="math-display" alt="| | | |A-|¬A--| |0 | 1 | | | | |1 | 0 | | | "/>

<p><span class="cmss-10x-x-109">AND (</span><span class="cmsy-10x-x-109">∧</span><span class="cmss-10x-x-109">) and OR (</span><span class="cmsy-10x-x-109">∨</span><span class="cmss-10x-x-109">) connect two propositions. </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∧</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is true if both </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">are true, while </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∨</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is true if either one is.</span></p>
<div class="math-disply">
<img src="../media/file1607.png" width="150" class="math-display" alt="| | | | | |A-|B--|A-∧-B--|A-∨-B-| |0 | 0 | 0 | 0 | | | | | | |0 | 1 | 0 | 1 | |1 | 0 | 0 | 1 | | | | | | |1 | 1 | 1 | 1 | | | "/>
</div>
<p><span class="cmss-10x-x-109">The implication connective THEN (</span><span class="cmsy-10x-x-109">→</span><span class="cmss-10x-x-109">) formalizes the deduction of</span><span id="dx1-279002"></span> <span class="cmss-10x-x-109">a conclusion </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">from a premise </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. By definition, </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is true if </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is true or both </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">are false. An example: IF </span><span class="cmssi-10x-x-109">“it’s raining outside”</span><span class="cmss-10x-x-109">, THEN </span><span class="cmssi-10x-x-109">“the sidewalk is wet”</span><span class="cmss-10x-x-109">.</span></p>
<div class="math-dispay">
<img src="../media/file1608.png" width="150" class="math-display" alt="| | | | |A--|B-|A--→-B--| | 0 |0 | 1 | | | | | | 0 |1 | 1 | | 1 |0 | 0 | | | | | | 1 |1 | 1 | | | "/>
</div>
<p><span class="cmss-10x-x-109">Note that </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">does not imply </span><span class="cmmi-10x-x-109">B </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. This common logical fallacy is called </span><span class="cmssi-10x-x-109">affirming the consequent</span><span class="cmss-10x-x-109">, and we’ve all fallen victim to it at some point in our lives. To see a concrete example: if </span><span class="cmssi-10x-x-109">“it’s raining outside”</span><span class="cmss-10x-x-109">, then </span><span class="cmssi-10x-x-109">“the sidewalk is wet”</span><span class="cmss-10x-x-109">, but not the other way around. The sidewalk can be wet for other reasons, such as someone spilling a barrel of water.</span></p>
<p><span class="cmss-10x-x-109">Connectives correspond to set operations. Why? Let’s take a look at the </span><span class="cmssi-10x-x-109">formal </span><span class="cmss-10x-x-109">definition of set operations.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-279003r75"></span> <span class="cmbx-10x-x-109">Definition 75.</span> </span><span class="cmbx-10x-x-109">(The (reasonably) formal definition of set operations and relations)</span></p>
<p>Let <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B </span>be two sets.</p>
<p><span class="cmti-10x-x-109">(a) </span>The union of <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B </span>is defined by</p>
<div class="math-display">
<img src="../media/file1609.png" class="math-display" alt="A ∪ B := {x : (x ∈ A )∨ (x ∈ B )}, "/>
</div>
<p>that is, <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∪</span><span class="cmmi-10x-x-109">B </span>contains all elements that are in <span class="cmmi-10x-x-109">A </span>or <span class="cmmi-10x-x-109">B</span>.</p>
<p><span class="cmti-10x-x-109">(b) </span>The intersection of <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B </span>is defined by</p>
<div class="math-display">
<img src="../media/file1610.png" class="math-display" alt="A ∩ B := {x : (x ∈ A )∧ (x ∈ B )}, "/>
</div>
<p>that is, <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">B </span>contains all elements that are in <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B</span>.</p>
<p><span class="cmti-10x-x-109">(c) </span>We say that <span class="cmmi-10x-x-109">A </span>is a subset of <span class="cmmi-10x-x-109">B</span>, that is, <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">B </span>if</p>
<div class="math-display">
<img src="../media/file1611.png" class="math-display" alt="(x ∈ A ) → (x ∈ B ) "/>
</div>
<p>is true for all <span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">A</span>.</p>
<p><span class="cmti-10x-x-109">(d) </span>The complement of <span class="cmmi-10x-x-109">A </span>with respect to an Ω <span class="cmsy-10x-x-109">⊃</span><span class="cmmi-10x-x-109">A </span>is defined by</p>
<div class="math-display">
<img src="../media/file1612.png" class="math-display" alt="Ω ∖A := {x ∈ Ω : ¬ (x ∈ A )}, "/>
</div>
<p>that is, Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A </span>contains all elements that are in Ω, but not in <span class="cmmi-10x-x-109">A</span>.</p>
</div>
<p><span class="cmss-10x-x-109">If you</span> <span id="dx1-279004"></span><span class="cmss-10x-x-109">carefully read through the definitions, you can see how connectives and set operations relate. </span><span class="cmsy-10x-x-109">∧ </span><span class="cmss-10x-x-109">is intersection, </span><span class="cmsy-10x-x-109">∨ </span><span class="cmss-10x-x-109">is union, </span><span class="cmsy-10x-x-109">¬ </span><span class="cmss-10x-x-109">is the complement, and </span><span class="cmsy-10x-x-109">→ </span><span class="cmss-10x-x-109">is the subset relation. This is illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">18.1</span></a><span class="cmss-10x-x-109">. (I’ve slightly abused the notation here, as statements such as </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∧</span><span class="cmmi-10x-x-109">B ⟺ A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">are mathematically incorrect. </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">cannot be a proposition and a set at the same time, thus the equivalence is not precise. )</span></p>
<div class="minipage">
<p><img src="../media/file1613.png" width="569" alt="PIC"/> <span id="x1-279005r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.1: Connectives and set operations</span> </span>
</div>
<p><span class="cmss-10x-x-109">Why is this important? Because probability operates on sets, and sets play the role of propositions. We’ll see this later, but first, let’s dive deep into how mathematical logic formalizes scientific thinking.</span></p>
<p><span class="cmss-10x-x-109">Let’s refine the inference process of mathematical logic. A proposition is either true or false, fair and square. How can we determine that in practice? For example, how do we find the truth value of the proposition “there are infinitely many prime numbers“?</span></p>
<p><span class="cmss-10x-x-109">By using evidence and deduction. Like Sherlock Holmes solving a crime by connecting facts, we rely on knowledge of the form </span><span class="cmssi-10x-x-109">“if </span><span class="cmmi-10x-x-109">A</span><span class="cmssi-10x-x-109">, then </span><span class="cmmi-10x-x-109">B</span><span class="cmssi-10x-x-109">“</span><span class="cmss-10x-x-109">. Our knowledge about the world is stored in true implications. For example:</span></p>
<ul>
<li><span class="cmssi-10x-x-109">“If it is raining, then the sidewalk is wet.“</span></li>
<li><span class="cmssi-10x-x-109">“If </span><img src="../media/file1614.png" class="math" alt="ABC "/> <span class="cmssi-10x-x-109">is a right triangle, then </span><span class="cmmi-10x-x-109">A</span><sup><span class="cmr-8">2</span></sup> + <span class="cmmi-10x-x-109">B</span><sup><span class="cmr-8">2</span></sup> = <span class="cmmi-10x-x-109">C</span><sup><span class="cmr-8">2</span></sup><span class="cmssi-10x-x-109">.“</span></li>
<li><span class="cmssi-10x-x-109">“If a system is closed, then its entropy cannot decrease.“</span></li>
</ul>
<p><span class="cmss-10x-x-109">As we have seen, the</span><span id="dx1-279006"></span> <span class="cmss-10x-x-109">implication can be translated into the language of set theory (as all the other connectives). While </span><span class="cmsy-10x-x-109">∧ </span><span class="cmss-10x-x-109">corresponds to intersection and </span><span class="cmsy-10x-x-109">∨ </span><span class="cmss-10x-x-109">to union, the implication is the subset relation. Keep this in mind, as it’s going to be important.</span></p>
<p><span class="cmss-10x-x-109">During inference, we use implications in the following way:</span></p>
<ol>
<li><span id="x1-279008x1"><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-279010x2"><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-279012x3"><span class="cmss-10x-x-109">Therefore, </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">This is</span> <span id="dx1-279013"></span><span class="cmss-10x-x-109">called the </span><span class="cmssi-10x-x-109">modus ponens</span><span class="cmss-10x-x-109">. If it sounds abstract, here is a concrete example:</span></p>
<ol>
<li><span id="x1-279015x1"><span class="cmss-10x-x-109">If it is raining, the sidewalk is wet.</span></span></li>
<li><span id="x1-279017x2"><span class="cmss-10x-x-109">It is raining.</span></span></li>
<li><span id="x1-279019x3"><span class="cmss-10x-x-109">Therefore, the sidewalk is wet.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Thus, we can infer the state of the sidewalk without looking at it. This is bigger than it sounds: modus ponens is a cornerstone of scientific thinking. We would still be living in caves without it. Modus ponens enables us to build robust skyscrapers of knowledge.</span></p>
<p><span class="cmss-10x-x-109">However, it’s not all perfect. Classical deductive logic might help to prove the infinity of prime numbers, but it fails spectacularly when confronted with inference problems outside the realms of mathematics and philosophy.</span></p>
<p><span class="cmss-10x-x-109">Classical logic has a fatal flaw: it is unable to deal with uncertainty. Think about the simple proposition </span><span class="cmssi-10x-x-109">“it is raining outside”</span><span class="cmss-10x-x-109">. If we are unable to actually observe the weather but have some indirect evidence (such as the fact that the sidewalk is wet, or the sky is cloudy, or it’s autumn out there), </span><span class="cmssi-10x-x-109">“it is raining outside” </span><span class="cmss-10x-x-109">is probable but not certain.</span></p>
<p><span class="cmss-10x-x-109">We need a tool to measure the truth value on a </span>0 <span class="cmsy-10x-x-109">− </span>1 <span class="cmss-10x-x-109">scale. This is where probabilities come in.</span></p>
</section>
<section id="thinking-in-probabilities" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_247"><span class="titlemark"><span class="cmss-10x-x-109">18.1.2 </span></span> <span id="x1-28000022.1.2"></span><span class="cmss-10x-x-109">Thinking in probabilities</span></h3>
<p><span class="cmss-10x-x-109">In a</span> <span id="dx1-280001"></span><span class="cmss-10x-x-109">mathematical sense, probability is a function that assigns a numerical value between zero and one to various </span><span class="cmssi-10x-x-109">sets </span><span class="cmss-10x-x-109">that represent events. (You can think of events as propositions.) Events are subsets of the event space, often denoted with the capital Greek letter omega (</span>Ω<span class="cmss-10x-x-109">). This is illustrated in </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">18.2</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1615.png" width="456" alt="PIC"/> <span id="x1-280002r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.2: Events and the event space</span> </span>
</div>
<p><span class="cmss-10x-x-109">This sounds quite abstract, so let’s see a simple example: rolling a fair six-sided dice. We can encode all possible outcomes with the event space </span>Ω = <span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>3<span class="cmmi-10x-x-109">,</span>4<span class="cmmi-10x-x-109">,</span>5<span class="cmmi-10x-x-109">,</span>6<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. Events such as </span><span class="cmmi-10x-x-109">A </span>= <span class="cmss-10x-x-109">”the outcome is even” or </span><span class="cmmi-10x-x-109">B </span>= <span class="cmss-10x-x-109">”the outcome is larger than 3” are represented by the sets</span></p>
<img src="../media/file1616.png" width="150" class="math-display" alt="A = {2,4,6}, B = {4,5,6}. "/>
<p><span class="cmss-10x-x-109">As the dice is fair, the probability of each outcome is the same:</span></p>
<div class="math-display">
<img src="../media/file1617.png" class="math-display" alt=" 1- P ({1}) = ⋅⋅⋅ = P({6}) = 6. "/>
</div>
<p><span class="cmss-10x-x-109">There are two properties that make such a function </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">a proper measure of probability:</span></p>
<ol>
<li><span id="x1-280004x1"><span class="cmss-10x-x-109">the probability of the event space is one,</span></span></li>
<li><span id="x1-280006x2"><span class="cmss-10x-x-109">and the probability of the union of disjoint events is the sum of probabilities.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">In our dice-rolling example, this is translated to, for instance,</span></p>
<img src="../media/file1618.png" class="math-display" alt="P (the outcome is even) = P({2,4,6}) = P({2}) + P({4}) + P({6}) 1 1 1 = --+ --+ -- 6 6 6 = 1. 2 " width="450"/>

<p><span class="cmss-10x-x-109">We’ll talk about these properties extensively in the next section. As logical</span> <span id="dx1-280007"></span><span class="cmss-10x-x-109">connectives can be represented in the language of set theory, set operations translate the semantics of logic into probabilities. Intersection is joint occurrence of events. Union is the occurrence of either one.</span></p>
<div class="minipage">
<p><img src="../media/file1619.png" width="456" alt="PIC"/> <span id="x1-280008r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.3: The probabilities of intersection and union</span> </span>
</div>
<p><span class="cmss-10x-x-109">In this way, we are able to build models involving uncertainty and develop a calculus to work with said models. In the Tower of Babel that is mathematics, </span><span class="cmssi-10x-x-109">statistics </span><span class="cmss-10x-x-109">deals with the modeling part, and </span><span class="cmssi-10x-x-109">probability theory </span><span class="cmss-10x-x-109">deals with the calculus part.</span></p>
<p><span class="cmss-10x-x-109">Even technically well-trained engineers conflate modeling and working with models. For instance, when we talk about flipping fair coins, the probability of heads and tails are both </span>1<span class="cmmi-10x-x-109">∕</span>2<span class="cmss-10x-x-109">. Even when we are absolutely sure about the model but have ten heads in a row, most would immediately jump to the conclusion that our coin is biased.</span></p>
<p><span class="cmss-10x-x-109">To make sure we are not making this mistake, first, we are going to learn what probability is.</span></p>
</section>
</section>
<section id="the-axioms-of-probability" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_248"><span class="titlemark"><span class="cmss-10x-x-109">18.2 </span></span> <span id="x1-28100022.2"></span><span class="cmss-10x-x-109">The axioms of probability</span></h2>
<p><span class="cmss-10x-x-109">In the</span><span id="dx1-281001"></span> <span class="cmss-10x-x-109">previous section, we have talked about probability as an extension of mathematical logic. Just like formal logic, probability has its axioms, which we need to understand to work with probability models. Now, we are going to seek the answer to a fundamental question: what is the mathematical model of probability and how do we work with it?</span></p>
<p><span class="cmss-10x-x-109">Probabilities are defined in the context of experiments and outcomes. To talk about probabilities, we need to define what we assign probabilities to. Formally speaking, we denote the probability of the event </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">by </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>)<span class="cmss-10x-x-109">. First, we’ll talk about what events are.</span></p>
<section id="event-spaces-and-algebras" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_249"><span class="titlemark"><span class="cmss-10x-x-109">18.2.1 </span></span> <span id="x1-28200022.2.1"></span><span class="cmss-10x-x-109">Event spaces and </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras</span></h3>
<p><span class="cmss-10x-x-109">Let’s revisit the six-sided example from the previous section. There are</span><span id="dx1-282001"></span> <span class="cmss-10x-x-109">six different mutually exclusive outcomes (that is, events that cannot occur at the same time), and together they form the </span><span class="cmssi-10x-x-109">event space</span><span class="cmss-10x-x-109">, denoted by</span> Ω<span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file1620.png" class="math-display" alt="Ω := {1,2,3,4,5,6}. "/>
</div>
<p><span class="cmss-10x-x-109">In general, the event space is the collection of all mutually exclusive outcomes. It can be any set.</span></p>
<p><span class="cmss-10x-x-109">What kind of events can we assign probabilities to? Obviously, the individual outcomes come to mind. However, we can think of events such as </span><span class="cmssi-10x-x-109">“the result is an odd number”</span><span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">“the result is 2 or 6”</span><span class="cmss-10x-x-109">, or </span><span class="cmssi-10x-x-109">“the result is not 1”</span><span class="cmss-10x-x-109">. Following this logic, our expectations are that for any two events </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">,</span></p>
<ul>
<li><span class="cmssi-10x-x-109">“</span><span class="cmmi-10x-x-109">A </span><span class="cmssi-10x-x-109">or </span><span class="cmmi-10x-x-109">B</span><span class="cmssi-10x-x-109">”</span><span class="cmss-10x-x-109">,</span></li>
<li><span class="cmssi-10x-x-109">“</span><span class="cmmi-10x-x-109">A </span><span class="cmssi-10x-x-109">and </span><span class="cmmi-10x-x-109">B</span><span class="cmssi-10x-x-109">”</span><span class="cmss-10x-x-109">,</span></li>
<li><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">“not </span><span class="cmmi-10x-x-109">A</span><span class="cmssi-10x-x-109">”</span></li>
</ul>
<p><span class="cmss-10x-x-109">are events as</span> <span id="dx1-282002"></span><span class="cmss-10x-x-109">well. These</span> <span id="dx1-282003"></span><span class="cmss-10x-x-109">can be translated to the language of set theory and are formalized by the notion of </span><span class="cmmi-10x-x-109">σ</span><span class="cmssi-10x-x-109">-algebras</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-282004r76"></span> <span class="cmbx-10x-x-109">Definition 76.</span> </span></p>
<p><span class="cmbx-10x-x-109">(</span><span class="cmmi-10x-x-109">σ</span><span class="cmbx-10x-x-109">-algebras)</span></p>
<p>Let Ω be an event space. A collection of its subsets Σ <span class="cmsy-10x-x-109">⊆ </span>2<sup><span class="cmr-8">Ω</span></sup> is called an <span class="cmmi-10x-x-109">σ</span>-algebra over Ω if the following properties hold:</p>
<p><span class="cmti-10x-x-109">(a) </span>Ω <span class="cmsy-10x-x-109">∈ </span>Σ. (That is, the set of all outcomes is an event.)</p>
<p><span class="cmti-10x-x-109">(b) </span>For all <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ, the set Ω<span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A </span>is also an element of Σ. (That is, <span class="cmmi-10x-x-109">σ</span>-algebras are closed to complements.)</p>
<p><span class="cmti-10x-x-109">(c) </span>For all <span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ, the set <span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> is also an element of Σ. (That is, <span class="cmmi-10x-x-109">σ</span>-algebras are closed to unions.)</p>
</div>
<p><span class="cmss-10x-x-109">Since events are modeled by sets, logical concepts such as </span><span class="cmssi-10x-x-109">“and”</span><span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">“or”</span><span class="cmss-10x-x-109">, and </span><span class="cmssi-10x-x-109">“not” </span><span class="cmss-10x-x-109">can be translated into set operations:</span></p>
<ul>
<li><span class="cmss-10x-x-109">the joint occurrence of events </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is equivalent to </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">,</span></li>
<li><span class="cmss-10x-x-109">“</span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">or </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">” is equivalent to </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∪</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">,</span></li>
<li><span class="cmss-10x-x-109">and “not </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">” is equivalent to </span>Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></li>
</ul>
<p><span class="cmss-10x-x-109">An immediate</span> <span id="dx1-282005"></span><span class="cmss-10x-x-109">consequence of the definition is that, for any events </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ<span class="cmss-10x-x-109">, their intersection </span><span class="cmsy-10x-x-109">∩</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is also a member of </span>Σ<span class="cmss-10x-x-109">. Indeed, as De Morgan’s laws (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch037.xhtml#x1-377003r153"><span class="cmssi-10x-x-109">153</span></a><span class="cmss-10x-x-109">) suggest,</span></p>
<div class="math-display">
<img src="../media/file1623.png" class="math-display" alt=" ∞ ∞ Ω ∖(∩ n=1 An ) = ∪n=1(Ω ∖ An). "/>
</div>
<p><span class="cmss-10x-x-109">Since </span>Ω <span class="cmsy-10x-x-109">∖ </span>(Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span>) = <span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file1624.png" class="math-display" alt=" ∞ ∞ ∩ n=1An = Ω ∖ (Ω ∖(∩n=1An )) ∞ = Ω ∖ ∪n=1( Ω◟-∖◝A◜n◞). ∈Σ "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the</span> <span id="dx1-282006"></span><span class="cmss-10x-x-109">defining properties of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras guarantee that </span><span class="cmsy-10x-x-109">∩</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is indeed an element of </span>Σ<span class="cmss-10x-x-109">. Another immediate</span> <span id="dx1-282007"></span><span class="cmss-10x-x-109">consequence of the definition is that since </span>Ω <span class="cmsy-10x-x-109">∈ </span>Σ<span class="cmss-10x-x-109">, the empty set </span><span class="cmsy-10x-x-109">∅ </span><span class="cmss-10x-x-109">is also a member of </span>Σ<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">At first glance, </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras seem a bit abstract. As usual, a bit of abstraction now will pay us huge dividends later in our studies. To bring this concept closer, here is a summary of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras in English:</span></p>
<ul>
<li><span class="cmss-10x-x-109">The set of all possible outcomes is an event.</span></li>
<li><span class="cmss-10x-x-109">For any event, it not occurring is an event as well.</span></li>
<li><span class="cmss-10x-x-109">For any events, their joint occurrence is an event as well.</span></li>
<li><span class="cmss-10x-x-109">For any events, at least one of them occurring is an event as well.</span></li>
</ul>
<p><span class="cmss-10x-x-109">Now that we have the formal definition under our belt, let’s see the first example.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">Rolling a six-sided dice. There, the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra is simply the power set of the event space:</span></p>
<div class="math-display">
<img src="../media/file1625.png" class="math-display" alt="Ω = {1,2,3,4,5,6}, Σ = 2Ω. "/>
</div>
<p><span class="cmss-10x-x-109">(Recall that the power set of </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is the set </span>2<sup><span class="cmmi-8">A</span></sup> <span class="cmss-10x-x-109">containing all subsets of </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, as defined in </span><span class="cmssi-10x-x-109">Definition </span><a href="ch037.xhtml#x1-374009r108"><span class="cmssi-10x-x-109">108</span></a><span class="cmss-10x-x-109">.) Even though this is one of the simplest examples, it will serve as a prototype and a building block for constructing more complicated event spaces.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">Tossing a coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times. A single toss has two</span><span id="dx1-282008"></span> <span class="cmss-10x-x-109">possible outcomes: heads or tails. For simplicity, we are going to encode heads with </span>0 <span class="cmss-10x-x-109">and tails with </span>1<span class="cmss-10x-x-109">. Since we are tossing the coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times, the result of an experiment will be an </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-long sequence of ones and zeros, like this: </span>(0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,…,</span>0<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">. Thus, the complete event space is</span> Ω = <span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmsy-10x-x-109">}</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">(We are not talking about probabilities just yet, but feel free to spend some time figuring out how to assign them to these events. Don’t worry if this is not clear; we will go through it in detail.)</span></p>
<p><span class="cmss-10x-x-109">As in the previous example, the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra </span>2<sup><span class="cmr-8">Ω</span></sup> <span class="cmss-10x-x-109">is a good choice. This covers all events that we need, for instance, </span><span class="cmssi-10x-x-109">“the number of tails is </span><span class="cmmi-10x-x-109">k</span><span class="cmssi-10x-x-109">”</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In practice, </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras are rarely given explicitly. Sure, for</span> <span id="dx1-282009"></span><span class="cmss-10x-x-109">simple cases such as the</span> <span id="dx1-282010"></span><span class="cmss-10x-x-109">above, it is possible.</span></p>
<p><span class="cmss-10x-x-109">What about cases where the event spaces are not countable? For instance, suppose that we are picking a random number between </span>0 <span class="cmss-10x-x-109">and </span>1<span class="cmss-10x-x-109">. Then, </span>Ω = [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, but selecting </span>Σ = 2<sup><span class="cmr-8">[0</span><span class="cmmi-8">,</span><span class="cmr-8">1]</span></sup> <span class="cmss-10x-x-109">is extremely problematic. Recall that we want to assign a probability to every event in </span>Σ<span class="cmss-10x-x-109">. The power set </span>2<sup><span class="cmr-8">[0</span><span class="cmmi-8">,</span><span class="cmr-8">1]</span></sup> <span class="cmss-10x-x-109">is so large that </span><span class="cmssi-10x-x-109">very </span><span class="cmss-10x-x-109">strange things can occur. In certain scenarios, we can cut up sets into a finite number of pieces and reassemble </span><span class="cmssi-10x-x-109">two identical copies of the set </span><span class="cmss-10x-x-109">from its pieces. (If you are interested in more, check out the </span><a href="https://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox"><span class="cmss-10x-x-109">Banach-Tarski paradox</span></a><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">To avoid weird things like the above-mentioned, we need another way to describe </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras.</span></p>
</section>
<section id="describing-algebras" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_250"><span class="titlemark"><span class="cmss-10x-x-109">18.2.2 </span></span> <span id="x1-28300022.2.2"></span><span class="cmss-10x-x-109">Describing </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras</span></h3>
<p><span class="cmss-10x-x-109">Let’s start</span> <span id="dx1-283001"></span><span class="cmss-10x-x-109">with a simple yet fundamental property of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras that we’ll soon use to give a friendly description of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-283002r109"></span> <span class="cmbx-10x-x-109">Theorem 109.</span> </span> <span class="cmbxti-10x-x-109">(Intersection of event algebras.)</span></p>
<p><span class="cmti-10x-x-109">Let </span>Ω <span class="cmti-10x-x-109">be a sample space, and let </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmti-10x-x-109">and </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmti-10x-x-109">be two </span><span class="cmmi-10x-x-109">σ</span><span class="cmti-10x-x-109">-algebras over it. Then</span> Σ<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∩ </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmti-10x-x-109">is also a </span><span class="cmmi-10x-x-109">σ</span><span class="cmti-10x-x-109">-algebra.</span></p>
</div>
<div id="tcolobox-291" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As we saw in the definition of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch030.xhtml#x1-282004r76"><span class="cmssi-10x-x-109">76</span></a><span class="cmss-10x-x-109">), there are three properties we need to verify to show that </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∩ </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">is indeed a </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra. This is very simple to check, so I suggest taking a shot by yourself first before reading my explanation.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">As both </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">are </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras, </span>Ω <span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span>Ω <span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">both hold. Thus, by definition of the intersection, </span>Ω <span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∩ </span>Σ<sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∩ </span>Σ<sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. As both of them are </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras, </span>Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and</span> Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">. Thus, </span>Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is an element of the intersection as well.</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅ <span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∩ </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">be arbitrary events. We can use the exact same argument as before: since both </span>Σ<sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span>Σ<sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">are </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras, we have</span></p>
<div class="math-display">
<img src="../media/file1627.png" class="math-display" alt="⋃∞ ∞⋃ An ∈ Σ1 and An ∈ Σ2. n=1 n=1 "/>
</div>
<p><span class="cmss-10x-x-109">So, the union is also a member of the intersection, i.e.,</span></p>
<div class="math-dislay">
<img src="../media/file1628.png" width="150" class="math-display" alt="⋃∞ An ∈ Σ1 ∩ Σ2. n=1 "/>
</div>
</div>
</div>
<p><span class="cmss-10x-x-109">With all that, we are ready to describe </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras with a generating set.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-283003r110"></span> <span class="cmbx-10x-x-109">Theorem 110.</span> </span> <span class="cmbxti-10x-x-109">(Generated </span><span class="cmmi-10x-x-109">σ</span><span class="cmbxti-10x-x-109">-algebras)</span></p>
<p><span class="cmti-10x-x-109">Let </span>Ω <span class="cmti-10x-x-109">be an event space and </span><span class="cmmi-10x-x-109">S </span><span class="cmsy-10x-x-109">⊆ </span>2<sup><span class="cmr-8">Ω</span></sup> <span class="cmti-10x-x-109">be an arbitrary collection of its sets. Then there is an unique smallest </span><span class="cmmi-10x-x-109">σ</span><span class="cmti-10x-x-109">-algebra </span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">S</span>) <span class="cmti-10x-x-109">that contains </span><span class="cmmi-10x-x-109">S</span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">(By smallest, we mean that if </span>Σ <span class="cmss-10x-x-109">is an </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra containing </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">S</span>) <span class="cmsy-10x-x-109">⊆ </span>Σ<span class="cmss-10x-x-109">.)</span></p>
<div id="tcolobox-292" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Our previous result shows that the intersection of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras is also an </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra. So, let’s take all </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras that contain </span><span class="cmmi-10x-x-109">S </span><span class="cmss-10x-x-109">and take their intersection. Formally, we define</span></p>
<div class="math-display">
<img src="../media/file1629.png" class="math-display" alt="σ (S ) = ∩ {Σ : Σ is an σ-algebra and S ⊆ Σ}. "/>
</div>
<p><span class="cmss-10x-x-109">By definition, </span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">S</span>) <span class="cmss-10x-x-109">is clearly the smallest, and it also contains </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">It’s also clear that the generated </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra is unique as, if there would be another</span> σ̂(<span class="cmmi-10x-x-109">S</span>) <span class="cmss-10x-x-109">satisfying the conditions, then due to the construction,</span> σ̂
(<span class="cmmi-10x-x-109">S</span>) <span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">S</span>) <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">S</span>) <span class="cmsy-10x-x-109">⊆</span>σ̂(<span class="cmmi-10x-x-109">S</span>)<span class="cmss-10x-x-109">, hence</span> σ̂(<span class="cmmi-10x-x-109">S</span>) = <span class="cmmi-10x-x-109">σ</span>(<span class="cmmi-10x-x-109">S</span>)<span class="cmss-10x-x-109">.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Right away, we</span> <span id="dx1-283004"></span><span class="cmss-10x-x-109">can use this to precisely construct the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra for an extremely common task: picking a number between </span>0 <span class="cmss-10x-x-109">and </span>1<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmss-10x-x-109">Selecting a random number between </span>0 <span class="cmss-10x-x-109">and </span>1<span class="cmss-10x-x-109">. It is clear that the event space is </span>Ω = [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">. What about the events? In this situation, we want to ask questions such as the probability of a random number </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">falling between some </span><span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">. That is, events such as </span>(<span class="cmmi-10x-x-109">a,b</span>)<span class="cmmi-10x-x-109">,</span>(<span class="cmmi-10x-x-109">a,b</span>]<span class="cmmi-10x-x-109">,</span>[<span class="cmmi-10x-x-109">a,b</span>)<span class="cmmi-10x-x-109">,</span>[<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">(whether or not we want strict inequality regarding </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">b</span><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">So, a proper </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra can be given by the algebra generated by events of the form</span> (<span class="cmmi-10x-x-109">a,b</span>]<span class="cmss-10x-x-109">. That is,</span></p>
<div class="math-display">
<img src="../media/file1634.png" class="math-display" alt="Σ = σ({(a,b] : 0 ≤ a &lt;b ≤ 1}). "/>
</div>
<p><span class="cmss-10x-x-109">This </span>Σ <span class="cmss-10x-x-109">has a rich structure. For instance, it contains simple events such as </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">x</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, but also more complex ones such as “</span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is a rational number” or “</span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is an irrational number”. Give yourself a few minutes to see why this is true. Don’t worry if you don’t see the solution—we’ll work this out in the problems section. (If you think this through, you’ll also see why we chose intervals of the form </span>(<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">instead of others such as </span>(<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">or</span> [<span class="cmmi-10x-x-109">a,b</span>]<span class="cmss-10x-x-109">.)</span></p>
</section>
<section id="algebras-over-real-numbers" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_251"><span class="titlemark"><span class="cmss-10x-x-109">18.2.3 </span></span> <span id="x1-28400022.2.3"></span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras over real numbers</span></h3>
<p><span class="cmss-10x-x-109">From all the</span><span id="dx1-284001"></span> <span class="cmss-10x-x-109">examples we have seen so far, it is clear that most commonly, we define probability spaces on </span><span class="msbm-10x-x-109">ℕ </span><span class="cmss-10x-x-109">or on </span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">. When </span>Ω <span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℕ</span><span class="cmss-10x-x-109">, the choice of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra is clear, as</span> Σ = 2<sup><span class="cmr-8">Ω</span></sup> <span class="cmss-10x-x-109">will always work.</span></p>
<p><span class="cmss-10x-x-109">However, as suggested in Example 3 above, selecting </span>Σ = 2<sup><span class="cmr-8">Ω</span></sup> <span class="cmss-10x-x-109">when </span>Ω <span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">can lead to some weird stuff. Because we are interested in the probability of events such as</span> [<span class="cmmi-10x-x-109">a,b</span>]<span class="cmss-10x-x-109">, our standard choice is going to be the generated </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra</span></p>
<div class="math-display">
  <span>
    ℬ(ℝ) = σ({ (a,b) : a,b ∈ ℝ }),
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.1)</span>
</div>

<p><span class="cmss-10x-x-109">called the Borel algebra, named after the famous French mathematician </span><a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel"><span class="cmss-10x-x-109">Émile Borel</span></a><span class="cmss-10x-x-109">. Due to its construction, </span><span class="cmsy-10x-x-109">ℬ </span><span class="cmss-10x-x-109">contains all events that are important to us, such as intervals and unions of intervals. Elements of </span><span class="cmsy-10x-x-109">ℬ </span><span class="cmss-10x-x-109">are called Borel sets.</span></p>
<p><span class="cmss-10x-x-109">Because </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras are closed to unions, you can see that all types of intervals can be found in </span><span class="cmsy-10x-x-109">ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmss-10x-x-109">. This is summarized by the following theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-284002r111"></span> <span class="cmbx-10x-x-109">Theorem 111.</span> </span></p>
<p><span class="cmti-10x-x-109">For all </span><span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><span class="cmti-10x-x-109">, the sets </span>[<span class="cmmi-10x-x-109">a,b</span>]<span class="cmti-10x-x-109">, </span>(<span class="cmmi-10x-x-109">a,b</span>]<span class="cmti-10x-x-109">, </span>[<span class="cmmi-10x-x-109">a,b</span>)<span class="cmti-10x-x-109">, </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,a</span>]<span class="cmti-10x-x-109">, </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,a</span>)<span class="cmti-10x-x-109">, </span>(<span class="cmmi-10x-x-109">a,</span><span class="cmsy-10x-x-109">∞</span>)<span class="cmti-10x-x-109">, and</span> [<span class="cmmi-10x-x-109">a,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmti-10x-x-109">are elements of </span><span class="cmsy-10x-x-109">ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">As an exercise, try to come up with the proof by yourself. One trick to get the ideas flowing is to start drawing some figures. If you can visualize what happens, you’ll discover a proof quickly.</span></p>
<div id="tcolobox-293" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">In general, for a given set </span><span class="cmmi-10x-x-109">S</span><span class="cmss-10x-x-109">, we can show that it belongs to </span><span class="cmsy-10x-x-109">ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>) <span class="cmss-10x-x-109">by writing it as the union/intersection/difference of known Borel sets. First, we have</span></p>
<div class="math-dispay">
<img src="../media/file1637.png" width="150" class="math-display" alt=" ∞ ⋃ (a,∞ ) = (a,n), n=1 "/>
</div>
<p><span class="cmss-10x-x-109">so </span>(<span class="cmmi-10x-x-109">a,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmsy-10x-x-109">∈ ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmss-10x-x-109">. With a similar argument, we see that </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">∈ ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Next,</span></p>
<div class="math-display">
<img src="../media/file1638.png" class="math-display" alt="(− ∞, a] = ℝ ∖ (a,∞ ), [a,∞ ) = ℝ ∖ (− ∞, a), "/>
</div>
<p><span class="cmss-10x-x-109">so </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,a</span>]<span class="cmmi-10x-x-109">,</span>[<span class="cmmi-10x-x-109">a,</span><span class="cmsy-10x-x-109">∞</span>) <span class="cmsy-10x-x-109">∈ ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>) <span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">a</span><span class="cmss-10x-x-109">. From these, the sets</span> [<span class="cmmi-10x-x-109">a,b</span>]<span class="cmmi-10x-x-109">,</span>(<span class="cmmi-10x-x-109">a,b</span>]<span class="cmmi-10x-x-109">,</span>[<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">can be produced by intersections.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Now that</span> <span id="dx1-284003"></span><span class="cmss-10x-x-109">we understand what events and </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras are, we can take our first detailed look at </span><span class="cmssi-10x-x-109">probability</span><span class="cmss-10x-x-109">. In the next section, we will introduce its precise mathematical definition.</span></p>
</section>
<section id="probability-measures" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_252"><span class="titlemark"><span class="cmss-10x-x-109">18.2.4 </span></span> <span id="x1-28500022.2.4"></span><span class="cmss-10x-x-109">Probability measures</span></h3>
<p><span class="cmss-10x-x-109">Let’s recap what</span> <span id="dx1-285001"></span><span class="cmss-10x-x-109">we have learned so far! In the language of mathematics, experiments with intrinsic uncertainty are described using outcomes, event spaces, and events.</span></p>
<p><span class="cmss-10x-x-109">The collection of all possible mutually exclusive outcomes of an experiment is called the </span><span class="cmssi-10x-x-109">event space</span><span class="cmss-10x-x-109">, denoted by </span>Ω<span class="cmss-10x-x-109">. Certain subsets of </span>Ω <span class="cmss-10x-x-109">are called </span><span class="cmssi-10x-x-109">events</span><span class="cmss-10x-x-109">, to which we want to assign probabilities.</span></p>
<p><span class="cmss-10x-x-109">These events form what is known as a </span><span class="cmmi-10x-x-109">σ</span><span class="cmssi-10x-x-109">-algebra</span><span class="cmss-10x-x-109">, denoted by </span>Σ<span class="cmss-10x-x-109">. We denote the probability of an event </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">by </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Intuitively speaking, we have three reasonable expectations about probability:</span></p>
<ol>
<li><span id="x1-285003x1"><span class="cmmi-10x-x-109">P</span>(Ω) = 1<span class="cmss-10x-x-109">, that is, the probability that at least one outcome occurs is </span>1<span class="cmss-10x-x-109">. In other words, our event space is a complete description of the experiment.</span></span></li>
<li><span id="x1-285005x2"><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">∅</span>) = 0<span class="cmss-10x-x-109">, that is, the probability that </span><span class="cmssi-10x-x-109">none </span><span class="cmss-10x-x-109">of the outcomes occur is </span>0<span class="cmss-10x-x-109">. Again, this means that our event space is complete.</span></span></li>
<li><span id="x1-285007x3"><span class="cmss-10x-x-109">The probability that either of two events occurs for two mutually exclusive events is the sum of the individual probabilities.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">These are formalized by the following definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-285008r77"></span> <span class="cmbx-10x-x-109">Definition 77.</span> </span><span class="cmbx-10x-x-109">(Probability measures and spaces) </span>Let Ω be an event space and Σ be an <span class="cmmi-10x-x-109">σ</span>-algebra over Ω. We say that the function <span class="cmmi-10x-x-109">P </span>: Σ <span class="cmsy-10x-x-109">→ </span>[0<span class="cmmi-10x-x-109">,</span>1] is a <span class="cmti-10x-x-109">probability measure </span>on Σ if the following properties hold:</p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">P</span>(Ω) = 1.</p>
<p><span class="cmti-10x-x-109">(b) </span>If <span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span> are mutually disjoint events (that is, <span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">j</span></sub> = <span class="cmsy-10x-x-109">∅ </span>for all <span class="cmmi-10x-x-109">i≠j</span>), then <span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>) = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>). This property is called the <span class="cmmi-10x-x-109">σ</span>-additivity of probability measures.</p>
<p>Along with the probability measure <span class="cmmi-10x-x-109">P</span>, the structure (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) is said to form a <span class="cmti-10x-x-109">probability space</span>.</p>
</div>
<p><span class="cmss-10x-x-109">As usual, let’s see some concrete examples first! We are going to continue with the ones we worked out when discussing </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1, continued. </span><span class="cmss-10x-x-109">Rolling a six-sided dice. Recall that the event space and algebra were defined by</span></p>
<div class="math-display">
<img src="../media/file1639.png" class="math-display" alt=" Ω Ω = {1,2,3,4,5,6}, Σ = 2 . "/>
</div>
<p><span class="cmss-10x-x-109">If we don’t</span> <span id="dx1-285009"></span><span class="cmss-10x-x-109">have any extra knowledge about our dice, it is reasonable to assume that each outcome is equally probable. That is, since there are six possible outcomes, we have</span></p>
<div class="math-display">
<img src="../media/file1640.png" class="math-display" alt="P ({1}) = ⋅⋅⋅ = P({6}) = 1. 6 "/>
</div>
<p><span class="cmss-10x-x-109">Notice that, in this case, knowing the probabilities for the individual outcomes is enough to determine the probability of any event. This is due to the (</span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-)additivity of the probability. For instance, the event </span><span class="cmssi-10x-x-109">“the outcome of the dice roll is an odd number” </span><span class="cmss-10x-x-109">is described by</span></p>
<div class="math-display">
<img src="../media/file1641.png" class="math-display" alt=" 3 P ({1,3,5} ) = P ({1})+ P ({3 })+ P ({5} ) = 6-. "/>
</div>
<p><span class="cmss-10x-x-109">In English, the probability of any event can be written down with the following formula:</span></p>
<div class="math-display">
<img src="../media/file1642.png" class="math-display" alt="P (event) = -favorable outcomes-. all possible outcomes "/>
</div>
<p><span class="cmss-10x-x-109">You might remember this from your elementary and high school studies (depending on the curriculum in your country). This is a useful formula, but there is a caveat: it only works if we assume that each outcome has an equal probability.</span></p>
<p><span class="cmss-10x-x-109">In the case when our dice is not uniformly weighted, the occurrences of individual outcomes are not equal. (Just think of a lead dice, where one side is significantly heavier than the others.) For now, we are not going to be concerned with this case. Later, this generalization will be discussed in detail.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2, continued. </span><span class="cmss-10x-x-109">Tossing a coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times. Here, our event space and algebra was </span>Ω = <span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmsy-10x-x-109">}</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">and </span>Σ = 2<sup><span class="cmr-8">Ω</span></sup><span class="cmss-10x-x-109">. For simplicity, let’s assume that </span><span class="cmmi-10x-x-109">n </span>= 5<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">What is the probability of a particular result, say HHTTT? Going step by step, the probability that the first toss will be </span><span class="cmssi-10x-x-109">heads </span><span class="cmss-10x-x-109">is </span>1<span class="cmmi-10x-x-109">∕</span>2<span class="cmss-10x-x-109">. That is,</span></p>
<div class="math-display">
<img src="../media/file1643.png" class="math-display" alt=" 1 P(first toss is heads) =- 2 "/>
</div>
<p><span class="cmss-10x-x-109">Since the first toss is independent of the second,</span></p>
<div class="math-display">
<img src="../media/file1644.png" class="math-display" alt="P(second toss is heads) = 1 2 "/>
</div>
<p><span class="cmss-10x-x-109">as well. To</span> <span id="dx1-285010"></span><span class="cmss-10x-x-109">combine this and calculate the probability that the </span><span class="cmssi-10x-x-109">first two </span><span class="cmss-10x-x-109">tosses are both heads, we can think in the following way. Among the outcomes where the first toss is heads, exactly half of them will have the second toss heads as well. So, we are looking for the </span><span class="cmssi-10x-x-109">half of the half</span><span class="cmss-10x-x-109">. That is,</span></p>

<img src="../media/file1645.png" width="650" class="math-display" alt="P(first two tosses are heads) = P(first toss is heads)P(second toss is heads) 1- = 4. "/>
<p><span class="cmss-10x-x-109">Going further with the same logic, we obtain</span></p>
<div class="math-display">
<img src="../media/file1646.png" class="math-display" alt=" 1 1 P (HHTTT ) = -5 = --. 2 32 "/>
</div>
<p><span class="cmss-10x-x-109">If we look a bit deeper, we will notice that this follows the previously seen “favorable/all” formula. Indeed, as we can see with a bit of combinatorics, there are</span> 2<sup><span class="cmr-8">5</span></sup> <span class="cmss-10x-x-109">total possibilities, all of them having equal probability.</span></p>
<p><span class="cmss-10x-x-109">Considering this, what is the probability that out of our five tosses, exactly two of them are heads? In the language of sets, we can encode each five-toss experiment as a subset of </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>3<span class="cmmi-10x-x-109">,</span>4<span class="cmmi-10x-x-109">,</span>5<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, the elements signifying the toss that resulted in heads. (So, for example, </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>4<span class="cmmi-10x-x-109">,</span>5<span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">would encode the outcome HTTHH.) With this, the experiments when there are two heads are exactly the two-element subsets of </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>3<span class="cmmi-10x-x-109">,</span>4<span class="cmmi-10x-x-109">,</span>5<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">From our combinatorics studies, we know that the number of subsets with given elements is</span></p>
<div class="math-display">
<img src="../media/file1647.png" class="math-display" alt="( ) n = ---n!----, k k!(n − k)! "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">is the size of our set and </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">is the desired size of the subsets. So, in total, there are</span> <img src="../media/file1648.png" width="15" alt="(5) 2"/> <span class="cmss-10x-x-109">number of occurrences with exactly two heads. Thus, following the “favorable/all” formula, we have</span></p>
<div class="math-display">
<img src="../media/file1649.png" class="math-display" alt=" ( ) P(two heads out of five tosses) = 5 -1-= 10. 2 32 32 "/>
</div>
<p><span class="cmss-10x-x-109">One more example, and we are ready to move forward.</span></p>
<p><span class="cmssbx-10x-x-109">Example 3, continued. </span><span class="cmss-10x-x-109">Selecting a random number between </span>0 <span class="cmss-10x-x-109">and </span>1<span class="cmss-10x-x-109">. Here, our event space was </span>Ω = [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, and our </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra was the generated algebra</span></p>
<div class="math-display">
<img src="../media/file1650.png" class="math-display" alt=" ( ) Σ = σ {(a,b] : 0 ≤ a &lt;b ≤ 1} . "/>
</div>
<p><span class="cmss-10x-x-109">Without any further information, it is reasonable to assume that every</span><span id="dx1-285011"></span> <span class="cmss-10x-x-109">number can be selected with an equal probability. What does this even mean for an infinite event space such as </span>Ω = [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">? We can’t divide </span>1 <span class="cmss-10x-x-109">into infinitely many equal parts.</span></p>
<p><span class="cmss-10x-x-109">So, instead of thinking about individual outcomes, we should start thinking about events. Let’s denote our randomly selected number with </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. If all numbers are “equally likely”, what is </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∈ </span>(0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">∕</span>2])<span class="cmss-10x-x-109">? Intuitively, given our equally likely hypothesis, this probability should be proportional to the size of</span> [0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">∕</span>2]<span class="cmss-10x-x-109">. Thus,</span></p>
<div class="math-display">
<img src="../media/file1651.png" class="math-display" alt="P (X ∈ I) = |I|, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">I </span><span class="cmss-10x-x-109">is some interval and </span><span class="cmmi-10x-x-109">jIj </span><span class="cmss-10x-x-109">is its length. For instance,</span></p>
<div class="math-display">
<img src="../media/file1652.png" class="math-display" alt="P (a &lt;X &lt;b) = P(a ≤ X &lt; b) = P(a &lt;X ≤ b) = P(a ≤ X ≤ b) = b− a. "/>
</div>
<p><span class="cmss-10x-x-109">By giving the probabilities on the generating set of the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra, the probabilities for all other events can be deduced. For instance,</span></p>
<div class="math-display">
<img src="../media/file1653.png" class="math-display" alt="P (X = x) = P(0 ≤ X ≤ x)− P (0 ≤ X &lt;x) = x − x = 0. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the probability of picking a given number is zero. There is an important lesson here: </span><span class="cmssbx-10x-x-109">events with zero probability can happen. </span><span class="cmss-10x-x-109">This sounds counterintuitive at first, but based on the above example, you can see that it is true.</span></p>
</section>
<section id="fundamental-properties-of-probability" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_253"><span class="titlemark"><span class="cmss-10x-x-109">18.2.5 </span></span> <span id="x1-28600022.2.5"></span><span class="cmss-10x-x-109">Fundamental properties of probability</span></h3>
<p><span class="cmss-10x-x-109">Now that we are</span><span id="dx1-286001"></span> <span class="cmss-10x-x-109">familiar with the mathematical model of probability, we can start working with them. Manipulating expressions of probabilities gives us the ability to deal with complex scenarios.</span></p>
<p><span class="cmss-10x-x-109">If you recall, probability measures had three simple defining properties (see </span><span class="cmssi-10x-x-109">Definition </span><a href="ch030.xhtml#x1-285008r77"><span class="cmssi-10x-x-109">77</span></a><span class="cmss-10x-x-109">):</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmmi-10x-x-109">P</span>(Ω) = 1<span class="cmss-10x-x-109">,</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">∅</span>) = 0<span class="cmss-10x-x-109">, and</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmmi-10x-x-109">P</span><img src="../media/file1654.png" width="55" data-align="middle" alt="⋃ ∞ ( n=1 An)"/> = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">, if the events </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">are mutually disjoint.</span></p>
<p><span class="cmss-10x-x-109">From these properties, many others can be deduced. For simplicity, here</span><span id="dx1-286002"></span> <span class="cmss-10x-x-109">is a theorem summarizing the most important ones.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-286003r112"></span> <span class="cmbx-10x-x-109">Theorem 112.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be two arbitrary events.</span></p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∪</span><span class="cmmi-10x-x-109">B</span>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>) + <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span>) <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">B</span>)<span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">(b) </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">B</span>) + <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">B</span>)<span class="cmti-10x-x-109">. Specifically, </span><span class="cmmi-10x-x-109">P</span>(Ω <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span>) + <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>) = 1<span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">(c) If </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">B</span><span class="cmti-10x-x-109">, then </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>) <span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span>)<span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">The proof of this is so simple that it is left to you as an exercise. All of these follow from the additivity of probability measures with respect to disjoint events. (If you don’t see the solution, sketch some Venn diagrams!)</span></p>
<p><span class="cmss-10x-x-109">Another fundamental tool is the </span><span class="cmssi-10x-x-109">law of total probability</span><span class="cmss-10x-x-109">, which is used all the time when dealing with more complex events.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-286004r113"></span> <span class="cmbx-10x-x-109">Theorem 113.</span> </span> <span class="cmbxti-10x-x-109">(Law of total probability)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be an arbitrary event. If </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">are mutually disjoint events (that is, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">j</span></sub> = <span class="cmsy-10x-x-109">∅</span><span class="cmti-10x-x-109">if </span><span class="cmmi-10x-x-109">i≠j</span><span class="cmti-10x-x-109">) for which </span><span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> = Ω<span class="cmti-10x-x-109">, then</span></p>
<div class="math-display">
  <span>
    P(A) = ∑<sub>n=1</sub><sup>∞</sup> P(A ∩ A<sub>n</sub>).
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.2)</span>
</div>

<p><span class="cmti-10x-x-109">We call mutually disjoint events whose union is the entire event space partitions.</span></p>
</div>
<div id="tcolobox-294" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This simply follows from the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-additivity of probability measures. Feel free to give the proof a shot by yourself to test your understanding.</span></p>
<p><span class="cmss-10x-x-109">If you can’t see this, no worries. Here is a brief explanation. Since </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,… </span><span class="cmss-10x-x-109">are mutually disjoint, </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,… </span><span class="cmss-10x-x-109">are mutually disjoint as well. Moreover, since </span><span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> = Ω<span class="cmss-10x-x-109">, we also have</span></p>
<div class="math-display">
<img src="../media/file1656.png" class="math-display" alt="∞⋃ ( ∞⋃ ) (An ∩ A) = An ∩ A n=1 n=1 = Ω ∩ A = A. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-additivity of probability measures implies that</span></p>
<div class="math-display">
<img src="../media/file1657.png" class="math-display" alt=" ∞ P (A) = ∑ P (A ∩ A ), n=1 n "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s see an example right away! Suppose that we toss </span><span class="cmssi-10x-x-109">two </span><span class="cmss-10x-x-109">dice. What is</span> <span id="dx1-286005"></span><span class="cmss-10x-x-109">the probability that the sum of the results is </span>7<span class="cmss-10x-x-109">?</span></p>
<p><span class="cmss-10x-x-109">First, we should properly describe the probability space. For notational simplicity, let’s denote the result of the throws with </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">. What we are looking for is </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>+ <span class="cmmi-10x-x-109">Y </span>= 7)<span class="cmss-10x-x-109">. Modeling the toss with two dice is the simplest if we impose order among them. With this in mind, the event space </span>Ω <span class="cmss-10x-x-109">is described by the Cartesian product</span></p>
<div class="math-display">
<img src="../media/file1658.png" class="math-display" alt="Ω = {1,2,3,4,5,6}× {1,2,3,4,5,6} = {(i,j) : i,j ∈ {1,2,3,4,5,6}}, "/>
</div>
<p><span class="cmss-10x-x-109">and the outcomes are tuples of the form </span>(<span class="cmmi-10x-x-109">i,j</span>)<span class="cmss-10x-x-109">. (That is, the tuple </span>(<span class="cmmi-10x-x-109">i,j</span>) <span class="cmss-10x-x-109">encodes the elementary event </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">i,Y </span>= <span class="cmmi-10x-x-109">j</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.) Since the tosses are independent of each other,</span></p>
<div class="math-display">
<img src="../media/file1659.png" class="math-display" alt="P (X = i,Y = j) = 1-⋅ 1-=-1-. 6 6 36 "/>
</div>
<p><span class="cmss-10x-x-109">(When it is clear, we omit the brackets of the event </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">i,Y </span>= <span class="cmmi-10x-x-109">j</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">Since the first throw falls between </span>1 <span class="cmss-10x-x-109">and </span>6<span class="cmss-10x-x-109">, we can partition the event space by forming</span></p>
<div class="math-display">
<img src="../media/file1660.png" class="math-display" alt="A := {X = n}, n = 1,...,6. n "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the law of total probability gives</span></p>
<div class="math-display">
<img src="../media/file1661.png" class="math-display" alt=" 6 P (X + Y = 7) = ∑ P ({X + Y = 7} and {X = n }). n=1 "/>
</div>
<p><span class="cmss-10x-x-109">However, if we know that </span><span class="cmmi-10x-x-109">X </span>+ <span class="cmmi-10x-x-109">Y </span>= 7 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">, we know that </span><span class="cmmi-10x-x-109">Y </span>= 7 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">must hold as well. So, continuing the calculation above,</span></p>
<div class="math-display">
<img src="../media/file1662.png" class="math-display" alt=" ∑6 ( ) P (X + Y = 7) = P {X + Y = 7} and {X = n} n=1 ∑6 = P(X = n,Y = 7 − n) n=1 6 ∑ 1-- = 36 n=1 = 1. 6 "/>
</div>
<p><span class="cmss-10x-x-109">So, the law of total probability helps us deal with complex events by</span><span id="dx1-286006"></span> <span class="cmss-10x-x-109">decomposing them into simpler ones. We have seen this pattern dozens of times now, and once again, it proves to be essential.</span></p>
<p><span class="cmss-10x-x-109">As yet another consequence of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-additivity, we can calculate the probability of an increasing sequence of events by taking the limit.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-286007r114"></span> <span class="cmbx-10x-x-109">Theorem 114.</span> </span> <span class="cmbxti-10x-x-109">(Lower continuity of probability measures)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">⊆</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be an increasing sequence of events. Then,</span></p>
<div class="math-display">
  <span>
    P(∪<sub>n=1</sub><sup>∞</sup> A<sub>n</sub>) = lim<sub>n→∞</sub> P(A<sub>n</sub>)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.3)</span>
</div>

<p><span class="cmti-10x-x-109">holds. This property is called the lower continuity of probability measures.</span></p>
</div>
<div id="tcolobox-295" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Since the events are increasing, that is, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">⊆ </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, we can write </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">as</span></p>
<div class="math-display">
<img src="../media/file1664.png" class="math-display" alt="An = An−1 ∪ (An ∖An −1), "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">are disjoint.</span></p>
<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file1665.png" class="math-display" alt=" ∞⋃ ∞⋃ An = (An ∖ An− 1), A0 := ∅, n=1 n=1 "/>
</div>
<p><span class="cmss-10x-x-109">which gives</span></p>
<div class="math-display">
<img src="../media/file1666.png" class="math-display" alt=" ∑∞ P (∪∞n=1An ) = P(An ∖ An− 1) n=1 N∑ = lim P (An ∖ An− 1) N → ∞n=1 N = lim ∑ [P(A )− P (A )] N → ∞ n n−1 n=1 = lim P (AN ), N → ∞ "/>
</div>
<p><span class="cmss-10x-x-109">where we used that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">∅</span>) = 0<span class="cmss-10x-x-109">.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">We can state an</span> <span id="dx1-286008"></span><span class="cmss-10x-x-109">analogue of the above theorem for a decreasing sequence of events.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-286009r115"></span> <span class="cmbx-10x-x-109">Theorem 115.</span> </span> <span class="cmbxti-10x-x-109">(Upper continuity of probability measures)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">⊇</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub> <span class="cmsy-10x-x-109">⊇</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be a decreasing sequence of events. Then,</span></p>
<div class="math-display">
  <span>
    P(∩<sub>n=1</sub><sup>∞</sup> A<sub>n</sub>) = lim<sub>n→∞</sub> P(A<sub>n</sub>)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.4)</span>
</div>

<p><span class="cmti-10x-x-109">holds. This property is called the upper continuity of probability measures.</span></p>
</div>
<div id="tcolobox-296" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">For simplicity, let’s denote the infinite intersection by </span><span class="cmmi-10x-x-109">A </span>:= <span class="cmsy-10x-x-109">∩</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">By defining </span><span class="cmmi-10x-x-109">B</span><sub><span class="cmmi-8">n</span></sub> := <span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, we have </span><span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">B</span><sub><span class="cmmi-8">n</span></sub> = <span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. Since </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is decreasing, </span><span class="cmmi-10x-x-109">B</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is increasing, so we can apply </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286007r114"><span class="cmssi-10x-x-109">114</span></a> <span class="cmss-10x-x-109">to obtain</span></p>
<div class="math-display">
<img src="../media/file1668.png" class="math-display" alt="P (A1 ∖A ) = nl→im∞ P (A1 ∖ An ) = P (A1) − lni→m∞ P (An). "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∖</span><span class="cmmi-10x-x-109">A</span>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub>) <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>)<span class="cmss-10x-x-109">, we obtain </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>) = lim<sub><span class="cmmi-8">n</span><span class="cmsy-8">→∞</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Now that we have a mathematical definition of a probabilistic model, it is time to take a step toward the space where machine learning is done: </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">.</span></p>
</section>
<section id="probability-spaces-on-rn" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_254"><span class="titlemark"><span class="cmss-10x-x-109">18.2.6 </span></span> <span id="x1-28700022.2.6"></span><span class="cmss-10x-x-109">Probability spaces on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup></h3>
<p><span class="cmss-10x-x-109">In machine learning, every data point is an elementary outcome, located</span><span id="dx1-287001"></span> <span class="cmss-10x-x-109">somewhere in the Euclidean space </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. Because of this, we are interested in modeling experiments there.</span></p>
<p><span class="cmss-10x-x-109">How can we define a probability space on </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">? As we did with the real line in </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#event-spaces-and-algebras"><span class="cmssi-10x-x-109">18.2.1</span></a><span class="cmss-10x-x-109">, we describe a convenient </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra by generating. There, we can use the higher dimensional counterpart of the </span>(<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">intervals: </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional spheres. For this, we define the set</span></p>
<div class="math-display">
<img src="../media/file1669.png" class="math-display" alt=" n B (x,r) := {y ∈ ℝ : ∥x − y ∥ &lt;r}, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">x </span><span class="cmss-10x-x-109">is the center of the sphere, </span><span class="cmmi-10x-x-109">r/span&gt;0 <span class="cmss-10x-x-109">is its radius, and </span><span class="cmsy-10x-x-109">∥⋅∥ </span><span class="cmss-10x-x-109">denotes the usual Euclidean norm. (The </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">denotes the word </span><span class="cmssi-10x-x-109">ball</span><span class="cmss-10x-x-109">. In mathematics, </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-dimensional spheres are often called balls.) Similar to the real line, the Borel algebra is defined by</span> </span></p>
<div class="math-display">
  <span>
    ℬ(ℝ<sup>n</sup>) := σ(<span class="big">{</span>B(x, r) : x ∈ ℝ<sup>n</sup>, r &gt; 0<span class="big">}</span>)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.5)</span>
</div>

<p><span class="cmss-10x-x-109">As we saw on the real line (see </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#algebras-over-real-numbers"><span class="cmssi-10x-x-109">18.2.3</span></a><span class="cmss-10x-x-109">), the structure of </span><span class="cmsy-10x-x-109">ℬ</span>(<span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>) <span class="cmss-10x-x-109">is richer than what the definition suggests at first glance. Here, the analogue of interval is a rectangle, defined by</span></p>
<div class="math-display">
<img src="../media/file1672.png" class="math-display" alt="(a,b) = (a1,b1)× ⋅⋅⋅× (an,bn) n = {x ∈ ℝ : ai &lt;xi &lt;bi,i = 1,...,n}, "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">×</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is the Cartesian product. (See </span><span class="cmssi-10x-x-109">Definition </span><a href="ch037.xhtml#x1-378002r110"><span class="cmssi-10x-x-109">110</span></a><span class="cmss-10x-x-109">.) Similarly, we can define</span> [<span class="cmmi-10x-x-109">a,b</span>]<span class="cmmi-10x-x-109">,</span>(<span class="cmmi-10x-x-109">a,b</span>]<span class="cmmi-10x-x-109">,</span>[<span class="cmmi-10x-x-109">a,b</span>)<span class="cmss-10x-x-109">, and others.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-287002r116"></span> <span class="cmbx-10x-x-109">Theorem 116.</span> </span></p>
<p><span class="cmti-10x-x-109">For any </span><span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b </span><span class="cmsy-10x-x-109">∈ </span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup><span class="cmti-10x-x-109">, the sets</span> [<span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b</span>]<span class="cmmi-10x-x-109">,</span>[<span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b</span>)<span class="cmmi-10x-x-109">,</span>(<span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b</span>]<span class="cmmi-10x-x-109">,</span>(<span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>)<span class="cmmi-10x-x-109">,</span>[<span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>)<span class="cmmi-10x-x-109">,</span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">a</span>)<span class="cmmi-10x-x-109">,</span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b</span>] <span class="cmti-10x-x-109">are elements of </span><span class="cmsy-10x-x-109">ℬ</span>(<span class="msbm-10x-x-109">ℝ</span><sup><span class="cmmi-8">n</span></sup>)<span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-297" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">The proof goes along the same line as the counterpart for </span><span class="cmsy-10x-x-109">ℬ</span>(<span class="msbm-10x-x-109">ℝ</span>)<span class="cmss-10x-x-109">. As such, it is left as an exercise to you.</span></p>
<p><span class="cmss-10x-x-109">As a hint, first, we can show that </span>(<span class="cmbx-10x-x-109">a</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">b</span>) <span class="cmss-10x-x-109">can be written as a countable union of balls. We can also show that this holds true for sets such as</span></p>
<div class="math-display">
<img src="../media/file1673.png" class="math-display" alt="ℝ × ⋅⋅⋅× (◟−-∞◝,◜-ai)◞ × ⋅⋅⋅× ℝ i- th component "/>
</div>
<p><span class="cmss-10x-x-109">as well. From these two, we can write the others as unions/intersections/differences.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">As an example, let’s throw a few darts at a rectangular wall. Suppose that we are terrible darts players and hitting any point on the wall is equally likely.</span></p>
<p><span class="cmss-10x-x-109">We can</span> <span id="dx1-287003"></span><span class="cmss-10x-x-109">model this event space with </span>Ω = [0<span class="cmmi-10x-x-109">,</span>1] <span class="cmsy-10x-x-109">× </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, representing our wall. What are the possible events? For instance, there is a circular darts board hanging on the wall, and we want to find the probability of hitting it. In this scenario, we can restrict the Borel sets defined by (</span><a href="ch030.xhtml#probability-spaces-on-rn"><span class="cmss-10x-x-109">22.2.6</span></a><span class="cmss-10x-x-109">) to</span></p>
<div class="math-display">
<img src="../media/file1674.png" class="math-display" alt=" ( ) ℬ Ω := {A ∩Ω : A ∈ ℬ(ℝn )}. "/>
</div>
<p><span class="cmss-10x-x-109">Now that the event space and algebra is clear, we need to think about assigning probabilities. Our assumption is that hitting any point is equally likely. So, by generalizing the</span> <img src="../media/file1675.png" class="frac" data-align="middle" alt="faallvo proassblibele ouotcutocmomeess"/> <span class="cmss-10x-x-109">formula we have seen in the discrete case, we define the probability measure by</span></p>
<div class="math-display">
<img src="../media/file1676.png" class="math-display" alt="P (A ) = volume-(A-). volume (Ω ) "/>
</div>
<p><span class="cmss-10x-x-109">(In two dimensions, we have the area instead of volume.) This is illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">18.4</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1677.png" width="284" alt="PIC"/> <span id="x1-287004r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.4: Probability space of throwing darts at a wall. Source: https://unsplash.com/photos/black-and-white-round-logo-i3WlrO7oAHA</span> </span>
</div>
<p><span class="cmss-10x-x-109">As we shall see later, this is a special case of </span><span class="cmssi-10x-x-109">uniform distributions</span><span class="cmss-10x-x-109">, one of the most prevalent distributions in probability theory. However, there is a lot to talk about until then. Before we conclude our discussion of the fundamentals of probability, let’s discuss how can we interpret them.</span></p>
</section>
<section id="how-to-interpret-probability" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_255"><span class="titlemark"><span class="cmss-10x-x-109">18.2.7 </span></span> <span id="x1-28800022.2.7"></span><span class="cmss-10x-x-109">How to interpret probability</span></h3>
<p><span class="cmss-10x-x-109">Now that we</span> <span id="dx1-288001"></span><span class="cmss-10x-x-109">know how to work with probabilities, it is time to study how can we assign probabilities to real-life events.</span></p>
<p><span class="cmss-10x-x-109">First, we are going to take a look at the </span><span class="cmssi-10x-x-109">frequentist </span><span class="cmss-10x-x-109">interpretation, explaining probabilities with relative frequencies. (If you are one of those people who are religious about this question, calm down. We’ll discuss the Bayesian interpretation in detail, but it is not time yet.)</span></p>
<p><span class="cmss-10x-x-109">Let’s go back to the beginning and consider the coin-tossing experiment. If I toss a fair coin 1000 times, how many of them will be heads? Most people immediately answer 500, but this is not correct. There is no right answer, as any number of heads between 0 and 1000 can happen. Of course, most probably it will be around 500, but with a very small probability, there can be zero heads as well.</span></p>
<p><span class="cmss-10x-x-109">In general, the probability of an event describes its </span><span class="cmssi-10x-x-109">relative frequency </span><span class="cmss-10x-x-109">among infinitely many attempts. That is,</span></p>
<div class="math-display">
<img src="../media/file1678.png" class="math-display" alt=" number of occurrences P (event) ≈ --------------------. number of attempts "/>
</div>
<p><span class="cmss-10x-x-109">When the number of attempts goes toward infinity, the relative frequency of occurrences converges to the true underlying probability. In other words, if </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">quantitatively describes our </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th attempt by</span></p>
<div class="math-display">
<img src="../media/file1679.png" class="math-display" alt=" ( |{ 1 if the event occurs Xi = | ( 0 otherwise, "/>
</div>
<p><span class="cmss-10x-x-109">then</span></p>
<div class="math-display">
<img src="../media/file1680.png" class="math-display" alt=" X + ⋅⋅⋅+ X P (event) = lim --1---------n. n→ ∞ n "/>
</div>
<p><span class="cmss-10x-x-109">We can illustrate this by doing a quick simulation using the coin-tossing example. Don’t worry if you don’t understand the code; we’ll talk about it in detail in the next chapters.</span></p>
<div id="tcolobox-298" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import numpy as np 
from scipy.stats import randint 
 
 
n_tosses = 1000 
# coin tosses: 0 for tails and 1 for heads 
coin_tosses = [randint.rvs(low=0, high=2) for _ in range(n_tosses)] 
averages = [np.mean(coin_tosses[:k+1]) for k in range(n_tosses)]</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s plot the results for some insight:</span></p>
<div id="tcolobox-299" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import matplotlib.pyplot as plt 
 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    plt.figure(figsize=(16, 8)) 
    plt.title("/span&gt;Relative frequency of the coin tosses 
    plt.xlabel("/span&gt;Number of tosses 
    plt.ylabel("/span&gt;Relative frequency 
 
    # plotting the averages 
    plt.plot(range(n_tosses), averages, linewidth=3) # the averages 
 
    # plotting the true expected value 
    plt.plot([-100, n_tosses+100], [0.5, 0.5], c="/span&gt;k 
    plt.xlim(-10, n_tosses+10) 
    plt.ylim(0, 1) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1681.png" width="569" alt="PIC"/> <span id="x1-288027r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.5: The relative frequency of coin tosses</span> </span>
</div>
<p><span class="cmss-10x-x-109">The relative frequency quite nicely stabilizes around </span>1<span class="cmmi-10x-x-109">∕</span>2<span class="cmss-10x-x-109">, which is the</span><span id="dx1-288028"></span> <span class="cmss-10x-x-109">true probability of our fair coin landing on its heads. Is this an accident? No.</span></p>
<p><span class="cmss-10x-x-109">We will make all of this mathematically precise when talking about </span><span class="cmssi-10x-x-109">the law of large numbers </span><span class="cmss-10x-x-109">in </span><span class="cmssi-10x-x-109">Section </span><a href="ch032.xhtml#the-law-of-large-numbers"><span class="cmssi-10x-x-109">20.5</span></a><span class="cmss-10x-x-109">, but first, we’ll introduce the Bayesian viewpoint, a probabilistic framework for updating our models given new observations.</span></p>
</section>
</section>
<section id="conditional-probability" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_256"><span class="titlemark"><span class="cmss-10x-x-109">18.3 </span></span> <span id="x1-28900022.3"></span><span class="cmss-10x-x-109">Conditional probability</span></h2>
<p><span class="cmss-10x-x-109">In the previous sections, we learned the foundations of probability. Now</span><span id="dx1-289001"></span> <span class="cmss-10x-x-109">we can speak in</span> <span id="dx1-289002"></span><span class="cmss-10x-x-109">terms of outcomes, events, and chances. However, in real-life applications, these basic tools are not enough to build useful predictive models.</span></p>
<p><span class="cmss-10x-x-109">To illustrate this, let’s build a probabilistic spam filter! For every email we receive, we want to estimate the probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmss-10x-x-109">email is spam</span>)<span class="cmss-10x-x-109">. The closer this is to </span>1<span class="cmss-10x-x-109">, the more likely that we are looking at a spam email.</span></p>
<p><span class="cmss-10x-x-109">Based on our inbox, we might calculate the relative frequency of spam emails and obtain that</span></p>
<div class="math-display">
<img src="../media/file1682.png" class="math-display" alt=" number of our spam emails P(email is spam ) ≈-------------------------. number of emails in our inbox "/>
</div>
<p><span class="cmss-10x-x-109">However, this doesn’t help us at all. Based on this, we can randomly discard every email with probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmss-10x-x-109">email is spam</span>)<span class="cmss-10x-x-109">, but that would be a horrible spam filter.</span></p>
<p><span class="cmss-10x-x-109">To improve, we need to dig a bit deeper. When analyzing spam emails, we start to notice patterns. For instance, the phrase “act now” can be found almost exclusively in spam. After a quick count, we get that</span></p>

<img src="../media/file1683.png" class="math-display" alt=" #spam emails with the phrase ”act now” P(email containing ”act now” is a spam ) = ---#emails with-the phrase-”act now”- ≈ 0.95. " width="450"/>

<p><span class="cmss-10x-x-109">This looks much more useful for our spam filtering efforts. By checking for the presence of the phrase “act now”, we can confidently classify an email as spam.</span></p>
<p><span class="cmss-10x-x-109">Of course, there is much more to spam filtering, but this example demonstrates the importance of probabilities </span><span class="cmssi-10x-x-109">conditional </span><span class="cmss-10x-x-109">on other events. To put this into mathematical form, we introduce the following definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-289003r78"></span> <span class="cmbx-10x-x-109">Definition 78.</span> </span><span class="cmbx-10x-x-109">(Conditional probability)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space, let <span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span>Σ be two events, and suppose that <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>)<span class="cmmi-10x-x-109">/span&gt;0. The <span class="cmti-10x-x-109">conditional probability </span>of <span class="cmmi-10x-x-109">B </span>given <span class="cmmi-10x-x-109">A </span>is defined by </span></p>

<img src="../media/file1684.png" class="math-display" alt="P (B | A ) := P-(A-∩-B). P (A) " width="150"/>
</div>
<p><span class="cmss-10x-x-109">You can think about </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">as restricting the event space to </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">18.6</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1685.png" width="199" alt="PIC"/> <span id="x1-289004r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.6: A visual representation of conditional probability</span> </span>
</div>
<p><span class="cmss-10x-x-109">When there</span> <span id="dx1-289005"></span><span class="cmss-10x-x-109">are more conditions, say </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, the definition takes the form</span></p>
<div class="math-display">
<img src="../media/file1686.png" class="math-display" alt=" P-(B-∩-A1-∩A2-) P(B | A1,A2) = P (A1 ∩A2 ) , "/>
</div>
<p><span class="cmss-10x-x-109">and so on for even more events.</span></p>
<p><span class="cmss-10x-x-109">To bring this concept closer, let’s revisit the dice-rolling experiment. Suppose that your friend rolls a six-sided dice and tells you that the outcome is an odd number. Given this information, what is the probability that the result is 3? For simplicity, let’s denote the outcome of the roll with </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. Mathematically speaking, this can be calculated by</span></p>
<div class="math-display">
<img src="../media/file1687.png" class="math-display" alt="P (X = 3 | X ∈ {1,3,5}) = P-(X-=-3-and-X-∈-{1,3,5}) P (X ∈ {1,3,5}) ---P-(X--=-3)--- = P (X ∈ {1,3,5}) = 1∕6- 1∕2 1- = 3. "/>
</div>
<p><span class="cmss-10x-x-109">This is the number that we expected.</span></p>
<p><span class="cmss-10x-x-109">Although this simple example doesn’t demonstrate the usefulness of conditional probability, this is a cornerstone in machine learning. In essence, learning from data can be formulated as estimating </span><span class="cmmi-10x-x-109">P</span>(<span class="cmss-10x-x-109">label</span><span class="cmsy-10x-x-109">∣</span><span class="cmss-10x-x-109">data</span>)<span class="cmss-10x-x-109">. We are going to expand on this idea later in this chapter.</span></p>
<section id="independence" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_257"><span class="titlemark"><span class="cmss-10x-x-109">18.3.1 </span></span> <span id="x1-29000022.3.1"></span><span class="cmss-10x-x-109">Independence</span></h3>
<p><span class="cmss-10x-x-109">The idea behind conditional probability is that observing certain events</span><span id="dx1-290001"></span> <span class="cmss-10x-x-109">changes the probability of others. Is this always the case, though?</span></p>
<p><span class="cmss-10x-x-109">In probabilistic modeling, recognizing when observing an event doesn’t influence another is equally important. This motivates the concept of</span><span id="dx1-290002"></span> <span class="cmssi-10x-x-109">independence</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-290003r79"></span> <span class="cmbx-10x-x-109">Definition 79.</span> </span><span class="cmbx-10x-x-109">(Independence of events)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space and let <span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span>Σ be two events. We say that <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B </span>are <span class="cmti-10x-x-109">independent </span>if</p>
<div class="math-display">
<img src="../media/file1688.png" class="math-display" alt="P (A ∩ B) = P (A )P(B ) "/>
</div>
<p>holds.</p>
</div>
<p><span class="cmss-10x-x-109">Equivalently, this can be formulated in terms of conditional probabilities. By the definition, if </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">are independent, we have</span></p>
<div class="math-display">
<img src="../media/file1689.png" class="math-display" alt="P (B | A ) = P(A-∩-B-) P(A ) P(A-)P(B-) = P (A) = P(B ). "/>
</div>
<p><span class="cmss-10x-x-109">To see an example, let’s go back to coin tossing and suppose that we toss a coin two times. Let the result of the first and second toss be denoted by </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">, respectively. What is the probability that both of these tosses are heads? As we saw when discussing this example in </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#probability-measures"><span class="cmssi-10x-x-109">18.2.4</span></a><span class="cmss-10x-x-109">, we can see that</span></p>
<img src="../media/file1690.png" width="450" class="math-display" alt="P (X1 = heads and X2 = heads) = P(X1 = heads)P (X2 = heads) = 1. 4 "/>
<p><span class="cmss-10x-x-109">That is, the two events are independent of each other.</span></p>
<p><span class="cmss-10x-x-109">Regarding probability, there are many common misconceptions. One is about the interpretation of independence. Suppose that I toss a fair coin ten times, all of them resulting in heads. What is the probability that my next toss will be heads?</span></p>
<p><span class="cmss-10x-x-109">Most would immediately conclude that this must be very small since having eleven heads in a row is highly unlikely. However, once we have the ten results available, we no longer talk about the probability of eleven coin tosses, just the last one! Since the coin tosses are independent of each other, the chance of heads for the eleventh toss (given the results of the previous ten) is still 50%.</span></p>
<p><span class="cmss-10x-x-109">This phenomenon is called the </span><span class="cmssi-10x-x-109">gambler’s fallacy</span><span class="cmss-10x-x-109">, and I am pretty sure that at some point in your life, you fell victim to it. (I sure did.)</span></p>
<p><span class="cmss-10x-x-109">In practical scenarios, working with conditional probabilities might be</span><span id="dx1-290004"></span> <span class="cmss-10x-x-109">easier. (For instance, sometimes we can estimate them directly, while the standard probabilities are difficult to gauge.) Because of this, we need tools to work with them.</span></p>
</section>
<section id="the-law-of-total-probability-revisited" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_258"><span class="titlemark"><span class="cmss-10x-x-109">18.3.2 </span></span> <span id="x1-29100022.3.2"></span><span class="cmss-10x-x-109">The law of total probability revisited</span></h3>
<p><span class="cmss-10x-x-109">Remember the law of</span> <span id="dx1-291001"></span><span class="cmss-10x-x-109">total</span> <span id="dx1-291002"></span><span class="cmss-10x-x-109">probability from </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286004r113"><span class="cmssi-10x-x-109">113</span></a><span class="cmss-10x-x-109">? We can use conditional probabilities to put it into a slightly different form.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-291003r117"></span> <span class="cmbx-10x-x-109">Theorem 117.</span> </span> <span class="cmbxti-10x-x-109">(Law of total probability, conditional version)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be an arbitrary event. If </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">are mutually disjoint events (that is, </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">j</span></sub> = <span class="cmsy-10x-x-109">∅</span><span class="cmti-10x-x-109">if </span><span class="cmmi-10x-x-109">i≠j</span><span class="cmti-10x-x-109">) for which </span><span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> = Ω<span class="cmti-10x-x-109">, then</span></p>
<div class="math-display">
  <span>
    P(A) = ∑<sub>k=1</sub><sup>∞</sup> P(A | A<sub>k</sub>) P(A<sub>k</sub>)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.6)</span>
</div>

</div>
<div id="tcolobox-300" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">The proof is the trivial application of the law of total probability (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286004r113"><span class="cmssi-10x-x-109">113</span></a><span class="cmss-10x-x-109">) and the definition of conditional probabilities: as </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
  <span>
    P(A) = ∑<sub>k=1</sub><sup>∞</sup> P(A ∩ A<sub>k</sub>)
  </span>
  <span>
   = ∑<sub>k=1</sub><sup>∞</sup> P(A | A<sub>k</sub>) P(A<sub>k</sub>)
  </span>
</div>

<p><span class="cmss-10x-x-109">holds, which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Why is this useful for us? Let’s demonstrate this with an example. Suppose that we have three urns containing light and dark colored balls.</span></p>
<p><span class="cmss-10x-x-109">The first one contains 4 dark, the second one contains 2 light and 2 dark, while the last one contains 1 light and 3 dark balls.</span></p>
<div class="minipage">
<p><img src="../media/file1692.png" width="284" alt="PIC"/> <span id="x1-291004r7"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.7: Urns with colored balls</span> </span>
</div>
<p><span class="cmss-10x-x-109">We</span> <span id="dx1-291005"></span><span class="cmss-10x-x-109">randomly pick an urn; however, picking the first one is twice as likely as picking the other two. (That is, we pick the first urn 50% of the time, while the second and the third 25%–25% of the time.) From that urn, we also randomly pick a ball. What is the probability that we select a light ball? Without using the law of total probability, this is difficult to compute.</span></p>
<p><span class="cmss-10x-x-109">Let’s denote the color of the selected ball by </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and suppose that the event </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">describes picking the </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-th urn. Then, we have</span></p>
<div class="math-diplay">
<img src="../media/file1693.png" width="450" class="math-display" alt=" ∑3 ∑3 P (X = light) = P ({X = light} ∩ Ak) = P (X = light | Ak )P (Ak). k=1 k=1 "/>
</div>
<p><span class="cmss-10x-x-109">Without using conditional probabilities, calculating </span><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X </span>= <span class="cmss-10x-x-109">light</span><span class="cmsy-10x-x-109">}∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub>) <span class="cmss-10x-x-109">is difficult (since we are not picking each urn with equal probability). However, we can simply calculate the conditionals by counting the number of light balls in each urn. That is, we have</span></p>
<div class="math-dsplay">
<img src="../media/file1694.png" width="150" class="math-display" alt="P(X = light | A1 ) = 0 2 P(X = light | A2 ) = 4 1 P(X = light | A3 ) =-. 4 "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub>) = 1<span class="cmmi-10x-x-109">∕</span>2<span class="cmmi-10x-x-109">,P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub>) = 1<span class="cmmi-10x-x-109">∕</span>4<span class="cmss-10x-x-109">, and </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">3</span></sub>) = 1<span class="cmmi-10x-x-109">∕</span>4<span class="cmss-10x-x-109">, the probability we are looking for is</span></p>
<div class="math-display">
<img src="../media/file1695.png" class="math-display" alt=" ∑3 P (X = light) = P (X = light | Ak)P (Ak) k=1 1 2 1 1 1 = 0⋅ 2 + 4 ⋅ 4 + 4 ⋅ 4 3 = --. 16 "/>
</div>
<p><span class="cmss-10x-x-109">Note that because the urns are not selected with equal probability,</span></p>
<div class="math-display">
<img src="../media/file1696.png" class="math-display" alt=" number of light balls P (X = light) ⁄= ------------------, number of balls "/>
</div>
<p><span class="cmss-10x-x-109">as one would naively guess.</span></p>
<p><span class="cmss-10x-x-109">Another</span> <span id="dx1-291006"></span><span class="cmss-10x-x-109">useful property of the conditional probability is that, due to its definition, we can use it to express the </span><span class="cmssi-10x-x-109">joint probability </span><span class="cmss-10x-x-109">of events:</span></p>
<div class="math-display">
<img src="../media/file1697.png" class="math-display" alt="P (A ∩ B ) = P (B | A )P (A). "/>
</div>
<p><span class="cmss-10x-x-109">Even though this sounds trivial, there are cases when we can estimate/compute the conditional probability but not the joint probability. In fact, this simple identity can be generalized for an arbitrary number of conditions. This is called the </span><span class="cmssi-10x-x-109">chain rule</span><span class="cmss-10x-x-109">. (Despite its name, it has nothing to do with the chain rule for differentiation.)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-291007r118"></span> <span class="cmbx-10x-x-109">Theorem 118.</span> </span> <span class="cmbxti-10x-x-109">(The chain rule)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be arbitrary events. Then,</span></p>
<div class="math-display">
  <span>
    P(A) = ∑<sub>k=1</sub><sup>∞</sup> P(A | A<sub>k</sub>) P(A<sub>k</sub>)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(18.7)</span>
</div>

<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<div id="tcolobox-301" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">First, we notice that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">∩</span>⋅⋅⋅<span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-dislay">
<img src="../media/file1701.png" width="450" class="math-display" alt=" P (A1 ∩ ⋅⋅⋅∩An ) P (A1 ∩ ⋅⋅⋅∩ An −1) P (A1 ∩ A2) P(A1 ∩⋅⋅⋅∩An ) =------------------------------------...-----------P (A1), P (A1 ∩ ⋅⋅⋅ ∩An −1)P (A1 ∩ ⋅⋅⋅∩ An −2) P(A1 ) "/>
</div>
<p><span class="cmss-10x-x-109">because the terms cancel out each other.</span></p>
<p><span class="cmss-10x-x-109">Since</span></p>
<div class="math-display">
<img src="../media/file1702.png" class="math-display" alt="-P(A1-∩-Ak)-- P(A1 ∩ Ak−1) = P (Ak | A1,...,Ak− 1), "/>
</div>
<p><span class="cmss-10x-x-109">the chain rule (</span><a href="ch030.xhtml#x1-291007r118"><span class="cmss-10x-x-109">18.7</span></a><span class="cmss-10x-x-109">) follows.</span></p>
</div>
</div>
</section>
<section id="the-bayes-theorem" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_259"><span class="titlemark"><span class="cmss-10x-x-109">18.3.3 </span></span> <span id="x1-29200022.3.3"></span><span class="cmss-10x-x-109">The Bayes theorem</span></h3>
<p><span class="cmss-10x-x-109">In essence, machine</span><span id="dx1-292001"></span> <span class="cmss-10x-x-109">learning is about</span><span id="dx1-292002"></span> <span class="cmss-10x-x-109">turning observations into predictive models. Probability theory gives us a language to express our models. For instance, going back to our spam filter example, we can notice that 5% of our emails are spam. However, this is not enough information to filter out spam emails. Upon inspection, we have observed that 95% of emails that contain the phrase “act now” are spam (but only 1% of all the emails contain “act now”). In the language of conditional probabilities, we have concluded that</span></p>
<div class="math-display">
<img src="../media/file1703.png" class="math-display" alt="P (spam | contains ”act now ”) = 0.95. "/>
</div>
<p><span class="cmss-10x-x-109">With this, we can start looking for emails containing the phrase “act now” and discard them with 95% confidence. Is this spam filter effective? Not really, since there can be other frequent keywords in spam mails that we don’t check. How can we check this?</span></p>
<p><span class="cmss-10x-x-109">For one, we can take a look at the conditional probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmss-10x-x-109">contains ”act now”</span><span class="cmsy-10x-x-109">∣</span><span class="cmss-10x-x-109">spam</span>)<span class="cmss-10x-x-109">, describing the frequency of the “act now” keyword among all the spam emails. A low frequency means that we are missing out on other keywords that we can use for filtering.</span></p>
<p><span class="cmss-10x-x-109">Generally speaking, we often want to compute/estimate the quantity </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span>)<span class="cmss-10x-x-109">, but our observations only allow us to infer </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>)<span class="cmss-10x-x-109">. So, we need a way to reverse the condition and the event. With a bit of algebra, we</span> <span id="dx1-292003"></span><span class="cmss-10x-x-109">can do this easily.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-292004r119"></span> <span class="cmbx-10x-x-109">Theorem 119.</span> </span> <span class="cmbxti-10x-x-109">(The Bayes formula)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, </span><span class="cmmi-10x-x-109">A,B </span><span class="cmti-10x-x-109">be two arbitrary events, and suppose that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>)<span class="cmmi-10x-x-109">,P</span>(<span class="cmmi-10x-x-109">B</span>)<span class="cmmi-10x-x-109">/span&gt;0<span class="cmti-10x-x-109">. Then,</span> </span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(26).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(18.8)
  </div>
</div>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<div id="tcolobox-302" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">By the definition of conditional probability, we have</span></p>
<div class="math-display">
<img src="../media/file1705.png" class="math-display" alt=" P(A-∩-B-) P(B | A ) = P(A ) = P(A-∩-B-)P(B-) P(B )P(A ) P-(B-) = P(A | B )P (A), "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">To see how it works in action, let’s put it to the test in our spam filtering example. Given the information we know, we have</span></p>
<div class="math-display">
<img src="../media/file1706.png" class="math-display" alt="P (spam | contains ”act now ”) = 0.95, P(contains ”act now ”) = 0.01, P (spam ) = 0.05. "/>
</div>
<p><span class="cmss-10x-x-109">So, according to the Bayes formula,</span></p>
<div class="math-diplay">
<img src="../media/file1707.png" width="550" class="math-display" alt=" P-(spam--| contains-”act now-”)P(contains-”act now-”) P(contains ”act now” | spam) = P(spam) 0.95⋅0.01 = --------- 0.05 = 0.19. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, by</span> <span id="dx1-292005"></span><span class="cmss-10x-x-109">filtering only for the phrase “act now”, we are missing a </span><span class="cmssi-10x-x-109">lot </span><span class="cmss-10x-x-109">of spam.</span></p>
<p><span class="cmss-10x-x-109">We can take the Bayes formula one step further by combining it with the</span><span id="dx1-292006"></span> <span class="cmss-10x-x-109">law of total probability in </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-291003r117"><span class="cmssi-10x-x-109">117</span></a><span class="cmss-10x-x-109">. (See the equation (</span><a href="ch030.xhtml#x1-291003r117"><span class="cmss-10x-x-109">18.6</span></a><span class="cmss-10x-x-109">).)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-292007r120"></span> <span class="cmbx-10x-x-109">Theorem 120.</span> </span> <span class="cmbxti-10x-x-109">(The Bayes theorem)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be arbitrary events. Moreover, let </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be a partition of the event space </span>Ω<span class="cmti-10x-x-109">. (That is, the </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub><span class="cmti-10x-x-109">-s are pairwise disjoint and their union is the entire event space.) Then,</span></p>
<div class="math-display">
<img src="../media/file1709.png" class="math-display" alt=" ----P-(A-| B-)P-(B)--- P (B | A) = ∑ ∞ P (A | An)P (An) n=1 "/>
</div>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<div id="tcolobox-303" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">The proof immediately follows from the Bayes formula (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-292004r119"><span class="cmssi-10x-x-109">119</span></a><span class="cmss-10x-x-109">) and the law of total probability (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-291003r117"><span class="cmssi-10x-x-109">117</span></a><span class="cmss-10x-x-109">.)</span></p>
</div>
</div>
</section>
<section id="the-bayesian-interpretation-of-probability" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_260"><span class="titlemark"><span class="cmss-10x-x-109">18.3.4 </span></span> <span id="x1-29300022.3.4"></span><span class="cmss-10x-x-109">The Bayesian interpretation of probability</span></h3>
<p><span class="cmss-10x-x-109">Historically, probability was introduced as the relative frequency of</span><span id="dx1-293001"></span> <span class="cmss-10x-x-109">observed events in </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#how-to-interpret-probability"><span class="cmssi-10x-x-109">18.2.7</span></a><span class="cmss-10x-x-109">. However, the invention of conditional probabilities and the Bayes formula enabled another interpretation that slowly became prevalent in statistics and machine learning.</span></p>
<p><span class="cmss-10x-x-109">In pure English, the Bayes formula can be thought of as updating our probabilistic models using new observations. Suppose that we are interested in the event </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">. Without observing anything, we can formulate a probabilistic model by assigning a probability to </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">, that is, estimating </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span>)<span class="cmss-10x-x-109">. This is what we call the </span><span class="cmssi-10x-x-109">prior</span><span class="cmss-10x-x-109">. However, observing another event </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">might change our probabilistic model.</span></p>
<p><span class="cmss-10x-x-109">Thus, we would like to estimate the </span><span class="cmssi-10x-x-109">posterior </span><span class="cmss-10x-x-109">probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>)<span class="cmss-10x-x-109">. We can’t do this directly, but thanks to our prior model, we can tell </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span>)<span class="cmss-10x-x-109">. The quantity </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span>) <span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">likelihood</span><span class="cmss-10x-x-109">. Combining these with the Bayes formula, we can see that the</span> <span id="dx1-293002"></span><span class="cmss-10x-x-109">posterior is proportional to the likelihood and the prior.</span></p>
<div class="minipage">
<p><img src="../media/file1710.png" width="284" alt="PIC"/> <span id="x1-293003r8"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.8: The Bayes formula, as the product of the </span><span class="cmssi-10x-x-109">likelihood </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">prior</span> </span>
</div>
<p><span class="cmss-10x-x-109">Let’s see a concrete example that will make the idea clear. Suppose that we are creating a diagnostic test for an exotic disease. How likely is the disease present in a random person?</span></p>
<p><span class="cmss-10x-x-109">Without knowing any specifics about the situation, we can only use statistics to formulate the probability model. Let’s say that only 2% of the population is affected. So, our probabilistic model is</span></p>
<div class="math-display">
<img src="../media/file1711.png" class="math-display" alt="P (infected) = 0.02, P(healthy) = 0.98. "/>
</div>
<p><span class="cmss-10x-x-109">However, once someone produces a positive test, things change. The goal is to estimate the posterior probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmss-10x-x-109">infected</span><span class="cmsy-10x-x-109">∣</span><span class="cmss-10x-x-109">positive</span>)<span class="cmss-10x-x-109">, a more accurate model.</span></p>
<p><span class="cmss-10x-x-109">Since no medical test is perfect, false positives and false negatives can happen. From the manufacturer, we know that it gives true positives 99% of the time, but the chance of a false positive is 5%. In probabilistic terms, we have</span></p>
<div class="math-display">
<img src="../media/file1712.png" class="math-display" alt="P (positive | infected) = 0.99, P (positive | healthy) = 0.05. "/>
</div>
<p><span class="cmss-10x-x-109">With these, the Bayes theorem gives us</span></p>

<img src="../media/file1713.png" width="650" class="math-display" alt=" P (positive | infected)P(infected) P(infected | positive) = P-(positive-| infected)P(infected)+-P-(positive-| healthy)P-(healthy) = ------0.99⋅0.02------ 0.99⋅0.02 + 0.05 ⋅0.98 ≈ 0.29. "/>

<p><span class="cmss-10x-x-109">So, the</span> <span id="dx1-293004"></span><span class="cmss-10x-x-109">chance of being infected upon producing a positive test is surprisingly 29% (given these specific true and false positive rates).</span></p>
<p><span class="cmss-10x-x-109">These probabilistic thinking principles are also valid for machine learning. If we abstract away the process of learning from data, we are essentially 1) making observations, 2) updating our models given the new observations, and 3) starting the process all over again. The Bayes theorem gives a concrete tool for the job.</span></p>
</section>
<section id="the-probabilistic-inference-process" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_261"><span class="titlemark"><span class="cmss-10x-x-109">18.3.5 </span></span> <span id="x1-29400022.3.5"></span><span class="cmss-10x-x-109">The probabilistic inference process</span></h3>
<p><span class="cmss-10x-x-109">As we</span> <span id="dx1-294001"></span><span class="cmss-10x-x-109">have seen before, probability theory is the extension of mathematical logic. So far, we have discussed how logical connectives correspond to set operations and how probability generalizes the truth value by adding the component of uncertainty. What about the probabilistic inference process? Can we generalize classical inference and use probabilistic reasoning to construct arguments? Yes.</span></p>
<p><span class="cmss-10x-x-109">To illustrate, let’s start with a story. It’s 6:00 AM. The alarm clock is blasting but you are having a hard time getting out of bed. You don’t feel well. Your muscles are weak, and your head is exploding. After a brief struggle, you manage to call a doctor and list all the symptoms. Your sore throat makes speaking painful.</span></p>
<p><span class="cmssi-10x-x-109">“It’s probably just the flu,“ </span><span class="cmss-10x-x-109">they say.</span></p>
<p><span class="cmss-10x-x-109">Interactions like this are everyday occurrences. Yet, we hardly think about the reasoning process behind them. After all, you could have been hungover. Similarly, if the police find a murder weapon at your house, they’ll suspect that you are the killer. The two are related but not the same. For instance, the murder weapon could have been planted.</span></p>
<p><span class="cmss-10x-x-109">The bulk of humanity’s knowledge is obtained in this manner: we collect evidence, then build hypotheses. How do we infer the underlying cause from observing the effect? Most importantly, how can we avoid fooling ourselves into false conclusions?</span></p>
<p><span class="cmss-10x-x-109">Let’s focus on </span><span class="cmssi-10x-x-109">“muscle fatigue, headache, sore throat </span><span class="cmsy-10x-x-109">→</span><span class="cmssi-10x-x-109">flu“</span><span class="cmss-10x-x-109">. This is certainly not true in an absolute sense, as these symptoms resemble how you would feel after shouting and drinking excessively during a metal concert, which is far from the flu. Yet, a positive diagnosis of flu is plausible. Given the evidence at hand, our belief is increased in the hypothesis.</span></p>
<p><span class="cmss-10x-x-109">Unfortunately, classical logic cannot deal with plausible, only with the absolute. Probability theory solves this problem by measuring plausibility on a </span>0 <span class="cmsy-10x-x-109">− </span>1 <span class="cmss-10x-x-109">scale, instead of being stuck at the extremes. Zero is impossible. One is certain. All the values in between represent degrees of uncertainty.</span></p>
<p><span class="cmss-10x-x-109">Let’s put this into mathematical terms!</span></p>
<p><span class="cmss-10x-x-109">How</span> <span id="dx1-294002"></span><span class="cmss-10x-x-109">can we establish a probabilistic link between cause and effect? In classical logic, events are interesting in the context of other events. Before, implication and modus ponens provided the context. Translated to the language of probability, the question is the following: What is the probability of </span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">, given that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is observed? The answer: conditional probabilities.</span></p>
<p><span class="cmss-10x-x-109">Why does conditional probability generalize the concept of implication? It’s easier to draw a picture, so consider the two extreme cases in </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">18.9</span></a><span class="cmss-10x-x-109">. (Recall that implication corresponds to the subset relation, as we saw earlier.)</span></p>
<div class="minipage">
<p><img src="../media/file1714.png" width="284" alt="PIC"/> <span id="x1-294003r9"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.9: Conditional probability as logical implication</span> </span>
</div>
<p><span class="cmss-10x-x-109">Essentially, </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) = 1 <span class="cmss-10x-x-109">means that </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is true, while </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) = 0 <span class="cmss-10x-x-109">means that it is not. We can take this analogy further: a small </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">means that </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is likely to be false, and a large </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">means that it is likely to be true.</span></p>
<p><span class="cmss-10x-x-109">This is illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">18.10</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1715.png" width="284" alt="PIC"/> <span id="x1-294004r10"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.10: Conditional probability as the extension of logical implication</span> </span>
</div>
<p><span class="cmss-10x-x-109">Thus, the </span><span class="cmssi-10x-x-109">“probabilistic modus ponens” </span><span class="cmss-10x-x-109">goes like this:</span></p>
<ol>
<li><span id="x1-294006x1"><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) <span class="cmsy-10x-x-109">≈ </span>1<span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294008x2"><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294010x3"><span class="cmss-10x-x-109">Therefore, </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">is probable.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">This is</span> <span id="dx1-294011"></span><span class="cmss-10x-x-109">quite a relief, as now we have a solid theoretical justification for most of our decisions. Thus, the diagnostic process that kicked up our investigation makes a lot more sense now:</span></p>
<ol>
<li><span id="x1-294013x1"><span class="cmmi-10x-x-109">P</span>(<span class="cmss-10x-x-109">flu</span><span class="cmsy-10x-x-109">∣</span><span class="cmss-10x-x-109">headache, muscle fatigue, sore throat</span>) <span class="cmsy-10x-x-109">≈ </span>1<span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294015x2"><span class="cmssi-10x-x-109">“Headache and muscle fatigue”</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294017x3"><span class="cmss-10x-x-109">Therefore, </span><span class="cmssi-10x-x-109">“flu” </span><span class="cmss-10x-x-109">is probable.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">However, one burning question remains. How do we know that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) <span class="cmsy-10x-x-109">≈ </span>1 <span class="cmss-10x-x-109">holds?</span></p>
<p><span class="cmss-10x-x-109">Let’s focus on the probabilistic version of </span><span class="cmssi-10x-x-109">“headache, sore throat, muscle fatigue </span><span class="cmsy-10x-x-109">→</span><span class="cmssi-10x-x-109">flu“</span><span class="cmss-10x-x-109">. We know that this is not certain, only plausible. Yet, the reverse implication </span><span class="cmssi-10x-x-109">“flu </span><span class="cmsy-10x-x-109">→</span><span class="cmssi-10x-x-109">headache, sore throat, muscle fatigue“ </span><span class="cmss-10x-x-109">is almost certain.</span></p>
<p><span class="cmss-10x-x-109">When naively arguing that the evidence implies the hypothesis, we have the opposite in mind. Instead of applying the modus ponens, we use the faulty argument</span></p>
<ol>
<li><span id="x1-294019x1"><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294021x2"><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294023x3"><span class="cmss-10x-x-109">Therefore, </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">We have talked about this before: this logical fallacy is called </span><span class="cmssi-10x-x-109">affirming the consequent</span><span class="cmss-10x-x-109">, and it’s completely wrong from a purely logical standpoint. However, the Bayes theorem provides a probabilistic twist.</span></p>
<p><span class="cmss-10x-x-109">The proposition </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">translates to </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">B</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">A</span>) = 1<span class="cmss-10x-x-109">, which implies that when </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is observed, </span><span class="cmmi-10x-x-109">B </span><span class="cmss-10x-x-109">occurs as well. Why? Because then we have</span></p>
<div class="math-display">
<img src="../media/file1716.png" class="math-display" alt=" P-(B-| A-)P-(A) P(A | B) = P (B) = P-(A) P (B) ≥ P(A ). "/>
</div>
<p><span class="cmss-10x-x-109">This is</span> <span id="dx1-294024"></span><span class="cmss-10x-x-109">good news, as reversing the implication is not totally wrong. Instead, we have the </span><span class="cmssi-10x-x-109">probabilistic affirming the consequent</span><span class="cmss-10x-x-109">:</span></p>
<ol>
<li><span id="x1-294026x1"><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294028x2"><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></span></li>
<li><span id="x1-294030x3"><span class="cmss-10x-x-109">Therefore, </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is more probable.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">With this, the probabilistic reasoning process makes perfect sense. To recall, the issue with arguments such as </span><span class="cmssi-10x-x-109">“if you have muscle fatigue, sore throat, and a headache, then you have the flu“ </span><span class="cmss-10x-x-109">is that the symptoms can be caused by other conditions, and in rare cases, the flu does not carry all of these symptoms.</span></p>
<p><span class="cmss-10x-x-109">Yet, this kind of thinking can be surprisingly effective in real-life decision-making. Probability and conditional probability extend our reasoning toolkit with inductive methods in three steps:</span></p>
<ol>
<li><span id="x1-294032x1"><span class="cmss-10x-x-109">Generalizes the binary </span>0 <span class="cmsy-10x-x-109">− </span>1 <span class="cmss-10x-x-109">truth values to allow the representation of uncertainty.</span></span></li>
<li><span id="x1-294034x2"><span class="cmss-10x-x-109">Defines the analogue of </span><span class="cmssi-10x-x-109">“if </span><span class="cmmi-10x-x-109">A</span><span class="cmssi-10x-x-109">, then </span><span class="cmmi-10x-x-109">B</span><span class="cmssi-10x-x-109">“</span><span class="cmss-10x-x-109">-type implications using conditional probability.</span></span></li>
<li><span id="x1-294036x3"><span class="cmss-10x-x-109">Provides a method to infer the cause from observing the effect.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">These three ideas are seriously powerful, and their inception has enabled science to perform unbelievable feats. (If you are interested in learning more about the relation of probability theory and logic, I recommend you the great book </span><span class="cmssi-10x-x-109">Probability Theory: The Logic of Science </span><span class="cmss-10x-x-109">by E. T. Jaynes.)</span></p>
<p><span class="cmss-10x-x-109">There’s one more thing I would like to show you. Let’s go back to the mid-twentieth century and look at how a TV show shaped probabilistic thinking.</span></p>
</section>
<section id="the-monty-hall-paradox" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_262"><span class="titlemark"><span class="cmss-10x-x-109">18.3.6 </span></span> <span id="x1-29500022.3.6"></span><span class="cmss-10x-x-109">The Monty Hall paradox</span></h3>
<p><span class="cmss-10x-x-109">Before we</span> <span id="dx1-295001"></span><span class="cmss-10x-x-109">finish with conditional</span> <span id="dx1-295002"></span><span class="cmss-10x-x-109">probability, we’ll touch on an important problem. Regarding probability, we often have seemingly contradictory phenomena, going against our intuitive expectations. These are called </span><span class="cmssi-10x-x-109">paradoxes</span><span class="cmss-10x-x-109">. To master probabilistic thinking, we need to resolve them and eliminate common fallacies from our thinking processes. So far, we have already seen the gambler’s fallacy when talking about the concept of independence in </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#independence"><span class="cmssi-10x-x-109">18.3.1</span></a><span class="cmss-10x-x-109">. Now, we’ll discuss the famous Monty Hall paradox.</span></p>
<p><span class="cmss-10x-x-109">In the ’60s, there was a TV show in the United States called Let’s Make a Deal (</span> <a href="https://en.wikipedia.org/wiki/Lets_Make_a_Deal" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Let\%27s_Make_a_Deal</span></a><span class="cmss-10x-x-109">). As a contestant, you faced three closed doors, one having a car behind it (that you could take home), while the others had nothing. You had the opportunity to open one.</span></p>
<div class="minipage">
<p><img src="../media/file1717.png" width="284" alt="PIC"/> <span id="x1-295003r11"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.11: Three closed doors, one of which contains a reward behind it</span> </span>
</div>
<p><span class="cmss-10x-x-109">Suppose</span> <span id="dx1-295004"></span><span class="cmss-10x-x-109">that after selecting door no. 1, Monty Hall — the show host — opens the third door, showing that it was not the winning one. Now, you have the opportunity</span><span id="dx1-295005"></span> <span class="cmss-10x-x-109">to change your mind and open door no. 2 instead of the first one. Do you take it?</span></p>
<div class="minipage">
<p><img src="../media/file1718.png" width="284" alt="PIC"/> <span id="x1-295006r12"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 18.12: Monty opened the third door for you. Do you switch?</span> </span>
</div>
<p><span class="cmss-10x-x-109">At first glance, your chances are 50%/50%, so you might not be better off by switching. However, this is not true!</span></p>
<p><span class="cmss-10x-x-109">To set things straight, let’s do a careful probabilistic analysis. Let </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">denote the event that the prize is behind the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th door, while </span><span class="cmmi-10x-x-109">B</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is the event of Monty opening the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th door. Before Monty opens the third one, our model is</span></p>
<div class="math-display">
<img src="../media/file1719.png" class="math-display" alt="P (A1) = P(A2 ) = P (A3) = 1, 3 "/>
</div>
<p><span class="cmss-10x-x-109">and we want to calculate </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span><sub><span class="cmr-8">3</span></sub>) <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span><sub><span class="cmr-8">3</span></sub>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">By thinking from the perspective of the show host, which door would you open? If you know that the prize is behind the 1st door, you open the 2nd and 3rd one with equal probability. However, if the prize is actually behind the 2nd door (and the contestant selected the 1st one), you always open the 3rd one. That is,</span></p>
<div class="math-display">
<img src="../media/file1720.png" class="math-display" alt="P (B | A ) = P (B | A ) = 1, 3 1 2 1 2 P (B3 | A2) = 1. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, by applying the Bayes formula, we have</span></p>
<div class="math-display">
<img src="../media/file1721.png" class="math-display" alt=" P(B3 | A1 )P(A1) P(A1 | B3 ) =---------------- P(B3 ) = -1∕6--, P(B3 ) "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1722.png" class="math-display" alt=" P(B3 | A2 )P(A2) P(A2 | B3 ) =---------------- P(B3 ) = -1∕3--. P(B3 ) "/>
</div>
<p><span class="cmss-10x-x-109">In conclusion, </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span><sub><span class="cmr-8">3</span></sub>) <span class="cmss-10x-x-109">is twice as large as </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">B</span><sub><span class="cmr-8">3</span></sub>)<span class="cmss-10x-x-109">, from which we deduce</span></p>
<div class="math-display">
<img src="../media/file1723.png" class="math-display" alt="P (A1 | B3) = 1, P (A2 | B3) = 2-. 3 3 "/>
</div>
<p><span class="cmss-10x-x-109">So, you should </span><span class="cmssi-10x-x-109">always </span><span class="cmss-10x-x-109">switch doors. Surprising, isn’t it? Here, the paradox</span><span id="dx1-295007"></span> <span class="cmss-10x-x-109">is that</span> <span id="dx1-295008"></span><span class="cmss-10x-x-109">contrary to what we might expect, changing our minds is the better option. With clear probabilistic thinking, we can easily resolve this.</span></p>
</section>
</section>
<section id="summary17" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_263"><span class="titlemark"><span class="cmss-10x-x-109">18.4 </span></span> <span id="x1-29600022.4"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">Phew! We are at the end of an intimidatingly long, albeit extremely essential chapter. Although we’ve talked about the mathematical details of probability for a couple of dozen pages, the most important takeaway can be summarized in a sentence: probability theory extends our reasoning toolkit by handling uncertainty. Instead of measuring the truthiness of a proposition on a true-or-false binary scale, it opens up a spectrum between </span>0 <span class="cmss-10x-x-109">and </span>1<span class="cmss-10x-x-109">, where </span>0 <span class="cmss-10x-x-109">represents (almost) impossible, and </span>1 <span class="cmss-10x-x-109">represents (almost) certain.</span></p>
<p><span class="cmss-10x-x-109">Mathematically speaking, probabilistic models are defined by </span><span class="cmssi-10x-x-109">probability measures and spaces</span><span class="cmss-10x-x-109">, that is, structures of the form </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>)<span class="cmss-10x-x-109">, where </span>Ω <span class="cmss-10x-x-109">is the set of possible elementary outcomes, </span>Σ <span class="cmss-10x-x-109">is the collection of events, and </span><span class="cmmi-10x-x-109">P </span><span class="cmss-10x-x-109">is a probability measure, satisfying</span></p>
<ol>
<li><span id="x1-296002x1"><span class="cmmi-10x-x-109">P</span>(Ω) = 1</span></li>
<li><span id="x1-296004x2"><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>) = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">n</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">for all mutually disjoint </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">∈ </span>Σ<span class="cmss-10x-x-109">,</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">which are called the Kolmogorov axioms. Thinking in probabilities enables us to reason under uncertainty: if </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">is the probabilistic version of the truth value of a statement, then the conditional probability</span></p>
<div class="math-display">
<img src="../media/file1724.png" class="math-display" alt="P (B | A) = P-(A-∩-B) P (A) "/>
</div>
<p><span class="cmss-10x-x-109">is the probabilistic version of the implication </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">However, all the tools we learned are just the tips of a massive iceberg. To build truly beefy and useful models, we need to once more turn qualitative into quantitative, as we did for many advances in science and mathematics. Can you recall the dice rolling experiment, where we used a mysterious variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">to represent the outcome of the roll? Thus, we could talk about events such as “</span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">”, turning a probability space into a sequence of numbers.</span></p>
<p><span class="cmss-10x-x-109">It’s not a coincidence; it’s a method. </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is an instance of a </span><span class="cmssi-10x-x-109">random variable</span><span class="cmss-10x-x-109">, the premier object of probability theory and statistics. Random variables translate between abstract probability spaces to numbers and vectors, our old friends. Let’s make them a permanent tool in our belt.</span></p>
</section>
<section id="problems16" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_264"><span class="titlemark"><span class="cmss-10x-x-109">18.5 </span></span> <span id="x1-29700022.5"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Let’s roll two six-sided dice! Describe the event space, </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra, and the corresponding probabilities for this experiment.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span>Ω = [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, and the corresponding </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra be the generated algebra</span></p>
<div class="math-display">
<img src="../media/file1725.png" class="math-display" alt=" ( ) Σ = σ {(a,b] : 0 ≤ a &lt;b ≤ 1} . "/>
</div>
<p><span class="cmss-10x-x-109">Show that the following events are members of </span>Σ<span class="cmss-10x-x-109">:</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">1</span></sub> = <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">x</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">2</span></sub> = <span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup>(<span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">,b</span><sub><span class="cmmi-8">i</span></sub>)<span class="cmss-10x-x-109">. (Show that this is also true when the intervals</span> [<span class="cmmi-10x-x-109">…</span>] <span class="cmss-10x-x-109">are replaced with open and half-open versions </span>(<span class="cmmi-10x-x-109">…</span>)<span class="cmmi-10x-x-109">,</span>(<span class="cmmi-10x-x-109">…</span>]<span class="cmmi-10x-x-109">,</span>[<span class="cmmi-10x-x-109">…</span>)<span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">3</span></sub> = [0<span class="cmmi-10x-x-109">,</span>1] <span class="cmsy-10x-x-109">∩</span><span class="msbm-10x-x-109">ℚ</span><span class="cmss-10x-x-109">. (That is, the set of rational numbers in</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmssi-10x-x-109">(d) </span><span class="cmmi-10x-x-109">S</span><sub><span class="cmr-8">4</span></sub> = [0<span class="cmmi-10x-x-109">,</span>1] <span class="cmsy-10x-x-109">∩</span><span class="big">(</span><span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">∖</span><span class="msbm-10x-x-109">ℚ</span><span class="big">)</span><span class="cmss-10x-x-109">. (That is, the set of irrational numbers in</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let’s roll two six-sided dice. What is the probability that</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Both rolls are odd numbers?</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">At least one of them is an odd number?</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">None of them are odd numbers?</span></p>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span>Ω = <span class="msbm-10x-x-109">ℝ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">be the event space, where we define the open disks</span></p>
=
<img src="../media/file1728.png" width="450" class="math-display" alt="D (x,r) := {z ∈ ℝ2 : ∥x − z∥ &lt;r}, x = (x1,x2) ∈ ℝ2, r &gt;0, "/>
<p><span class="cmss-10x-x-109">and the open rectangles by</span></p>
<img src="../media/file1729.png" class="math-display" alt="R(x,y ) = (x1,y1)× (x2,y2) 2 = {z = (z1,z2) ∈ ℝ : x1 &lt;z1 &lt;y1,x2 &lt;z2 &lt;y2}. " width="450"/>
<p><span class="cmss-10x-x-109">Show that the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras generated by these sets are the same, that is,</span></p>
<img src="../media/file1730.png" class="math-display" alt=" ( 2 ) ( 2 ) σ {D (x, r) : x ∈ ℝ ,r &gt;0} = σ {R (x,y) : x,y ∈ ℝ } . " width="450"/>

<p><span class="cmssbx-10x-x-109">Problem 5. </span><span class="cmss-10x-x-109">Let’s consider a variant of the Monty Hall problem. Suppose there are a hundred doors instead of three; only one contains a reward. Upon picking a door, Monty opens ninety-eight other doors, all of which are empty. Should you switch now?</span></p>
</section>
<section id="join-our-community-on-discord18" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_265"><span id="x1-298000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>