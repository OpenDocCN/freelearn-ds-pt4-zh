- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Probability?
  prefs: []
  type: TYPE_NORMAL
- en: When going about our lives, we almost always think in binary terms. A statement
    is either true or false. An outcome has either occurred or not.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we rarely have the comfort of certainty. We have to operate with
    incomplete information. When a scientist observes the outcome of an experiment,
    can they verify their hypothesis with 100% certainty? No. Because they do not
    have complete control over all the variables (such as the weather or the alignment
    of stars), the observed effect might be unintentional. Each result will either
    strengthen or weaken our belief in the hypothesis, but none will provide ultimate
    proof.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, our job is not simply to provide a prediction about some
    class label but to formulate a mathematical model that summarizes our knowledge
    about the data in a way that conveys information about the degree of our certainty
    in the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, fitting a parametric function f : ℝ^n →ℝ^m to model the relation between
    the data and the variable to be predicted is not enough. We will need an entirely
    new vocabulary to formulate such models. We need to think in terms of probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 18.1 The language of thinking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s talk about how we think. On the most basic level, our knowledge
    about the world is stored in propositions. In a mathematical sense, a proposition
    is a declaration that is either true or false. (In binary terms, true is denoted
    by 1 and false is denoted by 0.)
  prefs: []
  type: TYPE_NORMAL
- en: “The sky is blue.”
  prefs: []
  type: TYPE_NORMAL
- en: “There are infinitely many prime numbers.”
  prefs: []
  type: TYPE_NORMAL
- en: “1 + 1 = 3.”
  prefs: []
  type: TYPE_NORMAL
- en: “I got the flu.”
  prefs: []
  type: TYPE_NORMAL
- en: Propositions are often abbreviated as variables such as A = ”it’s raining outside”.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the truth value of a given proposition using evidence and reasoning
    is called inference. To be able to formulate valid arguments and understand how
    inference works, we’ll take a quick visit to the world of mathematical logic.
  prefs: []
  type: TYPE_NORMAL
- en: 18.1.1 Thinking in absolutes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So, we have propositions such as A = ”it’s raining outside” or B = ”the sidewalk
    is wet”. We need more expressive power: propositions are building blocks, and
    we want to combine them, yielding more complex propositions. (We’ll review the
    fundamentals of mathematical logic here, but check out Appendix [A](ch035.xhtml#its-just-logic)
    for more.)'
  prefs: []
  type: TYPE_NORMAL
- en: We can formulate complex propositions from simpler ones with logical connectives.
    Consider the proposition “if it is raining outside, then the sidewalk is wet”.
    This is the combination of A and B, strung together by the implication connective.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four essential connectives:'
  prefs: []
  type: TYPE_NORMAL
- en: NOT (¬), also known as negation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AND ( ∧), also known as conjunction,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OR (∨), also known as disjunction,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: THEN (→), also known as implication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Connectives are defined by the truth values of the resulting propositions.
    For instance, if A is true, then ¬A is false; if A is false, then ¬A is true.
    Denoting true by 1 and false by 0, we can describe connectives with truth tables.
    Here is the one for negation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![| | | |A-|¬A--| |0 | 1 | | | | |1 | 0 | | | ](img/file1606.png)'
  prefs: []
  type: TYPE_IMG
- en: AND (∧) and OR (∨) connect two propositions. A ∧B is true if both A and B are
    true, while A ∨B is true if either one is.
  prefs: []
  type: TYPE_NORMAL
- en: '![| | | | | |A-|B--|A-∧-B--|A-∨-B-| |0 | 0 | 0 | 0 | | | | | | |0 | 1 | 0 |
    1 | |1 | 0 | 0 | 1 | | | | | | |1 | 1 | 1 | 1 | | | ](img/file1607.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The implication connective THEN (→) formalizes the deduction of a conclusion
    B from a premise A. By definition, A →B is true if B is true or both A and B are
    false. An example: IF “it’s raining outside”, THEN “the sidewalk is wet”.'
  prefs: []
  type: TYPE_NORMAL
- en: '![| | | | |A--|B-|A--→-B--| | 0 |0 | 1 | | | | | | 0 |1 | 1 | | 1 |0 | 0 |
    | | | | | 1 |1 | 1 | | | ](img/file1608.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that A →B does not imply B →A. This common logical fallacy is called affirming
    the consequent, and we’ve all fallen victim to it at some point in our lives.
    To see a concrete example: if “it’s raining outside”, then “the sidewalk is wet”,
    but not the other way around. The sidewalk can be wet for other reasons, such
    as someone spilling a barrel of water.'
  prefs: []
  type: TYPE_NORMAL
- en: Connectives correspond to set operations. Why? Let’s take a look at the formal
    definition of set operations.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 75\. (The (reasonably) formal definition of set operations and relations)
  prefs: []
  type: TYPE_NORMAL
- en: Let A and B be two sets.
  prefs: []
  type: TYPE_NORMAL
- en: (a) The union of A and B is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![A ∪ B := {x : (x ∈ A )∨ (x ∈ B )}, ](img/file1609.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, A ∪B contains all elements that are in A or B.
  prefs: []
  type: TYPE_NORMAL
- en: (b) The intersection of A and B is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![A ∩ B := {x : (x ∈ A )∧ (x ∈ B )}, ](img/file1610.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, A ∩B contains all elements that are in A and B.
  prefs: []
  type: TYPE_NORMAL
- en: (c) We say that A is a subset of B, that is, A ⊆B if
  prefs: []
  type: TYPE_NORMAL
- en: '![(x ∈ A ) → (x ∈ B ) ](img/file1611.png)'
  prefs: []
  type: TYPE_IMG
- en: is true for all x ∈A.
  prefs: []
  type: TYPE_NORMAL
- en: (d) The complement of A with respect to an Ω ⊃A is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![Ω ∖A := {x ∈ Ω : ¬ (x ∈ A )}, ](img/file1612.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, Ω ∖A contains all elements that are in Ω, but not in A.
  prefs: []
  type: TYPE_NORMAL
- en: If you carefully read through the definitions, you can see how connectives and
    set operations relate. ∧ is intersection, ∨ is union, ¬ is the complement, and
    → is the subset relation. This is illustrated by Figure [18.1](#). (I’ve slightly
    abused the notation here, as statements such as A ∧B ⟺ A ∩B are mathematically
    incorrect. A and B cannot be a proposition and a set at the same time, thus the
    equivalence is not precise. )
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.1: Connectives and set operations'
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? Because probability operates on sets, and sets play the
    role of propositions. We’ll see this later, but first, let’s dive deep into how
    mathematical logic formalizes scientific thinking.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s refine the inference process of mathematical logic. A proposition is either
    true or false, fair and square. How can we determine that in practice? For example,
    how do we find the truth value of the proposition “there are infinitely many prime
    numbers“?
  prefs: []
  type: TYPE_NORMAL
- en: 'By using evidence and deduction. Like Sherlock Holmes solving a crime by connecting
    facts, we rely on knowledge of the form “if A, then B“. Our knowledge about the
    world is stored in true implications. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: “If it is raining, then the sidewalk is wet.“
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “If ![ABC ](img/file1614.png) is a right triangle, then A² + B² = C².“
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “If a system is closed, then its entropy cannot decrease.“
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen, the implication can be translated into the language of set
    theory (as all the other connectives). While ∧ corresponds to intersection and
    ∨ to union, the implication is the subset relation. Keep this in mind, as it’s
    going to be important.
  prefs: []
  type: TYPE_NORMAL
- en: 'During inference, we use implications in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: If A, then B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is called the modus ponens. If it sounds abstract, here is a concrete
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: If it is raining, the sidewalk is wet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is raining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, the sidewalk is wet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thus, we can infer the state of the sidewalk without looking at it. This is
    bigger than it sounds: modus ponens is a cornerstone of scientific thinking. We
    would still be living in caves without it. Modus ponens enables us to build robust
    skyscrapers of knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s not all perfect. Classical deductive logic might help to prove
    the infinity of prime numbers, but it fails spectacularly when confronted with
    inference problems outside the realms of mathematics and philosophy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classical logic has a fatal flaw: it is unable to deal with uncertainty. Think
    about the simple proposition “it is raining outside”. If we are unable to actually
    observe the weather but have some indirect evidence (such as the fact that the
    sidewalk is wet, or the sky is cloudy, or it’s autumn out there), “it is raining
    outside” is probable but not certain.'
  prefs: []
  type: TYPE_NORMAL
- en: We need a tool to measure the truth value on a 0 − 1 scale. This is where probabilities
    come in.
  prefs: []
  type: TYPE_NORMAL
- en: 18.1.2 Thinking in probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a mathematical sense, probability is a function that assigns a numerical
    value between zero and one to various sets that represent events. (You can think
    of events as propositions.) Events are subsets of the event space, often denoted
    with the capital Greek letter omega (Ω). This is illustrated in Figure [18.2](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1615.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.2: Events and the event space'
  prefs: []
  type: TYPE_NORMAL
- en: 'This sounds quite abstract, so let’s see a simple example: rolling a fair six-sided
    dice. We can encode all possible outcomes with the event space Ω = {1,2,3,4,5,6}.
    Events such as A = ”the outcome is even” or B = ”the outcome is larger than 3”
    are represented by the sets'
  prefs: []
  type: TYPE_NORMAL
- en: '![A = {2,4,6}, B = {4,5,6}. ](img/file1616.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the dice is fair, the probability of each outcome is the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1- P ({1}) = ⋅⋅⋅ = P({6}) = 6\. ](img/file1617.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two properties that make such a function P a proper measure of probability:'
  prefs: []
  type: TYPE_NORMAL
- en: the probability of the event space is one,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and the probability of the union of disjoint events is the sum of probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our dice-rolling example, this is translated to, for instance,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (the outcome is even) = P({2,4,6}) = P({2}) + P({4}) + P({6}) 1 1 1 = --+
    --+ -- 6 6 6 = 1\. 2 ](img/file1618.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ll talk about these properties extensively in the next section. As logical
    connectives can be represented in the language of set theory, set operations translate
    the semantics of logic into probabilities. Intersection is joint occurrence of
    events. Union is the occurrence of either one.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1619.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.3: The probabilities of intersection and union'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we are able to build models involving uncertainty and develop a
    calculus to work with said models. In the Tower of Babel that is mathematics,
    statistics deals with the modeling part, and probability theory deals with the
    calculus part.
  prefs: []
  type: TYPE_NORMAL
- en: Even technically well-trained engineers conflate modeling and working with models.
    For instance, when we talk about flipping fair coins, the probability of heads
    and tails are both 1∕2\. Even when we are absolutely sure about the model but
    have ten heads in a row, most would immediately jump to the conclusion that our
    coin is biased.
  prefs: []
  type: TYPE_NORMAL
- en: To make sure we are not making this mistake, first, we are going to learn what
    probability is.
  prefs: []
  type: TYPE_NORMAL
- en: 18.2 The axioms of probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we have talked about probability as an extension of
    mathematical logic. Just like formal logic, probability has its axioms, which
    we need to understand to work with probability models. Now, we are going to seek
    the answer to a fundamental question: what is the mathematical model of probability
    and how do we work with it?'
  prefs: []
  type: TYPE_NORMAL
- en: Probabilities are defined in the context of experiments and outcomes. To talk
    about probabilities, we need to define what we assign probabilities to. Formally
    speaking, we denote the probability of the event A by P(A). First, we’ll talk
    about what events are.
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.1 Event spaces and σ-algebras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s revisit the six-sided example from the previous section. There are six
    different mutually exclusive outcomes (that is, events that cannot occur at the
    same time), and together they form the event space, denoted by Ω:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ω := {1,2,3,4,5,6}. ](img/file1620.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, the event space is the collection of all mutually exclusive outcomes.
    It can be any set.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of events can we assign probabilities to? Obviously, the individual
    outcomes come to mind. However, we can think of events such as “the result is
    an odd number”, “the result is 2 or 6”, or “the result is not 1”. Following this
    logic, our expectations are that for any two events A and B,
  prefs: []
  type: TYPE_NORMAL
- en: “A or B”,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “A and B”,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and “not A”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: are events as well. These can be translated to the language of set theory and
    are formalized by the notion of σ-algebras.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 76\.
  prefs: []
  type: TYPE_NORMAL
- en: (σ-algebras)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let Ω be an event space. A collection of its subsets Σ ⊆ 2^Ω is called an σ-algebra
    over Ω if the following properties hold:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Ω ∈ Σ. (That is, the set of all outcomes is an event.)
  prefs: []
  type: TYPE_NORMAL
- en: (b) For all A ∈ Σ, the set Ω∖A is also an element of Σ. (That is, σ-algebras
    are closed to complements.)
  prefs: []
  type: TYPE_NORMAL
- en: (c) For all A[1],A[2],⋅⋅⋅∈ Σ, the set ∪[n=1]^∞A[n] is also an element of Σ.
    (That is, σ-algebras are closed to unions.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Since events are modeled by sets, logical concepts such as “and”, “or”, and
    “not” can be translated into set operations:'
  prefs: []
  type: TYPE_NORMAL
- en: the joint occurrence of events A and B is equivalent to A ∩B,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “A or B” is equivalent to A ∪B,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and “not A” is equivalent to Ω ∖A.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An immediate consequence of the definition is that, for any events A[1],A[2],⋅⋅⋅∈
    Σ, their intersection ∩[n=1]^∞A[n] is also a member of Σ. Indeed, as De Morgan’s
    laws (Theorem [153](ch037.xhtml#x1-377003r153)) suggest,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ ∞ Ω ∖(∩ n=1 An ) = ∪n=1(Ω ∖ An). ](img/file1623.png)'
  prefs: []
  type: TYPE_IMG
- en: Since Ω ∖ (Ω ∖A) = A, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ ∞ ∩ n=1An = Ω ∖ (Ω ∖(∩n=1An )) ∞ = Ω ∖ ∪n=1( Ω◟-∖◝A◜n◞). ∈Σ ](img/file1624.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the defining properties of σ-algebras guarantee that ∩[n=1]^∞A[n] is indeed
    an element of Σ. Another immediate consequence of the definition is that since
    Ω ∈ Σ, the empty set ∅ is also a member of Σ.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, σ-algebras seem a bit abstract. As usual, a bit of abstraction
    now will pay us huge dividends later in our studies. To bring this concept closer,
    here is a summary of σ-algebras in English:'
  prefs: []
  type: TYPE_NORMAL
- en: The set of all possible outcomes is an event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any event, it not occurring is an event as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any events, their joint occurrence is an event as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any events, at least one of them occurring is an event as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have the formal definition under our belt, let’s see the first example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1\. Rolling a six-sided dice. There, the σ-algebra is simply the power
    set of the event space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ω = {1,2,3,4,5,6}, Σ = 2Ω. ](img/file1625.png)'
  prefs: []
  type: TYPE_IMG
- en: (Recall that the power set of A is the set 2^A containing all subsets of A,
    as defined in Definition [108](ch037.xhtml#x1-374009r108).) Even though this is
    one of the simplest examples, it will serve as a prototype and a building block
    for constructing more complicated event spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2\. Tossing a coin n times. A single toss has two possible outcomes:
    heads or tails. For simplicity, we are going to encode heads with 0 and tails
    with 1\. Since we are tossing the coin n times, the result of an experiment will
    be an n-long sequence of ones and zeros, like this: (0,1,1,1,…,0,1). Thus, the
    complete event space is Ω = {0,1}^n.'
  prefs: []
  type: TYPE_NORMAL
- en: (We are not talking about probabilities just yet, but feel free to spend some
    time figuring out how to assign them to these events. Don’t worry if this is not
    clear; we will go through it in detail.)
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous example, the σ-algebra 2^Ω is a good choice. This covers
    all events that we need, for instance, “the number of tails is k”.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, σ-algebras are rarely given explicitly. Sure, for simple cases
    such as the above, it is possible.
  prefs: []
  type: TYPE_NORMAL
- en: What about cases where the event spaces are not countable? For instance, suppose
    that we are picking a random number between 0 and 1\. Then, Ω = [0,1], but selecting
    Σ = 2^([0,1]) is extremely problematic. Recall that we want to assign a probability
    to every event in Σ. The power set 2^([0,1]) is so large that very strange things
    can occur. In certain scenarios, we can cut up sets into a finite number of pieces
    and reassemble two identical copies of the set from its pieces. (If you are interested
    in more, check out the [Banach-Tarski paradox](https://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox).)
  prefs: []
  type: TYPE_NORMAL
- en: To avoid weird things like the above-mentioned, we need another way to describe
    σ-algebras.
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.2 Describing σ-algebras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start with a simple yet fundamental property of σ-algebras that we’ll
    soon use to give a friendly description of σ-algebras.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 109\. (Intersection of event algebras.)
  prefs: []
  type: TYPE_NORMAL
- en: Let Ω be a sample space, and let Σ[1] and Σ[2] be two σ-algebras over it. Then
    Σ[1] ∩ Σ[2] is also a σ-algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As we saw in the definition of σ-algebras (Definition [76](ch030.xhtml#x1-282004r76)),
    there are three properties we need to verify to show that Σ[1] ∩ Σ[2] is indeed
    a σ-algebra. This is very simple to check, so I suggest taking a shot by yourself
    first before reading my explanation.
  prefs: []
  type: TYPE_NORMAL
- en: (a) As both Σ[1] and Σ[2] are σ-algebras, Ω ∈ Σ[1] and Ω ∈ Σ[2] both hold. Thus,
    by definition of the intersection, Ω ∈ Σ[1] ∩ Σ[2].
  prefs: []
  type: TYPE_NORMAL
- en: (b) Let A ∈ Σ[1] ∩ Σ[2]. As both of them are σ-algebras, Ω ∖A ∈ Σ[1] and Ω ∖A
    ∈ Σ[2]. Thus, Ω ∖A is an element of the intersection as well.
  prefs: []
  type: TYPE_NORMAL
- en: '(c) Let A[1],A[2],⋅⋅⋅ ∈ Σ[1] ∩ Σ[2] be arbitrary events. We can use the exact
    same argument as before: since both Σ[1] and Σ[2] are σ-algebras, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![⋃∞ ∞⋃ An ∈ Σ1 and An ∈ Σ2\. n=1 n=1 ](img/file1627.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the union is also a member of the intersection, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![⋃∞ An ∈ Σ1 ∩ Σ2\. n=1 ](img/file1628.png)'
  prefs: []
  type: TYPE_IMG
- en: With all that, we are ready to describe σ-algebras with a generating set.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 110\. (Generated σ-algebras)
  prefs: []
  type: TYPE_NORMAL
- en: Let Ω be an event space and S ⊆ 2^Ω be an arbitrary collection of its sets.
    Then there is an unique smallest σ-algebra σ(S) that contains S.
  prefs: []
  type: TYPE_NORMAL
- en: (By smallest, we mean that if Σ is an σ-algebra containing S, then σ(S) ⊆ Σ.)
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Our previous result shows that the intersection of σ-algebras is also
    an σ-algebra. So, let’s take all σ-algebras that contain S and take their intersection.
    Formally, we define
  prefs: []
  type: TYPE_NORMAL
- en: '![σ (S ) = ∩ {Σ : Σ is an σ-algebra and S ⊆ Σ}. ](img/file1629.png)'
  prefs: []
  type: TYPE_IMG
- en: By definition, σ(S) is clearly the smallest, and it also contains S.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also clear that the generated σ-algebra is unique as, if there would be
    another σ̂(S) satisfying the conditions, then due to the construction, σ̂ (S)
    ⊆σ(S) and σ(S) ⊆σ̂(S), hence σ̂(S) = σ(S).
  prefs: []
  type: TYPE_NORMAL
- en: 'Right away, we can use this to precisely construct the σ-algebra for an extremely
    common task: picking a number between 0 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. Selecting a random number between 0 and 1\. It is clear that the
    event space is Ω = [0,1]. What about the events? In this situation, we want to
    ask questions such as the probability of a random number X falling between some
    a,b ∈ [0,1]. That is, events such as (a,b),(a,b],[a,b),[a,b] (whether or not we
    want strict inequality regarding a and b).
  prefs: []
  type: TYPE_NORMAL
- en: So, a proper σ-algebra can be given by the algebra generated by events of the
    form (a,b]. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![Σ = σ({(a,b] : 0 ≤ a <b ≤ 1}). ](img/file1634.png)'
  prefs: []
  type: TYPE_IMG
- en: This Σ has a rich structure. For instance, it contains simple events such as
    {x}, where x ∈ [0,1], but also more complex ones such as “X is a rational number”
    or “X is an irrational number”. Give yourself a few minutes to see why this is
    true. Don’t worry if you don’t see the solution—we’ll work this out in the problems
    section. (If you think this through, you’ll also see why we chose intervals of
    the form (a,b] instead of others such as (a,b) or [a,b].)
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.3 σ-algebras over real numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From all the examples we have seen so far, it is clear that most commonly, we
    define probability spaces on ℕ or on ℝ. When Ω ⊆ℕ, the choice of σ-algebra is
    clear, as Σ = 2^Ω will always work.
  prefs: []
  type: TYPE_NORMAL
- en: However, as suggested in Example 3 above, selecting Σ = 2^Ω when Ω ⊆ℝ can lead
    to some weird stuff. Because we are interested in the probability of events such
    as [a,b], our standard choice is going to be the generated σ-algebra
  prefs: []
  type: TYPE_NORMAL
- en: 'ℬ(ℝ) = σ({ (a,b) : a,b ∈ ℝ }), (18.1)'
  prefs: []
  type: TYPE_NORMAL
- en: called the Borel algebra, named after the famous French mathematician [Émile
    Borel](https://en.wikipedia.org/wiki/%C3%89mile_Borel). Due to its construction,
    ℬ contains all events that are important to us, such as intervals and unions of
    intervals. Elements of ℬ are called Borel sets.
  prefs: []
  type: TYPE_NORMAL
- en: Because σ-algebras are closed to unions, you can see that all types of intervals
    can be found in ℬ(ℝ). This is summarized by the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 111\.
  prefs: []
  type: TYPE_NORMAL
- en: For all a,b ∈ℝ, the sets [a,b], (a,b], [a,b), (−∞,a], (−∞,a), (a,∞), and [a,∞)
    are elements of ℬ(ℝ).
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, try to come up with the proof by yourself. One trick to get
    the ideas flowing is to start drawing some figures. If you can visualize what
    happens, you’ll discover a proof quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. In general, for a given set S, we can show that it belongs to ℬ(ℝ) by
    writing it as the union/intersection/difference of known Borel sets. First, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ ⋃ (a,∞ ) = (a,n), n=1 ](img/file1637.png)'
  prefs: []
  type: TYPE_IMG
- en: so (a,∞) ∈ ℬ(ℝ). With a similar argument, we see that (−∞,a) ∈ ℬ(ℝ).
  prefs: []
  type: TYPE_NORMAL
- en: Next,
  prefs: []
  type: TYPE_NORMAL
- en: '![(− ∞, a] = ℝ ∖ (a,∞ ), [a,∞ ) = ℝ ∖ (− ∞, a), ](img/file1638.png)'
  prefs: []
  type: TYPE_IMG
- en: so (−∞,a],[a,∞) ∈ ℬ(ℝ) for all a. From these, the sets [a,b],(a,b],[a,b) can
    be produced by intersections.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what events and σ-algebras are, we can take our first
    detailed look at probability. In the next section, we will introduce its precise
    mathematical definition.
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.4 Probability measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s recap what we have learned so far! In the language of mathematics, experiments
    with intrinsic uncertainty are described using outcomes, event spaces, and events.
  prefs: []
  type: TYPE_NORMAL
- en: The collection of all possible mutually exclusive outcomes of an experiment
    is called the event space, denoted by Ω. Certain subsets of Ω are called events,
    to which we want to assign probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: These events form what is known as a σ-algebra, denoted by Σ. We denote the
    probability of an event A by P(A).
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively speaking, we have three reasonable expectations about probability:'
  prefs: []
  type: TYPE_NORMAL
- en: P(Ω) = 1, that is, the probability that at least one outcome occurs is 1\. In
    other words, our event space is a complete description of the experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P(∅) = 0, that is, the probability that none of the outcomes occur is 0\. Again,
    this means that our event space is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The probability that either of two events occurs for two mutually exclusive
    events is the sum of the individual probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are formalized by the following definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 77\. (Probability measures and spaces) Let Ω be an event space and
    Σ be an σ-algebra over Ω. We say that the function P : Σ → [0,1] is a probability
    measure on Σ if the following properties hold:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) P(Ω) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: (b) If A[1],A[2],… are mutually disjoint events (that is, A[i]∩A[j] = ∅ for
    all i≠j), then P(∪[n=1]^∞A[n]) = ∑ [n=1]^∞P(A[n]). This property is called the
    σ-additivity of probability measures.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the probability measure P, the structure (Ω,Σ,P) is said to form
    a probability space.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, let’s see some concrete examples first! We are going to continue with
    the ones we worked out when discussing σ-algebras.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1, continued. Rolling a six-sided dice. Recall that the event space
    and algebra were defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ Ω Ω = {1,2,3,4,5,6}, Σ = 2 . ](img/file1639.png)'
  prefs: []
  type: TYPE_IMG
- en: If we don’t have any extra knowledge about our dice, it is reasonable to assume
    that each outcome is equally probable. That is, since there are six possible outcomes,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P ({1}) = ⋅⋅⋅ = P({6}) = 1\. 6 ](img/file1640.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that, in this case, knowing the probabilities for the individual outcomes
    is enough to determine the probability of any event. This is due to the (σ-)additivity
    of the probability. For instance, the event “the outcome of the dice roll is an
    odd number” is described by
  prefs: []
  type: TYPE_NORMAL
- en: '![ 3 P ({1,3,5} ) = P ({1})+ P ({3 })+ P ({5} ) = 6-. ](img/file1641.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In English, the probability of any event can be written down with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![P (event) = -favorable outcomes-. all possible outcomes ](img/file1642.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You might remember this from your elementary and high school studies (depending
    on the curriculum in your country). This is a useful formula, but there is a caveat:
    it only works if we assume that each outcome has an equal probability.'
  prefs: []
  type: TYPE_NORMAL
- en: In the case when our dice is not uniformly weighted, the occurrences of individual
    outcomes are not equal. (Just think of a lead dice, where one side is significantly
    heavier than the others.) For now, we are not going to be concerned with this
    case. Later, this generalization will be discussed in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2, continued. Tossing a coin n times. Here, our event space and algebra
    was Ω = {0,1}^n and Σ = 2^Ω. For simplicity, let’s assume that n = 5.
  prefs: []
  type: TYPE_NORMAL
- en: What is the probability of a particular result, say HHTTT? Going step by step,
    the probability that the first toss will be heads is 1∕2\. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 P(first toss is heads) =- 2 ](img/file1643.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the first toss is independent of the second,
  prefs: []
  type: TYPE_NORMAL
- en: '![P(second toss is heads) = 1 2 ](img/file1644.png)'
  prefs: []
  type: TYPE_IMG
- en: as well. To combine this and calculate the probability that the first two tosses
    are both heads, we can think in the following way. Among the outcomes where the
    first toss is heads, exactly half of them will have the second toss heads as well.
    So, we are looking for the half of the half. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![P(first two tosses are heads) = P(first toss is heads)P(second toss is heads)
    1- = 4\. ](img/file1645.png)'
  prefs: []
  type: TYPE_IMG
- en: Going further with the same logic, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 1 P (HHTTT ) = -5 = --. 2 32 ](img/file1646.png)'
  prefs: []
  type: TYPE_IMG
- en: If we look a bit deeper, we will notice that this follows the previously seen
    “favorable/all” formula. Indeed, as we can see with a bit of combinatorics, there
    are 2⁵ total possibilities, all of them having equal probability.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, what is the probability that out of our five tosses, exactly
    two of them are heads? In the language of sets, we can encode each five-toss experiment
    as a subset of {1,2,3,4,5}, the elements signifying the toss that resulted in
    heads. (So, for example, {1,4,5} would encode the outcome HTTHH.) With this, the
    experiments when there are two heads are exactly the two-element subsets of {1,2,3,4,5}.
  prefs: []
  type: TYPE_NORMAL
- en: From our combinatorics studies, we know that the number of subsets with given
    elements is
  prefs: []
  type: TYPE_NORMAL
- en: '![( ) n = ---n!----, k k!(n − k)! ](img/file1647.png)'
  prefs: []
  type: TYPE_IMG
- en: where n is the size of our set and k is the desired size of the subsets. So,
    in total, there are ![(5) 2](img/file1648.png) number of occurrences with exactly
    two heads. Thus, following the “favorable/all” formula, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) P(two heads out of five tosses) = 5 -1-= 10\. 2 32 32 ](img/file1649.png)'
  prefs: []
  type: TYPE_IMG
- en: One more example, and we are ready to move forward.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3, continued. Selecting a random number between 0 and 1\. Here, our
    event space was Ω = [0,1], and our σ-algebra was the generated algebra
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) Σ = σ {(a,b] : 0 ≤ a <b ≤ 1} . ](img/file1650.png)'
  prefs: []
  type: TYPE_IMG
- en: Without any further information, it is reasonable to assume that every number
    can be selected with an equal probability. What does this even mean for an infinite
    event space such as Ω = [0,1]? We can’t divide 1 into infinitely many equal parts.
  prefs: []
  type: TYPE_NORMAL
- en: So, instead of thinking about individual outcomes, we should start thinking
    about events. Let’s denote our randomly selected number with X. If all numbers
    are “equally likely”, what is P(X ∈ (0,1∕2])? Intuitively, given our equally likely
    hypothesis, this probability should be proportional to the size of [0,1∕2]. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X ∈ I) = |I|, ](img/file1651.png)'
  prefs: []
  type: TYPE_IMG
- en: where I is some interval and jIj is its length. For instance,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (a <X <b) = P(a ≤ X < b) = P(a <X ≤ b) = P(a ≤ X ≤ b) = b− a. ](img/file1652.png)'
  prefs: []
  type: TYPE_IMG
- en: By giving the probabilities on the generating set of the σ-algebra, the probabilities
    for all other events can be deduced. For instance,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X = x) = P(0 ≤ X ≤ x)− P (0 ≤ X <x) = x − x = 0\. ](img/file1653.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the probability of picking a given number is zero. There is an important
    lesson here: events with zero probability can happen. This sounds counterintuitive
    at first, but based on the above example, you can see that it is true.'
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.5 Fundamental properties of probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we are familiar with the mathematical model of probability, we can
    start working with them. Manipulating expressions of probabilities gives us the
    ability to deal with complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, probability measures had three simple defining properties (see
    Definition [77](ch030.xhtml#x1-285008r77)):'
  prefs: []
  type: TYPE_NORMAL
- en: (a) P(Ω) = 1,
  prefs: []
  type: TYPE_NORMAL
- en: (b) P(∅) = 0, and
  prefs: []
  type: TYPE_NORMAL
- en: (c) P![⋃ ∞ ( n=1 An)](img/file1654.png) = ∑ [n=1]^∞P(A[n]), if the events A[n]
    are mutually disjoint.
  prefs: []
  type: TYPE_NORMAL
- en: From these properties, many others can be deduced. For simplicity, here is a
    theorem summarizing the most important ones.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 112\.
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A,B ∈ Σ be two arbitrary events.
  prefs: []
  type: TYPE_NORMAL
- en: (a) P(A ∪B) = P(A) + P(B) −P(A ∩B).
  prefs: []
  type: TYPE_NORMAL
- en: (b) P(A) = P(A ∩B) + P(A ∖B). Specifically, P(Ω ∖A) + P(A) = 1.
  prefs: []
  type: TYPE_NORMAL
- en: (c) If A ⊆B, then P(A) ≤P(B).
  prefs: []
  type: TYPE_NORMAL
- en: The proof of this is so simple that it is left to you as an exercise. All of
    these follow from the additivity of probability measures with respect to disjoint
    events. (If you don’t see the solution, sketch some Venn diagrams!)
  prefs: []
  type: TYPE_NORMAL
- en: Another fundamental tool is the law of total probability, which is used all
    the time when dealing with more complex events.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 113\. (Law of total probability)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A ∈ Σ be an arbitrary event. If A[1],A[2],⋅⋅⋅∈
    Σ are mutually disjoint events (that is, A[i] ∩A[j] = ∅if i≠j) for which ∪[n=1]^∞A[n]
    = Ω, then
  prefs: []
  type: TYPE_NORMAL
- en: P(A) = ∑[n=1]^∞ P(A ∩ A[n]). (18.2)
  prefs: []
  type: TYPE_NORMAL
- en: We call mutually disjoint events whose union is the entire event space partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. This simply follows from the σ-additivity of probability measures. Feel
    free to give the proof a shot by yourself to test your understanding.
  prefs: []
  type: TYPE_NORMAL
- en: If you can’t see this, no worries. Here is a brief explanation. Since A[1],A[2],…
    are mutually disjoint, A ∩A[1],A ∩A[2],… are mutually disjoint as well. Moreover,
    since ∪[n=1]^∞A[n] = Ω, we also have
  prefs: []
  type: TYPE_NORMAL
- en: '![∞⋃ ( ∞⋃ ) (An ∩ A) = An ∩ A n=1 n=1 = Ω ∩ A = A. ](img/file1656.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the σ-additivity of probability measures implies that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ P (A) = ∑ P (A ∩ A ), n=1 n ](img/file1657.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see an example right away! Suppose that we toss two dice. What is the
    probability that the sum of the results is 7?
  prefs: []
  type: TYPE_NORMAL
- en: First, we should properly describe the probability space. For notational simplicity,
    let’s denote the result of the throws with X and Y . What we are looking for is
    P(X + Y = 7). Modeling the toss with two dice is the simplest if we impose order
    among them. With this in mind, the event space Ω is described by the Cartesian
    product
  prefs: []
  type: TYPE_NORMAL
- en: '![Ω = {1,2,3,4,5,6}× {1,2,3,4,5,6} = {(i,j) : i,j ∈ {1,2,3,4,5,6}}, ](img/file1658.png)'
  prefs: []
  type: TYPE_IMG
- en: and the outcomes are tuples of the form (i,j). (That is, the tuple (i,j) encodes
    the elementary event {X = i,Y = j}.) Since the tosses are independent of each
    other,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X = i,Y = j) = 1-⋅ 1-=-1-. 6 6 36 ](img/file1659.png)'
  prefs: []
  type: TYPE_IMG
- en: (When it is clear, we omit the brackets of the event {X = i,Y = j}.)
  prefs: []
  type: TYPE_NORMAL
- en: Since the first throw falls between 1 and 6, we can partition the event space
    by forming
  prefs: []
  type: TYPE_NORMAL
- en: '![A := {X = n}, n = 1,...,6\. n ](img/file1660.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the law of total probability gives
  prefs: []
  type: TYPE_NORMAL
- en: '![ 6 P (X + Y = 7) = ∑ P ({X + Y = 7} and {X = n }). n=1 ](img/file1661.png)'
  prefs: []
  type: TYPE_IMG
- en: However, if we know that X + Y = 7 and X = n, we know that Y = 7 −n must hold
    as well. So, continuing the calculation above,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑6 ( ) P (X + Y = 7) = P {X + Y = 7} and {X = n} n=1 ∑6 = P(X = n,Y = 7
    − n) n=1 6 ∑ 1-- = 36 n=1 = 1\. 6 ](img/file1662.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the law of total probability helps us deal with complex events by decomposing
    them into simpler ones. We have seen this pattern dozens of times now, and once
    again, it proves to be essential.
  prefs: []
  type: TYPE_NORMAL
- en: As yet another consequence of σ-additivity, we can calculate the probability
    of an increasing sequence of events by taking the limit.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 114\. (Lower continuity of probability measures)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A[1] ⊆A[2] ⊆⋅⋅⋅∈ Σ be an increasing
    sequence of events. Then,
  prefs: []
  type: TYPE_NORMAL
- en: P(∪[n=1]^∞ A[n]) = lim[n→∞] P(A[n]) (18.3)
  prefs: []
  type: TYPE_NORMAL
- en: holds. This property is called the lower continuity of probability measures.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. Since the events are increasing, that is, A[n−1] ⊆ A[n], we can write
    A[n] as
  prefs: []
  type: TYPE_NORMAL
- en: '![An = An−1 ∪ (An ∖An −1), ](img/file1664.png)'
  prefs: []
  type: TYPE_IMG
- en: where A[n−1] and A[n] ∖A[n−1] are disjoint.
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞⋃ ∞⋃ An = (An ∖ An− 1), A0 := ∅, n=1 n=1 ](img/file1665.png)'
  prefs: []
  type: TYPE_IMG
- en: which gives
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑∞ P (∪∞n=1An ) = P(An ∖ An− 1) n=1 N∑ = lim P (An ∖ An− 1) N → ∞n=1 N =
    lim ∑ [P(A )− P (A )] N → ∞ n n−1 n=1 = lim P (AN ), N → ∞ ](img/file1666.png)'
  prefs: []
  type: TYPE_IMG
- en: where we used that P(∅) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: We can state an analogue of the above theorem for a decreasing sequence of events.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 115\. (Upper continuity of probability measures)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A[1] ⊇A[2] ⊇⋅⋅⋅∈ Σ be a decreasing
    sequence of events. Then,
  prefs: []
  type: TYPE_NORMAL
- en: P(∩[n=1]^∞ A[n]) = lim[n→∞] P(A[n]) (18.4)
  prefs: []
  type: TYPE_NORMAL
- en: holds. This property is called the upper continuity of probability measures.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. For simplicity, let’s denote the infinite intersection by A := ∩[n=1]^∞A[n].
  prefs: []
  type: TYPE_NORMAL
- en: By defining B[n] := A[1] ∖A[n], we have ∪[n=1]^∞B[n] = A[1] ∖A. Since A[n] is
    decreasing, B[n] is increasing, so we can apply Theorem [114](ch030.xhtml#x1-286007r114)
    to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A1 ∖A ) = nl→im∞ P (A1 ∖ An ) = P (A1) − lni→m∞ P (An). ](img/file1668.png)'
  prefs: []
  type: TYPE_IMG
- en: Since P(A[1] ∖A) = P(A[1]) −P(A), we obtain P(A) = lim[n→∞]P(A[n]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a mathematical definition of a probabilistic model, it is
    time to take a step toward the space where machine learning is done: ℝ^n.'
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.6 Probability spaces on ℝ^n
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In machine learning, every data point is an elementary outcome, located somewhere
    in the Euclidean space ℝ^n. Because of this, we are interested in modeling experiments
    there.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we define a probability space on ℝ^n? As we did with the real line
    in Section [18.2.1](ch030.xhtml#event-spaces-and-algebras), we describe a convenient
    σ-algebra by generating. There, we can use the higher dimensional counterpart
    of the (a,b) intervals: n-dimensional spheres. For this, we define the set'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n B (x,r) := {y ∈ ℝ : ∥x − y ∥ <r}, ](img/file1669.png)'
  prefs: []
  type: TYPE_IMG
- en: where x is the center of the sphere, r/span>0 is its radius, and ∥⋅∥ denotes
    the usual Euclidean norm. (The B denotes the word ball. In mathematics, n-dimensional
    spheres are often called balls.) Similar to the real line, the Borel algebra is
    defined by
  prefs: []
  type: TYPE_NORMAL
- en: 'ℬ(ℝ^n) := σ({B(x, r) : x ∈ ℝ^n, r > 0}) (18.5)'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw on the real line (see Section [18.2.3](ch030.xhtml#algebras-over-real-numbers)),
    the structure of ℬ(ℝ^n) is richer than what the definition suggests at first glance.
    Here, the analogue of interval is a rectangle, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![(a,b) = (a1,b1)× ⋅⋅⋅× (an,bn) n = {x ∈ ℝ : ai <xi <bi,i = 1,...,n}, ](img/file1672.png)'
  prefs: []
  type: TYPE_IMG
- en: where A×B is the Cartesian product. (See Definition [110](ch037.xhtml#x1-378002r110).)
    Similarly, we can define [a,b],(a,b],[a,b), and others.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 116\.
  prefs: []
  type: TYPE_NORMAL
- en: For any a,b ∈ ℝ^n, the sets [a,b],[a,b),(a,b],(a,∞),[a,∞),(−∞,a),(−∞,b] are
    elements of ℬ(ℝ^n).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. The proof goes along the same line as the counterpart for ℬ(ℝ). As such,
    it is left as an exercise to you.
  prefs: []
  type: TYPE_NORMAL
- en: As a hint, first, we can show that (a,b) can be written as a countable union
    of balls. We can also show that this holds true for sets such as
  prefs: []
  type: TYPE_NORMAL
- en: '![ℝ × ⋅⋅⋅× (◟−-∞◝,◜-ai)◞ × ⋅⋅⋅× ℝ i- th component ](img/file1673.png)'
  prefs: []
  type: TYPE_IMG
- en: as well. From these two, we can write the others as unions/intersections/differences.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s throw a few darts at a rectangular wall. Suppose that we
    are terrible darts players and hitting any point on the wall is equally likely.
  prefs: []
  type: TYPE_NORMAL
- en: We can model this event space with Ω = [0,1] × [0,1] ⊆ℝ², representing our wall.
    What are the possible events? For instance, there is a circular darts board hanging
    on the wall, and we want to find the probability of hitting it. In this scenario,
    we can restrict the Borel sets defined by ([22.2.6](ch030.xhtml#probability-spaces-on-rn))
    to
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) ℬ Ω := {A ∩Ω : A ∈ ℬ(ℝn )}. ](img/file1674.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the event space and algebra is clear, we need to think about assigning
    probabilities. Our assumption is that hitting any point is equally likely. So,
    by generalizing the ![faallvo proassblibele ouotcutocmomeess](img/file1675.png)
    formula we have seen in the discrete case, we define the probability measure by
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A ) = volume-(A-). volume (Ω ) ](img/file1676.png)'
  prefs: []
  type: TYPE_IMG
- en: (In two dimensions, we have the area instead of volume.) This is illustrated
    by Figure [18.4](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1677.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.4: Probability space of throwing darts at a wall. Source: https://unsplash.com/photos/black-and-white-round-logo-i3WlrO7oAHA'
  prefs: []
  type: TYPE_NORMAL
- en: As we shall see later, this is a special case of uniform distributions, one
    of the most prevalent distributions in probability theory. However, there is a
    lot to talk about until then. Before we conclude our discussion of the fundamentals
    of probability, let’s discuss how can we interpret them.
  prefs: []
  type: TYPE_NORMAL
- en: 18.2.7 How to interpret probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know how to work with probabilities, it is time to study how can
    we assign probabilities to real-life events.
  prefs: []
  type: TYPE_NORMAL
- en: First, we are going to take a look at the frequentist interpretation, explaining
    probabilities with relative frequencies. (If you are one of those people who are
    religious about this question, calm down. We’ll discuss the Bayesian interpretation
    in detail, but it is not time yet.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the beginning and consider the coin-tossing experiment. If
    I toss a fair coin 1000 times, how many of them will be heads? Most people immediately
    answer 500, but this is not correct. There is no right answer, as any number of
    heads between 0 and 1000 can happen. Of course, most probably it will be around
    500, but with a very small probability, there can be zero heads as well.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the probability of an event describes its relative frequency among
    infinitely many attempts. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![ number of occurrences P (event) ≈ --------------------. number of attempts
    ](img/file1678.png)'
  prefs: []
  type: TYPE_IMG
- en: When the number of attempts goes toward infinity, the relative frequency of
    occurrences converges to the true underlying probability. In other words, if X[i]
    quantitatively describes our i-th attempt by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 1 if the event occurs Xi = | ( 0 otherwise, ](img/file1679.png)'
  prefs: []
  type: TYPE_IMG
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '![ X + ⋅⋅⋅+ X P (event) = lim --1---------n. n→ ∞ n ](img/file1680.png)'
  prefs: []
  type: TYPE_IMG
- en: We can illustrate this by doing a quick simulation using the coin-tossing example.
    Don’t worry if you don’t understand the code; we’ll talk about it in detail in
    the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the results for some insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1681.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.5: The relative frequency of coin tosses'
  prefs: []
  type: TYPE_NORMAL
- en: The relative frequency quite nicely stabilizes around 1∕2, which is the true
    probability of our fair coin landing on its heads. Is this an accident? No.
  prefs: []
  type: TYPE_NORMAL
- en: We will make all of this mathematically precise when talking about the law of
    large numbers in Section [20.5](ch032.xhtml#the-law-of-large-numbers), but first,
    we’ll introduce the Bayesian viewpoint, a probabilistic framework for updating
    our models given new observations.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3 Conditional probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we learned the foundations of probability. Now we
    can speak in terms of outcomes, events, and chances. However, in real-life applications,
    these basic tools are not enough to build useful predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let’s build a probabilistic spam filter! For every email
    we receive, we want to estimate the probability P(email is spam). The closer this
    is to 1, the more likely that we are looking at a spam email.
  prefs: []
  type: TYPE_NORMAL
- en: Based on our inbox, we might calculate the relative frequency of spam emails
    and obtain that
  prefs: []
  type: TYPE_NORMAL
- en: '![ number of our spam emails P(email is spam ) ≈-------------------------.
    number of emails in our inbox ](img/file1682.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this doesn’t help us at all. Based on this, we can randomly discard
    every email with probability P(email is spam), but that would be a horrible spam
    filter.
  prefs: []
  type: TYPE_NORMAL
- en: To improve, we need to dig a bit deeper. When analyzing spam emails, we start
    to notice patterns. For instance, the phrase “act now” can be found almost exclusively
    in spam. After a quick count, we get that
  prefs: []
  type: TYPE_NORMAL
- en: '![ #spam emails with the phrase ”act now” P(email containing ”act now” is a
    spam ) = ---#emails with-the phrase-”act now”- ≈ 0.95\. ](img/file1683.png)'
  prefs: []
  type: TYPE_IMG
- en: This looks much more useful for our spam filtering efforts. By checking for
    the presence of the phrase “act now”, we can confidently classify an email as
    spam.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is much more to spam filtering, but this example demonstrates
    the importance of probabilities conditional on other events. To put this into
    mathematical form, we introduce the following definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 78\. (Conditional probability)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space, let A,B ∈ Σ be two events, and suppose that
    P(A)/span>0\. The conditional probability of B given A is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![P (B | A ) := P-(A-∩-B). P (A) ](img/file1684.png)'
  prefs: []
  type: TYPE_IMG
- en: You can think about P(B∣A) as restricting the event space to A, as illustrated
    by Figure [18.6](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1685.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.6: A visual representation of conditional probability'
  prefs: []
  type: TYPE_NORMAL
- en: When there are more conditions, say A[1] and A[2], the definition takes the
    form
  prefs: []
  type: TYPE_NORMAL
- en: '![ P-(B-∩-A1-∩A2-) P(B | A1,A2) = P (A1 ∩A2 ) , ](img/file1686.png)'
  prefs: []
  type: TYPE_IMG
- en: and so on for even more events.
  prefs: []
  type: TYPE_NORMAL
- en: To bring this concept closer, let’s revisit the dice-rolling experiment. Suppose
    that your friend rolls a six-sided dice and tells you that the outcome is an odd
    number. Given this information, what is the probability that the result is 3?
    For simplicity, let’s denote the outcome of the roll with X. Mathematically speaking,
    this can be calculated by
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X = 3 | X ∈ {1,3,5}) = P-(X-=-3-and-X-∈-{1,3,5}) P (X ∈ {1,3,5}) ---P-(X--=-3)---
    = P (X ∈ {1,3,5}) = 1∕6- 1∕2 1- = 3\. ](img/file1687.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the number that we expected.
  prefs: []
  type: TYPE_NORMAL
- en: Although this simple example doesn’t demonstrate the usefulness of conditional
    probability, this is a cornerstone in machine learning. In essence, learning from
    data can be formulated as estimating P(label∣data). We are going to expand on
    this idea later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3.1 Independence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea behind conditional probability is that observing certain events changes
    the probability of others. Is this always the case, though?
  prefs: []
  type: TYPE_NORMAL
- en: In probabilistic modeling, recognizing when observing an event doesn’t influence
    another is equally important. This motivates the concept of independence.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 79\. (Independence of events)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A,B ∈ Σ be two events. We say that
    A and B are independent if
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A ∩ B) = P (A )P(B ) ](img/file1688.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Equivalently, this can be formulated in terms of conditional probabilities.
    By the definition, if A and B are independent, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P (B | A ) = P(A-∩-B-) P(A ) P(A-)P(B-) = P (A) = P(B ). ](img/file1689.png)'
  prefs: []
  type: TYPE_IMG
- en: To see an example, let’s go back to coin tossing and suppose that we toss a
    coin two times. Let the result of the first and second toss be denoted by X[1]
    and X[2], respectively. What is the probability that both of these tosses are
    heads? As we saw when discussing this example in Section [18.2.4](ch030.xhtml#probability-measures),
    we can see that
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X1 = heads and X2 = heads) = P(X1 = heads)P (X2 = heads) = 1\. 4 ](img/file1690.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the two events are independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding probability, there are many common misconceptions. One is about the
    interpretation of independence. Suppose that I toss a fair coin ten times, all
    of them resulting in heads. What is the probability that my next toss will be
    heads?
  prefs: []
  type: TYPE_NORMAL
- en: Most would immediately conclude that this must be very small since having eleven
    heads in a row is highly unlikely. However, once we have the ten results available,
    we no longer talk about the probability of eleven coin tosses, just the last one!
    Since the coin tosses are independent of each other, the chance of heads for the
    eleventh toss (given the results of the previous ten) is still 50%.
  prefs: []
  type: TYPE_NORMAL
- en: This phenomenon is called the gambler’s fallacy, and I am pretty sure that at
    some point in your life, you fell victim to it. (I sure did.)
  prefs: []
  type: TYPE_NORMAL
- en: In practical scenarios, working with conditional probabilities might be easier.
    (For instance, sometimes we can estimate them directly, while the standard probabilities
    are difficult to gauge.) Because of this, we need tools to work with them.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3.2 The law of total probability revisited
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember the law of total probability from Theorem [113](ch030.xhtml#x1-286004r113)?
    We can use conditional probabilities to put it into a slightly different form.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 117\. (Law of total probability, conditional version)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A ∈ Σ be an arbitrary event. If A[1],A[2],⋅⋅⋅∈
    Σ are mutually disjoint events (that is, A[i] ∩A[j] = ∅if i≠j) for which ∪[n=1]^∞A[n]
    = Ω, then
  prefs: []
  type: TYPE_NORMAL
- en: P(A) = ∑[k=1]^∞ P(A | A[k]) P(A[k]) (18.6)
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof. The proof is the trivial application of the law of total probability
    (Theorem [113](ch030.xhtml#x1-286004r113)) and the definition of conditional probabilities:
    as P(A ∩A[k]) = P(A∣A[k])P(A[k]),'
  prefs: []
  type: TYPE_NORMAL
- en: P(A) = ∑[k=1]^∞ P(A ∩ A[k]) = ∑[k=1]^∞ P(A | A[k]) P(A[k])
  prefs: []
  type: TYPE_NORMAL
- en: holds, which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this useful for us? Let’s demonstrate this with an example. Suppose that
    we have three urns containing light and dark colored balls.
  prefs: []
  type: TYPE_NORMAL
- en: The first one contains 4 dark, the second one contains 2 light and 2 dark, while
    the last one contains 1 light and 3 dark balls.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1692.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.7: Urns with colored balls'
  prefs: []
  type: TYPE_NORMAL
- en: We randomly pick an urn; however, picking the first one is twice as likely as
    picking the other two. (That is, we pick the first urn 50% of the time, while
    the second and the third 25%–25% of the time.) From that urn, we also randomly
    pick a ball. What is the probability that we select a light ball? Without using
    the law of total probability, this is difficult to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s denote the color of the selected ball by X and suppose that the event
    A[n] describes picking the n-th urn. Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑3 ∑3 P (X = light) = P ({X = light} ∩ Ak) = P (X = light | Ak )P (Ak).
    k=1 k=1 ](img/file1693.png)'
  prefs: []
  type: TYPE_IMG
- en: Without using conditional probabilities, calculating P({X = light}∩A[k]) is
    difficult (since we are not picking each urn with equal probability). However,
    we can simply calculate the conditionals by counting the number of light balls
    in each urn. That is, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P(X = light | A1 ) = 0 2 P(X = light | A2 ) = 4 1 P(X = light | A3 ) =-.
    4 ](img/file1694.png)'
  prefs: []
  type: TYPE_IMG
- en: Since P(A[1]) = 1∕2,P(A[2]) = 1∕4, and P(A[3]) = 1∕4, the probability we are
    looking for is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑3 P (X = light) = P (X = light | Ak)P (Ak) k=1 1 2 1 1 1 = 0⋅ 2 + 4 ⋅ 4
    + 4 ⋅ 4 3 = --. 16 ](img/file1695.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that because the urns are not selected with equal probability,
  prefs: []
  type: TYPE_NORMAL
- en: '![ number of light balls P (X = light) ⁄= ------------------, number of balls
    ](img/file1696.png)'
  prefs: []
  type: TYPE_IMG
- en: as one would naively guess.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful property of the conditional probability is that, due to its
    definition, we can use it to express the joint probability of events:'
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A ∩ B ) = P (B | A )P (A). ](img/file1697.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though this sounds trivial, there are cases when we can estimate/compute
    the conditional probability but not the joint probability. In fact, this simple
    identity can be generalized for an arbitrary number of conditions. This is called
    the chain rule. (Despite its name, it has nothing to do with the chain rule for
    differentiation.)
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 118\. (The chain rule)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and A[1],A[2],⋅⋅⋅∈ Σ be arbitrary events.
    Then,
  prefs: []
  type: TYPE_NORMAL
- en: P(A) = ∑[k=1]^∞ P(A | A[k]) P(A[k]) (18.7)
  prefs: []
  type: TYPE_NORMAL
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. First, we notice that P(A[1] ∩⋅⋅⋅∩A[n]) can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ P (A1 ∩ ⋅⋅⋅∩An ) P (A1 ∩ ⋅⋅⋅∩ An −1) P (A1 ∩ A2) P(A1 ∩⋅⋅⋅∩An ) =------------------------------------...-----------P
    (A1), P (A1 ∩ ⋅⋅⋅ ∩An −1)P (A1 ∩ ⋅⋅⋅∩ An −2) P(A1 ) ](img/file1701.png)'
  prefs: []
  type: TYPE_IMG
- en: because the terms cancel out each other.
  prefs: []
  type: TYPE_NORMAL
- en: Since
  prefs: []
  type: TYPE_NORMAL
- en: '![-P(A1-∩-Ak)-- P(A1 ∩ Ak−1) = P (Ak | A1,...,Ak− 1), ](img/file1702.png)'
  prefs: []
  type: TYPE_IMG
- en: the chain rule ([18.7](ch030.xhtml#x1-291007r118)) follows.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3.3 The Bayes theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In essence, machine learning is about turning observations into predictive models.
    Probability theory gives us a language to express our models. For instance, going
    back to our spam filter example, we can notice that 5% of our emails are spam.
    However, this is not enough information to filter out spam emails. Upon inspection,
    we have observed that 95% of emails that contain the phrase “act now” are spam
    (but only 1% of all the emails contain “act now”). In the language of conditional
    probabilities, we have concluded that
  prefs: []
  type: TYPE_NORMAL
- en: '![P (spam | contains ”act now ”) = 0.95\. ](img/file1703.png)'
  prefs: []
  type: TYPE_IMG
- en: With this, we can start looking for emails containing the phrase “act now” and
    discard them with 95% confidence. Is this spam filter effective? Not really, since
    there can be other frequent keywords in spam mails that we don’t check. How can
    we check this?
  prefs: []
  type: TYPE_NORMAL
- en: For one, we can take a look at the conditional probability P(contains ”act now”∣spam),
    describing the frequency of the “act now” keyword among all the spam emails. A
    low frequency means that we are missing out on other keywords that we can use
    for filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, we often want to compute/estimate the quantity P(A∣B), but
    our observations only allow us to infer P(B∣A). So, we need a way to reverse the
    condition and the event. With a bit of algebra, we can do this easily.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 119\. (The Bayes formula)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space, A,B be two arbitrary events, and suppose
    that P(A),P(B)/span>0\. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(26).png)(18.8)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. By the definition of conditional probability, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ P(A-∩-B-) P(B | A ) = P(A ) = P(A-∩-B-)P(B-) P(B )P(A ) P-(B-) = P(A | B
    )P (A), ](img/file1705.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: To see how it works in action, let’s put it to the test in our spam filtering
    example. Given the information we know, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P (spam | contains ”act now ”) = 0.95, P(contains ”act now ”) = 0.01, P (spam
    ) = 0.05\. ](img/file1706.png)'
  prefs: []
  type: TYPE_IMG
- en: So, according to the Bayes formula,
  prefs: []
  type: TYPE_NORMAL
- en: '![ P-(spam--| contains-”act now-”)P(contains-”act now-”) P(contains ”act now”
    | spam) = P(spam) 0.95⋅0.01 = --------- 0.05 = 0.19\. ](img/file1707.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, by filtering only for the phrase “act now”, we are missing a lot of spam.
  prefs: []
  type: TYPE_NORMAL
- en: We can take the Bayes formula one step further by combining it with the law
    of total probability in Theorem [117](ch030.xhtml#x1-291003r117). (See the equation
    ([18.6](ch030.xhtml#x1-291003r117)).)
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 120\. (The Bayes theorem)
  prefs: []
  type: TYPE_NORMAL
- en: Let (Ω,Σ,P) be a probability space and let A,B ∈ Σ be arbitrary events. Moreover,
    let A[1],A[2],⋅⋅⋅∈ Σ be a partition of the event space Ω. (That is, the A[n]-s
    are pairwise disjoint and their union is the entire event space.) Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ----P-(A-| B-)P-(B)--- P (B | A) = ∑ ∞ P (A | An)P (An) n=1 ](img/file1709.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. The proof immediately follows from the Bayes formula (Theorem [119](ch030.xhtml#x1-292004r119))
    and the law of total probability (Theorem [117](ch030.xhtml#x1-291003r117).)
  prefs: []
  type: TYPE_NORMAL
- en: 18.3.4 The Bayesian interpretation of probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Historically, probability was introduced as the relative frequency of observed
    events in Section [18.2.7](ch030.xhtml#how-to-interpret-probability). However,
    the invention of conditional probabilities and the Bayes formula enabled another
    interpretation that slowly became prevalent in statistics and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In pure English, the Bayes formula can be thought of as updating our probabilistic
    models using new observations. Suppose that we are interested in the event B.
    Without observing anything, we can formulate a probabilistic model by assigning
    a probability to B, that is, estimating P(B). This is what we call the prior.
    However, observing another event A might change our probabilistic model.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we would like to estimate the posterior probability P(B∣A). We can’t do
    this directly, but thanks to our prior model, we can tell P(A∣B). The quantity
    P(A∣B) is called the likelihood. Combining these with the Bayes formula, we can
    see that the posterior is proportional to the likelihood and the prior.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1710.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.8: The Bayes formula, as the product of the likelihood and prior'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see a concrete example that will make the idea clear. Suppose that we
    are creating a diagnostic test for an exotic disease. How likely is the disease
    present in a random person?
  prefs: []
  type: TYPE_NORMAL
- en: Without knowing any specifics about the situation, we can only use statistics
    to formulate the probability model. Let’s say that only 2% of the population is
    affected. So, our probabilistic model is
  prefs: []
  type: TYPE_NORMAL
- en: '![P (infected) = 0.02, P(healthy) = 0.98\. ](img/file1711.png)'
  prefs: []
  type: TYPE_IMG
- en: However, once someone produces a positive test, things change. The goal is to
    estimate the posterior probability P(infected∣positive), a more accurate model.
  prefs: []
  type: TYPE_NORMAL
- en: Since no medical test is perfect, false positives and false negatives can happen.
    From the manufacturer, we know that it gives true positives 99% of the time, but
    the chance of a false positive is 5%. In probabilistic terms, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P (positive | infected) = 0.99, P (positive | healthy) = 0.05\. ](img/file1712.png)'
  prefs: []
  type: TYPE_IMG
- en: With these, the Bayes theorem gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![ P (positive | infected)P(infected) P(infected | positive) = P-(positive-|
    infected)P(infected)+-P-(positive-| healthy)P-(healthy) = ------0.99⋅0.02------
    0.99⋅0.02 + 0.05 ⋅0.98 ≈ 0.29\. ](img/file1713.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the chance of being infected upon producing a positive test is surprisingly
    29% (given these specific true and false positive rates).
  prefs: []
  type: TYPE_NORMAL
- en: These probabilistic thinking principles are also valid for machine learning.
    If we abstract away the process of learning from data, we are essentially 1) making
    observations, 2) updating our models given the new observations, and 3) starting
    the process all over again. The Bayes theorem gives a concrete tool for the job.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3.5 The probabilistic inference process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen before, probability theory is the extension of mathematical
    logic. So far, we have discussed how logical connectives correspond to set operations
    and how probability generalizes the truth value by adding the component of uncertainty.
    What about the probabilistic inference process? Can we generalize classical inference
    and use probabilistic reasoning to construct arguments? Yes.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let’s start with a story. It’s 6:00 AM. The alarm clock is blasting
    but you are having a hard time getting out of bed. You don’t feel well. Your muscles
    are weak, and your head is exploding. After a brief struggle, you manage to call
    a doctor and list all the symptoms. Your sore throat makes speaking painful.
  prefs: []
  type: TYPE_NORMAL
- en: “It’s probably just the flu,“ they say.
  prefs: []
  type: TYPE_NORMAL
- en: Interactions like this are everyday occurrences. Yet, we hardly think about
    the reasoning process behind them. After all, you could have been hungover. Similarly,
    if the police find a murder weapon at your house, they’ll suspect that you are
    the killer. The two are related but not the same. For instance, the murder weapon
    could have been planted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bulk of humanity’s knowledge is obtained in this manner: we collect evidence,
    then build hypotheses. How do we infer the underlying cause from observing the
    effect? Most importantly, how can we avoid fooling ourselves into false conclusions?'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on “muscle fatigue, headache, sore throat →flu“. This is certainly
    not true in an absolute sense, as these symptoms resemble how you would feel after
    shouting and drinking excessively during a metal concert, which is far from the
    flu. Yet, a positive diagnosis of flu is plausible. Given the evidence at hand,
    our belief is increased in the hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, classical logic cannot deal with plausible, only with the absolute.
    Probability theory solves this problem by measuring plausibility on a 0 − 1 scale,
    instead of being stuck at the extremes. Zero is impossible. One is certain. All
    the values in between represent degrees of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put this into mathematical terms!
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we establish a probabilistic link between cause and effect? In classical
    logic, events are interesting in the context of other events. Before, implication
    and modus ponens provided the context. Translated to the language of probability,
    the question is the following: What is the probability of B, given that A is observed?
    The answer: conditional probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Why does conditional probability generalize the concept of implication? It’s
    easier to draw a picture, so consider the two extreme cases in Figure [18.9](#).
    (Recall that implication corresponds to the subset relation, as we saw earlier.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1714.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.9: Conditional probability as logical implication'
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, P(B∣A) = 1 means that A →B is true, while P(B∣A) = 0 means that
    it is not. We can take this analogy further: a small P(B∣A) means that A →B is
    likely to be false, and a large P(B∣A) means that it is likely to be true.'
  prefs: []
  type: TYPE_NORMAL
- en: This is illustrated by Figure [18.10](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1715.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.10: Conditional probability as the extension of logical implication'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the “probabilistic modus ponens” goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: P(B∣A) ≈ 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, B is probable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is quite a relief, as now we have a solid theoretical justification for
    most of our decisions. Thus, the diagnostic process that kicked up our investigation
    makes a lot more sense now:'
  prefs: []
  type: TYPE_NORMAL
- en: P(flu∣headache, muscle fatigue, sore throat) ≈ 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Headache and muscle fatigue”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, “flu” is probable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, one burning question remains. How do we know that P(B∣A) ≈ 1 holds?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on the probabilistic version of “headache, sore throat, muscle fatigue
    →flu“. We know that this is not certain, only plausible. Yet, the reverse implication
    “flu →headache, sore throat, muscle fatigue“ is almost certain.
  prefs: []
  type: TYPE_NORMAL
- en: When naively arguing that the evidence implies the hypothesis, we have the opposite
    in mind. Instead of applying the modus ponens, we use the faulty argument
  prefs: []
  type: TYPE_NORMAL
- en: A →B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, A.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have talked about this before: this logical fallacy is called affirming
    the consequent, and it’s completely wrong from a purely logical standpoint. However,
    the Bayes theorem provides a probabilistic twist.'
  prefs: []
  type: TYPE_NORMAL
- en: The proposition A →B translates to P(B∣A) = 1, which implies that when A is
    observed, B occurs as well. Why? Because then we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ P-(B-| A-)P-(A) P(A | B) = P (B) = P-(A) P (B) ≥ P(A ). ](img/file1716.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is good news, as reversing the implication is not totally wrong. Instead,
    we have the probabilistic affirming the consequent:'
  prefs: []
  type: TYPE_NORMAL
- en: A →B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, A is more probable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this, the probabilistic reasoning process makes perfect sense. To recall,
    the issue with arguments such as “if you have muscle fatigue, sore throat, and
    a headache, then you have the flu“ is that the symptoms can be caused by other
    conditions, and in rare cases, the flu does not carry all of these symptoms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, this kind of thinking can be surprisingly effective in real-life decision-making.
    Probability and conditional probability extend our reasoning toolkit with inductive
    methods in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generalizes the binary 0 − 1 truth values to allow the representation of uncertainty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defines the analogue of “if A, then B“-type implications using conditional probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provides a method to infer the cause from observing the effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These three ideas are seriously powerful, and their inception has enabled science
    to perform unbelievable feats. (If you are interested in learning more about the
    relation of probability theory and logic, I recommend you the great book Probability
    Theory: The Logic of Science by E. T. Jaynes.)'
  prefs: []
  type: TYPE_NORMAL
- en: There’s one more thing I would like to show you. Let’s go back to the mid-twentieth
    century and look at how a TV show shaped probabilistic thinking.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3.6 The Monty Hall paradox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we finish with conditional probability, we’ll touch on an important problem.
    Regarding probability, we often have seemingly contradictory phenomena, going
    against our intuitive expectations. These are called paradoxes. To master probabilistic
    thinking, we need to resolve them and eliminate common fallacies from our thinking
    processes. So far, we have already seen the gambler’s fallacy when talking about
    the concept of independence in Section [18.3.1](ch030.xhtml#independence). Now,
    we’ll discuss the famous Monty Hall paradox.
  prefs: []
  type: TYPE_NORMAL
- en: In the ’60s, there was a TV show in the United States called Let’s Make a Deal
    ( [https://en.wikipedia.org/wiki/Let\%27s_Make_a_Deal](https://en.wikipedia.org/wiki/Lets_Make_a_Deal)).
    As a contestant, you faced three closed doors, one having a car behind it (that
    you could take home), while the others had nothing. You had the opportunity to
    open one.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1717.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.11: Three closed doors, one of which contains a reward behind it'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that after selecting door no. 1, Monty Hall — the show host — opens
    the third door, showing that it was not the winning one. Now, you have the opportunity
    to change your mind and open door no. 2 instead of the first one. Do you take
    it?
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.12: Monty opened the third door for you. Do you switch?'
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, your chances are 50%/50%, so you might not be better off by
    switching. However, this is not true!
  prefs: []
  type: TYPE_NORMAL
- en: To set things straight, let’s do a careful probabilistic analysis. Let A[i]
    denote the event that the prize is behind the i-th door, while B[i] is the event
    of Monty opening the i-th door. Before Monty opens the third one, our model is
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A1) = P(A2 ) = P (A3) = 1, 3 ](img/file1719.png)'
  prefs: []
  type: TYPE_IMG
- en: and we want to calculate P(A[1]∣B[3]) and P(A[2]∣B[3]).
  prefs: []
  type: TYPE_NORMAL
- en: By thinking from the perspective of the show host, which door would you open?
    If you know that the prize is behind the 1st door, you open the 2nd and 3rd one
    with equal probability. However, if the prize is actually behind the 2nd door
    (and the contestant selected the 1st one), you always open the 3rd one. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (B | A ) = P (B | A ) = 1, 3 1 2 1 2 P (B3 | A2) = 1\. ](img/file1720.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, by applying the Bayes formula, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ P(B3 | A1 )P(A1) P(A1 | B3 ) =---------------- P(B3 ) = -1∕6--, P(B3 ) ](img/file1721.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ P(B3 | A2 )P(A2) P(A2 | B3 ) =---------------- P(B3 ) = -1∕3--. P(B3 ) ](img/file1722.png)'
  prefs: []
  type: TYPE_IMG
- en: In conclusion, P(A[2]∣B[3]) is twice as large as P(A[1]∣B[3]), from which we
    deduce
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A1 | B3) = 1, P (A2 | B3) = 2-. 3 3 ](img/file1723.png)'
  prefs: []
  type: TYPE_IMG
- en: So, you should always switch doors. Surprising, isn’t it? Here, the paradox
    is that contrary to what we might expect, changing our minds is the better option.
    With clear probabilistic thinking, we can easily resolve this.
  prefs: []
  type: TYPE_NORMAL
- en: 18.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Phew! We are at the end of an intimidatingly long, albeit extremely essential
    chapter. Although we’ve talked about the mathematical details of probability for
    a couple of dozen pages, the most important takeaway can be summarized in a sentence:
    probability theory extends our reasoning toolkit by handling uncertainty. Instead
    of measuring the truthiness of a proposition on a true-or-false binary scale,
    it opens up a spectrum between 0 and 1, where 0 represents (almost) impossible,
    and 1 represents (almost) certain.'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically speaking, probabilistic models are defined by probability measures
    and spaces, that is, structures of the form (Ω,Σ,P), where Ω is the set of possible
    elementary outcomes, Σ is the collection of events, and P is a probability measure,
    satisfying
  prefs: []
  type: TYPE_NORMAL
- en: P(Ω) = 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and P(∪[n=1]^∞−A[n]) = ∑ [n=1]^∞P(A[n]) for all mutually disjoint A[n] ∈ Σ,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'which are called the Kolmogorov axioms. Thinking in probabilities enables us
    to reason under uncertainty: if P(A) is the probabilistic version of the truth
    value of a statement, then the conditional probability'
  prefs: []
  type: TYPE_NORMAL
- en: '![P (B | A) = P-(A-∩-B) P (A) ](img/file1724.png)'
  prefs: []
  type: TYPE_IMG
- en: is the probabilistic version of the implication A →B.
  prefs: []
  type: TYPE_NORMAL
- en: However, all the tools we learned are just the tips of a massive iceberg. To
    build truly beefy and useful models, we need to once more turn qualitative into
    quantitative, as we did for many advances in science and mathematics. Can you
    recall the dice rolling experiment, where we used a mysterious variable X to represent
    the outcome of the roll? Thus, we could talk about events such as “X = k”, turning
    a probability space into a sequence of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not a coincidence; it’s a method. X is an instance of a random variable,
    the premier object of probability theory and statistics. Random variables translate
    between abstract probability spaces to numbers and vectors, our old friends. Let’s
    make them a permanent tool in our belt.
  prefs: []
  type: TYPE_NORMAL
- en: 18.5 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Let’s roll two six-sided dice! Describe the event space, σ-algebra,
    and the corresponding probabilities for this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Let Ω = [0,1], and the corresponding σ-algebra be the generated
    algebra
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) Σ = σ {(a,b] : 0 ≤ a <b ≤ 1} . ](img/file1725.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Show that the following events are members of Σ:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) S[1] = {x} for all x ∈ [0,1].
  prefs: []
  type: TYPE_NORMAL
- en: (b) S[2] = ∪[i=1]^n(a[i],b[i]). (Show that this is also true when the intervals
    […] are replaced with open and half-open versions (…),(…],[…).)
  prefs: []
  type: TYPE_NORMAL
- en: (c) S[3] = [0,1] ∩ℚ. (That is, the set of rational numbers in [0,1].)
  prefs: []
  type: TYPE_NORMAL
- en: (d) S[4] = [0,1] ∩(ℝ ∖ℚ). (That is, the set of irrational numbers in [0,1].)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Let’s roll two six-sided dice. What is the probability that
  prefs: []
  type: TYPE_NORMAL
- en: (a) Both rolls are odd numbers?
  prefs: []
  type: TYPE_NORMAL
- en: (b) At least one of them is an odd number?
  prefs: []
  type: TYPE_NORMAL
- en: (c) None of them are odd numbers?
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let Ω = ℝ² be the event space, where we define the open disks
  prefs: []
  type: TYPE_NORMAL
- en: '= ![D (x,r) := {z ∈ ℝ2 : ∥x − z∥ <r}, x = (x1,x2) ∈ ℝ2, r >0, ](img/file1728.png)'
  prefs: []
  type: TYPE_NORMAL
- en: and the open rectangles by
  prefs: []
  type: TYPE_NORMAL
- en: '![R(x,y ) = (x1,y1)× (x2,y2) 2 = {z = (z1,z2) ∈ ℝ : x1 <z1 <y1,x2 <z2 <y2}.
    ](img/file1729.png)'
  prefs: []
  type: TYPE_IMG
- en: Show that the σ-algebras generated by these sets are the same, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( 2 ) ( 2 ) σ {D (x, r) : x ∈ ℝ ,r >0} = σ {R (x,y) : x,y ∈ ℝ } . ](img/file1730.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 5\. Let’s consider a variant of the Monty Hall problem. Suppose there
    are a hundred doors instead of three; only one contains a reward. Upon picking
    a door, Monty opens ninety-eight other doors, all of which are empty. Should you
    switch now?
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
