<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch031.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="random-variables-and-distributions" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">19</span></span><br/>
<span id="x1-29900023"></span><span class="cmss-10x-x-109">Random Variables and Distributions</span></h1>
<p><span class="cmss-10x-x-109">Having a probability space to model our experiments and observations is fine and all, but in almost all of the cases, we are interested in a quantitative measure of the outcome. To give you an example, let’s consider an already familiar situation: tossing coins. Suppose that we are tossing a fair coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times but we are only interested in the number of heads. How do we model the probability space this time?</span></p>
<p><span class="cmss-10x-x-109">By taking things one step at a time; first, we construct an event space by enumerating all possible outcomes in a single set, just like we already did in </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#event-spaces-and-algebras"><span class="cmssi-10x-x-109">18.2.1</span></a><span class="cmss-10x-x-109">:</span></p>
<div class="math-display">
<img src="../media/file1731.png" class="math-display" alt="Ω = {0,1}n, Σ = 2Ω. "/>
</div>
<p><span class="cmss-10x-x-109">Since the coin is fair, each outcome </span><span class="cmmi-10x-x-109">ω </span><span class="cmss-10x-x-109">has the probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">ω</span>) = <img src="../media/file1732.png" width="15" data-align="middle" alt="12n"/><span class="cmss-10x-x-109">. This probability space </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmss-10x-x-109">is nice and simple so far. Using the additivity of probability measures (see </span><span class="cmssi-10x-x-109">Definition </span><a href="ch030.xhtml#x1-285008r77"><span class="cmssi-10x-x-109">77</span></a><span class="cmss-10x-x-109">), we can calculate the probability of any event. That is, for any </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ<span class="cmss-10x-x-109">, we have</span></p>
<img src="../media/file1733.png" width="150" class="math-display" alt="P (A) = |A|, |Ω| "/>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">j </span><span class="cmsy-10x-x-109">⋅</span><span class="cmmi-10x-x-109">j </span><span class="cmss-10x-x-109">denotes the number of elements in a given set.</span></p>
<p><span class="cmss-10x-x-109">However, as mentioned, we are only interested in the number of heads. Should we just incorporate this information somewhere in the probability space? Sure, we could do that, but that would couple the elementary outcomes (that is, a series of heads or tails) with the measurements. This can significantly complicate our model.</span></p>
<p><span class="cmss-10x-x-109">Instead of overloading this probability space to directly deal with the desired measurements, we can do something much simpler: introduce a function </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℕ</span><span class="cmss-10x-x-109">, mapping </span><span class="cmssi-10x-x-109">outcomes </span><span class="cmss-10x-x-109">to </span><span class="cmssi-10x-x-109">measurements</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">These functions are called </span><span class="cmssi-10x-x-109">random variables</span><span class="cmss-10x-x-109">, and they are at the very center of probability theory and statistics. By collecting data, we are observing random variables, and by fitting predictive models, we approximate them using the observations. Now that we understand why we need them, we are going to make this notion mathematically precise.</span></p>
<section id="random-variables" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_266"><span class="titlemark"><span class="cmss-10x-x-109">19.1 </span></span> <span id="x1-30000023.1"></span><span class="cmss-10x-x-109">Random variables</span></h2>
<p><span class="cmss-10x-x-109">Hold</span> <span id="dx1-300001"></span><span class="cmss-10x-x-109">your horses, though; it’s not that simple. Random variables are hard to understand in their general form, so we’ll slow down and focus on special cases, taking one step at a time. This is how learning is done most effectively, and we’ll follow this path as well.</span></p>
<p><span class="cmss-10x-x-109">Let’s deal with so-called </span><span class="cmssi-10x-x-109">discrete </span><span class="cmss-10x-x-109">random variables (such as the above example) first, real random variables second, and the general case last.</span></p>
<section id="discrete-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_267"><span class="titlemark"><span class="cmss-10x-x-109">19.1.1 </span></span> <span id="x1-30100023.1.1"></span><span class="cmss-10x-x-109">Discrete random variables</span></h3>
<p><span class="cmss-10x-x-109">Following our</span> <span id="dx1-301001"></span><span class="cmss-10x-x-109">motivating example describing the number of heads in </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">coin tosses, we can create a formal definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-301002r80"></span> <span class="cmbx-10x-x-109">Definition 80.</span> </span><span class="cmbx-10x-x-109">(Discrete random variables)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space and <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup> be an arbitrary sequence of real numbers. The function <span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→ {</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmsy-10x-x-109">} </span>is called a <span class="cmti-10x-x-109">discrete random variable </span>if the sets</p>
<div class="math-display">
<img src="../media/file1734.png" class="math-display" alt="S = {ω ∈ Ω : X (ω) = x } k k "/>
</div>
<p>are events for any integer <span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℤ </span>(that is, <span class="cmmi-10x-x-109">S</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">∈ </span>Σ).</p>
</div>
<p><span class="cmss-10x-x-109">You might ask why we are requiring the sets </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">ω </span><span class="cmsy-10x-x-109">∈ </span>Ω : <span class="cmmi-10x-x-109">X</span>(<span class="cmmi-10x-x-109">ω</span>) = <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">to be events. It seems like just another technical condition, but this plays an essential role. Ultimately, we are defining random variables because we want to measure the probabilities of our observations. This condition ensures that we can do this.</span></p>
<p><span class="cmss-10x-x-109">To simplify our notations, we write</span></p>
<div class="math-display">
<img src="../media/file1735.png" class="math-display" alt=" ( ) P (X = xk) := P {ω ∈ Ω : X (ω) = xk} "/>
</div>
<p><span class="cmss-10x-x-109">whenever we talk about these probabilities. Let’s see a concrete example!</span></p>
<p><span class="cmss-10x-x-109">In the case of the coin tossing above, our random variable is defined by</span></p>
<div class="math-display">
<img src="../media/file1736.png" class="math-display" alt="X = number of heads. "/>
</div>
<p><span class="cmss-10x-x-109">Even though we can define </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">in terms of formulas by</span></p>
<div class="math-display">
<img src="../media/file1737.png" class="math-display" alt=" ∑n X (ω) = ωk, ω = (ω1, ...,ωn ) ∈ Ω, k=1 "/>
</div>
<p><span class="cmss-10x-x-109">this is not needed. Often, such a thing is not even possible. Regarding</span><span id="dx1-301003"></span> <span class="cmss-10x-x-109">our random variables, we are not interested in knowing the entire mapping, only in questions such as the probability of </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">heads among </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">tosses.</span></p>
<p><span class="cmss-10x-x-109">If we record the “timestamps” where the outcome is heads, we can encode each </span><span class="cmmi-10x-x-109">ω </span><span class="cmss-10x-x-109">as a subset of </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. For instance, if the 1st, 3rd, and 37th tosses are heads and the rest are tails, this is </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>3<span class="cmmi-10x-x-109">,</span>37<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. To calculate the probability of </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">heads, we need to count the number of </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-sized subsets for a set of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">elements. This is given by the binomial coefficient</span> <img src="../media/file1738.png" alt="(n) k" width="15"/><span class="cmss-10x-x-109">. So,</span></p>
<div class="math-display">
<img src="../media/file1739.png" class="math-display" alt=" ( ) P (X = k) = n 1-. k 2n "/>
</div>
<p><span class="cmss-10x-x-109">We’ll see this in detail when talking about the </span><span class="cmssi-10x-x-109">binomial distribution</span><span class="cmss-10x-x-109">, whatever it might be. For now, we are ready to generalize our random variables!</span></p>
</section>
<section id="realvalued-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_268"><span class="titlemark"><span class="cmss-10x-x-109">19.1.2 </span></span> <span id="x1-30200023.1.2"></span><span class="cmss-10x-x-109">Real-valued random variables</span></h3>
<p><span class="cmss-10x-x-109">What if our measurements are not discrete? For instance, suppose that we</span><span id="dx1-302001"></span> <span class="cmss-10x-x-109">have a class of students in front of us. We are interested in the distribution of their body height. So, we pick one student at random and measure their height with our shiny new tool, which is capable of measuring height with perfect precision.</span></p>
<p><span class="cmss-10x-x-109">In this case, discrete random variables are not enough, but we can define something similar.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-302002r81"></span> <span class="cmbx-10x-x-109">Definition 81.</span> </span><span class="cmbx-10x-x-109">(Real-valued random variables)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space. The function <span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℝ </span>is called a <span class="cmti-10x-x-109">random variable </span>if the set</p>
<div class="math-display">
<img src="../media/file1740.png" class="math-display" alt=" ( ) X −1 (a,b) := {ω ∈ Ω : a &lt;X (ω) &lt;b} "/>
</div>
<p>is an event for all <span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span>. (That is, <span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmmi-10x-x-109">a,b</span>)<span class="big">)</span> <span class="cmsy-10x-x-109">∈ </span>Σ for all <span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span>.)</p>
</div>
<p><span class="cmss-10x-x-109">Let’s unwrap this definition. First of all, </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is a mapping from</span><span id="dx1-302003"></span> <span class="cmss-10x-x-109">the event space </span>Ω <span class="cmss-10x-x-109">to the set of real numbers </span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">, as illustrated by </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">19.1</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1743.png" width="284" alt="PIC"/> <span id="x1-302004r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.1: A real-valued random variable is a mapping from the event space to the set of real numbers</span> </span>
</div>
<p><span class="cmss-10x-x-109">Similarly to the discrete case, we are interested in the probabilities of events such as </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmmi-10x-x-109">a,b</span>)<span class="big">)</span><span class="cmss-10x-x-109">. Again, for simplicity, we write</span></p>
<div class="math-display">
<img src="../media/file1746.png" class="math-display" alt=" ( −1( )) P (a &lt;X &lt;b) = P X (a,b) . "/>
</div>
<p><span class="cmss-10x-x-109">You can imagine </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>((<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">as the subset of </span>Ω <span class="cmss-10x-x-109">that is mapped to </span>(<span class="cmmi-10x-x-109">a,b</span>)<span class="cmss-10x-x-109">. (In general, sets of the form </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">are called </span><span class="cmssi-10x-x-109">inverse images</span><span class="cmss-10x-x-109">.)</span></p>
<div class="minipage">
<p><img src="../media/file1747.png" width="456" alt="PIC"/> <span id="x1-302005r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.2: Inverse image of an interval</span> </span>
</div>
<p><span class="cmss-10x-x-109">Let’s see an example right away. Suppose that we are throwing darts at a</span><span id="dx1-302006"></span> <span class="cmss-10x-x-109">circular board on the wall. (For simplicity, assume that we are so good that we always hit the board.) As we have seen when discussing </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras in higher dimensions (</span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#probability-spaces-on-rn"><span class="cmssi-10x-x-109">18.2.6</span></a><span class="cmss-10x-x-109">), we can model this by selecting</span></p>
<div class="math-display">
<img src="../media/file1748.png" class="math-display" alt="Ω = B(0,1) = {x ∈ ℝ2 : ∥x∥ &lt;1} "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1749.png" class="math-display" alt=" ( ) Σ = ℬ B (0,1) ( n ) = σ {A ∩ B (0,1) : A ∈ ℬ (ℝ ) , "/>
</div>
<p><span class="cmss-10x-x-109">while</span></p>
<div class="math-display">
<img src="../media/file1750.png" class="math-display" alt="P (A) = area(A)-= area(A). area(Ω) π "/>
</div>
<p><span class="cmss-10x-x-109">Since dart boards are subdivided into concentric circles, scoring is determined by the distance from the center. So, we might as well define our random variable by</span></p>
<div class="math-display">
<img src="../media/file1751.png" class="math-display" alt="X = distance of the impact point from the center. "/>
</div>
<p><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">encodes all that we are interested in, in terms of scoring. In general, we have</span></p>
<div class="math-display">
<img src="../media/file1752.png" class="math-display" alt=" ( || 0 if r ≤ 0, ||{ P(X &lt;r) = r2 if 0 &lt;r &lt;1, ||| |( 1 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">What if we</span> <span id="dx1-302007"></span><span class="cmss-10x-x-109">have more than one measurement? For instance, in the case of the famous Iris dataset (</span><a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Iris_flower_data_set</span></a><span class="cmss-10x-x-109">) (one that we have seen a few times so far), we have four measurements. Sure, we can just define four random variables, but then we cannot take advantage of all the heavy machinery we have built so far: linear algebra and multivariate calculus.</span></p>
<p><span class="cmss-10x-x-109">For this, we will take a look at random variables in the general case.</span></p>
</section>
<section id="random-variables-in-general" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_269"><span class="titlemark"><span class="cmss-10x-x-109">19.1.3 </span></span> <span id="x1-30300023.1.3"></span><span class="cmss-10x-x-109">Random variables in general</span></h3>
<p><span class="cmss-10x-x-109">Let’s cut to</span> <span id="dx1-303001"></span><span class="cmss-10x-x-109">the chase.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-303002r82"></span> <span class="cmbx-10x-x-109">Definition 82.</span> </span><span class="cmbx-10x-x-109">(Random variables)</span></p>
<p>Let (Ω<sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span>Σ<sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,P</span><sub><span class="cmr-8">1</span></sub>) be a probability space and let (Ω<sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>Σ<sub><span class="cmr-8">2</span></sub>) be another event space Ω<sub><span class="cmr-8">2</span></sub> with <span class="cmmi-10x-x-109">σ</span>-algebra Σ<sub><span class="cmr-8">2</span></sub>. The function <span class="cmmi-10x-x-109">X </span>: Ω<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">→ </span>Ω<sub><span class="cmr-8">2</span></sub> is a <span class="cmti-10x-x-109">random variable</span> if, for every <span class="cmmi-10x-x-109">E </span><span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">2</span></sub>, the set</p>
<div class="math-display">
<img src="../media/file1753.png" class="math-display" alt="X −1(E ) := {ω ∈ Ω1 : X (ω) ∈ E } "/>
</div>
<p>is a member of Σ<sub><span class="cmr-8">1</span></sub>. (That is, <span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">E</span>) <span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">1</span></sub>.)</p>
</div>
<p><span class="cmss-10x-x-109">In mathematical literature, random variables are usually denoted by either capital Latin letters such as </span><span class="cmmi-10x-x-109">X,Y </span><span class="cmss-10x-x-109">, or Greek letters (mostly starting from </span><span class="cmmi-10x-x-109">ξ</span><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">Random variables essentially push probability measures forward from abstract probability spaces to more tractable ones. On the event space </span>(Ω<sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>Σ<sub><span class="cmr-8">2</span></sub>)<span class="cmss-10x-x-109">, we can define a probability measure </span><span class="cmmi-10x-x-109">P</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">by</span></p>
<div class="math-display">
<img src="../media/file1754.png" class="math-display" alt=" ( −1 ) P2(E ) := P1 X (E) , E ∈ Σ2, "/>
</div>
<p><span class="cmss-10x-x-109">making it possible to transform one probability space to another while keeping the underlying probabilistic model intact.</span></p>
<p><span class="cmss-10x-x-109">This general case covers all the mathematical objects we are interested in for machine learning. Staying with the Iris dataset (</span> <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Iris_flower_data_set</span></a><span class="cmss-10x-x-109">), the random variable</span></p>
<img src="../media/file1755.png" width="550" class="math-display" alt="X : set of iris flowers → ℝ4, iris flower ↦→ (petal width,petal length,sepal width,sepal length) "/>
<p><span class="cmss-10x-x-109">describes the</span> <span id="dx1-303003"></span><span class="cmss-10x-x-109">generating distribution for the dataset, while for classification tasks, we are interested in approximating the random variable</span></p>

<img width="550" src="../media/file1756.png" class="math-display" alt="Y : set of iris flowers → {setosa,versicolor,virginica}, iris flower ↦→ class label. "/>

<p><span class="cmss-10x-x-109">Now we will take a deeper look at why random variables are defined this way. This will be a bit technical, so feel free to skip it. It won’t adversely affect your ability to work with random variables.</span></p>
</section>
<section id="behind-the-definition-of-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_270"><span class="titlemark"><span class="cmss-10x-x-109">19.1.4 </span></span> <span id="x1-30400023.1.4"></span><span class="cmss-10x-x-109">Behind the definition of random variables</span></h3>
<p><span class="cmss-10x-x-109">So, random variables are functions, mapping the probability space onto a</span><span id="dx1-304001"></span> <span class="cmss-10x-x-109">measurement space. The only question is, why are the sets </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">E</span>) <span class="cmss-10x-x-109">so special? Let’s revisit one of our motivating examples: picking a random student and measuring their height. We are interested in questions such as the probability of a student having a body height between 155 cm and 185 cm. (If you prefer using the imperial metric system, then 155 cm is roughly 5.09 feet and 185 cm is around 6.07 feet.) Translating this into formulas, we are interested in</span></p>
<div class="math-display">
<img src="../media/file1757.png" class="math-display" alt=" ( ( )) P(155 ≤ X ≤ 185) = P X − 1[155,185] . "/>
</div>
<p><span class="cmss-10x-x-109">(In the above formula, I wrote the same thing using two different notations.)</span></p>
<p><span class="cmss-10x-x-109">So, how is </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>[155<span class="cmmi-10x-x-109">,</span>185]<span class="big">)</span> <span class="cmss-10x-x-109">an event? To find this out, let’s look at </span><span class="cmssi-10x-x-109">inverse images </span><span class="cmss-10x-x-109">in general.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-304002r83"></span> <span class="cmbx-10x-x-109">Definition 83.</span> </span><span class="cmbx-10x-x-109">(Inverse image of sets with respect to functions)</span></p>
<p>Let <span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">E </span><span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">H </span>be a function between the two sets <span class="cmmi-10x-x-109">E </span>and <span class="cmmi-10x-x-109">H</span>, and let <span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">H</span> be an arbitrary set. The <span class="cmti-10x-x-109">inverse image </span>of <span class="cmmi-10x-x-109">A </span>with respect to the function <span class="cmmi-10x-x-109">f</span> is defined by</p>
<div class="math-display">
<img src="../media/file1760.png" class="math-display" alt="f− 1(A ) := {x ∈ E : f (x ) ∈ A}. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">We like inverse images of sets because they behave nicely under set operations.</span></p>
<p><span class="cmss-10x-x-109">This is formalized by the following theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-304003r121"></span> <span class="cmbx-10x-x-109">Theorem 121.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">f </span>: <span class="cmmi-10x-x-109">E </span><span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">H </span><span class="cmti-10x-x-109">be a function between the two sets </span><span class="cmmi-10x-x-109">E </span><span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">H</span><span class="cmti-10x-x-109">. For any </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,A</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>⋅⋅⋅<span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">H</span><span class="cmti-10x-x-109">, the following hold:</span></p>
<p><span class="cmti-10x-x-109">(a)</span></p>
<div class="math-display">
<img src="../media/file1762.png" class="math-display" alt=" ( ) −1 ⋃∞ ∞⋃ −1 f An = f (An ), n=1 n=1 "/>
</div>
<p><span class="cmti-10x-x-109">(b)</span></p>
<div class="math-display">
<img src="../media/file1763.png" class="math-display" alt="f− 1(A1 ∖ A2) = f−1(A1) ∖f− 1(A2 ), "/>
</div>
<p><span class="cmti-10x-x-109">(c)</span></p>
<div class="math-display">
<img src="../media/file1764.png" class="math-display" alt=" ( ∞ ) ∞ −1 ⋂ ⋂ −1 f An = f (An ). n=1 n=1 "/>
</div>
</div>
<div id="tcolobox-304" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. (a) </span><span class="cmss-10x-x-109">We can easily see this by simply writing out the definitions. That is, we have</span></p>
<div class="math-display">
<img src="../media/file1765.png" class="math-display" alt=" ( ) −1 ⋃∞ { ∞ } f An = x ∈ E : f(x) ∈ ∪ n=1An n=1 ∞⋃ { } = x ∈ E : f(x) ∈ An n=1 ∞⋃ −1 = f (An ), n=1 "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show. (If you are not comfortable with working with sets, feel free to review </span><span class="cmssi-10x-x-109">Appendix C </span><span class="cmss-10x-x-109">on introductory set theory.)</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">This can be done in the same manner as </span><span class="cmssi-10x-x-109">(a)</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">The De Morgan laws (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch037.xhtml#x1-377003r153"><span class="cmssi-10x-x-109">153</span></a><span class="cmss-10x-x-109">) imply that</span></p>
<div class="math-display">
<img src="../media/file1766.png" class="math-display" alt=" ( ⋃∞ ) ∞⋂ H ∖ An = (H ∖An ) n=1 n=1 "/>
</div>
<p><span class="cmss-10x-x-109">holds. Combining this with </span><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">(b)</span><span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">follows.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Why is this important? Recall</span> <span id="dx1-304004"></span><span class="cmss-10x-x-109">that the Borel sets, our standard </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra on real numbers (as seen in </span><span class="cmssi-10x-x-109">Section </span><a href="ch030.xhtml#algebras-over-real-numbers"><span class="cmssi-10x-x-109">18.2.3</span></a><span class="cmss-10x-x-109">), is defined by</span></p>
<div class="math-display">
  <span>
    ℬ := σ({(−∞, x] : x ∈ ℝ})
  </span>
  <span class="math-label" style="float: right;">(19.1)</span>
</div>

<p><span class="cmss-10x-x-109">These contain all the events that we are interested in regarding the measurements. Combined with our previous result, we can reveal what is not in plain sight about random variables.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-304005r122"></span> <span class="cmbx-10x-x-109">Theorem 122.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be a random variable, and let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ ℬ</span><span class="cmti-10x-x-109">, where </span><span class="cmsy-10x-x-109">ℬ </span><span class="cmti-10x-x-109">is the Borel algebra defined by (</span><a href="#"><span class="cmti-10x-x-109">23.1.4</span></a><span class="cmti-10x-x-109">). Then, </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">A</span>) <span class="cmsy-10x-x-109">∈ </span>Σ<span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">That is, we</span> <span id="dx1-304006"></span><span class="cmss-10x-x-109">can measure the probability of </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">for any Borel set </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">. Without this, our random variables would not be that useful. To make our notations more intuitive, we write</span></p>
<div class="math-display">
<img src="../media/file1769.png" class="math-display" alt=" ( −1 ) P (X ∈ A ) := P X (A ) . "/>
</div>
<p><span class="cmss-10x-x-109">In plain English, </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">is the probability of our measurement </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">falling into the set </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Now that we understand what all of this means, let’s see the simple proof!</span></p>
<div id="tcolobox-305" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This is a simple consequence of the fact that </span><span class="cmsy-10x-x-109">ℬ </span><span class="cmss-10x-x-109">is the </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra generated by sets of the form </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,x</span>]<span class="cmss-10x-x-109">, and the inverse images behave nicely under set operations (as </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch031.xhtml#x1-304003r121"><span class="cmssi-10x-x-109">121</span></a> <span class="cmss-10x-x-109">suggests).</span></p>
</div>
</div>
</section>
<section id="independence-of-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_271"><span class="titlemark"><span class="cmss-10x-x-109">19.1.5 </span></span> <span id="x1-30500023.1.5"></span><span class="cmss-10x-x-109">Independence of random variables</span></h3>
<p><span class="cmss-10x-x-109">When building probabilistic models of the external world, the assumption</span><span id="dx1-305001"></span> <span class="cmss-10x-x-109">of independence significantly simplifies the subsequent mathematical analysis. Recall that on a probability space </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>)<span class="cmss-10x-x-109">, the events </span><span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmss-10x-x-109">are independent if</span></p>
<div class="math-display">
<img src="../media/file1770.png" class="math-display" alt="P (A ∩B ) = P(A )P(B ), "/>
</div>
<p><span class="cmss-10x-x-109">or equivalently,</span></p>
<div class="math-display">
<img src="../media/file1771.png" class="math-display" alt="P (A | B) = P (A ). "/>
</div>
<p><span class="cmss-10x-x-109">In plain English, observing one event doesn’t change our probabilistic belief about the other.</span></p>
<p><span class="cmss-10x-x-109">Since a random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is described by events of the form </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup>(<span class="cmmi-10x-x-109">E</span>)<span class="cmss-10x-x-109">, we can generalize the notion of independence to random variables.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-305002r84"></span> <span class="cmbx-10x-x-109">Definition 84.</span> </span><span class="cmbx-10x-x-109">(Independence of random variables)</span></p>
<p>Let <span class="cmmi-10x-x-109">X,Y </span>: Ω<sub><span class="cmr-8">1</span></sub> <span class="cmsy-10x-x-109">→ </span>Ω<sub><span class="cmr-8">2</span></sub> be two random variables between the probability space (Ω<sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span>Σ<sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,P</span>) and <span class="cmmi-10x-x-109">σ</span>-algebra (Ω<sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span>Σ<sub><span class="cmr-8">2</span></sub>).</p>
<p>We say that <span class="cmmi-10x-x-109">X </span>and <span class="cmmi-10x-x-109">Y </span>are independent if, for every <span class="cmmi-10x-x-109">A,B </span><span class="cmsy-10x-x-109">∈ </span>Σ<sub><span class="cmr-8">2</span></sub>,</p>
<div class="math-display">
<img src="../media/file1772.png" class="math-display" alt="P (X ∈ A, Y ∈ B ) = P (X ∈ A )P(Y ∈ B ) "/>
</div>
<p>holds.</p>
</div>
<p><span class="cmss-10x-x-109">Again, think about two coin tosses. </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">describes the first coin toss and </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">describes the other. Since the tosses are independent, no observation of the first one reveals any extra information about the</span><span id="dx1-305003"></span> <span class="cmss-10x-x-109">second one. This is formalized by the definition above.</span></p>
<p><span class="cmss-10x-x-109">On the other hand, to show two </span><span class="cmssi-10x-x-109">dependent </span><span class="cmss-10x-x-109">random variables, consider the following. We’ll roll a six-sided dice, and denote the result as </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. After that, we roll with </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">pieces of six-sided dice and denote the sum total of their values as </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">. </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">are dependent on each other. For instance, consider that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= 1<span class="cmmi-10x-x-109">,Y /span&gt;7) = 0<span class="cmss-10x-x-109">, but neither </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= 1) <span class="cmss-10x-x-109">nor </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">Y /span&gt;7) <span class="cmss-10x-x-109">are zero.</span> </span></span></p>
<p><span class="cmss-10x-x-109">Independence is an assumption that we often make. When working with sequences of random variables represented by </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X,</span>2<span class="cmmi-10x-x-109">,…</span><span class="cmss-10x-x-109">, we almost always assume that they are </span><span class="cmssi-10x-x-109">independent and identically distributed</span><span class="cmss-10x-x-109">; that is, </span><span class="cmssi-10x-x-109">i.i.d. </span><span class="cmss-10x-x-109">random variables.</span></p>
<p><span class="cmss-10x-x-109">Now that we understand how to work with random variables, it’s time to show how to represent them in a compact form.</span></p>
</section>
</section>
<section id="discrete-distributions" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_272"><span class="titlemark"><span class="cmss-10x-x-109">19.2 </span></span> <span id="x1-30600023.2"></span><span class="cmss-10x-x-109">Discrete distributions</span></h2>
<p><span class="cmss-10x-x-109">Let’s recap what</span> <span id="dx1-306001"></span><span class="cmss-10x-x-109">we have learned so far. In probability theory, our goal is to first model real-life scenarios affected by uncertainty and then to analyze them using mathematical tools such as calculus.</span></p>
<p><span class="cmss-10x-x-109">For the latter purpose, probability spaces are not easy to work with. A probability measure is a function defined on an </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebra, so we can’t really use calculus there.</span></p>
<p><span class="cmss-10x-x-109">Random variables bring us one step closer to the solution, but they can also be difficult to work with. Even though a real-valued random variable </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">maps an abstract probablity space to the set of real numbers, there are some complications. </span>Ω <span class="cmss-10x-x-109">can be anything, and if you recall, we might not even have a tractable formula for </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">For example, if </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">denotes the lifetime of a lightbulb, we don’t have a formula. So, again, we can’t use calculus. However, there is a way to represent the information contained by a random variable in a sequence, a vector-scalar function, or a scalar-scalar function.</span></p>
<p><span class="cmss-10x-x-109">Enter </span><span class="cmssi-10x-x-109">probability distributions </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">density functions</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Consider a simple experiment, such as tossing a fair coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times and counting the number of heads, denoting it with </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. As we have seen before in </span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-301002r80"><span class="cmssi-10x-x-109">80</span></a><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is a discrete random variable with</span></p>
<div class="math-display">
<img src="../media/file1773.png" class="math-display" alt=" (| ( ) { nk 12n if k = 0,1,...,n, P (X = k) = | ( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">However, the sequence </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=0</span></sub><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">fully describes the random variable </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">!</span></p>
<p><span class="cmss-10x-x-109">Think about it. As our event space is </span>Ω = <span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, any event is of the form </span><span class="cmmi-10x-x-109">A </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…,a</span><sub><span class="cmmi-8">l</span></sub><span class="cmsy-10x-x-109">}⊂ </span>Ω <span class="cmss-10x-x-109">for some </span><span class="cmmi-10x-x-109">l </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">n </span>+ 1<span class="cmss-10x-x-109">. Thus,</span></p>
<div class="math-display">
<img src="../media/file1774.png" class="math-display" alt=" l ∑ P (X ∈ A ) = P(X = ai), i=1 "/>
</div>
<p><span class="cmss-10x-x-109">where we used the (</span><img src="../media/file1775.png" class="math" alt="σ "/><span class="cmss-10x-x-109">-)additivity of probability. The sequence </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=0</span></sub><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is all the information we need.</span></p>
<p><span class="cmss-10x-x-109">As a consequence, instead of working with </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℕ</span><span class="cmss-10x-x-109">, we can forget about it and use only </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=0</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmss-10x-x-109">. Why is this good for us?</span></p>
<p><span class="cmss-10x-x-109">Because</span> <span id="dx1-306002"></span><span class="cmss-10x-x-109">sequences are awesome. As opposed to the mysterious random variables, we have a lot of tools to work with them. Most importantly, we can represent them in a programming language as an array of numbers. We can’t do such a thing with pure random variables.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-306003r85"></span> <span class="cmbx-10x-x-109">Definition 85.</span> </span><span class="cmbx-10x-x-109">(Probability mass function)</span></p>
<p>Let <span class="cmmi-10x-x-109">X </span>be a real-valued discrete random variable. The function <span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">X</span></sub> : <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span> [0<span class="cmmi-10x-x-109">,</span>1] defined by</p>
<div class="math-display">
<img src="../media/file1776.png" class="math-display" alt="p (x) = P(X = x), x ∈ ℝ X "/>
</div>
<p>is called the <span class="cmti-10x-x-109">probability mass function </span>(or PMF in short) of the discrete random variable <span class="cmmi-10x-x-109">X</span>.</p>
</div>
<p><span class="cmss-10x-x-109">In general, a sequence of real numbers defines a </span><span class="cmssi-10x-x-109">discrete distribution </span><span class="cmss-10x-x-109">if its elements are non-negative and it sums up to one.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-306004r86"></span> <span class="cmbx-10x-x-109">Definition 86.</span> </span><span class="cmbx-10x-x-109">(Discrete probability distribution)</span></p>
<p>Let <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup> be a sequence of real numbers. We say that <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">} </span>is a <span class="cmti-10x-x-109">discrete probability distribution </span>if</p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">≥ </span>0 for all <span class="cmmi-10x-x-109">k</span>,</p>
<p><span class="cmti-10x-x-109">(b) </span>and <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub> = 1.</p>
</div>
<div class="newtheorem">
<p><span class="head"> <span id="x1-306005r12"></span> <span class="cmbx-10x-x-109">Remark 12.</span> </span></p>
<p>Note that if the random variable assumes finitely many values (such as in our coin tossing example before), only finitely many values are nonzero in the distribution.</p>
</div>
<p><span class="cmss-10x-x-109">As recently</span> <span id="dx1-306006"></span><span class="cmss-10x-x-109">hinted, every discrete random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">defines the distribution </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmss-10x-x-109">, where </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">are the possible values that </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">can take. This also holds in the reverse direction: given a discrete distribution </span><span class="cmbx-10x-x-109">p </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmss-10x-x-109">, we can construct a random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">whose </span><span class="cmssbx-10x-x-109">probability mass function </span><span class="cmss-10x-x-109">(</span><span class="cmssbx-10x-x-109">PMF</span><span class="cmss-10x-x-109">) is </span><span class="cmbx-10x-x-109">p</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Thus, the probability mass function of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is also referred to as its distribution. I know, it is a bit confusing, as the word “distribution” is quite overloaded in math. You’ll get used to it.</span></p>
<p><span class="cmss-10x-x-109">These discrete probability distributions are well suited for performing</span><span id="dx1-306007"></span> <span class="cmss-10x-x-109">quantitative analysis, as opposed to the base form of random variables. As an additional benefit, think about how distributions generalize random variables. No matter whether we talk about coin tosses or medical tests, the rate of success is given by the above discrete probability distribution.</span></p>
<p><span class="cmss-10x-x-109">Before moving on to discussing the basic properties of discrete distributions, let’s see some examples!</span></p>
<section id="the-bernoulli-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_273"><span class="titlemark"><span class="cmss-10x-x-109">19.2.1 </span></span> <span id="x1-30700023.2.1"></span><span class="cmss-10x-x-109">The Bernoulli distribution</span></h3>
<p><span class="cmss-10x-x-109">Let’s start the</span> <span id="dx1-307001"></span><span class="cmss-10x-x-109">long line of examples with the most basic probability</span><span id="dx1-307002"></span> <span class="cmss-10x-x-109">distribution possible: the Bernoulli distribution, describing a simple coin-tossing experiment. We are tossing a coin having probability </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">of coming up heads and probability </span>1 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">of coming up tails. The experiment is encoded in the random variable </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, which takes the value </span>1 <span class="cmss-10x-x-109">if the toss results in heads, </span>0 <span class="cmss-10x-x-109">otherwise:</span></p>
<div class="math-display">
<img src="../media/file1777.png" class="math-display" alt=" ( |{ 1 if the toss results in heads, X = |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file1778.png" class="math-display" alt=" ( || ||{ 1 − p if k = 0, P (X = k) = p if k = 1, ||| |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">When a random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is distributed according to this, we write</span></p>
<img src="../media/file1779.png" width="150" class="math-display" alt="X ∼ Bernoulli(p), "/>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">p </span><span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">is a parameter of the distribution.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-307003r13"></span> <span class="cmbx-10x-x-109">Remark 13.</span> </span><span class="cmbx-10x-x-109">(An alternative form of the Bernoulli distribution)</span></p>
<p>There is a clever alternative formulation of the Bernoulli distribution that gets rid of the if-else definition. As <span class="cmmi-10x-x-109">k </span>is either zero or one, <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>) can be written as</p>
<div class="math-display">
<img src="../media/file1780.png" class="math-display" alt=" k 1−k P (X = k) = p (1− p) . "/>
</div>
<p>Keep this form in mind, as it’ll be extremely useful later down the line.</p>
</div>
<p><span class="cmss-10x-x-109">It’s time to</span> <span id="dx1-307004"></span><span class="cmss-10x-x-109">talk about distributions in practice. There are several</span><span id="dx1-307005"></span> <span class="cmss-10x-x-109">stats packages for Python, but we’ll use the almighty </span><span class="cmtt-10x-x-109">scipy </span><span class="cmss-10x-x-109">(which is not exactly a stats package, but it has an excellent statistical module):</span></p>
<div id="tcolobox-306" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats import bernoulli</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">We can generate random values using the </span><span class="cmtt-10x-x-109">rvs </span><span class="cmss-10x-x-109">method of the </span><span class="cmtt-10x-x-109">bernoulli </span><span class="cmss-10x-x-109">object (just like for any other distribution from </span><span class="cmtt-10x-x-109">scipy</span><span class="cmss-10x-x-109">):</span></p>
<div id="tcolobox-307" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>[bernoulli.rvs(p=0.5) for _ in range(10)]    # ten Bernoulli(0.5)-distributed random numbers</code></pre>
</div>
</div>
<pre class="lstlisting"><code>[1, 1, 1, 1, 0, 1, 0, 1, 1, 0]</code></pre>
<p><span class="cmss-10x-x-109">In </span><span class="cmtt-10x-x-109">scipy</span><span class="cmss-10x-x-109">, the probability mass function is implemented in the </span><span class="cmtt-10x-x-109">pmf </span><span class="cmss-10x-x-109">method.</span></p>
<p><span class="cmss-10x-x-109">We can even visualize the distribution using Matplotlib:</span></p>
<div id="tcolobox-308" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import matplotlib.pyplot as plt 
 
params = [0.25, 0.5, 0.75] 
 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    fig, axs = plt.subplots(1, len(params), figsize=(4*len(params), 4), sharey=True) 
    fig.suptitle("/span&gt;The Bernoulli distribution 
    for ax, p in zip(axs, params): 
        x = range(2) 
        y = [bernoulli.pmf(k=k, p=p) for k in x] 
        ax.bar(x, y) 
        ax.set_title(f/span&gt;p = {p}" 
        ax.set_ylabel("/span&gt;P(X = k) 
        ax.set_xlabel("/span&gt;k 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1781.png" width="569" alt="PIC"/> <span id="x1-307025r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.3: The Bernoulli distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">If you are</span> <span id="dx1-307026"></span><span class="cmss-10x-x-109">interested</span> <span id="dx1-307027"></span><span class="cmss-10x-x-109">in the details, feel free to check out the SciPy documentation (</span> <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html" class="url"><span class="cmtt-10x-x-109">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html</span></a><span class="cmss-10x-x-109">) for further methods!</span></p>
</section>
<section id="the-binomial-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_274"><span class="titlemark"><span class="cmss-10x-x-109">19.2.2 </span></span> <span id="x1-30800023.2.2"></span><span class="cmss-10x-x-109">The binomial distribution</span></h3>
<p><span class="cmss-10x-x-109">Let’s take our previous coin-tossing example one step further. Suppose</span><span id="dx1-308001"></span> <span class="cmss-10x-x-109">that we toss the same coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times, and </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">denotes the number of heads out of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">tosses. What is the probability of getting exactly </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">heads?</span></p>
<p><span class="cmss-10x-x-109">Say, </span><span class="cmmi-10x-x-109">n </span>= 5 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">k </span>= 3<span class="cmss-10x-x-109">. For example, the configuration </span>11010 <span class="cmss-10x-x-109">(where </span>0 <span class="cmss-10x-x-109">denotes tails and </span>1 <span class="cmss-10x-x-109">denotes heads) has the probability </span><span class="cmmi-10x-x-109">p</span><sup><span class="cmr-8">3</span></sup>(1 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">p</span>)<sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, as there are three heads and two tails from five independent (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-305002r84"><span class="cmssi-10x-x-109">84</span></a><span class="cmss-10x-x-109">) tosses.</span></p>
<p><span class="cmss-10x-x-109">How many such configurations are available? Selecting the position of the three heads is the same as selecting a three-element subset out of a set of five elements. Thus, there are</span> <img src="../media/file1783.png" width="15" alt="(5) 3"/> <span class="cmss-10x-x-109">possibilities. In general, there are</span> <img src="../media/file1784.png" alt="(n) k" width="15"/> <span class="cmss-10x-x-109">possibilities for selecting a </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-element subset out of a set of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">elements.</span></p>
<p><span class="cmss-10x-x-109">Combining this, we have</span></p>
<div class="math-display">
<img src="../media/file1785.png" class="math-display" alt=" ( |{ (n) k n−k P (X = k ) = k p (1− p) if k = 0,1,...,n, |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">This is called</span> <span id="dx1-308002"></span><span class="cmss-10x-x-109">the binomial distribution, one of the most frequently encountered ones</span> <span id="dx1-308003"></span><span class="cmss-10x-x-109">in probability and statistics. In notation, we write</span></p>
<img src="../media/file1786.png" width="150" class="math-display" alt="X ∼ Binomial(n,p), "/>
<p><span class="cmss-10x-x-109">where the </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℕ </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">p </span><span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">are its two parameters. Let’s visualize the distribution!</span></p>
<div id="tcolobox-309" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats.distributions import binom 
 
params = [(20, 0.25), (20, 0.5), (20, 0.75)] 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    fig, axs = plt.subplots(1, len(params), figsize=(4*len(params), 4), sharey=True) 
    fig.suptitle("/span&gt;The binomial distribution 
    for ax, (n, p) in zip(axs, params): 
        x = range(n+1) 
        y = [binom.pmf(n=n, p=p, k=k) for k in x] 
        ax.bar(x, y) 
        ax.set_title(f/span&gt;n = {n}, p = {p}" 
        ax.set_ylabel("/span&gt;P(X = k) 
        ax.set_xlabel("/span&gt;k 
 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1787.png" width="760" alt="PIC"/> <span id="x1-308020r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.4: The binomial distribution</span> </span>
</div>
</section>
<section id="the-geometric-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_275"><span class="titlemark"><span class="cmss-10x-x-109">19.2.3 </span></span> <span id="x1-30900023.2.3"></span><span class="cmss-10x-x-109">The geometric distribution</span></h3>
<p><span class="cmss-10x-x-109">A bit more</span> <span id="dx1-309001"></span><span class="cmss-10x-x-109">coin</span> <span id="dx1-309002"></span><span class="cmss-10x-x-109">tossing. We toss the same coin until a heads turn up. Let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">denote the number of tosses needed. With some elementary probabilistic thinking, we can deduce that</span></p>
<div class="math-display">
<img src="../media/file1788.png" class="math-display" alt=" ( |{ k−1 P(X = k) = (1 − p) p if k = 1,2,... |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">(Since if heads turn up first for the </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-th toss, we tossed </span><span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">− </span>1 <span class="cmss-10x-x-109">tails previously.) This is called the geometric distribution and is denoted as</span></p>

<img src="../media/file1789.png" width="150" class="math-display" alt="X ∼ Geo (p), "/>

<p><span class="cmss-10x-x-109">with </span><span class="cmmi-10x-x-109">p </span><span class="cmsy-10x-x-109">∈ </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">being the only parameter. Similarly, we can plot the histograms to visualize the distribution family:</span></p>
<div id="tcolobox-310" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats import geom 
 
 
params = [0.2, 0.5, 0.8] 
 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    fig, axs = plt.subplots(1, len(params), figsize=(5*len(params), 5), sharey=True) 
    fig.suptitle("/span&gt;The geometric distribution 
    for ax, p in zip(axs, params): 
        x = range(1, 20) 
        y = [geom.pmf(p=p, k=k) for k in x] 
        ax.bar(x, y) 
        ax.set_title(f/span&gt;p = {p}" 
        ax.set_ylabel("/span&gt;P(X = k) 
        ax.set_xlabel("/span&gt;k 
 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1790.png" width="560" alt="PIC"/> <span id="x1-309021r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.5: The geometric distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">Note that </span><span class="cmssi-10x-x-109">none </span><span class="cmss-10x-x-109">of the probabilities </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>) <span class="cmss-10x-x-109">are zero, but as </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">grows, they become extremely small. (The closer </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">is to </span>1<span class="cmss-10x-x-109">, the faster the decay.)</span></p>
<p><span class="cmss-10x-x-109">It might not be</span> <span id="dx1-309022"></span><span class="cmss-10x-x-109">immediately obvious that </span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup>(1 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">p</span>)<sup><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">p </span>= 1<span class="cmss-10x-x-109">. To do that, we’ll apply a magic trick. (You know. Paraphrasing the famous Arthur C. Clarke quote, “Any sufficiently advanced mathematics is indistinguishable from magic.”)</span></p>
<p><span class="cmss-10x-x-109">In fact, for an arbitrary </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈ </span>(<span class="cmsy-10x-x-109">−</span>1<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">, the astounding identity</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(27).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(19.2)
  </div>
</div>
<p><span class="cmss-10x-x-109">holds.</span></p>
<p><span class="cmss-10x-x-109">This is the famous geometric series. Using (</span><a href="#"><span class="cmss-10x-x-109">23.2.3</span></a><span class="cmss-10x-x-109">), we have</span></p>
<div class="math-display">
<img src="../media/file1792.png" class="math-display" alt=" ∞ ∞ ∑ ∑ k−1 P (X = k) = (1− p ) p k=1 k=1∞ ∑ k = p (1 − p) k=0 = p-----1---- 1 − (1− p) = 1. "/>
</div>
<p><span class="cmss-10x-x-109">Using the geometric series is one of the most common tricks up a mathematician’s sleeve. We’ll use this, for instance, when talking about expected values for certain distributions.</span></p>
</section>
<section id="the-uniform-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_276"><span class="titlemark"><span class="cmss-10x-x-109">19.2.4 </span></span> <span id="x1-31000023.2.4"></span><span class="cmss-10x-x-109">The uniform distribution</span></h3>
<p><span class="cmss-10x-x-109">Let’s</span> <span id="dx1-310001"></span><span class="cmss-10x-x-109">discard the coin and</span><span id="dx1-310002"></span> <span class="cmss-10x-x-109">roll a six-sided dice instead. We’ve seen this before: the probability of each outcome is the same, that is,</span></p>
<div class="math-display">
<img src="../media/file1793.png" class="math-display" alt="P(X = 1) = P(X = 2) = ⋅⋅⋅ = P(X = 6) = 1, 6 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">denotes the outcome of the roll. This is a special instance of the </span><span class="cmssi-10x-x-109">uniform distribution</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">In general, let </span><span class="cmmi-10x-x-109">A </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…,a</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">be a finite set. The discrete random variable </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">is </span><span class="cmssi-10x-x-109">uniformly distributed on </span><span class="cmmi-10x-x-109">A</span><span class="cmss-10x-x-109">, that is,</span></p>
<div class="math-display">
<img src="../media/file1794.png" class="math-display" alt="X ∼ Uniform(A ), "/>
</div>
<p><span class="cmss-10x-x-109">if</span></p>
<div class="math-display">
<img src="../media/file1795.png" class="math-display" alt=" 1- P (X = a1) = P (X = a2) = ⋅⋅⋅ = P(X = an) = n. "/>
</div>
<p><span class="cmss-10x-x-109">Note that </span><span class="cmmi-10x-x-109">A </span><span class="cmss-10x-x-109">must be a finite set: no discrete uniform distribution exists on infinite sets. When we have an uniform distribution on </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,…,n</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, we often abbreviate it as</span> Uniform(<span class="cmmi-10x-x-109">n</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Here is the probability mass function for rolling a six-sided dice. Not the most exciting one, I know:</span></p>
<div id="tcolobox-311" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats import randint 
 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    fig = plt.figure(figsize=(10, 5)) 
    plt.title("/span&gt;The uniform distribution 
 
    x = range(-1, 9) 
    y = [randint.pmf(k=k, low=1, high=7) for k in x] 
    plt.bar(x, y) 
    plt.ylim(0, 1) 
    plt.ylabel("/span&gt;P(X = k) 
    plt.xlabel("/span&gt;k 
 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1796.png" width="569" alt="PIC"/> <span id="x1-310018r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.6: The (discrete) uniform distribution</span> </span>
</div>
</section>
<section id="the-singlepoint-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_277"><span class="titlemark"><span class="cmss-10x-x-109">19.2.5 </span></span> <span id="x1-31100023.2.5"></span><span class="cmss-10x-x-109">The single-point distribution</span></h3>
<p><span class="cmss-10x-x-109">We’ve left</span> <span id="dx1-311001"></span><span class="cmss-10x-x-109">the</span> <span id="dx1-311002"></span><span class="cmss-10x-x-109">simplest one till last: the single-point distribution. For that, let </span><span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">be an arbitrary real number. We say that the random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is distributed according to </span><span class="cmmi-10x-x-109">δ</span>(<span class="cmmi-10x-x-109">a</span>) <span class="cmss-10x-x-109">if</span></p>
<div class="math-display">
<img src="../media/file1797.png" class="math-display" alt=" ( |{ 1 if x = a, P (X = x) = |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">That is, </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">assumes </span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">with probability </span>1<span class="cmss-10x-x-109">. Their corresponding cumulative distribution function is</span></p>
<div class="math-display">
<img src="../media/file1798.png" class="math-display" alt=" (| { 1 if x ≥ a, FX (x) = | ( 0 otherwise, "/>
</div>
<p><span class="cmss-10x-x-109">which is a simple step function with a single jump.</span></p>
<p><span class="cmss-10x-x-109">Trust me, explicitly naming such a simple distribution is immensely useful. There are two main reasons that come to mind. First, the single-point distribution often arises as the limit distribution of sequences of random variables.</span></p>
<p><span class="cmss-10x-x-109">Second, every discrete distribution can be written in terms of single-point distributions. This is not absolutely necessary for you to understand right now, but it’ll be essential on a more advanced level.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-311003r14"></span> <span class="cmbx-10x-x-109">Remark 14.</span> </span><span class="cmbx-10x-x-109">(Discrete distributions as the linear combination of single-point distributions)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space and let <span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→{</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmsy-10x-x-109">} </span>be a discrete random variable with probability mass function <span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub>).</p>
<p>By introducing the single-point distributions <span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∼</span><span class="cmmi-10x-x-109">δ</span>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub>), we have</p>
<div class="math-display">
<img src="../media/file1799.png" class="math-display" alt=" ∑∞ FX (x) = piFX (x). i=1 i "/>
</div>
<p>This decomposition can be extremely useful.</p>
</div>
</section>
<section id="law-of-total-probability-revisited-once-more" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_278"><span class="titlemark"><span class="cmss-10x-x-109">19.2.6 </span></span> <span id="x1-31200023.2.6"></span><span class="cmss-10x-x-109">Law of total probability, revisited once more</span></h3>
<p><span class="cmss-10x-x-109">With the</span> <span id="dx1-312001"></span><span class="cmss-10x-x-109">help</span> <span id="dx1-312002"></span><span class="cmss-10x-x-109">of discrete random variables, we can dress the law of total probability (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-291003r117"><span class="cmssi-10x-x-109">117</span></a><span class="cmss-10x-x-109">) in new clothes.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-312003r123"></span> <span class="cmbx-10x-x-109">Theorem 123.</span> </span><span class="cmbxti-10x-x-109">(Law of total probability, discrete random variable version)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">∈ </span>Σ <span class="cmti-10x-x-109">be an arbitrary event. If </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→{</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmsy-10x-x-109">}</span><span class="cmti-10x-x-109">is a discrete random variable, then</span></p>
<div class="math-display">
  <span>
    P(A) = ∑<sub>k=1</sub><sup>∞</sup> P(A | X = x<sub>k</sub>) P(X = x<sub>k</sub>).
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(19.3)</span>
</div>

</div>
<div id="tcolobox-312" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">For any discrete random variable </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→ {</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, the events </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">partition the event space: they are mutually disjoint, and their union gives </span>Ω<span class="cmss-10x-x-109">. Thus, the law of total probability can be applied, obtaining</span></p>
<div class="math-display">
<img src="../media/file1800.png" class="math-display" alt=" ∑∞ P(A ) = P(A, X = xk) k=1 ∑∞ = P(A | X = xk)P (X = xk), k=1 "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to prove.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">In other words, we can study events in the context of discrete random variables. This is extremely useful in practice. (Soon, we’ll see that it’s not only for the discrete case.)</span></p>
<p><span class="cmss-10x-x-109">Let’s put (</span><a href="ch031.xhtml#x1-312003r123"><span class="cmss-10x-x-109">19.3</span></a><span class="cmss-10x-x-109">) to work right away.</span></p>
</section>
<section id="sums-of-discrete-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_279"><span class="titlemark"><span class="cmss-10x-x-109">19.2.7 </span></span> <span id="x1-31300023.2.7"></span><span class="cmss-10x-x-109">Sums of discrete random variables</span></h3>
<p><span class="cmss-10x-x-109">Since discrete</span> <span id="dx1-313001"></span><span class="cmss-10x-x-109">probability distributions are represented by sequences, we can use a wide array of tools from mathematical analysis to work with them. (This was the whole reason behind switching random variables to distributions.) As a consequence, we can easily describe more complex random variables by constructing them from simpler ones.</span></p>
<p><span class="cmss-10x-x-109">For instance, consider rolling two dice, where we are interested in the distribution of the sum. So, we can write this as the sum of random variables </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub> <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">, </span><span class="cmss-10x-x-109">denoting the outcome of the first and second toss, respectively. We know that</span></p>
<div class="math-display">
<img src="../media/file1801.png" class="math-display" alt=" (| { 16 if k = 1,2,...,6, P(Xi = k) = | ( 0 otherwise "/>
</div>
<p><span class="cmss-10x-x-109">for </span><span class="cmmi-10x-x-109">i </span>= 1<span class="cmmi-10x-x-109">,</span>2<span class="cmss-10x-x-109">. Using (</span><a href="ch031.xhtml#x1-312003r123"><span class="cmss-10x-x-109">19.3</span></a><span class="cmss-10x-x-109">) and the fact that the two outcomes are independent, we have</span></p>
<div class="math-display">
<img src="../media/file1802.png" class="math-display" alt=" 6 P (X + X = k) = ∑ P(X + X = k | X = l)P (X = l) 1 2 l=1 1 2 2 2 6 ∑ = P(X1 = k − l)P(X2 = l) l=1 "/>
</div>
<p><span class="cmss-10x-x-109">If this looks</span><span id="dx1-313002"></span> <span class="cmss-10x-x-109">familiar, it is not an accident.</span></p>
<p><span class="cmss-10x-x-109">What you see here is the famous convolution operation in action.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-313003r87"></span> <span class="cmbx-10x-x-109">Definition 87.</span> </span><span class="cmbx-10x-x-109">(Discrete convolution)</span></p>
<p>Let <span class="cmmi-10x-x-109">a </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup> and <span class="cmmi-10x-x-109">b </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">k</span></sub><span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup> be two arbitrary sequences. Their <span class="cmti-10x-x-109">convolution </span>is defined by</p>
<div class="math-display">
<img src="../media/file1803.png" class="math-display" alt=" { ∞∑ } ∞ a∗ b := ak−lbl . l= −∞ k=−∞ "/>
</div>
</div>
<p><span class="cmss-10x-x-109">That is, the </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-th element of the sequence </span><span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">∗</span><span class="cmmi-10x-x-109">b </span><span class="cmss-10x-x-109">is defined by the sum </span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">l</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmmi-8">l</span></sub><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">l</span></sub><span class="cmss-10x-x-109">. This might be hard to imagine, but thinking about the probabilistic interpretation makes the definition clear. The random variable </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">2</span></sub> <span class="cmss-10x-x-109">can assume the value </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">if </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub> = <span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">l </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">2</span></sub> = <span class="cmmi-10x-x-109">l</span><span class="cmss-10x-x-109">, for all possible </span><span class="cmmi-10x-x-109">l </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℤ</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-313004r15"></span> <span class="cmbx-10x-x-109">Remark 15.</span> </span><span class="cmbx-10x-x-109">Remark 19.2.4 (Switching up the indices)</span></p>
<p>Due to symmetry,</p>
<div class="math-display">
<img src="../media/file1804.png" class="math-display" alt=" ∑∞ ∑∞ ak−lbl = albk−l. l=−∞ l=−∞ "/>
</div>
<p>Thus, an alternative definition of <span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">∗</span><span class="cmmi-10x-x-109">b</span></p>
<div class="math-display">
<img src="../media/file1805.png" class="math-display" alt=" { ∑∞ }∞ a ∗b = albk−l . l=− ∞ k= −∞ "/>
</div>
<p>This trick is often extremely useful, as when <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">k</span></sub> and <span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">k</span></sub> is explicitly given, sometimes <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">l</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">l</span></sub><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmmi-8">l</span></sub> is simpler to calculate than <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">l</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">−</span><span class="cmmi-8">l</span></sub><span class="cmmi-10x-x-109">b</span><sub><span class="cmmi-8">l</span></sub>, and vice versa.</p>
</div>
<p><span class="cmss-10x-x-109">Convolution</span><span id="dx1-313005"></span> <span class="cmss-10x-x-109">is</span> <span id="dx1-313006"></span><span class="cmss-10x-x-109">supported by NumPy, so with its help, we can visualize the distribution of our </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">2</span></sub><span class="cmss-10x-x-109">:</span></p>
<div id="tcolobox-313" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import numpy as np 
 
dist_1 = [0, 1/6, 1/6, 1/6, 1/6, 1/6, 1/6] 
dist_2 = [0, 1/6, 1/6, 1/6, 1/6, 1/6, 1/6] 
sum_dist = np.convolve(dist_1, dist_1) 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    plt.figure(figsize=(10, 5)) 
    plt.bar(range(0, len(sum_dist)), sum_dist) 
    plt.title("/span&gt;Distribution of X1 + X2" 
    plt.ylabel("/span&gt;P(X1 + X1 = k) 
    plt.xlabel("/span&gt;k 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1806.png" width="569" alt="PIC"/> <span id="x1-313020r7"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.7: Distribution of the sum of two random variables</span> </span>
</div>
<p><span class="cmss-10x-x-109">Let’s talk about the general case. The pattern is clear, so we can formulate a theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-313021r124"></span> <span class="cmbx-10x-x-109">Theorem 124.</span> </span><span class="cmbxti-10x-x-109">(Sums of discrete random variables.)</span></p>
<p><span class="cmti-10x-x-109">If </span><span class="cmmi-10x-x-109">X,Y </span>: Ω <span class="cmsy-10x-x-109">→ </span><span class="msbm-10x-x-109">ℤ </span><span class="cmti-10x-x-109">are both integer-valued random variables, then the distribution of </span><span class="cmmi-10x-x-109">X </span>+ <span class="cmmi-10x-x-109">Y </span><span class="cmti-10x-x-109">is given by the convolution of the respective distributions:</span></p>
<div class="math-display">
<img src="../media/file1807.png" class="math-display" alt=" ∞ ∑ P (X + Y = k ) = P (X = k − l)P (Y = l), l=−∞ "/>
</div>
<p><span class="cmti-10x-x-109">that is,</span></p>
<img src="../media/file1808.png" class="math-display" alt="pX+Y = pX ∗ pY. " width="150"/>
</div>
<div id="tcolobox-314" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">The proof is a straightforward application of the law of total probability (</span><a href="ch031.xhtml#x1-312003r123"><span class="cmss-10x-x-109">19.3</span></a><span class="cmss-10x-x-109">):</span></p>
<p><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>+ <span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">k</span>) = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">l</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>+ <span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">k</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">l</span>)<span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">l</span>) = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">l</span><span class="cmr-8">=</span><span class="cmsy-8">−∞</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">l</span>)<span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">l</span>) = (<span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">X</span></sub> <span class="cmsy-10x-x-109">∗</span><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">Y</span></sub> )(<span class="cmmi-10x-x-109">k</span>)<span class="cmmi-10x-x-109">,</span></p>
<p><span class="cmss-10x-x-109">which is what we had to prove.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Another</span> <span id="dx1-313022"></span><span class="cmss-10x-x-109">example of random variable sums is the binomial distribution itself. Instead</span> <span id="dx1-313023"></span><span class="cmss-10x-x-109">of thinking about the number of successes of an experiment out of </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">independent tries, we can model the core experiment as a Bernoulli distribution. That is, if </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is a</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">distributed random variable describing the success of the </span><span class="cmmi-10x-x-109">i</span><span class="cmss-10x-x-109">-th attempt, we have</span></p>
<img src="../media/file1809.png" width="450" class="math-display" alt=" ∑ P (X1 + ⋅⋅⋅ + Xn = k) = P (X1 = i1,...,Xn = in) i1+ ⋅⋅⋅+in=k = ∑ P (X = i )...P(X = i) ◟---1----1-◝◜-----n---n◞ i1+ ⋅⋅⋅+in=k X1,...,Xn are independent = ∑ pk(1 − p)n− k i1(+ ⋅⋅⋅)+in=k = n pk(1 − p)n−k, k "/>

<p><span class="cmss-10x-x-109">where the sum </span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><sub><span class="cmr-6">1</span></sub><span class="cmr-8">+</span>⋅⋅⋅<span class="cmr-8">+</span><span class="cmmi-8">i</span><sub><span class="cmmi-6">n</span></sub><span class="cmr-8">=</span><span class="cmmi-8">k</span></sub> <span class="cmss-10x-x-109">traverses all tuples </span>(<span class="cmmi-10x-x-109">i</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,i</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmsy-10x-x-109">∈{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmsy-10x-x-109">}</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">for which </span><span class="cmmi-10x-x-109">i</span><sub><span class="cmr-8">1</span></sub> + ⋅⋅⋅ + <span class="cmmi-10x-x-109">i</span><sub><span class="cmmi-8">n</span></sub> = <span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">. (As there are</span> <img src="../media/file1812.png" width="15" alt="(n) k"/> <span class="cmss-10x-x-109">of such tuples, we have </span><span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">i</span><sub><span class="cmr-6">1</span></sub><span class="cmr-8">+</span>⋅⋅⋅<span class="cmr-8">+</span><span class="cmmi-8">i</span><sub><span class="cmmi-6">n</span></sub><span class="cmr-8">=</span><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">p</span><sup><span class="cmmi-8">k</span></sup>(1 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">p</span>)<sup><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmmi-8">k</span></sup> = <img src="../media/file1814.png" alt="( ) nk" width="15"/><span class="cmmi-10x-x-109">p</span><sup><span class="cmmi-8">k</span></sup>(1 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">p</span>)<sup><span class="cmmi-8">n</span><span class="cmsy-8">−</span><span class="cmmi-8">k</span></sup> <span class="cmss-10x-x-109">in the last step.)</span></p>
</section>
</section>
<section id="realvalued-distributions" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_280"><span class="titlemark"><span class="cmss-10x-x-109">19.3 </span></span> <span id="x1-31400023.3"></span><span class="cmss-10x-x-109">Real-valued distributions</span></h2>
<p><span class="cmss-10x-x-109">So far, we have</span> <span id="dx1-314001"></span><span class="cmss-10x-x-109">talked about discrete random variables, that is, random variables with countably many values. However, not all experiments/observations/measurements are like this. For instance, the height of a person is a random variable that can assume a continuum of values.</span></p>
<p><span class="cmss-10x-x-109">To give a tractable example, let’s pick a number </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">from</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, with each one having an “equal chance.” In this context, equal chance means that</span></p>
<div class="math-display">
<img src="../media/file1815.png" class="math-display" alt="P (a &lt;X ≤ b) = |b − a|. "/>
</div>
<p><span class="cmss-10x-x-109">Can we describe </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">with a single real function? As in the discrete case, we can try</span></p>
<div class="math-display">
<img src="../media/file1816.png" class="math-display" alt="F(x) = P (X = x), "/>
</div>
<p><span class="cmss-10x-x-109">but this wouldn’t work. Why?</span></p>
<p><span class="cmss-10x-x-109">Because for each </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, we have </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span>) = 0<span class="cmss-10x-x-109">. That is, picking a particular number </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">has zero probability. Instead, we can try </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">, which is</span></p>
<div class="math-display">
<img src="../media/file1817.png" class="math-display" alt=" ( ||| 0 if x ≤ 0, |{ FX(x) = x if 0 &lt;x ≤ 1, |||| ( 1 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">We can plot this for visualization:</span></p>
<div id="tcolobox-315" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats import uniform 
X = np.linspace(-0.5, 1.5, 100) 
y = uniform.cdf(X) 
 
 
with plt.style.context(’seaborn-v0_8’): 
    plt.figure(figsize=(10, 5)) 
    plt.title("/span&gt;The uniform distribution 
    plt.plot(X, y) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1818.png" width="569" alt="PIC"/> <span id="x1-314012r8"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.8: The uniform distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">In the</span> <span id="dx1-314013"></span><span class="cmss-10x-x-109">following section, we will properly define and study this object in detail for all real-valued random variables.</span></p>
<section id="the-cumulative-distribution-function" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_281"><span class="titlemark"><span class="cmss-10x-x-109">19.3.1 </span></span> <span id="x1-31500023.3.1"></span><span class="cmss-10x-x-109">The cumulative distribution function</span></h3>
<p><span class="cmss-10x-x-109">What we</span> <span id="dx1-315001"></span><span class="cmss-10x-x-109">have seen in our motivating example is an instance of a </span><span class="cmssi-10x-x-109">cumulative distribution function</span><span class="cmss-10x-x-109">, or CDF in short. Let’s jump into the formal definition right away.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-315002r88"></span> <span class="cmbx-10x-x-109">Definition 88.</span> </span><span class="cmbx-10x-x-109">(Cumulative distribution function)</span></p>
<p>Let <span class="cmmi-10x-x-109">X </span>be a real-valued random variable. The function defined by</p>
<div class="math-display">
  <span>
    F<sub>X</sub>(x) := P(X ≤ x)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(19.4)</span>
</div>

<p>is called the <span class="cmti-10x-x-109">cumulative distribution function </span>(CDF) of <span class="cmmi-10x-x-109">X</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Again, let’s unpack this. Recall that in the definition of real-valued random variables (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-302002r81"><span class="cmssi-10x-x-109">81</span></a><span class="cmss-10x-x-109">), we have used the inverse images </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmmi-10x-x-109">a,b</span>)<span class="big">)</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Something similar is going on here. </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">is the abbreviation for </span><span class="cmmi-10x-x-109">P</span><span class="big">(</span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,x</span>]<span class="big">)</span><span class="big">)</span><span class="cmss-10x-x-109">, which we are too lazy to write. Similarly to </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmmi-10x-x-109">a,b</span>)<span class="big">)</span><span class="big">)</span><span class="cmss-10x-x-109">, you can visualize </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,x</span>]<span class="big">)</span><span class="big">)</span> <span class="cmss-10x-x-109">by pulling the interval </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,x</span>] <span class="cmss-10x-x-109">back to </span>Ω <span class="cmss-10x-x-109">using the mapping </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Sets of the form </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmsy-8">−</span><span class="cmr-8">1</span></sup><span class="big">(</span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,x</span>]<span class="big">)</span> <span class="cmss-10x-x-109">are called the </span><span class="cmssi-10x-x-109">level sets </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1833.png" width="456" alt="PIC"/> <span id="x1-315003r9"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.9: The level set of a random variable</span> </span>
</div>
<p><span class="cmss-10x-x-109">According to the Oxford English Dictionary, the word </span><span class="cmssi-10x-x-109">cumulative </span><span class="cmss-10x-x-109">means </span><span class="cmssi-10x-x-109">“increasing or increased in quantity, degree, or force by successive additions.” </span><span class="cmss-10x-x-109">For discrete random variables, using </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>) <span class="cmss-10x-x-109">was enough, but since real random variables are more nuanced, we have</span> <span id="dx1-315004"></span><span class="cmss-10x-x-109">to use the cumulative probabilities </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">to meaningfully describe them.</span></p>
<p><span class="cmss-10x-x-109">Why do we like to work with distribution functions? Because they condense all the relevant information about a random variable in a real function.</span></p>
<p><span class="cmss-10x-x-109">For instance, we can express probabilities like</span></p>
<div class="math-display">
<img src="../media/file1834.png" class="math-display" alt="P (a &lt;X ≤ b) = F (b) − F (a ). X X "/>
</div>
<p><span class="cmss-10x-x-109">To give an example, let’s revisit the introduction, where we were selecting a random number between zero and one. There, the random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">with CDF</span></p>
<div style="display: flex; justify-content: space-between; align-items: center; max-width: 600px;" class="equation math-display">
  <div class="math-display">
    <img src="../media/equation_(28).png" alt="L(U,V ) = {f : U → V | f is linear}" width="150"/>
  </div>
  <div style="padding-left: 1em; ">
    <!-- Label goes here, e.g. (4.2) -->(19.5)
  </div>
</div>
<p><span class="cmss-10x-x-109">is said to be </span><span class="cmssi-10x-x-109">uniformly distributed over</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, or </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼</span> Uniform(0<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">in short. We’ll see a ton of examples later, but keep note of this, as the uniform distribution will be our textbook example throughout this section.</span></p>
</section>
<section id="properties-of-the-distribution-function" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_282"><span class="titlemark"><span class="cmss-10x-x-109">19.3.2 </span></span> <span id="x1-31600023.3.2"></span><span class="cmss-10x-x-109">Properties of the distribution function</span></h3>
<p><span class="cmss-10x-x-109">Cumulative distribution functions</span> <span id="dx1-316001"></span><span class="cmss-10x-x-109">have three properties that characterize them: they are always non-decreasing, right-continuous (whatever that might be), and their limits are </span>0 <span class="cmss-10x-x-109">and </span>1 <span class="cmss-10x-x-109">toward </span><span class="cmsy-10x-x-109">−∞ </span><span class="cmss-10x-x-109">and </span><span class="cmsy-10x-x-109">∞ </span><span class="cmss-10x-x-109">respectively. You might have guessed some of this from the definition, but here is the formal theorem that summarizes</span> <span id="dx1-316002"></span><span class="cmss-10x-x-109">this.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-316003r125"></span> <span class="cmbx-10x-x-109">Theorem 125.</span> </span><span class="cmbxti-10x-x-109">(Properties of CDFs)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmti-10x-x-109">be a real-valued random variable with CDF </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub><span class="cmti-10x-x-109">. Then, </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub> <span class="cmti-10x-x-109">is</span></p>
<p><span class="cmti-10x-x-109">(a) non-decreasing (that is, </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">y </span><span class="cmti-10x-x-109">implies </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">y</span>)<span class="cmti-10x-x-109">),</span></p>
<p><span class="cmti-10x-x-109">(b) right-continuous (that is,</span> lim<sub><span class="cmmi-8">x</span><span class="cmsy-8">→</span><span class="cmmi-8">x</span><sub><span class="cmr-6">0</span></sub><span class="cmr-8">+</span></sub><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub>)<span class="cmti-10x-x-109">, or in other words, taking the right limit is interchangeable with </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub><span class="cmti-10x-x-109">),</span></p>
<p><span class="cmti-10x-x-109">(c) and the limits</span></p>
<div class="math-display">
<img src="../media/file1836.png" class="math-display" alt=" lim F (x ) = 0, lim F (x) = 1 x→ −∞ X x→ ∞ X "/>
</div>
<p><span class="cmti-10x-x-109">hold.</span></p>
</div>
<div id="tcolobox-316" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">The proofs are relatively straightforward. </span><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">follows from the fact that if </span><span class="cmmi-10x-x-109">x/span&gt;<span class="cmmi-10x-x-109">y</span><span class="cmss-10x-x-109">, then we have</span> </span></p>
<div class="math-display">
<img src="../media/file1837.png" class="math-display" alt=" ( ) ( ) X −1 (− ∞, x] ⊆ X −1 (− ∞, y]. "/>
</div>
<p><span class="cmss-10x-x-109">In other words, the event </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">is a subset of </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">y</span><span class="cmss-10x-x-109">. Thus, due to the monotonicity of probability measures, we have </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">x</span>) <span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">y</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Here, we need to show that</span> lim<sub><span class="cmmi-8">x</span><span class="cmsy-8">→</span><span class="cmmi-8">x</span><sub><span class="cmr-6">0</span></sub><span class="cmr-8">+</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub>)<span class="cmss-10x-x-109">. For this, note that for any </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">→ </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub> <span class="cmss-10x-x-109">with </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub><span class="cmss-10x-x-109">, the event sequence </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">ω </span><span class="cmsy-10x-x-109">∈ </span>Ω : <span class="cmmi-10x-x-109">X</span>(<span class="cmmi-10x-x-109">ω</span>) <span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is decreasing, and</span> </span></p>
<div class="math-display">
<img src="../media/file1838.png" class="math-display" alt="∩∞ X− 1((− ∞, xn]) = X− 1((− ∞, x0]). n=1 "/>
</div>
<p><span class="cmss-10x-x-109">Because of the upper continuity of probability measures (see </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286009r115"><span class="cmssi-10x-x-109">115</span></a><span class="cmss-10x-x-109">), the right continuity of </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub> <span class="cmss-10x-x-109">follows.</span></p>
<p><span class="cmssi-10x-x-109">(c) </span><span class="cmss-10x-x-109">Again, this follows from the fact that</span></p>
<div class="math-display">
<img src="../media/file1839.png" class="math-display" alt=" ( ) ∩ ∞n=1X −1 (− ∞, n ] = ∅ "/>
</div>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1840.png" class="math-display" alt="∪∞n=1X −1((− ∞, n]) = Ω. "/>
</div>
<p><span class="cmss-10x-x-109">Since </span><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">∅</span>) = 0 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">P</span>(Ω) = 1<span class="cmss-10x-x-109">, the statement follows from the upper and lower continuity of probability measures. (See </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286007r114"><span class="cmssi-10x-x-109">114</span></a> <span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286009r115"><span class="cmssi-10x-x-109">115</span></a><span class="cmss-10x-x-109">.)</span></p>
</div>
</div>
<div class="newtheorem">
<p><span class="head"> <span id="x1-316004r16"></span> <span class="cmbx-10x-x-109">Remark 16.</span> </span><span class="cmbx-10x-x-109">(Alternative definition of CDF-s)</span></p>
<p>In the literature, you can sometimes find that instead of (<a href="ch031.xhtml#x1-315002r88">19.4</a>), the CDF of <span class="cmmi-10x-x-109">X </span>is defined by</p>
<div class="math-display">
<img src="../media/file1841.png" class="math-display" alt="F∗ (x ) := P(X &lt;x), X "/>
</div>
<p>that is, <span class="cmmi-10x-x-109">X/span&gt;<span class="cmmi-10x-x-109">x </span>instead of <span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤ </span><span class="cmmi-10x-x-109">x</span>. This doesn’t change the big picture, but some details are slightly different. For instance, this change makes <span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub> <span class="cmti-10x-x-109">left-continuous </span>instead of right-continuous. These minute details matter if you dig really deep, but in machine learning, we’ll be fine without thinking too much about them. </span></p>
</div>
<p><span class="cmssi-10x-x-109">Theorem </span><a href="ch031.xhtml#x1-316003r125"><span class="cmssi-10x-x-109">125</span></a> <span class="cmss-10x-x-109">is true the</span> <span id="dx1-316005"></span><span class="cmss-10x-x-109">other way around: if you give me a non-decreasing right-continuous function </span><span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">with</span> lim<sub><span class="cmmi-8">x</span><span class="cmsy-8">→−∞</span></sub><span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">x</span>) = 0 <span class="cmss-10x-x-109">and</span> lim<sub><span class="cmmi-8">x</span><span class="cmsy-8">→∞</span></sub><span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">x</span>) = 1<span class="cmss-10x-x-109">, I can construct a random variable such that its distribution function matches </span><span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">.</span></p>
</section>
<section id="cumulative-distribution-functions-for-discrete-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_283"><span class="titlemark"><span class="cmss-10x-x-109">19.3.3 </span></span> <span id="x1-31700023.3.3"></span><span class="cmss-10x-x-109">Cumulative distribution functions for discrete random variables</span></h3>
<p><span class="cmss-10x-x-109">The</span> <span id="dx1-317001"></span><span class="cmss-10x-x-109">discrete and real-valued case is not</span> <span id="dx1-317002"></span><span class="cmss-10x-x-109">entirely disjoint: in fact, discrete random variables have cumulative distribution functions as well. (But not the other way around; that is, real-valued random variables cannot be described with sequences.)</span></p>
<p><span class="cmss-10x-x-109">Say, if </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is a discrete random variable taking the values </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmss-10x-x-109">, then its CDF is</span></p>
<div class="math-display">
<img src="../media/file1842.png" class="math-display" alt=" ∑ FX (x ) = P (X = xi), xi≤x "/>
</div>
<p><span class="cmss-10x-x-109">which is a piecewise continuous function. For example, </span><span class="cmssi-10x-x-109">Figure </span><a href="#"><span class="cmssi-10x-x-109">19.10</span></a> <span class="cmss-10x-x-109">illustrates the CDF of the binomial distribution.</span></p>
<div class="minipage">
<p><img src="../media/file1843.png" width="369" alt="PIC"/> <span id="x1-317003r10"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.10: The CDF of</span> Binomial(10<span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">.</span>5) </span>
</div>
<p><span class="cmss-10x-x-109">The strength or probability lies in its ability to translate real-world</span><span id="dx1-317004"></span> <span class="cmss-10x-x-109">phenomena</span> <span id="dx1-317005"></span><span class="cmss-10x-x-109">into coin tosses, dice rolls, dart throws, lightbulb lifespans, and many more. This is possible because of distributions. Distributions are the ribbons stringing together a vast bundle of random variables.</span></p>
<p><span class="cmss-10x-x-109">Let’s meet some of the most important ones!</span></p>
</section>
<section id="the-uniform-distribution2" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_284"><span class="titlemark"><span class="cmss-10x-x-109">19.3.4 </span></span> <span id="x1-31800023.3.4"></span><span class="cmss-10x-x-109">The uniform distribution</span></h3>
<p><span class="cmss-10x-x-109">We have already seen a special case of the uniform distribution: selecting a random number from the interval</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, such that</span><span id="dx1-318001"></span> <span class="cmss-10x-x-109">all outcomes are “equally likely.” The general uniform distribution captures the same concept, except on an arbitrary interval</span> [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">for any </span><span class="cmmi-10x-x-109">a/span&gt;<span class="cmmi-10x-x-109">b</span><span class="cmss-10x-x-109">. That is, the random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is uniformly distributed on the interval</span> [<span class="cmmi-10x-x-109">a,b</span>]<span class="cmss-10x-x-109">, or </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼</span> Uniform(<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">in symbols, if</span> </span></p>
<div class="math-display">
<img src="../media/file1844.png" class="math-display" alt=" | | P (α &lt;X ≤ β ) = -1--||[a,b]∩(α,β ]|| b− a "/>
</div>
<p><span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">α &lt;β</span><span class="cmss-10x-x-109">, where </span><span class="big">|</span>[<span class="cmmi-10x-x-109">c,d</span>]<span class="big">|</span> <span class="cmss-10x-x-109">denotes the length of the interval </span>[<span class="cmmi-10x-x-109">c,d</span>]<span class="cmss-10x-x-109">,</span></p>
<p><span class="cmss-10x-x-109">In other words, the probability of our random number falling into a given interval is proportional to the interval’s length. This is how the condition “equally likely” makes sense: as there are uncountably many possible outcomes, the probability of each individual outcome is zero, but equally long intervals have an equal chance.</span></p>
<p><span class="cmss-10x-x-109">In line</span> <span id="dx1-318002"></span><span class="cmss-10x-x-109">with the definition, the distribution function of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is</span></p>
<div class="math-display">
<img src="../media/file1847.png" class="math-display" alt=" ( |||| 0 if x ≤ a, { FX (x) = | xb−−aa- if a &lt;x ≤ b, |||( 1 otherwise. "/>
</div>
</section>
<section id="the-exponential-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_285"><span class="titlemark"><span class="cmss-10x-x-109">19.3.5 </span></span> <span id="x1-31900023.3.5"></span><span class="cmss-10x-x-109">The exponential distribution</span></h3>
<p><span class="cmss-10x-x-109">Let’s turn our</span> <span id="dx1-319001"></span><span class="cmss-10x-x-109">attention toward a different problem: lightbulbs. According to some mysterious (and probably totally inaccurate) lore, lightbulbs possess the so-called memoryless property. That is, their expected lifespan is the same at </span><span class="cmssi-10x-x-109">any </span><span class="cmss-10x-x-109">point in their life.</span></p>
<p><span class="cmss-10x-x-109">To put this into a mathematical form, let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">be a random variable denoting the lifespan of a given lightbulb. The memoryless property states that if the lightbulb has already lasted </span><span class="cmmi-10x-x-109">s </span><span class="cmss-10x-x-109">seconds, then the probability of lasting another </span><span class="cmmi-10x-x-109">t </span><span class="cmss-10x-x-109">is the same as in the very first moment of its life. That is,</span></p>
<div class="math-display">
<img src="../media/file1848.png" class="math-display" alt="P (X &gt;t + s | X &gt;s) = P(X &gt;t). "/>
</div>
<p><span class="cmss-10x-x-109">Expanding the left side, we have</span></p>
<div class="math-display">
<img src="../media/file1849.png" class="math-display" alt=" P (X &gt;t+ s,X &gt;s) P (X &gt;t + s | X &gt;s) =------------------- P(X &gt;s) P-(X--&gt;t+-s)- = P(X &gt; s) , "/>
</div>
<p><span class="cmss-10x-x-109">as </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X/span&gt;<span class="cmmi-10x-x-109">t </span>+ <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">}∩{</span><span class="cmmi-10x-x-109">X/span&gt;<span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">} </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X/span&gt;<span class="cmmi-10x-x-109">t </span>+ <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. Thus, the memoryless property implies that</span> </span></span></span></p>
<div class="math-display">
  <span>
    P(X &gt; t + s) = P(X &gt; t) P(X &gt; s).
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(19.6)</span>
</div>

<p><span class="cmss-10x-x-109">If we think about the probabilities as a function </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">t</span>) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X/span&gt;<span class="cmmi-10x-x-109">t</span>)<span class="cmss-10x-x-109">, (</span><a href="ch031.xhtml#the-exponential-distribution"><span class="cmss-10x-x-109">19.6</span></a><span class="cmss-10x-x-109">) can be viewed as a functional equation. And a famous one at that. Without going into the painful details, the only continuous solution is the exponential function </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">t</span>) = <span class="cmmi-10x-x-109">e</span><sup><span class="cmmi-8">at</span></sup><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">is a parameter.</span> </span></p>
<p><span class="cmss-10x-x-109">As we are talking about the lifespan of a lightbulb here, the probability of it lasting forever is zero. That is,</span></p>
<div class="math-display">
<img src="../media/file1850.png" class="math-display" alt="lim P (X &gt;t) = 0 t→ ∞ "/>
</div>
<p><span class="cmss-10x-x-109">holds. Thus, as</span></p>
<div class="math-display">
<img src="../media/file1851.png" class="math-display" alt=" ( || ||{ 0 if a &lt;0, lim eat = 1 if a = 0, t→ ∞ ||| |( ∞ if a &gt;0, "/>
</div>
<p><span class="cmss-10x-x-109">only the negative</span> <span id="dx1-319002"></span><span class="cmss-10x-x-109">parameters are valid in our case. This characterizes the exponential distribution. In general, </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼</span> exp(<span class="cmmi-10x-x-109">λ</span>) <span class="cmss-10x-x-109">for a </span><span class="cmmi-10x-x-109">λ/span&gt;0 <span class="cmss-10x-x-109">if</span> </span></p>
<div class="math-display">
<img src="../media/file1852.png" class="math-display" alt=" ( |{ 0 if x &lt;0, FX (x) = |( 1− e−λx if x ≥ 0. "/>
</div>
<p><span class="cmss-10x-x-109">Let’s plot this for visualization!</span></p>
<div id="tcolobox-317" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats import expon 
X = np.linspace(-0.5, 10, 100) 
params = [0.1, 1, 10] 
ys = [expon.cdf(X, scale=1/l) for l in params] 
 
with plt.style.context(’seaborn-v0_8’): 
    plt.figure(figsize=(10, 5)) 
 
    for l, y in zip(params, ys): 
        plt.plot(X, y, label=f/span&gt;lambda = {l}" 
 
    plt.title("/span&gt;The exponential distribution 
    plt.legend() 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1853.png" width="512" alt="PIC"/> <span id="x1-319017r11"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.11: The exponential distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">The exponential distribution is extremely useful and frequently encountered in real-life applications. For instance, it models the requests incoming</span> <span id="dx1-319018"></span><span class="cmss-10x-x-109">to a server, customers standing in a queue, buses arriving at a bus stop, and many more.</span></p>
<p><span class="cmss-10x-x-109">We’ll talk more about special distributions in later chapters, and we’ll add quite a few others as well.</span></p>
</section>
<section id="the-normal-distribution" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_286"><span class="titlemark"><span class="cmss-10x-x-109">19.3.6 </span></span> <span id="x1-32000023.3.6"></span><span class="cmss-10x-x-109">The normal distribution</span></h3>
<p><span class="cmss-10x-x-109">You have</span> <span id="dx1-320001"></span><span class="cmss-10x-x-109">probably seen the bell curve at one point in your life, as it is used to describe a wide range of statistical phenomena. Salaries, prices, height, intelligence: they all seem to follow the same symmetric bell-shaped distribution.</span></p>
<p><span class="cmss-10x-x-109">This is described by the famous normal distribution: we say that </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is normally distributed, or </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼𝒩</span>(<span class="cmmi-10x-x-109">μ,σ</span><sup><span class="cmr-8">2</span></sup>)<span class="cmss-10x-x-109">, if</span></p>
<div class="math-display">
<img src="../media/file1854.png" class="math-display" alt=" 1 ∫ x (t−-μ)2 FX (x ) =--√--- e− 2σ2 dt, σ 2 π −∞ "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">μ,σ </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">. The parameter </span><span class="cmmi-10x-x-109">μ </span><span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">mean </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, while </span><span class="cmmi-10x-x-109">σ</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">is its </span><span class="cmssi-10x-x-109">variance </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">σ </span><span class="cmss-10x-x-109">is its </span><span class="cmssi-10x-x-109">standard deviation</span><span class="cmss-10x-x-109">. (We’ll see more about these quantities when talking about the expected value and variance in </span><span class="cmssi-10x-x-109">Chapter </span><a href="ch032.xhtml#the-expected-value"><span class="cmssi-10x-x-109">20</span></a><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">Let’s see the plot the inner part first, which you know as the famous bell curves:</span></p>
<pre class="lstinputlisting"><code>from scipy.stats import norm
X = np.linspace(-10, 10, 1000)
sigmas = [0.5, 1, 2, 3]
ys = [norm.pdf(X, scale=sigma) for sigma in sigmas]


with plt.style.context('seaborn-v0_8'):
    plt.figure(figsize=(10, 5))
    
    for sigma, y in zip(sigmas, ys):
        plt.plot(X, y, label=f"sigma = {sigma}")
    
    plt.title("The bell curves")
    plt.savefig("bell_curve.png", dpi=300)
    plt.legend()
    plt.show()</code></pre>
<div class="minipage">
<p><img src="../media/file1857.png" width="569" alt="PIC"/> <span id="x1-320018r12"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.12: The bell curves</span> </span>
</div>
<p><span class="cmss-10x-x-109">Surprisingly, no</span> <span id="dx1-320019"></span><span class="cmss-10x-x-109">closed expression exists for its CDF</span></p>
<div class="math-display">
<img src="../media/file1858.png" class="math-display" alt=" 1 ∫ x − (t−-μ)2 FX (x ) =--√--- e 2σ2 dt. σ 2 π −∞ "/>
</div>
<p><span class="cmss-10x-x-109">No, it’s not that mathematicians were not smart enough to figure it out; it provably doesn’t exist. In the ancient days, statisticians used to read out its values from statistical tables, located in massive tomes.</span></p>
<p><span class="cmss-10x-x-109">Now, let’s plot </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub><span class="cmss-10x-x-109">:</span></p>
<div id="tcolobox-318" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>X = np.linspace(-10, 10, 1000) 
sigmas = [0.5, 1, 2, 3] 
ys = [norm.cdf(X, scale=sigma) for sigma in sigmas] 
 
 
with plt.style.context(’seaborn-v0_8’): 
    plt.figure(figsize=(10, 5)) 
 
    for sigma, y in zip(sigmas, ys): 
        plt.plot(X, y, label=f/span&gt;sigma = {sigma}" 
 
    plt.title("/span&gt;The normal distribution 
    plt.legend() 
 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1859.png" width="569" alt="PIC"/> <span id="x1-320035r13"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.13: The normal distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">Normal distribution</span><span id="dx1-320036"></span> <span class="cmss-10x-x-109">is the single most important one in statistics, and we’ll see it appearing everywhere, not just in practice but in theory as well.</span></p>
<p><span class="cmss-10x-x-109">To sum up, distributions are the lifeblood of probability theory, and distributions can be represented with cumulative distribution functions.</span></p>
<p><span class="cmss-10x-x-109">However, CDFs have a significant drawback: it’s hard to express the probability of more complex events with them. Later, we’ll see several concrete examples of where CDFs fail.</span></p>
<p><span class="cmss-10x-x-109">Without going into details, one example points toward multidimensional distributions. (I hope that their existence and importance are not surprising to you.) There, the distribution functions can be used to express the probability of rectangle-shaped events, but not, say, spheres.</span></p>
<p><span class="cmss-10x-x-109">To be a bit more precise, if </span><span class="cmmi-10x-x-109">X,Y </span><span class="cmsy-10x-x-109">∼</span> Uniform(0<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">, then the probability</span></p>
<img src="../media/file1860.png" class="math-display" width="150" alt="P(X2 + Y 2 &lt;1) "/>

<p><span class="cmss-10x-x-109">cannot be directly expressed in terms of the two-dimensional CDF </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X,Y</span></sub> (<span class="cmmi-10x-x-109">x,y</span>) <span class="cmss-10x-x-109">(whatever that may be). Fortunately, this is not our only tool. Recall the </span><span class="cmmi-10x-x-109">e</span><sup><span class="cmsy-8">−</span><img src="../media/file1861.png" class="frac" data-align="middle" alt="(x−2μσ2)2"/></sup> <span class="cmss-10x-x-109">part inside the CDF of the normal distribution? This is a special instance of </span><span class="cmssi-10x-x-109">density functions</span><span class="cmss-10x-x-109">, which is what we’ll learn about in the next section.</span></p>
</section>
</section>
<section id="density-functions" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_287"><span class="titlemark"><span class="cmss-10x-x-109">19.4 </span></span> <span id="x1-32100023.4"></span><span class="cmss-10x-x-109">Density functions</span></h2>
<p><span class="cmss-10x-x-109">Distribution functions</span> <span id="dx1-321001"></span><span class="cmss-10x-x-109">are not our only tool to describe real-valued random variables. If you have studied probability theory from a book/lecture/course written by a non-mathematician, you have probably seen a function such as</span></p>
<div class="math-display">
<img src="../media/file1862.png" class="math-display" alt=" √-1--− x22 p(x) = 2π e "/>
</div>
<p><span class="cmss-10x-x-109">referred to as “probability” at some point. Let me tell you, this is definitely </span><span class="cmssi-10x-x-109">not </span><span class="cmss-10x-x-109">a probability. I have seen this mistake so much that I decided to write short X/Twitter threads properly explaining probabilistic concepts, from which this book was grown out of. So, I take this issue to heart.</span></p>
<p><span class="cmss-10x-x-109">Here is the problem with cumulative distribution functions: they represent global information about local objects. Let’s unpack this idea. If </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is a real-valued random variable, the CDF</span></p>
<div class="math-display">
<img src="../media/file1863.png" class="math-display" alt="FX (x) = P(X ≤ x) "/>
</div>
<p><span class="cmss-10x-x-109">describes the probability of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">being smaller than a given </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">. But what if we are interested in what happens </span><span class="cmssi-10x-x-109">around </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">? Say, in the case of the uniform distribution (</span><a href="#"><span class="cmss-10x-x-109">19.5</span></a><span class="cmss-10x-x-109">), we have</span></p>
<div class="math-display">
<img src="../media/file1864.png" class="math-display" alt="P(X = x) = lim P (x− 𝜀 &lt;X ≤ x) 𝜀→0 = lim (FX (x)− FX (x − 𝜀)) 𝜀→0 = lim 𝜀 𝜀→0 = 0. "/>
</div>
<p><span class="cmss-10x-x-109">(We used </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-286009r115"><span class="cmssi-10x-x-109">115</span></a> <span class="cmss-10x-x-109">when taking the limit.)</span></p>
<p><span class="cmss-10x-x-109">Thus, as we have already seen, the probability of picking a particular point is zero. Contrary to the discrete case, </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">tells us nothing about how the distribution of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">behaves around </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">And the worst thing is, this is the same for a wide array of distributions. For instance, you can check it manually for the exponential distribution.</span></p>
<p><span class="cmss-10x-x-109">Isn’t this strange? The probability of individual outcomes is zero for both the uniform and exponential distribution, yet the distributions themselves couldn’t be more different. Let’s examine the problem from another perspective. By definition,</span></p>
<div class="math-display">
<img src="../media/file1865.png" class="math-display" alt="P(a &lt;X ≤ b) = FX (b)− FX (a) "/>
</div>
<p><span class="cmss-10x-x-109">holds. Does this look familiar to you? Increments of </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub> <span class="cmss-10x-x-109">on the right, probabilities on the left. Where have we seen </span><span class="cmssi-10x-x-109">increments </span><span class="cmss-10x-x-109">before?</span></p>
<p><span class="cmss-10x-x-109">In the</span> <span id="dx1-321002"></span><span class="cmss-10x-x-109">fundamental theorem of calculus (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch022.xhtml#x1-235004r92"><span class="cmssi-10x-x-109">92</span></a><span class="cmss-10x-x-109">), that’s where. That is, if </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub> <span class="cmss-10x-x-109">is differentiable and its derivative is </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub><sup><span class="cmsy-8">′</span></sup>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">, then</span></p>
<div class="math-display">
  <span>
    ∫<sub>a</sub><sup>b</sup> f<sub>X</sub>(x) dx = F<sub>X</sub>(b) − F<sub>X</sub>(a).
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(19.7)</span>
</div>

<p><span class="cmss-10x-x-109">The function </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">seems to be what we are looking for: it represents the local behavior of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">around </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">. But instead of describing the</span> <span id="dx1-321003"></span><span class="cmss-10x-x-109">probability, it describes its rate of change. This is called a </span><span class="cmssi-10x-x-109">probability density function</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">By turning this argument around, we can </span><span class="cmssi-10x-x-109">define </span><span class="cmss-10x-x-109">density functions using (</span><a href="ch031.xhtml#density-functions"><span class="cmss-10x-x-109">19.7</span></a><span class="cmss-10x-x-109">). Here is the mathematically precise version.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-321004r89"></span> <span class="cmbx-10x-x-109">Definition 89.</span> </span><span class="cmbx-10x-x-109">(Density functions)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space, and <span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be a real-valued random variable. The function <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub> : <span class="msbm-10x-x-109">ℝ </span><span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>is called the <span class="cmti-10x-x-109">probability density function</span> (PDF) of <span class="cmmi-10x-x-109">X</span>, if it is integrable, and</p>
<div class="math-display">
  <span>
    ∫<sub>a</sub><sup>b</sup> f<sub>X</sub>(x) dx = F<sub>X</sub>(b) − F<sub>X</sub>(a)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(19.8)</span>
</div>

<p>holds for all <span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Again, (</span><a href="ch031.xhtml#x1-321004r89"><span class="cmss-10x-x-109">19.8</span></a><span class="cmss-10x-x-109">) is the Newton-Leibniz formula (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch022.xhtml#x1-235004r92"><span class="cmssi-10x-x-109">92</span></a><span class="cmss-10x-x-109">) in disguise.</span></p>
<p><span class="cmss-10x-x-109">The following theorem makes this connection precise.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-321005r126"></span> <span class="cmbx-10x-x-109">Theorem 126.</span> </span><span class="cmbxti-10x-x-109">(The density function as derivative)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmti-10x-x-109">be a real-valued random variable. If the cumulative distribution function </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmti-10x-x-109">is everywhere differentiable, then</span></p>
<div class="math-display">
<img src="../media/file1866.png" class="math-display" alt="fX(x) = -d-FX (x ) dx "/>
</div>
<p><span class="cmti-10x-x-109">is a density function for </span><span class="cmmi-10x-x-109">X</span><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-319" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This is just a simple application of the fundamental theorem of calculus (</span><span class="cmssi-10x-x-109">Theorem </span><a href="ch022.xhtml#x1-235004r92"><span class="cmssi-10x-x-109">92</span></a><span class="cmss-10x-x-109">). If the derivative indeed exists, then</span></p>
<div class="math-display">
<img src="../media/file1867.png" class="math-display" alt="∫ b-d- a dx FX (x )dx = FX (b)− FX (a), "/>
</div>
<p><span class="cmss-10x-x-109">which means that </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) = <img src="../media/file1868.png" width="15" data-align="middle" alt="d- dx"/><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">is indeed a density function.</span></p>
</div>
</div>
<div class="newtheorem">
<p><span class="head"> <span id="x1-321006r17"></span> <span class="cmbx-10x-x-109">Remark 17.</span> </span><span class="cmbx-10x-x-109">(Density functions are not unique)</span></p>
<p>Note that density functions are <span class="cmti-10x-x-109">not </span>unique. If <span class="cmmi-10x-x-109">X </span>is a random variable with density <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>, then, say, modifying <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub> at a single point still functions as a density function for <span class="cmmi-10x-x-109">X</span>.</p>
<p>To be more precise, define</p>
<div class="math-display">
<img src="../media/file1869.png" class="math-display" alt=" ( |{ fX(x) if x ⁄= 0, f∗X(x) = |( fX(0)+ 1 if x = 0. "/>
</div>
<p>You can check by hand that <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub><sup><span class="cmsy-8">∗</span></sup> is still a density for <span class="cmmi-10x-x-109">X</span>, yet <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub><span class="cmmi-10x-x-109">≠f</span><sub><span class="cmmi-8">X</span></sub><sup><span class="cmsy-8">∗</span></sup>.</p>
</div>
<p><span class="cmss-10x-x-109">One</span> <span id="dx1-321007"></span><span class="cmss-10x-x-109">more thing before we move on. Recall that discrete random variables are characterized by probability mass functions (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-306003r85"><span class="cmssi-10x-x-109">85</span></a><span class="cmss-10x-x-109">). Mass functions and densities are two sides of the same coin.</span></p>
<p><span class="cmss-10x-x-109">The probability mass function is analogous to the density function, yet we don’t have terminology for random variables with the latter. We’ll fix this now.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-321008r90"></span> <span class="cmbx-10x-x-109">Definition 90.</span> </span><span class="cmbx-10x-x-109">(Continuous random variables)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space, and <span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be a real-valued random variable. We say that <span class="cmmi-10x-x-109">X </span>is <span class="cmti-10x-x-109">continuous </span>if it has a probability density function.</p>
</div>
<p><span class="cmss-10x-x-109">Discrete and continuous random variables are the backbones of probability theory: the most interesting random variables are falling into either of these two classes. (Later in the chapter, we’ll see that there are more types, but these two are the most important.)</span></p>
<p><span class="cmss-10x-x-109">Now we are ready to get our hands dirty and see some density functions in practice.</span></p>
<section id="density-functions-in-practice" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_288"><span class="titlemark"><span class="cmss-10x-x-109">19.4.1 </span></span> <span id="x1-32200023.4.1"></span><span class="cmss-10x-x-109">Density functions in practice</span></h3>
<p><span class="cmss-10x-x-109">After all</span> <span id="dx1-322001"></span><span class="cmss-10x-x-109">this introduction, let’s see a few concrete examples. So far, we have seen two real-valued non-discrete distributions: uniform and exponential.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">Let’s start with </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼</span> Uniform(0<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">. Can we apply </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch031.xhtml#x1-321005r126"><span class="cmssi-10x-x-109">126</span></a> <span class="cmss-10x-x-109">directly? Not without a little snag. Or two, to be more precise.</span></p>
<p><span class="cmss-10x-x-109">Why? Because the distribution function</span></p>
<div class="math-display">
<img src="../media/file1870.png" class="math-display" alt=" (| ||| 0 if x ≤ 0, { FX(x) = | x if 0 &lt;x ≤ 1, |||( 1 if x &gt;1 "/>
</div>
<p><span class="cmss-10x-x-109">is not differentiable at </span><span class="cmmi-10x-x-109">x </span>= 0 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">x </span>= 1<span class="cmss-10x-x-109">. However, it is differentiable everywhere else, and its derivative</span></p>
<div class="math-display">
<img src="../media/file1871.png" class="math-display" alt=" ( || 0 if x &lt;0, ||{ F′X (x ) = 1 if 0 &lt;x &lt;1, ||| |( 0 if x &gt;1 "/>
</div>
<p><span class="cmss-10x-x-109">is indeed a density function. (You can check this by hand.) This density</span><span id="dx1-322002"></span> <span class="cmss-10x-x-109">is patched together from the derivative of </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">on the intervals </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,</span>0)<span class="cmss-10x-x-109">, </span>(0<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">, and</span> (1<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>)<span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1872.png" width="527" alt="PIC"/> <span id="x1-322003r14"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.14: Density function of the uniform distribution on</span> [0<span class="cmmi-10x-x-109">,</span>1] </span>
</div>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">In the case of the exponentially distributed random variable </span><span class="cmmi-10x-x-109">Y </span><span class="cmsy-10x-x-109">∼</span> exp(<span class="cmmi-10x-x-109">λ</span>)<span class="cmss-10x-x-109">, the function</span></p>
<div class="math-display">
<img src="../media/file1873.png" class="math-display" alt=" ( |{0 if x &lt;0, fY(x) = | (λe −λx if x ≥ 0 "/>
</div>
<p><span class="cmss-10x-x-109">is a proper density function, which we obtained by differentiating </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">Y</span></sub> (<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">whenever possible. Again, the density </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">is patched</span> <span id="dx1-322004"></span><span class="cmss-10x-x-109">together from the derivatives on the intervals </span>(<span class="cmsy-10x-x-109">−∞</span><span class="cmmi-10x-x-109">,</span>0) <span class="cmss-10x-x-109">and </span>(0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">∞</span>)<span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1874.png" width="569" alt="PIC"/> <span id="x1-322005r15"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.15: Density function of the</span> exp(1) <span class="cmss-10x-x-109">distribution</span> </span>
</div>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmss-10x-x-109">Now, I am going to turn everything upside down. Let </span><span class="cmmi-10x-x-109">Z </span><span class="cmsy-10x-x-109">∼</span> Bernoulli(1<span class="cmmi-10x-x-109">∕</span>2)<span class="cmss-10x-x-109">, which is a discrete random variable with probability mass function</span></p>
<div class="math-display">
<img src="../media/file1875.png" class="math-display" alt="pZ (0) = pZ(1) = 1, 2 "/>
</div>
<p><span class="cmss-10x-x-109">and cumulative distribution function</span></p>
<div class="math-display">
<img src="../media/file1876.png" class="math-display" alt=" ( || 0 if x &lt;0, ||{ FZ(x) = 1 if 0 ≤ x &lt;1, ||| 2 |( 1 if x ≥ 1. "/>
</div>
<p><span class="cmss-10x-x-109">Like the uniform and exponential distributions, this CDF is also differentiable except for a few points (which are </span>0 <span class="cmss-10x-x-109">and </span>1<span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">Thus, it</span><span id="dx1-322006"></span> <span class="cmss-10x-x-109">is natural to guess that, like before, we can patch its derivatives together to obtain a density function. However, there is a bigger snag: the derivative of </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">Z</span></sub> <span class="cmss-10x-x-109">is zero, at least wherever it exists. It turns out that </span><span class="cmmi-10x-x-109">Z </span><span class="cmssi-10x-x-109">does not </span><span class="cmss-10x-x-109">have a density function at all!</span></p>
<p><span class="cmss-10x-x-109">What’s the issue? I’ll tell you: the </span><span class="cmssi-10x-x-109">jump discontinuities </span><span class="cmss-10x-x-109">of </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">Z</span></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">at </span><span class="cmmi-10x-x-109">x </span>= 0 <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">x </span>= 1<span class="cmss-10x-x-109">. Although the CDFs of the uniform and exponential distributions were not differentiable at finitely many points, they did not include any jump discontinuities.</span></p>
<p><span class="cmss-10x-x-109">We are not going to dive deep into the details, but the gist is: if there is a jump discontinuity in the CDF, the density function does not exist.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-322007r18"></span> <span class="cmbx-10x-x-109">Remark 18.</span> </span><span class="cmbx-10x-x-109">(The non-existence of density despite the lack of jump discontinuities)</span></p>
<p>Unfortunately, the reverse direction of “jump discontinuity in the CDF ⇒ no PDF exists” is not true, I repeat, <span class="cmti-10x-x-109">not true</span>.</p>
<p>We can find random variables whose cumulative distribution functions are continuous, but their density does not exist. One famous example is the Cantor function (<a href="https://en.wikipedia.org/wiki/Cantor_function" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Cantor_function</span></a>), also known as the Devil’s staircase. (Only follow this link if you are brave enough or well-trained in real analysis, which is the same.)</p>
</div>
</section>
<section id="classification-of-realvalued-random-variables" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_289"><span class="titlemark"><span class="cmss-10x-x-109">19.4.2 </span></span> <span id="x1-32300023.4.2"></span><span class="cmss-10x-x-109">Classification of real-valued random variables</span></h3>
<p><span class="cmss-10x-x-109">So far, we</span> <span id="dx1-323001"></span><span class="cmss-10x-x-109">have been focusing on two special kinds of real-valued random variables: discrete random variables (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-301002r80"><span class="cmssi-10x-x-109">80</span></a><span class="cmss-10x-x-109">) and continuous ones (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-321008r90"><span class="cmssi-10x-x-109">90</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">We’ve seen all kinds of objects describing them. Every real-valued random variable has a cumulative distribution function (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-315002r88"><span class="cmssi-10x-x-109">88</span></a><span class="cmss-10x-x-109">), but while discrete ones are characterized by probability mass functions (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-306003r85"><span class="cmssi-10x-x-109">85</span></a><span class="cmss-10x-x-109">), the continuous ones are by density functions (</span><span class="cmssi-10x-x-109">Definition </span><a href="ch031.xhtml#x1-321004r89"><span class="cmssi-10x-x-109">89</span></a><span class="cmss-10x-x-109">).</span></p>
<p><span class="cmss-10x-x-109">Are these two all that’s out there?</span></p>
<p><span class="cmss-10x-x-109">No. There are mixed cases. For instance, consider the following example. We are selecting a random number from</span> [0<span class="cmmi-10x-x-109">,</span>1]<span class="cmss-10x-x-109">, but we add a little twist to the picking process. First, we toss a fair coin, and if it comes up heads, we pick </span>0<span class="cmss-10x-x-109">. Otherwise, we pick uniformly between zero and one.</span></p>
<p><span class="cmss-10x-x-109">To describe this weird process, let’s introduce two random variables: let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">be the final outcome and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">be the outcome of the coin toss. Then, using the conditional version of the law of total probability (see </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch030.xhtml#x1-291003r117"><span class="cmssi-10x-x-109">117</span></a><span class="cmss-10x-x-109">), we have</span></p>
<div class="math-disply">
<img src="../media/file1878.png" width="450" class="math-display" alt="P (X ≤ x) = P (X ≤ x | Y = heads )P (Y = heads) + P (X ≤ x | Y = tails)P (Y = tails). "/>
</div>
<p><span class="cmss-10x-x-109">As</span></p>
<div class="math-display">
<img src="../media/file1879.png" class="math-display" alt=" ( |{ P (X ≤ x | Y = heads) = 0 if x &lt;0, |( 1 if x ≥ 1, "/>
</div>
<p><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">x</span><span class="cmsy-10x-x-109">∣</span><span class="cmmi-10x-x-109">Y </span>= tails) = <span class="cmmi-10x-x-109">F</span><sub><span class="cmr-8">Uniform(0</span><span class="cmmi-8">,</span><span class="cmr-8">1)</span></sub>(<span class="cmmi-10x-x-109">x</span>)<span class="cmss-10x-x-109">, we ultimately have</span></p>
<div class="math-display">
<img src="../media/file1880.png" class="math-display" alt=" (| ||| 0 if x &lt;0, { x+1 FX (x) = | -2- if 0 ≤ x &lt;1, |||( 1 if x ≥ 1. "/>
</div>
<p><span class="cmss-10x-x-109">Ultimately, </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub> <span class="cmss-10x-x-109">is the </span><span class="cmssi-10x-x-109">convex combination </span><span class="cmss-10x-x-109">of two cumulative</span> <span id="dx1-323002"></span><span class="cmss-10x-x-109">distribution functions. (A convex combination is a linear combination where the coefficients are positive and their sum is </span>1<span class="cmss-10x-x-109">.)</span></p>
<div class="minipage">
<p><img src="../media/file1881.png" width="569" alt="PIC"/> <span id="x1-323003r16"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure 19.16: CDF of the mixed distribution </span><span class="cmmi-10x-x-109">X</span> </span>
</div>
<p><span class="cmss-10x-x-109">Thus, the random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is not discrete nor continuous. So, what is it?</span></p>
<p><span class="cmss-10x-x-109">It’s time to add order to chaos! In this section, we are going to provide a complete classification for our real-valued random variables. This is a beautiful, albeit advanced, topic so feel free to skip it on a first read.</span></p>
<p><span class="cmss-10x-x-109">Let’s start at a seemingly distant topic: subsets of </span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">that are so small that they practically vanish. Since </span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">is a one-dimensional object, we are usually talking about </span><span class="cmssi-10x-x-109">length </span><span class="cmss-10x-x-109">here, but let’s forget that terminology and talk about </span><span class="cmssi-10x-x-109">measure </span><span class="cmss-10x-x-109">instead. We’ll denote the measure of a set </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">by </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">A</span>)<span class="cmss-10x-x-109">, whatever that might be.</span></p>
<p><span class="cmss-10x-x-109">We are not going too deep into the details and will keep on using the notion of measure intuitively. For instance, the measure of an interval</span> [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">is </span><span class="cmmi-10x-x-109">λ</span>([<span class="cmmi-10x-x-109">a,b</span>]) = <span class="cmmi-10x-x-109">b</span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">a</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Our</span> <span id="dx1-323004"></span><span class="cmss-10x-x-109">measure </span><span class="cmmi-10x-x-109">λ </span><span class="cmss-10x-x-109">has some fundamental properties, for instance,</span></p>
<ol>
<li><span id="x1-323006x1"><span class="cmmi-10x-x-109">λ</span>(<span class="cmsy-10x-x-109">∅</span>) = 0<span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-323008x2"><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">A</span>) <span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">B</span>) <span class="cmss-10x-x-109">if </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">B</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-323010x3"><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub>) = <span class="cmex-10x-x-109">∑</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">k</span></sub>) <span class="cmss-10x-x-109">if </span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">∩</span><span class="cmmi-10x-x-109">A</span><sub><span class="cmmi-8">j</span></sub> = <span class="cmsy-10x-x-109">∅</span><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">This almost behaves like a probability measure, with one glaring exception: </span><span class="cmmi-10x-x-109">λ</span>(<span class="msbm-10x-x-109">ℝ</span>) = <span class="cmsy-10x-x-109">∞</span><span class="cmss-10x-x-109">. This is not an accident.</span></p>
<p><span class="cmss-10x-x-109">What is the measure of a finite set </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,a</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">? Intuitively, it is zero, and from this example, we’ll conjure up the concept of </span><span class="cmssi-10x-x-109">sets of zero measure</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-323011r127"></span> <span class="cmbx-10x-x-109">Theorem 127.</span> </span><span class="cmbxti-10x-x-109">(Sets of zero measure)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be an arbitrary set. Suppose that for any arbitrarily small </span><span class="cmmi-10x-x-109">𝜀/span&gt;0<span class="cmti-10x-x-109">, there exists a union of intervals </span><span class="cmmi-10x-x-109">E </span>= <span class="cmsy-10x-x-109">∪</span><sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">∞</span></sup>(<span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">,b</span><sub><span class="cmmi-8">i</span></sub>) <span class="cmti-10x-x-109">such that</span> </span></p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">E</span>)<span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">𝜀</span><span class="cmti-10x-x-109">, (b) and </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">E</span><span class="cmti-10x-x-109">,</span> </span></p>
<p><span class="cmti-10x-x-109">then, </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">A</span>) = 0<span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-320" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As </span><span class="cmmi-10x-x-109">A </span><span class="cmsy-10x-x-109">⊆</span><span class="cmmi-10x-x-109">E</span><span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">A</span>) <span class="cmsy-10x-x-109">≤</span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">E</span>) <span class="cmmi-10x-x-109">𝜀</span><span class="cmss-10x-x-109">. This means that </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmmi-10x-x-109">A</span>) <span class="cmss-10x-x-109">is smaller than any positive real number, thus it must be zero.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Let’s see some examples.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmss-10x-x-109">A set of a single element has zero measure. As any </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">can be covered by the interval </span>(<span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">𝜀,a </span>+ <span class="cmmi-10x-x-109">𝜀</span>) <span class="cmss-10x-x-109">for some </span><span class="cmmi-10x-x-109">𝜀/span&gt;0<span class="cmss-10x-x-109">. As </span><span class="cmmi-10x-x-109">λ</span><span class="big">(</span>(<span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">𝜀,a </span>+ <span class="cmmi-10x-x-109">𝜀</span>)<span class="big">)</span> = 2<span class="cmmi-10x-x-109">𝜀</span><span class="cmss-10x-x-109">, the conditions of </span><span class="cmssi-10x-x-109">Theorem </span><a href="ch031.xhtml#x1-323011r127"><span class="cmssi-10x-x-109">127</span></a> <span class="cmss-10x-x-109">apply, thus </span><span class="cmmi-10x-x-109">λ</span>(<span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">}</span>) = 0<span class="cmss-10x-x-109">.</span> </span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmss-10x-x-109">A finite set has zero measure. To see this, let </span><span class="cmmi-10x-x-109">A </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,…,a</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">be our finite set. The system of intervals</span></p>
<div class="math-display">
<img src="../media/file1884.png" class="math-display" alt=" n ( ) E = ⋃ a − -𝜀-,a + -𝜀- , 𝜀 &gt;0 k 2n k 2n k=1 "/>
</div>
<p><span class="cmss-10x-x-109">will do the job, as the intervals are mutually disjoint for a small enough </span><span class="cmmi-10x-x-109">𝜀</span><span class="cmss-10x-x-109">,</span></p>
<p><span class="cmss-10x-x-109">thus</span></p>
<div class="math-display">
<img src="../media/file1885.png" class="math-display" alt=" ∑n ( ( 𝜀 𝜀 )) λ(E) = λ ak − ---,ak +--- k=1 2n 2n ∑n 𝜀 = -- k=1n = 𝜀. "/>
</div>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmss-10x-x-109">A countable set has zero measure. For any </span><span class="cmmi-10x-x-109">A </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,…</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, the system of intervals</span></p>
<div class="math-display">
<img src="../media/file1886.png" class="math-display" alt=" ∞⋃ ( 𝜀 𝜀 ) E = ak − -k+1-,ak +-k+1- , 𝜀 &gt;0 k=1 2 2 "/>
</div>
<p><span class="cmss-10x-x-109">work perfectly, as</span></p>
<div class="math-display">
<img src="../media/file1887.png" class="math-display" alt=" ∞∑ -𝜀- λ (E ) ≤ 2k = 𝜀. k=1 "/>
</div>
<p><span class="cmss-10x-x-109">For instance, as</span> <span id="dx1-323012"></span><span class="cmss-10x-x-109">the set of integers and rational numbers are both countable, </span><span class="cmmi-10x-x-109">λ</span>(<span class="msbm-10x-x-109">ℤ</span>) = <span class="cmmi-10x-x-109">λ</span>(<span class="msbm-10x-x-109">ℚ</span>) = 0<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Overall, sets of zero measure are true to their name: they are small. (They are not necessarily countable though.) Why are these important? We’ll see this in the next section.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-323013r19"></span> <span class="cmbx-10x-x-109">Remark 19.</span> </span><span class="cmbx-10x-x-109">(Density functions are not unique, take two)</span></p>
<p>Do you recall <span class="cmti-10x-x-109">Remark </span><a href="ch031.xhtml#x1-321006r17"><span class="cmti-10x-x-109">17</span></a>, where we saw that changing the density function of <span class="cmmi-10x-x-109">X </span>at a single point is also a density for <span class="cmmi-10x-x-109">X</span>?</p>
<p>Turns out that you can actually modify <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub> at an entire set of measure zero. Say,</p>
<div class="math-display">
<img src="../media/file1888.png" class="math-display" alt=" ( | ∗ { fX (x) if x∈∕ℚ, fX (x) = |( 0 if x ∈ ℚ "/>
</div>
<p>is still a density function for <span class="cmmi-10x-x-109">X</span>. Unfortunately, we don’t have the tools to show this, as it would require moving beyond the good old Riemann integral, which is way beyond our scope.</p>
</div>
<p><span class="cmss-10x-x-109">The main difference between a discrete and a continuous random variable is the set where they live. Fundamentally, they are both real-valued random variables, but the range of a discrete variable is a set of measure zero.</span></p>
<p><span class="cmss-10x-x-109">Let’s introduce the concept of </span><span class="cmssi-10x-x-109">singular random variables </span><span class="cmss-10x-x-109">to make this notion precise.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-323014r91"></span> <span class="cmbx-10x-x-109">Definition 91.</span> </span><span class="cmbx-10x-x-109">(Singular random variables)</span></p>
<p>Let (Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) be a probability space and <span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span>be a real-valued random variable. We say that <span class="cmmi-10x-x-109">X </span>is <span class="cmti-10x-x-109">singular </span>if its range <span class="cmmi-10x-x-109">X</span>(Ω) = <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X</span>(<span class="cmmi-10x-x-109">ω</span>) : <span class="cmmi-10x-x-109">ω </span><span class="cmsy-10x-x-109">∈ </span>Ω<span class="cmsy-10x-x-109">}⊆</span><span class="msbm-10x-x-109">ℝ</span> is a set of zero measure, that is,</p>
<div class="math-displa">
<img src="../media/file1889.png" width="150" class="math-display" alt=" ( ) λ X (Ω ) = 0 "/>
</div>
<p>holds.</p>
</div>
<p><span class="cmss-10x-x-109">All discrete random variables are singular, but not the other way around. For instance, the Cantor function (</span><a href="https://en.wikipedia.org/wiki/Cantor_function" class="url"><span class="cmtt-10x-x-109">https://en.wikipedia.org/wiki/Cantor_function</span></a><span class="cmss-10x-x-109">) is a good example.</span></p>
<p><span class="cmss-10x-x-109">Why are</span> <span id="dx1-323015"></span><span class="cmss-10x-x-109">singular random variables so special? Because every distribution can be written as the sum of a singular and a continuous one! Here is the famous Lebesgue decomposition theorem.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-323016r128"></span> <span class="cmbx-10x-x-109">Theorem 128.</span> </span><span class="cmbxti-10x-x-109">(Lebesgue’s decomposition theorem)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Ω<span class="cmmi-10x-x-109">,</span>Σ<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and </span><span class="cmmi-10x-x-109">X </span>: Ω <span class="cmsy-10x-x-109">→</span><span class="msbm-10x-x-109">ℝ </span><span class="cmti-10x-x-109">be a real-valued random variable. Then, there exists a singular random variable </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">s</span></sub> <span class="cmti-10x-x-109">and a continuous random variable </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">c</span></sub> <span class="cmti-10x-x-109">such that</span></p>
<div class="math-display">
<img src="../media/file1890.png" class="math-display" alt="FX = αFXs + βFXc, "/>
</div>
<p><span class="cmti-10x-x-109">where </span><span class="cmmi-10x-x-109">α </span>+ <span class="cmmi-10x-x-109">β </span>= 1<span class="cmti-10x-x-109">, and </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub><span class="cmti-10x-x-109">, </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span><sub><span class="cmmi-6">s</span></sub></sub><span class="cmti-10x-x-109">, </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span><sub><span class="cmmi-6">c</span></sub></sub> <span class="cmti-10x-x-109">are the corresponding cumulative distribution functions.</span></p>
</div>
<p><span class="cmss-10x-x-109">We are not going to prove this here but the gist is this: there are singular random variables, continuous ones, and their sum.</span></p>
</section>
</section>
<section id="summary18" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_290"><span class="titlemark"><span class="cmss-10x-x-109">19.5 </span></span> <span id="x1-32400023.5"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">With the introduction of random variables, we learned to represent abstract probability spaces as random variables, mapping a sufficiently expressive collection of events to the real numbers. Instead of </span><span class="cmmi-10x-x-109">σ</span><span class="cmss-10x-x-109">-algebras and probability measures, now we can deal with numbers. As I told you, </span><span class="cmssi-10x-x-109">“The strength or probability lies in its ability to translate real-world phenomena into coin tosses, dice rolls, dart throws, lightbulb lifespans, and many more.”</span></p>
<p><span class="cmss-10x-x-109">Most common random variables come in two forms: discrete or continuous, meaning that either it can be described with a probability mass function</span></p>
<div class="math-display">
<img src="../media/file1891.png" class="math-display" alt="{ } ∞ P (X = xk ) k=1, "/>
</div>
<p><span class="cmss-10x-x-109">or with a density function </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub><span class="cmss-10x-x-109">, satisfying</span></p>
<div class="math-display">
<img src="../media/file1892.png" class="math-display" alt=" ∫ b P(a ≤ X ≤ b) = a fX (x)dx. "/>
</div>
<p><span class="cmss-10x-x-109">Translating experiments to distributions is the secret sauce of probability theory and statistics. For instance, the time between call center calls, bus arrivals, earthquakes, and insurance claims are all modeled with the exponential distribution, a mathematical object we can work with.</span></p>
<p><span class="cmss-10x-x-109">I know that learning takes a lifetime, but we must wrap this book up at some point. There is one more concept left that I want to tell you about: the expected value, enabling us to measure the statistical properties of our distributions. See you in the next chapter!</span></p>
</section>
<section id="problems17" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_291"><span class="titlemark"><span class="cmss-10x-x-109">19.6 </span></span> <span id="x1-32500023.6"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">be two independent random variables, and let </span><span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ </span><span class="cmss-10x-x-109">be two arbitrary constants. Show that </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">b </span><span class="cmss-10x-x-109">are also independent from each other.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">be a continuous random variable. Show that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span>) = 0 <span class="cmss-10x-x-109">for any </span><span class="cmmi-10x-x-109">x </span><span class="cmsy-10x-x-109">∈</span><span class="msbm-10x-x-109">ℝ</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmsy-10x-x-109">∼</span> Binomial(<span class="cmmi-10x-x-109">n,p</span>)<span class="cmss-10x-x-109">. Calculate the probability distribution of </span><span class="cmmi-10x-x-109">X </span>+ <span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">∼</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">be the result of a coin toss. We select a random number </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">from</span> [0<span class="cmmi-10x-x-109">,</span>2] <span class="cmss-10x-x-109">based on the result of the toss: if </span><span class="cmmi-10x-x-109">X </span>= 0<span class="cmss-10x-x-109">, we pick a number from</span> [0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">using the uniform distribution, but if </span><span class="cmmi-10x-x-109">X </span>= 1<span class="cmss-10x-x-109">, we pick a number from</span> [1<span class="cmmi-10x-x-109">,</span>2]<span class="cmss-10x-x-109">, once more using the uniform distribution. Find the cumulative distribution function of </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">. Does </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">have a density function? If yes, find it.</span></p>
</section>
<section id="join-our-community-on-discord19" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_292"><span id="x1-326000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>