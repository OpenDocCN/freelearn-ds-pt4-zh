- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Variables and Distributions
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a probability space to model our experiments and observations is fine
    and all, but in almost all of the cases, we are interested in a quantitative measure
    of the outcome. To give you an example, let’s consider an already familiar situation:
    tossing coins. Suppose that we are tossing a fair coin n times but we are only
    interested in the number of heads. How do we model the probability space this
    time?'
  prefs: []
  type: TYPE_NORMAL
- en: 'By taking things one step at a time; first, we construct an event space by
    enumerating all possible outcomes in a single set, just like we already did in
    Section [18.2.1](ch030.xhtml#event-spaces-and-algebras):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ω = {0,1}n, Σ = 2Ω. ](img/file1731.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the coin is fair, each outcome ω has the probability P(ω) = ![12n](img/file1732.png).
    This probability space (Ω,Σ,P) is nice and simple so far. Using the additivity
    of probability measures (see Definition [77](ch030.xhtml#x1-285008r77)), we can
    calculate the probability of any event. That is, for any A ∈ Σ, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A) = |A|, |Ω| ](img/file1733.png)'
  prefs: []
  type: TYPE_IMG
- en: where j ⋅j denotes the number of elements in a given set.
  prefs: []
  type: TYPE_NORMAL
- en: However, as mentioned, we are only interested in the number of heads. Should
    we just incorporate this information somewhere in the probability space? Sure,
    we could do that, but that would couple the elementary outcomes (that is, a series
    of heads or tails) with the measurements. This can significantly complicate our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of overloading this probability space to directly deal with the desired
    measurements, we can do something much simpler: introduce a function X : Ω →ℕ,
    mapping outcomes to measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: These functions are called random variables, and they are at the very center
    of probability theory and statistics. By collecting data, we are observing random
    variables, and by fitting predictive models, we approximate them using the observations.
    Now that we understand why we need them, we are going to make this notion mathematically
    precise.
  prefs: []
  type: TYPE_NORMAL
- en: 19.1 Random variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hold your horses, though; it’s not that simple. Random variables are hard to
    understand in their general form, so we’ll slow down and focus on special cases,
    taking one step at a time. This is how learning is done most effectively, and
    we’ll follow this path as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s deal with so-called discrete random variables (such as the above example)
    first, real random variables second, and the general case last.
  prefs: []
  type: TYPE_NORMAL
- en: 19.1.1 Discrete random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following our motivating example describing the number of heads in n coin tosses,
    we can create a formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 80\. (Discrete random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space and {x[k]}[k=1]^∞ be an arbitrary sequence
    of real numbers. The function X : Ω → {x[1],x[2],…} is called a discrete random
    variable if the sets'
  prefs: []
  type: TYPE_NORMAL
- en: '![S = {ω ∈ Ω : X (ω) = x } k k ](img/file1734.png)'
  prefs: []
  type: TYPE_IMG
- en: are events for any integer k ∈ℤ (that is, S[k] ∈ Σ).
  prefs: []
  type: TYPE_NORMAL
- en: 'You might ask why we are requiring the sets {ω ∈ Ω : X(ω) = x[k]} to be events.
    It seems like just another technical condition, but this plays an essential role.
    Ultimately, we are defining random variables because we want to measure the probabilities
    of our observations. This condition ensures that we can do this.'
  prefs: []
  type: TYPE_NORMAL
- en: To simplify our notations, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) P (X = xk) := P {ω ∈ Ω : X (ω) = xk} ](img/file1735.png)'
  prefs: []
  type: TYPE_IMG
- en: whenever we talk about these probabilities. Let’s see a concrete example!
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the coin tossing above, our random variable is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![X = number of heads. ](img/file1736.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though we can define X in terms of formulas by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n X (ω) = ωk, ω = (ω1, ...,ωn ) ∈ Ω, k=1 ](img/file1737.png)'
  prefs: []
  type: TYPE_IMG
- en: this is not needed. Often, such a thing is not even possible. Regarding our
    random variables, we are not interested in knowing the entire mapping, only in
    questions such as the probability of k heads among n tosses.
  prefs: []
  type: TYPE_NORMAL
- en: If we record the “timestamps” where the outcome is heads, we can encode each
    ω as a subset of {1,2,…,n}. For instance, if the 1st, 3rd, and 37th tosses are
    heads and the rest are tails, this is {1,3,37}. To calculate the probability of
    k heads, we need to count the number of k-sized subsets for a set of n elements.
    This is given by the binomial coefficient ![(n) k](img/file1738.png). So,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) P (X = k) = n 1-. k 2n ](img/file1739.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ll see this in detail when talking about the binomial distribution, whatever
    it might be. For now, we are ready to generalize our random variables!
  prefs: []
  type: TYPE_NORMAL
- en: 19.1.2 Real-valued random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What if our measurements are not discrete? For instance, suppose that we have
    a class of students in front of us. We are interested in the distribution of their
    body height. So, we pick one student at random and measure their height with our
    shiny new tool, which is capable of measuring height with perfect precision.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, discrete random variables are not enough, but we can define something
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 81\. (Real-valued random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space. The function X : Ω → ℝ is called a random
    variable if the set'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) X −1 (a,b) := {ω ∈ Ω : a <X (ω) <b} ](img/file1740.png)'
  prefs: []
  type: TYPE_IMG
- en: is an event for all a,b ∈ℝ. (That is, X^(−1)((a,b)) ∈ Σ for all a,b ∈ℝ.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s unwrap this definition. First of all, X is a mapping from the event space
    Ω to the set of real numbers ℝ, as illustrated by Figure [19.1](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1743.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.1: A real-valued random variable is a mapping from the event space
    to the set of real numbers'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to the discrete case, we are interested in the probabilities of events
    such as X^(−1)((a,b)). Again, for simplicity, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( −1( )) P (a <X <b) = P X (a,b) . ](img/file1746.png)'
  prefs: []
  type: TYPE_IMG
- en: You can imagine X^(−1)((a,b) as the subset of Ω that is mapped to (a,b). (In
    general, sets of the form X^(−1)(A) are called inverse images.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1747.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.2: Inverse image of an interval'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see an example right away. Suppose that we are throwing darts at a circular
    board on the wall. (For simplicity, assume that we are so good that we always
    hit the board.) As we have seen when discussing σ-algebras in higher dimensions
    (Section [18.2.6](ch030.xhtml#probability-spaces-on-rn)), we can model this by
    selecting
  prefs: []
  type: TYPE_NORMAL
- en: '![Ω = B(0,1) = {x ∈ ℝ2 : ∥x∥ <1} ](img/file1748.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) Σ = ℬ B (0,1) ( n ) = σ {A ∩ B (0,1) : A ∈ ℬ (ℝ ) , ](img/file1749.png)'
  prefs: []
  type: TYPE_IMG
- en: while
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A) = area(A)-= area(A). area(Ω) π ](img/file1750.png)'
  prefs: []
  type: TYPE_IMG
- en: Since dart boards are subdivided into concentric circles, scoring is determined
    by the distance from the center. So, we might as well define our random variable
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![X = distance of the impact point from the center. ](img/file1751.png)'
  prefs: []
  type: TYPE_IMG
- en: X encodes all that we are interested in, in terms of scoring. In general, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( || 0 if r ≤ 0, ||{ P(X <r) = r2 if 0 <r <1, ||| |( 1 otherwise. ](img/file1752.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What if we have more than one measurement? For instance, in the case of the
    famous Iris dataset ([https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set))
    (one that we have seen a few times so far), we have four measurements. Sure, we
    can just define four random variables, but then we cannot take advantage of all
    the heavy machinery we have built so far: linear algebra and multivariate calculus.'
  prefs: []
  type: TYPE_NORMAL
- en: For this, we will take a look at random variables in the general case.
  prefs: []
  type: TYPE_NORMAL
- en: 19.1.3 Random variables in general
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s cut to the chase.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 82\. (Random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω[1],Σ[1],P[1]) be a probability space and let (Ω[2],Σ[2]) be another
    event space Ω[2] with σ-algebra Σ[2]. The function X : Ω[1] → Ω[2] is a random
    variable if, for every E ∈ Σ[2], the set'
  prefs: []
  type: TYPE_NORMAL
- en: '![X −1(E ) := {ω ∈ Ω1 : X (ω) ∈ E } ](img/file1753.png)'
  prefs: []
  type: TYPE_IMG
- en: is a member of Σ[1]. (That is, X^(−1)(E) ∈ Σ[1].)
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical literature, random variables are usually denoted by either capital
    Latin letters such as X,Y , or Greek letters (mostly starting from ξ).
  prefs: []
  type: TYPE_NORMAL
- en: Random variables essentially push probability measures forward from abstract
    probability spaces to more tractable ones. On the event space (Ω[2],Σ[2]), we
    can define a probability measure P[2] by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( −1 ) P2(E ) := P1 X (E) , E ∈ Σ2, ](img/file1754.png)'
  prefs: []
  type: TYPE_IMG
- en: making it possible to transform one probability space to another while keeping
    the underlying probabilistic model intact.
  prefs: []
  type: TYPE_NORMAL
- en: This general case covers all the mathematical objects we are interested in for
    machine learning. Staying with the Iris dataset ( [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)),
    the random variable
  prefs: []
  type: TYPE_NORMAL
- en: '![X : set of iris flowers → ℝ4, iris flower ↦→ (petal width,petal length,sepal
    width,sepal length) ](img/file1755.png)'
  prefs: []
  type: TYPE_IMG
- en: describes the generating distribution for the dataset, while for classification
    tasks, we are interested in approximating the random variable
  prefs: []
  type: TYPE_NORMAL
- en: '![Y : set of iris flowers → {setosa,versicolor,virginica}, iris flower ↦→ class
    label. ](img/file1756.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we will take a deeper look at why random variables are defined this way.
    This will be a bit technical, so feel free to skip it. It won’t adversely affect
    your ability to work with random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 19.1.4 Behind the definition of random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So, random variables are functions, mapping the probability space onto a measurement
    space. The only question is, why are the sets X^(−1)(E) so special? Let’s revisit
    one of our motivating examples: picking a random student and measuring their height.
    We are interested in questions such as the probability of a student having a body
    height between 155 cm and 185 cm. (If you prefer using the imperial metric system,
    then 155 cm is roughly 5.09 feet and 185 cm is around 6.07 feet.) Translating
    this into formulas, we are interested in'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ( )) P(155 ≤ X ≤ 185) = P X − 1[155,185] . ](img/file1757.png)'
  prefs: []
  type: TYPE_IMG
- en: (In the above formula, I wrote the same thing using two different notations.)
  prefs: []
  type: TYPE_NORMAL
- en: So, how is X^(−1)([155,185]) an event? To find this out, let’s look at inverse
    images in general.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 83\. (Inverse image of sets with respect to functions)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : E →H be a function between the two sets E and H, and let A ⊆H be an
    arbitrary set. The inverse image of A with respect to the function f is defined
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '![f− 1(A ) := {x ∈ E : f (x ) ∈ A}. ](img/file1760.png)'
  prefs: []
  type: TYPE_IMG
- en: We like inverse images of sets because they behave nicely under set operations.
  prefs: []
  type: TYPE_NORMAL
- en: This is formalized by the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 121\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let f : E → H be a function between the two sets E and H. For any A[1],A[2],⋅⋅⋅⊆H,
    the following hold:'
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) −1 ⋃∞ ∞⋃ −1 f An = f (An ), n=1 n=1 ](img/file1762.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![f− 1(A1 ∖ A2) = f−1(A1) ∖f− 1(A2 ), ](img/file1763.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ∞ ) ∞ −1 ⋂ ⋂ −1 f An = f (An ). n=1 n=1 ](img/file1764.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. (a) We can easily see this by simply writing out the definitions. That
    is, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) −1 ⋃∞ { ∞ } f An = x ∈ E : f(x) ∈ ∪ n=1An n=1 ∞⋃ { } = x ∈ E : f(x)
    ∈ An n=1 ∞⋃ −1 = f (An ), n=1 ](img/file1765.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show. (If you are not comfortable with working with
    sets, feel free to review Appendix C on introductory set theory.)
  prefs: []
  type: TYPE_NORMAL
- en: (b) This can be done in the same manner as (a).
  prefs: []
  type: TYPE_NORMAL
- en: (c) The De Morgan laws (Theorem [153](ch037.xhtml#x1-377003r153)) imply that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ⋃∞ ) ∞⋂ H ∖ An = (H ∖An ) n=1 n=1 ](img/file1766.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. Combining this with (a) and (b), (c) follows.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? Recall that the Borel sets, our standard σ-algebra on
    real numbers (as seen in Section [18.2.3](ch030.xhtml#algebras-over-real-numbers)),
    is defined by
  prefs: []
  type: TYPE_NORMAL
- en: 'ℬ := σ({(−∞, x] : x ∈ ℝ}) (19.1)'
  prefs: []
  type: TYPE_NORMAL
- en: These contain all the events that we are interested in regarding the measurements.
    Combined with our previous result, we can reveal what is not in plain sight about
    random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 122\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space and X : Ω →ℝ be a random variable, and let
    A ∈ ℬ, where ℬ is the Borel algebra defined by ([23.1.4](#)). Then, X^(−1)(A)
    ∈ Σ.'
  prefs: []
  type: TYPE_NORMAL
- en: That is, we can measure the probability of X^(−1)(A) for any Borel set A. Without
    this, our random variables would not be that useful. To make our notations more
    intuitive, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( −1 ) P (X ∈ A ) := P X (A ) . ](img/file1769.png)'
  prefs: []
  type: TYPE_IMG
- en: In plain English, P(X ∈A) is the probability of our measurement X falling into
    the set A.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what all of this means, let’s see the simple proof!
  prefs: []
  type: TYPE_NORMAL
- en: Proof. This is a simple consequence of the fact that ℬ is the σ-algebra generated
    by sets of the form (−∞,x], and the inverse images behave nicely under set operations
    (as Theorem [121](ch031.xhtml#x1-304003r121) suggests).
  prefs: []
  type: TYPE_NORMAL
- en: 19.1.5 Independence of random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When building probabilistic models of the external world, the assumption of
    independence significantly simplifies the subsequent mathematical analysis. Recall
    that on a probability space (Ω,Σ,P), the events A,B ∈ Σ are independent if
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A ∩B ) = P(A )P(B ), ](img/file1770.png)'
  prefs: []
  type: TYPE_IMG
- en: or equivalently,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (A | B) = P (A ). ](img/file1771.png)'
  prefs: []
  type: TYPE_IMG
- en: In plain English, observing one event doesn’t change our probabilistic belief
    about the other.
  prefs: []
  type: TYPE_NORMAL
- en: Since a random variable X is described by events of the form X^(−1)(E), we can
    generalize the notion of independence to random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 84\. (Independence of random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let X,Y : Ω[1] → Ω[2] be two random variables between the probability space
    (Ω[1],Σ[1],P) and σ-algebra (Ω[2],Σ[2]).'
  prefs: []
  type: TYPE_NORMAL
- en: We say that X and Y are independent if, for every A,B ∈ Σ[2],
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X ∈ A, Y ∈ B ) = P (X ∈ A )P(Y ∈ B ) ](img/file1772.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Again, think about two coin tosses. X[1] describes the first coin toss and X[2]
    describes the other. Since the tosses are independent, no observation of the first
    one reveals any extra information about the second one. This is formalized by
    the definition above.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, to show two dependent random variables, consider the following.
    We’ll roll a six-sided dice, and denote the result as X. After that, we roll with
    X pieces of six-sided dice and denote the sum total of their values as Y . X and
    Y are dependent on each other. For instance, consider that P(X = 1,Y /span>7)
    = 0, but neither P(X = 1) nor P(Y /span>7) are zero.
  prefs: []
  type: TYPE_NORMAL
- en: Independence is an assumption that we often make. When working with sequences
    of random variables represented by X[1],X,2,…, we almost always assume that they
    are independent and identically distributed; that is, i.i.d. random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to work with random variables, it’s time to show
    how to represent them in a compact form.
  prefs: []
  type: TYPE_NORMAL
- en: 19.2 Discrete distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s recap what we have learned so far. In probability theory, our goal is
    to first model real-life scenarios affected by uncertainty and then to analyze
    them using mathematical tools such as calculus.
  prefs: []
  type: TYPE_NORMAL
- en: For the latter purpose, probability spaces are not easy to work with. A probability
    measure is a function defined on an σ-algebra, so we can’t really use calculus
    there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random variables bring us one step closer to the solution, but they can also
    be difficult to work with. Even though a real-valued random variable X : Ω →ℝ
    maps an abstract probablity space to the set of real numbers, there are some complications.
    Ω can be anything, and if you recall, we might not even have a tractable formula
    for X.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if X denotes the lifetime of a lightbulb, we don’t have a formula.
    So, again, we can’t use calculus. However, there is a way to represent the information
    contained by a random variable in a sequence, a vector-scalar function, or a scalar-scalar
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Enter probability distributions and density functions.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a simple experiment, such as tossing a fair coin n times and counting
    the number of heads, denoting it with X. As we have seen before in Definition [80](ch031.xhtml#x1-301002r80),
    X is a discrete random variable with
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ( ) { nk 12n if k = 0,1,...,n, P (X = k) = | ( 0 otherwise. ](img/file1773.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the sequence {P(X = k)}[k=0]^n fully describes the random variable
    X!
  prefs: []
  type: TYPE_NORMAL
- en: Think about it. As our event space is Ω = {0,1,…,n}, any event is of the form
    A = {a[1],a[2],…,a[l]}⊂ Ω for some l ≤n + 1\. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ l ∑ P (X ∈ A ) = P(X = ai), i=1 ](img/file1774.png)'
  prefs: []
  type: TYPE_IMG
- en: where we used the (![σ ](img/file1775.png)-)additivity of probability. The sequence
    {P(X = k)}[k=0]^n is all the information we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a consequence, instead of working with X : Ω →ℕ, we can forget about it
    and use only {P(X = k)}[k=0]^n. Why is this good for us?'
  prefs: []
  type: TYPE_NORMAL
- en: Because sequences are awesome. As opposed to the mysterious random variables,
    we have a lot of tools to work with them. Most importantly, we can represent them
    in a programming language as an array of numbers. We can’t do such a thing with
    pure random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 85\. (Probability mass function)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let X be a real-valued discrete random variable. The function p[X] : ℝ → [0,1]
    defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![p (x) = P(X = x), x ∈ ℝ X ](img/file1776.png)'
  prefs: []
  type: TYPE_IMG
- en: is called the probability mass function (or PMF in short) of the discrete random
    variable X.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a sequence of real numbers defines a discrete distribution if its
    elements are non-negative and it sums up to one.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 86\. (Discrete probability distribution)
  prefs: []
  type: TYPE_NORMAL
- en: Let {p[k]}[k=1]^∞ be a sequence of real numbers. We say that {p[k]} is a discrete
    probability distribution if
  prefs: []
  type: TYPE_NORMAL
- en: (a) p[k] ≥ 0 for all k,
  prefs: []
  type: TYPE_NORMAL
- en: (b) and ∑ [k=1]^∞p[k] = 1.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 12\.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if the random variable assumes finitely many values (such as in our
    coin tossing example before), only finitely many values are nonzero in the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As recently hinted, every discrete random variable X defines the distribution
    {P(X = x[k])}[k=1]^∞, where {x[1],x[2],…} are the possible values that X can take.
    This also holds in the reverse direction: given a discrete distribution p = {p[k]}[k=1]^∞,
    we can construct a random variable X whose probability mass function (PMF) is
    p.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the probability mass function of X is also referred to as its distribution.
    I know, it is a bit confusing, as the word “distribution” is quite overloaded
    in math. You’ll get used to it.
  prefs: []
  type: TYPE_NORMAL
- en: These discrete probability distributions are well suited for performing quantitative
    analysis, as opposed to the base form of random variables. As an additional benefit,
    think about how distributions generalize random variables. No matter whether we
    talk about coin tosses or medical tests, the rate of success is given by the above
    discrete probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to discussing the basic properties of discrete distributions,
    let’s see some examples!
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.1 The Bernoulli distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start the long line of examples with the most basic probability distribution
    possible: the Bernoulli distribution, describing a simple coin-tossing experiment.
    We are tossing a coin having probability p of coming up heads and probability
    1 −p of coming up tails. The experiment is encoded in the random variable X, which
    takes the value 1 if the toss results in heads, 0 otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 1 if the toss results in heads, X = |( 0 otherwise. ](img/file1777.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( || ||{ 1 − p if k = 0, P (X = k) = p if k = 1, ||| |( 0 otherwise. ](img/file1778.png)'
  prefs: []
  type: TYPE_IMG
- en: When a random variable X is distributed according to this, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![X ∼ Bernoulli(p), ](img/file1779.png)'
  prefs: []
  type: TYPE_IMG
- en: where p ∈ [0,1] is a parameter of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 13\. (An alternative form of the Bernoulli distribution)
  prefs: []
  type: TYPE_NORMAL
- en: There is a clever alternative formulation of the Bernoulli distribution that
    gets rid of the if-else definition. As k is either zero or one, P(X = k) can be
    written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ k 1−k P (X = k) = p (1− p) . ](img/file1780.png)'
  prefs: []
  type: TYPE_IMG
- en: Keep this form in mind, as it’ll be extremely useful later down the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to talk about distributions in practice. There are several stats
    packages for Python, but we’ll use the almighty scipy (which is not exactly a
    stats package, but it has an excellent statistical module):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can generate random values using the rvs method of the bernoulli object
    (just like for any other distribution from scipy):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In scipy, the probability mass function is implemented in the pmf method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even visualize the distribution using Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1781.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.3: The Bernoulli distribution'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in the details, feel free to check out the SciPy documentation
    ( [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html))
    for further methods!
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.2 The binomial distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s take our previous coin-tossing example one step further. Suppose that
    we toss the same coin n times, and X denotes the number of heads out of n tosses.
    What is the probability of getting exactly k heads?
  prefs: []
  type: TYPE_NORMAL
- en: Say, n = 5 and k = 3\. For example, the configuration 11010 (where 0 denotes
    tails and 1 denotes heads) has the probability p³(1 −p)², as there are three heads
    and two tails from five independent (Definition [84](ch031.xhtml#x1-305002r84))
    tosses.
  prefs: []
  type: TYPE_NORMAL
- en: How many such configurations are available? Selecting the position of the three
    heads is the same as selecting a three-element subset out of a set of five elements.
    Thus, there are ![(5) 3](img/file1783.png) possibilities. In general, there are
    ![(n) k](img/file1784.png) possibilities for selecting a k-element subset out
    of a set of n elements.
  prefs: []
  type: TYPE_NORMAL
- en: Combining this, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ (n) k n−k P (X = k ) = k p (1− p) if k = 0,1,...,n, |( 0 otherwise.
    ](img/file1785.png)'
  prefs: []
  type: TYPE_IMG
- en: This is called the binomial distribution, one of the most frequently encountered
    ones in probability and statistics. In notation, we write
  prefs: []
  type: TYPE_NORMAL
- en: '![X ∼ Binomial(n,p), ](img/file1786.png)'
  prefs: []
  type: TYPE_IMG
- en: where the n ∈ℕ and p ∈ [0,1] are its two parameters. Let’s visualize the distribution!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1787.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.4: The binomial distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.3 The geometric distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A bit more coin tossing. We toss the same coin until a heads turn up. Let X
    denote the number of tosses needed. With some elementary probabilistic thinking,
    we can deduce that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ k−1 P(X = k) = (1 − p) p if k = 1,2,... |( 0 otherwise. ](img/file1788.png)'
  prefs: []
  type: TYPE_IMG
- en: (Since if heads turn up first for the k-th toss, we tossed k − 1 tails previously.)
    This is called the geometric distribution and is denoted as
  prefs: []
  type: TYPE_NORMAL
- en: '![X ∼ Geo (p), ](img/file1789.png)'
  prefs: []
  type: TYPE_IMG
- en: 'with p ∈ [0,1] being the only parameter. Similarly, we can plot the histograms
    to visualize the distribution family:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1790.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.5: The geometric distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Note that none of the probabilities P(X = k) are zero, but as k grows, they
    become extremely small. (The closer p is to 1, the faster the decay.)
  prefs: []
  type: TYPE_NORMAL
- en: It might not be immediately obvious that ∑ [k=1]^∞(1 −p)^(k−1)p = 1\. To do
    that, we’ll apply a magic trick. (You know. Paraphrasing the famous Arthur C.
    Clarke quote, “Any sufficiently advanced mathematics is indistinguishable from
    magic.”)
  prefs: []
  type: TYPE_NORMAL
- en: In fact, for an arbitrary x ∈ (−1,1), the astounding identity
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(27).png)(19.2)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: This is the famous geometric series. Using ([23.2.3](#)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ ∞ ∑ ∑ k−1 P (X = k) = (1− p ) p k=1 k=1∞ ∑ k = p (1 − p) k=0 = p-----1----
    1 − (1− p) = 1\. ](img/file1792.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the geometric series is one of the most common tricks up a mathematician’s
    sleeve. We’ll use this, for instance, when talking about expected values for certain
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.4 The uniform distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s discard the coin and roll a six-sided dice instead. We’ve seen this before:
    the probability of each outcome is the same, that is,'
  prefs: []
  type: TYPE_NORMAL
- en: '![P(X = 1) = P(X = 2) = ⋅⋅⋅ = P(X = 6) = 1, 6 ](img/file1793.png)'
  prefs: []
  type: TYPE_IMG
- en: where X denotes the outcome of the roll. This is a special instance of the uniform
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, let A = {a[1],a[2],…,a[n]} be a finite set. The discrete random
    variable X : Ω →A is uniformly distributed on A, that is,'
  prefs: []
  type: TYPE_NORMAL
- en: '![X ∼ Uniform(A ), ](img/file1794.png)'
  prefs: []
  type: TYPE_IMG
- en: if
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1- P (X = a1) = P (X = a2) = ⋅⋅⋅ = P(X = an) = n. ](img/file1795.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that A must be a finite set: no discrete uniform distribution exists on
    infinite sets. When we have an uniform distribution on {1,2,…,n}, we often abbreviate
    it as Uniform(n).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the probability mass function for rolling a six-sided dice. Not the
    most exciting one, I know:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1796.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.6: The (discrete) uniform distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.5 The single-point distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve left the simplest one till last: the single-point distribution. For that,
    let a ∈ℝ be an arbitrary real number. We say that the random variable X is distributed
    according to δ(a) if'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 1 if x = a, P (X = x) = |( 0 otherwise. ](img/file1797.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, X assumes a with probability 1\. Their corresponding cumulative distribution
    function is
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| { 1 if x ≥ a, FX (x) = | ( 0 otherwise, ](img/file1798.png)'
  prefs: []
  type: TYPE_IMG
- en: which is a simple step function with a single jump.
  prefs: []
  type: TYPE_NORMAL
- en: Trust me, explicitly naming such a simple distribution is immensely useful.
    There are two main reasons that come to mind. First, the single-point distribution
    often arises as the limit distribution of sequences of random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Second, every discrete distribution can be written in terms of single-point
    distributions. This is not absolutely necessary for you to understand right now,
    but it’ll be essential on a more advanced level.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 14\. (Discrete distributions as the linear combination of single-point
    distributions)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space and let X : Ω →{x[1],x[2],…} be a discrete
    random variable with probability mass function p[i] = P(X = x[i]).'
  prefs: []
  type: TYPE_NORMAL
- en: By introducing the single-point distributions X[i] ∼δ(x[i]), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑∞ FX (x) = piFX (x). i=1 i ](img/file1799.png)'
  prefs: []
  type: TYPE_IMG
- en: This decomposition can be extremely useful.
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.6 Law of total probability, revisited once more
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the help of discrete random variables, we can dress the law of total probability
    (Theorem [117](ch030.xhtml#x1-291003r117)) in new clothes.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 123\. (Law of total probability, discrete random variable version)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space and let A ∈ Σ be an arbitrary event. If
    X : Ω →{x[1],x[2],…}is a discrete random variable, then'
  prefs: []
  type: TYPE_NORMAL
- en: P(A) = ∑[k=1]^∞ P(A | X = x[k]) P(X = x[k]). (19.3)
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof. For any discrete random variable X : Ω → {x[1],x[2],…}, the events {X
    = x[k]} partition the event space: they are mutually disjoint, and their union
    gives Ω. Thus, the law of total probability can be applied, obtaining'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑∞ P(A ) = P(A, X = xk) k=1 ∑∞ = P(A | X = xk)P (X = xk), k=1 ](img/file1800.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to prove.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we can study events in the context of discrete random variables.
    This is extremely useful in practice. (Soon, we’ll see that it’s not only for
    the discrete case.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put ([19.3](ch031.xhtml#x1-312003r123)) to work right away.
  prefs: []
  type: TYPE_NORMAL
- en: 19.2.7 Sums of discrete random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since discrete probability distributions are represented by sequences, we can
    use a wide array of tools from mathematical analysis to work with them. (This
    was the whole reason behind switching random variables to distributions.) As a
    consequence, we can easily describe more complex random variables by constructing
    them from simpler ones.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider rolling two dice, where we are interested in the distribution
    of the sum. So, we can write this as the sum of random variables X[1] and X[2],
    denoting the outcome of the first and second toss, respectively. We know that
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| { 16 if k = 1,2,...,6, P(Xi = k) = | ( 0 otherwise ](img/file1801.png)'
  prefs: []
  type: TYPE_IMG
- en: for i = 1,2\. Using ([19.3](ch031.xhtml#x1-312003r123)) and the fact that the
    two outcomes are independent, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ 6 P (X + X = k) = ∑ P(X + X = k | X = l)P (X = l) 1 2 l=1 1 2 2 2 6 ∑ =
    P(X1 = k − l)P(X2 = l) l=1 ](img/file1802.png)'
  prefs: []
  type: TYPE_IMG
- en: If this looks familiar, it is not an accident.
  prefs: []
  type: TYPE_NORMAL
- en: What you see here is the famous convolution operation in action.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 87\. (Discrete convolution)
  prefs: []
  type: TYPE_NORMAL
- en: Let a = {a[k]}[k=−∞]^∞ and b = {b[k]}[k=−∞]^∞ be two arbitrary sequences. Their
    convolution is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ { ∞∑ } ∞ a∗ b := ak−lbl . l= −∞ k=−∞ ](img/file1803.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the k-th element of the sequence a ∗b is defined by the sum ∑ [l=−∞]^∞a[k−l]b[l].
    This might be hard to imagine, but thinking about the probabilistic interpretation
    makes the definition clear. The random variable X[1] + X[2] can assume the value
    k if X[1] = k −l and X[2] = l, for all possible l ∈ℤ.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 15\. Remark 19.2.4 (Switching up the indices)
  prefs: []
  type: TYPE_NORMAL
- en: Due to symmetry,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑∞ ∑∞ ak−lbl = albk−l. l=−∞ l=−∞ ](img/file1804.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, an alternative definition of a ∗b
  prefs: []
  type: TYPE_NORMAL
- en: '![ { ∑∞ }∞ a ∗b = albk−l . l=− ∞ k= −∞ ](img/file1805.png)'
  prefs: []
  type: TYPE_IMG
- en: This trick is often extremely useful, as when a[k] and b[k] is explicitly given,
    sometimes ∑ [l=−∞]^∞a[l]b[k−l] is simpler to calculate than ∑ [l=−∞]^∞a[k−l]b[l],
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolution is supported by NumPy, so with its help, we can visualize the distribution
    of our X[1] + X[2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1806.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.7: Distribution of the sum of two random variables'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about the general case. The pattern is clear, so we can formulate
    a theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 124\. (Sums of discrete random variables.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If X,Y : Ω → ℤ are both integer-valued random variables, then the distribution
    of X + Y is given by the convolution of the respective distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞ ∑ P (X + Y = k ) = P (X = k − l)P (Y = l), l=−∞ ](img/file1807.png)'
  prefs: []
  type: TYPE_IMG
- en: that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![pX+Y = pX ∗ pY. ](img/file1808.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Proof. The proof is a straightforward application of the law of total probability
    ([19.3](ch031.xhtml#x1-312003r123)):'
  prefs: []
  type: TYPE_NORMAL
- en: P(X + Y = k) = ∑ [l=−∞]^∞P(X + Y = k∣Y = l)P(Y = l) = ∑ [l=−∞]^∞P(X = k −l)P(Y
    = l) = (p[X] ∗p[Y] )(k),
  prefs: []
  type: TYPE_NORMAL
- en: which is what we had to prove.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of random variable sums is the binomial distribution itself.
    Instead of thinking about the number of successes of an experiment out of n independent
    tries, we can model the core experiment as a Bernoulli distribution. That is,
    if X[i] is a Bernoulli(p) distributed random variable describing the success of
    the i-th attempt, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ P (X1 + ⋅⋅⋅ + Xn = k) = P (X1 = i1,...,Xn = in) i1+ ⋅⋅⋅+in=k = ∑ P (X
    = i )...P(X = i) ◟---1----1-◝◜-----n---n◞ i1+ ⋅⋅⋅+in=k X1,...,Xn are independent
    = ∑ pk(1 − p)n− k i1(+ ⋅⋅⋅)+in=k = n pk(1 − p)n−k, k ](img/file1809.png)'
  prefs: []
  type: TYPE_IMG
- en: where the sum ∑ [i[1]+⋅⋅⋅+i[n]=k] traverses all tuples (i[1],…,i[n]) ∈{0,1}^n
    for which i[1] + ⋅⋅⋅ + i[n] = k. (As there are ![(n) k](img/file1812.png) of such
    tuples, we have ∑ [i[1]+⋅⋅⋅+i[n]=k]p^k(1 −p)^(n−k) = ![( ) nk](img/file1814.png)p^k(1
    −p)^(n−k) in the last step.)
  prefs: []
  type: TYPE_NORMAL
- en: 19.3 Real-valued distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have talked about discrete random variables, that is, random variables
    with countably many values. However, not all experiments/observations/measurements
    are like this. For instance, the height of a person is a random variable that
    can assume a continuum of values.
  prefs: []
  type: TYPE_NORMAL
- en: To give a tractable example, let’s pick a number X from [0,1], with each one
    having an “equal chance.” In this context, equal chance means that
  prefs: []
  type: TYPE_NORMAL
- en: '![P (a <X ≤ b) = |b − a|. ](img/file1815.png)'
  prefs: []
  type: TYPE_IMG
- en: Can we describe X with a single real function? As in the discrete case, we can
    try
  prefs: []
  type: TYPE_NORMAL
- en: '![F(x) = P (X = x), ](img/file1816.png)'
  prefs: []
  type: TYPE_IMG
- en: but this wouldn’t work. Why?
  prefs: []
  type: TYPE_NORMAL
- en: Because for each x ∈X, we have P(X = x) = 0\. That is, picking a particular
    number x has zero probability. Instead, we can try F[X](x) = P(X ≤x), which is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ||| 0 if x ≤ 0, |{ FX(x) = x if 0 <x ≤ 1, |||| ( 1 otherwise. ](img/file1817.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can plot this for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1818.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.8: The uniform distribution'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will properly define and study this object in detail
    for all real-valued random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 19.3.1 The cumulative distribution function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What we have seen in our motivating example is an instance of a cumulative distribution
    function, or CDF in short. Let’s jump into the formal definition right away.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 88\. (Cumulative distribution function)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a real-valued random variable. The function defined by
  prefs: []
  type: TYPE_NORMAL
- en: F[X](x) := P(X ≤ x) (19.4)
  prefs: []
  type: TYPE_NORMAL
- en: is called the cumulative distribution function (CDF) of X.
  prefs: []
  type: TYPE_NORMAL
- en: Again, let’s unpack this. Recall that in the definition of real-valued random
    variables (Definition [81](ch031.xhtml#x1-302002r81)), we have used the inverse
    images X^(−1)((a,b)).
  prefs: []
  type: TYPE_NORMAL
- en: Something similar is going on here. P(X ≤x) is the abbreviation for P(X^(−1)((−∞,x])),
    which we are too lazy to write. Similarly to X^(−1)((a,b))), you can visualize
    X^(−1)((−∞,x])) by pulling the interval (−∞,x] back to Ω using the mapping X.
  prefs: []
  type: TYPE_NORMAL
- en: Sets of the form X^(−1)((−∞,x]) are called the level sets of X.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1833.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.9: The level set of a random variable'
  prefs: []
  type: TYPE_NORMAL
- en: According to the Oxford English Dictionary, the word cumulative means “increasing
    or increased in quantity, degree, or force by successive additions.” For discrete
    random variables, using P(X = k) was enough, but since real random variables are
    more nuanced, we have to use the cumulative probabilities P(X ≤x) to meaningfully
    describe them.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we like to work with distribution functions? Because they condense all
    the relevant information about a random variable in a real function.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can express probabilities like
  prefs: []
  type: TYPE_NORMAL
- en: '![P (a <X ≤ b) = F (b) − F (a ). X X ](img/file1834.png)'
  prefs: []
  type: TYPE_IMG
- en: To give an example, let’s revisit the introduction, where we were selecting
    a random number between zero and one. There, the random variable X with CDF
  prefs: []
  type: TYPE_NORMAL
- en: '![L(U,V ) = {f : U → V | f is linear}](img/equation_(28).png)(19.5)'
  prefs: []
  type: TYPE_IMG
- en: is said to be uniformly distributed over [0,1], or X ∼ Uniform(0,1) in short.
    We’ll see a ton of examples later, but keep note of this, as the uniform distribution
    will be our textbook example throughout this section.
  prefs: []
  type: TYPE_NORMAL
- en: 19.3.2 Properties of the distribution function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cumulative distribution functions have three properties that characterize them:
    they are always non-decreasing, right-continuous (whatever that might be), and
    their limits are 0 and 1 toward −∞ and ∞ respectively. You might have guessed
    some of this from the definition, but here is the formal theorem that summarizes
    this.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 125\. (Properties of CDFs)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a real-valued random variable with CDF F[X]. Then, F[X] is
  prefs: []
  type: TYPE_NORMAL
- en: (a) non-decreasing (that is, x ≤y implies F[X](x) ≤F[X](y)),
  prefs: []
  type: TYPE_NORMAL
- en: (b) right-continuous (that is, lim[x→x[0]+]F[X](x) = F[X](x[0]), or in other
    words, taking the right limit is interchangeable with F[X]),
  prefs: []
  type: TYPE_NORMAL
- en: (c) and the limits
  prefs: []
  type: TYPE_NORMAL
- en: '![ lim F (x ) = 0, lim F (x) = 1 x→ −∞ X x→ ∞ X ](img/file1836.png)'
  prefs: []
  type: TYPE_IMG
- en: hold.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. The proofs are relatively straightforward. (a) follows from the fact
    that if x/span>y, then we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) ( ) X −1 (− ∞, x] ⊆ X −1 (− ∞, y]. ](img/file1837.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the event X ≤ x is a subset of X ≤ y. Thus, due to the monotonicity
    of probability measures, we have P(X ≤ x) ≤ P(X ≤y).
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Here, we need to show that lim[x→x[0]+]P(X ≤ x) = P(X ≤ x[0]). For this,
    note that for any x[n] → x[0] with x[n]/span>x[0], the event sequence {ω ∈ Ω :
    X(ω) ≤x[n]} is decreasing, and'
  prefs: []
  type: TYPE_NORMAL
- en: '![∩∞ X− 1((− ∞, xn]) = X− 1((− ∞, x0]). n=1 ](img/file1838.png)'
  prefs: []
  type: TYPE_IMG
- en: Because of the upper continuity of probability measures (see Theorem [115](ch030.xhtml#x1-286009r115)),
    the right continuity of F[X] follows.
  prefs: []
  type: TYPE_NORMAL
- en: (c) Again, this follows from the fact that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) ∩ ∞n=1X −1 (− ∞, n ] = ∅ ](img/file1839.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![∪∞n=1X −1((− ∞, n]) = Ω. ](img/file1840.png)'
  prefs: []
  type: TYPE_IMG
- en: Since P(∅) = 0 and P(Ω) = 1, the statement follows from the upper and lower
    continuity of probability measures. (See Theorem [114](ch030.xhtml#x1-286007r114)
    and Theorem [115](ch030.xhtml#x1-286009r115).)
  prefs: []
  type: TYPE_NORMAL
- en: Remark 16\. (Alternative definition of CDF-s)
  prefs: []
  type: TYPE_NORMAL
- en: In the literature, you can sometimes find that instead of ([19.4](ch031.xhtml#x1-315002r88)),
    the CDF of X is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![F∗ (x ) := P(X <x), X ](img/file1841.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, X/span>x instead of X ≤ x. This doesn’t change the big picture, but
    some details are slightly different. For instance, this change makes F[X] left-continuous
    instead of right-continuous. These minute details matter if you dig really deep,
    but in machine learning, we’ll be fine without thinking too much about them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theorem [125](ch031.xhtml#x1-316003r125) is true the other way around: if you
    give me a non-decreasing right-continuous function F(x) with lim[x→−∞]F(x) = 0
    and lim[x→∞]F(x) = 1, I can construct a random variable such that its distribution
    function matches F(x).'
  prefs: []
  type: TYPE_NORMAL
- en: 19.3.3 Cumulative distribution functions for discrete random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The discrete and real-valued case is not entirely disjoint: in fact, discrete
    random variables have cumulative distribution functions as well. (But not the
    other way around; that is, real-valued random variables cannot be described with
    sequences.)'
  prefs: []
  type: TYPE_NORMAL
- en: Say, if X is a discrete random variable taking the values x[1],x[2],…, then
    its CDF is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ FX (x ) = P (X = xi), xi≤x ](img/file1842.png)'
  prefs: []
  type: TYPE_IMG
- en: which is a piecewise continuous function. For example, Figure [19.10](#) illustrates
    the CDF of the binomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.10: The CDF of Binomial(10,0.5)'
  prefs: []
  type: TYPE_NORMAL
- en: The strength or probability lies in its ability to translate real-world phenomena
    into coin tosses, dice rolls, dart throws, lightbulb lifespans, and many more.
    This is possible because of distributions. Distributions are the ribbons stringing
    together a vast bundle of random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s meet some of the most important ones!
  prefs: []
  type: TYPE_NORMAL
- en: 19.3.4 The uniform distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have already seen a special case of the uniform distribution: selecting
    a random number from the interval [0,1], such that all outcomes are “equally likely.”
    The general uniform distribution captures the same concept, except on an arbitrary
    interval [a,b] for any a/span>b. That is, the random variable X is uniformly distributed
    on the interval [a,b], or X ∼ Uniform(a,b) in symbols, if'
  prefs: []
  type: TYPE_NORMAL
- en: '![ | | P (α <X ≤ β ) = -1--||[a,b]∩(α,β ]|| b− a ](img/file1844.png)'
  prefs: []
  type: TYPE_IMG
- en: for all α <β, where |[c,d]| denotes the length of the interval [c,d],
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the probability of our random number falling into a given interval
    is proportional to the interval’s length. This is how the condition “equally likely”
    makes sense: as there are uncountably many possible outcomes, the probability
    of each individual outcome is zero, but equally long intervals have an equal chance.'
  prefs: []
  type: TYPE_NORMAL
- en: In line with the definition, the distribution function of X is
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |||| 0 if x ≤ a, { FX (x) = | xb−−aa- if a <x ≤ b, |||( 1 otherwise. ](img/file1847.png)'
  prefs: []
  type: TYPE_IMG
- en: 19.3.5 The exponential distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s turn our attention toward a different problem: lightbulbs. According
    to some mysterious (and probably totally inaccurate) lore, lightbulbs possess
    the so-called memoryless property. That is, their expected lifespan is the same
    at any point in their life.'
  prefs: []
  type: TYPE_NORMAL
- en: To put this into a mathematical form, let X be a random variable denoting the
    lifespan of a given lightbulb. The memoryless property states that if the lightbulb
    has already lasted s seconds, then the probability of lasting another t is the
    same as in the very first moment of its life. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X >t + s | X >s) = P(X >t). ](img/file1848.png)'
  prefs: []
  type: TYPE_IMG
- en: Expanding the left side, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ P (X >t+ s,X >s) P (X >t + s | X >s) =------------------- P(X >s) P-(X-->t+-s)-
    = P(X > s) , ](img/file1849.png)'
  prefs: []
  type: TYPE_IMG
- en: as {X/span>t + s}∩{X/span>s} = {X/span>t + s}. Thus, the memoryless property
    implies that
  prefs: []
  type: TYPE_NORMAL
- en: P(X > t + s) = P(X > t) P(X > s). (19.6)
  prefs: []
  type: TYPE_NORMAL
- en: If we think about the probabilities as a function f(t) = P(X/span>t), ([19.6](ch031.xhtml#the-exponential-distribution))
    can be viewed as a functional equation. And a famous one at that. Without going
    into the painful details, the only continuous solution is the exponential function
    f(t) = e^(at), where a ∈ℝ is a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: As we are talking about the lifespan of a lightbulb here, the probability of
    it lasting forever is zero. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![lim P (X >t) = 0 t→ ∞ ](img/file1850.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. Thus, as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( || ||{ 0 if a <0, lim eat = 1 if a = 0, t→ ∞ ||| |( ∞ if a >0, ](img/file1851.png)'
  prefs: []
  type: TYPE_IMG
- en: only the negative parameters are valid in our case. This characterizes the exponential
    distribution. In general, X ∼ exp(λ) for a λ/span>0 if
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ 0 if x <0, FX (x) = |( 1− e−λx if x ≥ 0\. ](img/file1852.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s plot this for visualization!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1853.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.11: The exponential distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The exponential distribution is extremely useful and frequently encountered
    in real-life applications. For instance, it models the requests incoming to a
    server, customers standing in a queue, buses arriving at a bus stop, and many
    more.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll talk more about special distributions in later chapters, and we’ll add
    quite a few others as well.
  prefs: []
  type: TYPE_NORMAL
- en: 19.3.6 The normal distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have probably seen the bell curve at one point in your life, as it is used
    to describe a wide range of statistical phenomena. Salaries, prices, height, intelligence:
    they all seem to follow the same symmetric bell-shaped distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is described by the famous normal distribution: we say that X is normally
    distributed, or X ∼𝒩(μ,σ²), if'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 ∫ x (t−-μ)2 FX (x ) =--√--- e− 2σ2 dt, σ 2 π −∞ ](img/file1854.png)'
  prefs: []
  type: TYPE_IMG
- en: where μ,σ ∈ℝ. The parameter μ is called the mean of X, while σ² is its variance
    and σ is its standard deviation. (We’ll see more about these quantities when talking
    about the expected value and variance in Chapter [20](ch032.xhtml#the-expected-value).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the plot the inner part first, which you know as the famous bell
    curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.12: The bell curves'
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, no closed expression exists for its CDF
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 ∫ x − (t−-μ)2 FX (x ) =--√--- e 2σ2 dt. σ 2 π −∞ ](img/file1858.png)'
  prefs: []
  type: TYPE_IMG
- en: No, it’s not that mathematicians were not smart enough to figure it out; it
    provably doesn’t exist. In the ancient days, statisticians used to read out its
    values from statistical tables, located in massive tomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s plot F[X]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1859.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.13: The normal distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Normal distribution is the single most important one in statistics, and we’ll
    see it appearing everywhere, not just in practice but in theory as well.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, distributions are the lifeblood of probability theory, and distributions
    can be represented with cumulative distribution functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, CDFs have a significant drawback: it’s hard to express the probability
    of more complex events with them. Later, we’ll see several concrete examples of
    where CDFs fail.'
  prefs: []
  type: TYPE_NORMAL
- en: Without going into details, one example points toward multidimensional distributions.
    (I hope that their existence and importance are not surprising to you.) There,
    the distribution functions can be used to express the probability of rectangle-shaped
    events, but not, say, spheres.
  prefs: []
  type: TYPE_NORMAL
- en: To be a bit more precise, if X,Y ∼ Uniform(0,1), then the probability
  prefs: []
  type: TYPE_NORMAL
- en: '![P(X2 + Y 2 <1) ](img/file1860.png)'
  prefs: []
  type: TYPE_IMG
- en: cannot be directly expressed in terms of the two-dimensional CDF F[X,Y] (x,y)
    (whatever that may be). Fortunately, this is not our only tool. Recall the e^(−![(x−2μσ2)2](img/file1861.png))
    part inside the CDF of the normal distribution? This is a special instance of
    density functions, which is what we’ll learn about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 19.4 Density functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distribution functions are not our only tool to describe real-valued random
    variables. If you have studied probability theory from a book/lecture/course written
    by a non-mathematician, you have probably seen a function such as
  prefs: []
  type: TYPE_NORMAL
- en: '![ √-1--− x22 p(x) = 2π e ](img/file1862.png)'
  prefs: []
  type: TYPE_IMG
- en: referred to as “probability” at some point. Let me tell you, this is definitely
    not a probability. I have seen this mistake so much that I decided to write short
    X/Twitter threads properly explaining probabilistic concepts, from which this
    book was grown out of. So, I take this issue to heart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the problem with cumulative distribution functions: they represent
    global information about local objects. Let’s unpack this idea. If X is a real-valued
    random variable, the CDF'
  prefs: []
  type: TYPE_NORMAL
- en: '![FX (x) = P(X ≤ x) ](img/file1863.png)'
  prefs: []
  type: TYPE_IMG
- en: describes the probability of X being smaller than a given x. But what if we
    are interested in what happens around x? Say, in the case of the uniform distribution
    ([19.5](#)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P(X = x) = lim P (x− 𝜀 <X ≤ x) 𝜀→0 = lim (FX (x)− FX (x − 𝜀)) 𝜀→0 = lim 𝜀
    𝜀→0 = 0\. ](img/file1864.png)'
  prefs: []
  type: TYPE_IMG
- en: (We used Theorem [115](ch030.xhtml#x1-286009r115) when taking the limit.)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, as we have already seen, the probability of picking a particular point
    is zero. Contrary to the discrete case, P(X = x) tells us nothing about how the
    distribution of X behaves around x.
  prefs: []
  type: TYPE_NORMAL
- en: And the worst thing is, this is the same for a wide array of distributions.
    For instance, you can check it manually for the exponential distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Isn’t this strange? The probability of individual outcomes is zero for both
    the uniform and exponential distribution, yet the distributions themselves couldn’t
    be more different. Let’s examine the problem from another perspective. By definition,
  prefs: []
  type: TYPE_NORMAL
- en: '![P(a <X ≤ b) = FX (b)− FX (a) ](img/file1865.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. Does this look familiar to you? Increments of F[X] on the right, probabilities
    on the left. Where have we seen increments before?
  prefs: []
  type: TYPE_NORMAL
- en: In the fundamental theorem of calculus (Theorem [92](ch022.xhtml#x1-235004r92)),
    that’s where. That is, if F[X] is differentiable and its derivative is F[X]^′(x)
    = f[X](x), then
  prefs: []
  type: TYPE_NORMAL
- en: ∫[a]^b f[X](x) dx = F[X](b) − F[X](a). (19.7)
  prefs: []
  type: TYPE_NORMAL
- en: 'The function f[X](x) seems to be what we are looking for: it represents the
    local behavior of X around x. But instead of describing the probability, it describes
    its rate of change. This is called a probability density function.'
  prefs: []
  type: TYPE_NORMAL
- en: By turning this argument around, we can define density functions using ([19.7](ch031.xhtml#density-functions)).
    Here is the mathematically precise version.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 89\. (Density functions)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space, and X : Ω →ℝ be a real-valued random variable.
    The function f[X] : ℝ →ℝ is called the probability density function (PDF) of X,
    if it is integrable, and'
  prefs: []
  type: TYPE_NORMAL
- en: ∫[a]^b f[X](x) dx = F[X](b) − F[X](a) (19.8)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all a,b ∈ℝ.
  prefs: []
  type: TYPE_NORMAL
- en: Again, ([19.8](ch031.xhtml#x1-321004r89)) is the Newton-Leibniz formula (Theorem [92](ch022.xhtml#x1-235004r92))
    in disguise.
  prefs: []
  type: TYPE_NORMAL
- en: The following theorem makes this connection precise.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 126\. (The density function as derivative)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a real-valued random variable. If the cumulative distribution function
    F[X](x) is everywhere differentiable, then
  prefs: []
  type: TYPE_NORMAL
- en: '![fX(x) = -d-FX (x ) dx ](img/file1866.png)'
  prefs: []
  type: TYPE_IMG
- en: is a density function for X.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. This is just a simple application of the fundamental theorem of calculus
    (Theorem [92](ch022.xhtml#x1-235004r92)). If the derivative indeed exists, then
  prefs: []
  type: TYPE_NORMAL
- en: '![∫ b-d- a dx FX (x )dx = FX (b)− FX (a), ](img/file1867.png)'
  prefs: []
  type: TYPE_IMG
- en: which means that f[X](x) = ![d- dx](img/file1868.png)F[X](x) is indeed a density
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 17\. (Density functions are not unique)
  prefs: []
  type: TYPE_NORMAL
- en: Note that density functions are not unique. If X is a random variable with density
    f[X], then, say, modifying f[X] at a single point still functions as a density
    function for X.
  prefs: []
  type: TYPE_NORMAL
- en: To be more precise, define
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ fX(x) if x ⁄= 0, f∗X(x) = |( fX(0)+ 1 if x = 0\. ](img/file1869.png)'
  prefs: []
  type: TYPE_IMG
- en: You can check by hand that f[X]^∗ is still a density for X, yet f[X]≠f[X]^∗.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing before we move on. Recall that discrete random variables are
    characterized by probability mass functions (Definition [85](ch031.xhtml#x1-306003r85)).
    Mass functions and densities are two sides of the same coin.
  prefs: []
  type: TYPE_NORMAL
- en: The probability mass function is analogous to the density function, yet we don’t
    have terminology for random variables with the latter. We’ll fix this now.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 90\. (Continuous random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space, and X : Ω →ℝ be a real-valued random variable.
    We say that X is continuous if it has a probability density function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discrete and continuous random variables are the backbones of probability theory:
    the most interesting random variables are falling into either of these two classes.
    (Later in the chapter, we’ll see that there are more types, but these two are
    the most important.)'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to get our hands dirty and see some density functions in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 19.4.1 Density functions in practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After all this introduction, let’s see a few concrete examples. So far, we
    have seen two real-valued non-discrete distributions: uniform and exponential.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. Let’s start with X ∼ Uniform(0,1). Can we apply Theorem [126](ch031.xhtml#x1-321005r126)
    directly? Not without a little snag. Or two, to be more precise.
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because the distribution function
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ||| 0 if x ≤ 0, { FX(x) = | x if 0 <x ≤ 1, |||( 1 if x >1 ](img/file1870.png)'
  prefs: []
  type: TYPE_IMG
- en: is not differentiable at x = 0 and x = 1\. However, it is differentiable everywhere
    else, and its derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( || 0 if x <0, ||{ F′X (x ) = 1 if 0 <x <1, ||| |( 0 if x >1 ](img/file1871.png)'
  prefs: []
  type: TYPE_IMG
- en: is indeed a density function. (You can check this by hand.) This density is
    patched together from the derivative of F[X](x) on the intervals (−∞,0), (0,1),
    and (1,∞).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1872.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.14: Density function of the uniform distribution on [0,1]'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. In the case of the exponentially distributed random variable Y ∼
    exp(λ), the function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{0 if x <0, fY(x) = | (λe −λx if x ≥ 0 ](img/file1873.png)'
  prefs: []
  type: TYPE_IMG
- en: is a proper density function, which we obtained by differentiating F[Y] (x)
    whenever possible. Again, the density f[X](x) is patched together from the derivatives
    on the intervals (−∞,0) and (0,∞).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1874.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.15: Density function of the exp(1) distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. Now, I am going to turn everything upside down. Let Z ∼ Bernoulli(1∕2),
    which is a discrete random variable with probability mass function
  prefs: []
  type: TYPE_NORMAL
- en: '![pZ (0) = pZ(1) = 1, 2 ](img/file1875.png)'
  prefs: []
  type: TYPE_IMG
- en: and cumulative distribution function
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( || 0 if x <0, ||{ FZ(x) = 1 if 0 ≤ x <1, ||| 2 |( 1 if x ≥ 1\. ](img/file1876.png)'
  prefs: []
  type: TYPE_IMG
- en: Like the uniform and exponential distributions, this CDF is also differentiable
    except for a few points (which are 0 and 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, it is natural to guess that, like before, we can patch its derivatives
    together to obtain a density function. However, there is a bigger snag: the derivative
    of F[Z] is zero, at least wherever it exists. It turns out that Z does not have
    a density function at all!'
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s the issue? I’ll tell you: the jump discontinuities of F[Z](x) at x =
    0 and x = 1\. Although the CDFs of the uniform and exponential distributions were
    not differentiable at finitely many points, they did not include any jump discontinuities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to dive deep into the details, but the gist is: if there is
    a jump discontinuity in the CDF, the density function does not exist.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 18\. (The non-existence of density despite the lack of jump discontinuities)
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the reverse direction of “jump discontinuity in the CDF ⇒ no
    PDF exists” is not true, I repeat, not true.
  prefs: []
  type: TYPE_NORMAL
- en: We can find random variables whose cumulative distribution functions are continuous,
    but their density does not exist. One famous example is the Cantor function ([https://en.wikipedia.org/wiki/Cantor_function](https://en.wikipedia.org/wiki/Cantor_function)),
    also known as the Devil’s staircase. (Only follow this link if you are brave enough
    or well-trained in real analysis, which is the same.)
  prefs: []
  type: TYPE_NORMAL
- en: 19.4.2 Classification of real-valued random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we have been focusing on two special kinds of real-valued random variables:
    discrete random variables (Definition [80](ch031.xhtml#x1-301002r80)) and continuous
    ones (Definition [90](ch031.xhtml#x1-321008r90)).'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen all kinds of objects describing them. Every real-valued random variable
    has a cumulative distribution function (Definition [88](ch031.xhtml#x1-315002r88)),
    but while discrete ones are characterized by probability mass functions (Definition [85](ch031.xhtml#x1-306003r85)),
    the continuous ones are by density functions (Definition [89](ch031.xhtml#x1-321004r89)).
  prefs: []
  type: TYPE_NORMAL
- en: Are these two all that’s out there?
  prefs: []
  type: TYPE_NORMAL
- en: No. There are mixed cases. For instance, consider the following example. We
    are selecting a random number from [0,1], but we add a little twist to the picking
    process. First, we toss a fair coin, and if it comes up heads, we pick 0\. Otherwise,
    we pick uniformly between zero and one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To describe this weird process, let’s introduce two random variables: let X
    be the final outcome and Y be the outcome of the coin toss. Then, using the conditional
    version of the law of total probability (see Theorem [117](ch030.xhtml#x1-291003r117)),
    we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X ≤ x) = P (X ≤ x | Y = heads )P (Y = heads) + P (X ≤ x | Y = tails)P
    (Y = tails). ](img/file1878.png)'
  prefs: []
  type: TYPE_IMG
- en: As
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ P (X ≤ x | Y = heads) = 0 if x <0, |( 1 if x ≥ 1, ](img/file1879.png)'
  prefs: []
  type: TYPE_IMG
- en: and P(X ≤x∣Y = tails) = F[Uniform(0,1)](x), we ultimately have
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| ||| 0 if x <0, { x+1 FX (x) = | -2- if 0 ≤ x <1, |||( 1 if x ≥ 1\. ](img/file1880.png)'
  prefs: []
  type: TYPE_IMG
- en: Ultimately, F[X] is the convex combination of two cumulative distribution functions.
    (A convex combination is a linear combination where the coefficients are positive
    and their sum is 1.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1881.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.16: CDF of the mixed distribution X'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the random variable X is not discrete nor continuous. So, what is it?
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to add order to chaos! In this section, we are going to provide a
    complete classification for our real-valued random variables. This is a beautiful,
    albeit advanced, topic so feel free to skip it on a first read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start at a seemingly distant topic: subsets of ℝ that are so small that
    they practically vanish. Since ℝ is a one-dimensional object, we are usually talking
    about length here, but let’s forget that terminology and talk about measure instead.
    We’ll denote the measure of a set A ⊆ℝ by λ(A), whatever that might be.'
  prefs: []
  type: TYPE_NORMAL
- en: We are not going too deep into the details and will keep on using the notion
    of measure intuitively. For instance, the measure of an interval [a,b] is λ([a,b])
    = b−a.
  prefs: []
  type: TYPE_NORMAL
- en: Our measure λ has some fundamental properties, for instance,
  prefs: []
  type: TYPE_NORMAL
- en: λ(∅) = 0,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: λ(A) ≤λ(B) if A ⊆B,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and λ(∪[k=1]^∞A[k]) = ∑ [k=1]^∞λ(A[k]) if A[i] ∩A[j] = ∅.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This almost behaves like a probability measure, with one glaring exception:
    λ(ℝ) = ∞. This is not an accident.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the measure of a finite set {a[1],…,a[n]}? Intuitively, it is zero,
    and from this example, we’ll conjure up the concept of sets of zero measure.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 127\. (Sets of zero measure)
  prefs: []
  type: TYPE_NORMAL
- en: Let A ⊆ℝ be an arbitrary set. Suppose that for any arbitrarily small 𝜀/span>0,
    there exists a union of intervals E = ∪[k=1]^∞(a[i],b[i]) such that
  prefs: []
  type: TYPE_NORMAL
- en: (a) λ(E)/span>𝜀, (b) and A ⊆E,
  prefs: []
  type: TYPE_NORMAL
- en: then, λ(A) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As A ⊆E, λ(A) ≤λ(E) 𝜀. This means that λ(A) is smaller than any positive
    real number, thus it must be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. A set of a single element has zero measure. As any {a} can be covered
    by the interval (a−𝜀,a + 𝜀) for some 𝜀/span>0\. As λ((a−𝜀,a + 𝜀)) = 2𝜀, the conditions
    of Theorem [127](ch031.xhtml#x1-323011r127) apply, thus λ({a}) = 0\.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. A finite set has zero measure. To see this, let A = {a[1],…,a[n]}
    be our finite set. The system of intervals
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ( ) E = ⋃ a − -𝜀-,a + -𝜀- , 𝜀 >0 k 2n k 2n k=1 ](img/file1884.png)'
  prefs: []
  type: TYPE_IMG
- en: will do the job, as the intervals are mutually disjoint for a small enough 𝜀,
  prefs: []
  type: TYPE_NORMAL
- en: thus
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ( ( 𝜀 𝜀 )) λ(E) = λ ak − ---,ak +--- k=1 2n 2n ∑n 𝜀 = -- k=1n = 𝜀. ](img/file1885.png)'
  prefs: []
  type: TYPE_IMG
- en: Example 3\. A countable set has zero measure. For any A = {a[1],a[2],…}, the
    system of intervals
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞⋃ ( 𝜀 𝜀 ) E = ak − -k+1-,ak +-k+1- , 𝜀 >0 k=1 2 2 ](img/file1886.png)'
  prefs: []
  type: TYPE_IMG
- en: work perfectly, as
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∞∑ -𝜀- λ (E ) ≤ 2k = 𝜀. k=1 ](img/file1887.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance, as the set of integers and rational numbers are both countable,
    λ(ℤ) = λ(ℚ) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, sets of zero measure are true to their name: they are small. (They
    are not necessarily countable though.) Why are these important? We’ll see this
    in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 19\. (Density functions are not unique, take two)
  prefs: []
  type: TYPE_NORMAL
- en: Do you recall Remark [17](ch031.xhtml#x1-321006r17), where we saw that changing
    the density function of X at a single point is also a density for X?
  prefs: []
  type: TYPE_NORMAL
- en: Turns out that you can actually modify f[X] at an entire set of measure zero.
    Say,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( | ∗ { fX (x) if x∈∕ℚ, fX (x) = |( 0 if x ∈ ℚ ](img/file1888.png)'
  prefs: []
  type: TYPE_IMG
- en: is still a density function for X. Unfortunately, we don’t have the tools to
    show this, as it would require moving beyond the good old Riemann integral, which
    is way beyond our scope.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between a discrete and a continuous random variable is the
    set where they live. Fundamentally, they are both real-valued random variables,
    but the range of a discrete variable is a set of measure zero.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s introduce the concept of singular random variables to make this notion
    precise.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 91\. (Singular random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space and X : Ω →ℝ be a real-valued random variable.
    We say that X is singular if its range X(Ω) = {X(ω) : ω ∈ Ω}⊆ℝ is a set of zero
    measure, that is,'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( ) λ X (Ω ) = 0 ](img/file1889.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: All discrete random variables are singular, but not the other way around. For
    instance, the Cantor function ([https://en.wikipedia.org/wiki/Cantor_function](https://en.wikipedia.org/wiki/Cantor_function))
    is a good example.
  prefs: []
  type: TYPE_NORMAL
- en: Why are singular random variables so special? Because every distribution can
    be written as the sum of a singular and a continuous one! Here is the famous Lebesgue
    decomposition theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 128\. (Lebesgue’s decomposition theorem)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Ω,Σ,P) be a probability space and X : Ω →ℝ be a real-valued random variable.
    Then, there exists a singular random variable X[s] and a continuous random variable
    X[c] such that'
  prefs: []
  type: TYPE_NORMAL
- en: '![FX = αFXs + βFXc, ](img/file1890.png)'
  prefs: []
  type: TYPE_IMG
- en: where α + β = 1, and F[X], F[X[s]], F[X[c]] are the corresponding cumulative
    distribution functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to prove this here but the gist is this: there are singular
    random variables, continuous ones, and their sum.'
  prefs: []
  type: TYPE_NORMAL
- en: 19.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the introduction of random variables, we learned to represent abstract
    probability spaces as random variables, mapping a sufficiently expressive collection
    of events to the real numbers. Instead of σ-algebras and probability measures,
    now we can deal with numbers. As I told you, “The strength or probability lies
    in its ability to translate real-world phenomena into coin tosses, dice rolls,
    dart throws, lightbulb lifespans, and many more.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Most common random variables come in two forms: discrete or continuous, meaning
    that either it can be described with a probability mass function'
  prefs: []
  type: TYPE_NORMAL
- en: '![{ } ∞ P (X = xk ) k=1, ](img/file1891.png)'
  prefs: []
  type: TYPE_IMG
- en: or with a density function f[X], satisfying
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∫ b P(a ≤ X ≤ b) = a fX (x)dx. ](img/file1892.png)'
  prefs: []
  type: TYPE_IMG
- en: Translating experiments to distributions is the secret sauce of probability
    theory and statistics. For instance, the time between call center calls, bus arrivals,
    earthquakes, and insurance claims are all modeled with the exponential distribution,
    a mathematical object we can work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'I know that learning takes a lifetime, but we must wrap this book up at some
    point. There is one more concept left that I want to tell you about: the expected
    value, enabling us to measure the statistical properties of our distributions.
    See you in the next chapter!'
  prefs: []
  type: TYPE_NORMAL
- en: 19.6 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Let X and Y be two independent random variables, and let a,b ∈ℝ
    be two arbitrary constants. Show that X −a and Y −b are also independent from
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2\. Let X be a continuous random variable. Show that P(X = x) = 0 for
    any x ∈ℝ.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Let X ∼ Bernoulli(p) and Y ∼ Binomial(n,p). Calculate the probability
    distribution of X + Y .
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 4\. Let X ∼ Bernoulli(p) be the result of a coin toss. We select a
    random number Y from [0,2] based on the result of the toss: if X = 0, we pick
    a number from [0,1] using the uniform distribution, but if X = 1, we pick a number
    from [1,2], once more using the uniform distribution. Find the cumulative distribution
    function of Y . Does Y have a density function? If yes, find it.'
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
