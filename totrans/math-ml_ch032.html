<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch032.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="the-expected-value" class="level2 chapterHead">
<h1 class="chapterHead"><span class="titlemark"><span class="cmss-10x-x-109">20</span></span><br/>
<span id="x1-32700024"></span><span class="cmss-10x-x-109">The Expected Value</span></h1>
<p><span class="cmss-10x-x-109">In the last chapter, we learned about probability distributions, the objects that represent probabilistic models as sequences or functions. After all, there is the entire field of calculus to help us deal with functions, so they open up a wide array of mathematical tools.</span></p>
<p><span class="cmss-10x-x-109">However, we might not need all the information available. Sometimes, simple descriptive statistics such as mean, variance, or median suffice. Even in machine learning, loss functions are given in terms of them. For instance, the famous mean-squared error</span></p>
<div class="math-display">
<img src="../media/file1893.png" class="math-display" alt=" n MSE (x,y ) =-1‚àë (f(x )‚àí y )2, x,y ‚àà ‚Ñùn n i i i=1 "/>
</div>
<p><span class="cmss-10x-x-109">is the variance of the prediction error. Deep down, these familiar quantities are rooted in probability theory, and we‚Äôll devote this chapter to learning about them.</span></p>
<section id="discrete-random-variables1" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_293"><span class="titlemark"><span class="cmss-10x-x-109">20.1 </span></span> <span id="x1-32800024.1"></span><span class="cmss-10x-x-109">Discrete random variables</span></h2>
<p><span class="cmss-10x-x-109">Let‚Äôs play a simple game. I toss a coin, and if it comes up heads, you win </span>$1 <span class="cmss-10x-x-109">. If it is tails, you lose </span>$2 <span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Up until now, we</span> <span id="dx1-328001"></span><span class="cmss-10x-x-109">were dealing with questions like the probability of winning. Say, for the coin toss, whether you win or lose, we have</span></p>
<div class="math-display">
<img src="../media/file1894.png" class="math-display" alt="P(heads) = P(tails) = 1. 2 "/>
</div>
<p><span class="cmss-10x-x-109">Despite the equal chances of winning and losing, should you play this game? Let‚Äôs find out.</span></p>
<p><span class="cmss-10x-x-109">After </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">rounds, your earnings can be calculated by the number of heads times </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">1 minus the number of tails times </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">2 . If we divide total earnings by </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">, we obtain your average winnings per round. That is,</span></p>
<div class="math-display">
<img src="../media/file1895.png" class="math-display" alt=" total-winnings- your average winnings = n 1‚ãÖ#heads ‚àí 2‚ãÖ#tails = -------------------- n = 1‚ãÖ #heads ‚àí 2‚ãÖ #tails, n n "/>
</div>
<p><span class="cmss-10x-x-109">where </span>#<span class="cmss-10x-x-109">heads and </span>#<span class="cmss-10x-x-109">tails denote the number of heads and tails respectively.</span></p>
<p><span class="cmss-10x-x-109">Recall the</span> <span id="dx1-328002"></span><span class="cmss-10x-x-109">frequentist interpretation of probability from </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch030.xhtml#how-to-interpret-probability"><span class="cmssi-10x-x-109">18.2.7</span></a><span class="cmss-10x-x-109">? According to our intuition, we should have</span></p>
<div class="math-display">
<img src="../media/file1896.png" class="math-display" alt=" lim #heads- = P(heads) = 1, n‚Üí ‚àû n 2 #tails- 1- lni‚Üím‚àû n = P(tails) = 2. "/>
</div>
<p><span class="cmss-10x-x-109">This means that if you play long enough, your average winnings per round is</span></p>
<div class="math-display">
<img src="../media/file1897.png" class="math-display" alt="your average winnings = 1‚ãÖP (heads)‚àí 2 ‚ãÖP(tails) = ‚àí 1. 2 "/>
</div>
<p><span class="cmss-10x-x-109">So, as you are losing half a dollar per round on average, you definitely shouldn‚Äôt play this game.</span></p>
<p><span class="cmss-10x-x-109">Let‚Äôs formalize this argument with a random variable. Say, if </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">describes your winnings per round, we have</span></p>
<div class="math-display">
<img src="../media/file1898.png" class="math-display" alt=" 1 P(X = 1) = P (X = ‚àí 2) =-, 2 "/>
</div>
<p><span class="cmss-10x-x-109">so the average winnings can be written as</span></p>
<div class="math-display">
<img src="../media/file1899.png" class="math-display" alt="average value of X = 1 ‚ãÖP(X = 1)‚àí 2 ‚ãÖP(X = ‚àí 2) 1 = ‚àí -. 2 "/>
</div>
<p><span class="cmss-10x-x-109">With a bit of a pattern matching, we find that for a general discrete random variable </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, the formula looks like</span></p>
<div class="math-display">
<img src="../media/file1900.png" class="math-display" alt=" ‚àë average value of X = (value)‚ãÖP (X = value). value "/>
</div>
<p><span class="cmss-10x-x-109">And from this, the definition of </span><span class="cmssi-10x-x-109">expected value </span><span class="cmss-10x-x-109">is born.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-328003r92"></span> <span class="cmbx-10x-x-109">Definition 92.</span> </span><span class="cmbx-10x-x-109">(The expected value of discrete random variables)</span></p>
<p>Let (Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) be a probability space, and <span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí{</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶</span><span class="cmsy-10x-x-109">} </span>be a discrete random variable. The <span class="cmti-10x-x-109">expected value </span>of <span class="cmmi-10x-x-109">X </span>is defined by</p>
<div class="math-display">
<img src="../media/file1901.png" class="math-display" alt="ùîº [X ] := ‚àë x P (X = x ). k k k "/>
</div>
<p>(Note that if <span class="cmmi-10x-x-109">X </span>assumes finitely many values, the sum only contains a finite number of terms.)</p>
</div>
<p><span class="cmss-10x-x-109">In English, the</span> <span id="dx1-328004"></span><span class="cmss-10x-x-109">expected value describes the average value of a random variable in the long run. The expected value is also called the </span><span class="cmssi-10x-x-109">mean </span><span class="cmss-10x-x-109">and is often denoted by </span><span class="cmmi-10x-x-109">Œº</span><span class="cmss-10x-x-109">. Instead of using random variables, we‚Äôll often use the expected value symbol by plugging in distributions, like </span><span class="msbm-10x-x-109">ùîº</span>[Bernoulli(<span class="cmmi-10x-x-109">p</span>)]<span class="cmss-10x-x-109">. Although this is mathematically not precise, 1) it is simpler in certain cases, 2) and the expected value only depends on the distribution anyway.</span></p>
<p><span class="cmss-10x-x-109">It‚Äôs time for examples.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmssi-10x-x-109">Expected value of the Bernoulli distribution. </span><span class="cmss-10x-x-109">(See the definition of the Bernoulli distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-bernoulli-distribution"><span class="cmssi-10x-x-109">19.2.1</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">. Its expected value is quite simple to compute, as</span></p>
<div class="math-display">
<img src="../media/file1902.png" class="math-display" alt="ùîº[X] = 0‚ãÖP (X = 0 )+ 1‚ãÖP (X = 1) = = 0‚ãÖ(1 ‚àí p)+ 1 ‚ãÖp = p. "/>
</div>
<p><span class="cmss-10x-x-109">We‚Äôve seen this before: the introductory example with the simple game is the transformed Bernoulli distribution </span>3 <span class="cmsy-10x-x-109">‚ãÖ</span> Bernoulli(1<span class="cmmi-10x-x-109">‚àï</span>2) <span class="cmsy-10x-x-109">‚àí </span>2<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmssi-10x-x-109">Expected value of the binomial distribution. </span><span class="cmss-10x-x-109">(See the definition of the binomial distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-binomial-distribution"><span class="cmssi-10x-x-109">19.2.2</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Binomial(<span class="cmmi-10x-x-109">n,p</span>)<span class="cmss-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file1903.png" class="math-display" alt=" ‚àën ùîº[X] = kP (X = k ) k=0 ‚àën (n ) = k pk(1 ‚àí p)n‚àík k=0 k ‚àën = k----n!--- pk(1 ‚àí p)n‚àík. k=0 k!(n ‚àí k)! "/>
</div>
<p><span class="cmss-10x-x-109">The plan is the following: absorb that </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">with the fraction</span> <img src="../media/file1904.png" class="frac" data-align="middle" alt="--n!--- k!(n‚àík)!"/><span class="cmss-10x-x-109">, and adjust the sum such that its terms form the probability</span> <span id="dx1-328005"></span><span class="cmss-10x-x-109">mass function for</span> Binomial(<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">‚àí </span>1<span class="cmmi-10x-x-109">,p</span>)<span class="cmss-10x-x-109">. As </span><span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">k </span>= (<span class="cmmi-10x-x-109">n </span><span class="cmsy-10x-x-109">‚àí </span>1) <span class="cmsy-10x-x-109">‚àí </span>(<span class="cmmi-10x-x-109">k </span><span class="cmsy-10x-x-109">‚àí </span>1)<span class="cmss-10x-x-109">, we have</span></p>
<div class="math-dispay">
<img src="../media/file1905.png" width="450" class="math-display" alt=" ‚àën ùîº[X ] = k ---n!----pk(1‚àí p)n‚àík k=0 k!(n ‚àí k)! ‚àën = np ---------(n-‚àí-1)!--------pk‚àí1(1 ‚àí p)(n‚àí1)‚àí(k‚àí1) k=1(k ‚àí 1)!((n ‚àí 1)‚àí (k ‚àí 1))! n‚àí1 = np ‚àë ---(n-‚àí-1)!--pk(1‚àí p)(n‚àí1‚àík) k!(n ‚àí 1‚àí k)! k=0 n‚àë‚àí1 = np P (Binomial(n‚àí 1,p) = k) k=0 = np. "/>
</div>
<p><span class="cmss-10x-x-109">This computation might not look like the simplest, but once you get familiar with the trick, it‚Äôll be like second nature for you.</span></p>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmssi-10x-x-109">Expected value of the geometric distribution. </span><span class="cmss-10x-x-109">(See the definition of the geometric distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-geometric-distribution"><span class="cmssi-10x-x-109">19.2.3</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Geo(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">. We need to calculate</span></p>
<div class="math-dislay">
<img src="../media/file1906.png" width="350" class="math-display" alt=" ‚àë‚àû ùîº[X ] = k (1 ‚àí p)k‚àí1p. k=1 "/>
</div>
<p><span class="cmss-10x-x-109">Do you remember the geometric series (</span><a href="#"><span class="cmss-10x-x-109">19.2</span></a><span class="cmss-10x-x-109">)? This is almost it, except for the </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">term, which throws a monkey wrench into our gears. To fix that, we‚Äôll use another magic trick. Recall that</span></p>
<div class="math-display">
<img src="../media/file1907.png" class="math-display" alt=" 1 ‚àë‚àû ----- = xk. 1 ‚àí x k=0 "/>
</div>
<p><span class="cmss-10x-x-109">Now, we are going to </span><span class="cmssi-10x-x-109">differentiate </span><span class="cmss-10x-x-109">the geometric series, thus obtaining</span></p>
<div class="math-display">
<img src="../media/file1908.png" class="math-display" alt=" d 1 d ‚àë‚àû k dx-1-‚àí-x = dx- x k=0 ‚àë‚àû d k = dx-x k=0 ‚àë‚àû k‚àí 1 = kx , k=1 "/>
</div>
<p><span class="cmss-10x-x-109">where we used the linearity of the derivative and the pleasant analytic properties of</span> <span id="dx1-328006"></span><span class="cmss-10x-x-109">the geometric series. Mathematicians would scream upon the sight of switching the derivative and the infinite sum, but don‚Äôt worry, everything here is correct as is. (Mathematicians are really afraid of interchanging limits. Mind you, for a good reason!)</span></p>
<p><span class="cmss-10x-x-109">On the other hand,</span></p>
<div class="math-display">
<img src="../media/file1909.png" class="math-display" alt="d---1--- ---1---- dx 1‚àí x = (1 ‚àí x)2, "/>
</div>
<p><span class="cmss-10x-x-109">thus</span></p>
<div class="math-display">
<img src="../media/file1910.png" class="math-display" alt=" ‚àû ‚àë k‚àí1 ---1---- kx = (1‚àí x )2. k=1 "/>
</div>
<p><span class="cmss-10x-x-109">Combining all of these, we finally have</span></p>
<div class="math-display">
<img src="../media/file1911.png" class="math-display" alt=" ‚àë‚àû ùîº[X ] = k (1 ‚àí p)k‚àí1p k=1 ‚àû‚àë = p k(1‚àí p)k‚àí1 k=1 1 1 = p-2 = -. p p "/>
</div>
<p><span class="cmssbx-10x-x-109">Example 4. </span><span class="cmssi-10x-x-109">Expected value of the constant random variable. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">c </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">be an arbitrary constant, and let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">be the random variable that assumes the value </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">everywhere. As </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is a discrete random variable, its expected value is simply</span></p>
<div class="math-display">
<img src="../media/file1912.png" class="math-display" alt="ùîº[X ] = c‚ãÖP (X = c) = c. "/>
</div>
<p><span class="cmss-10x-x-109">I know, this example looks silly, but it can be quite useful. When it is clear, we abuse the notation by denoting the constant </span><span class="cmmi-10x-x-109">c </span><span class="cmss-10x-x-109">as the random variable itself.</span></p>
<section id="the-expected-value-in-poker" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_294"><span class="titlemark"><span class="cmss-10x-x-109">20.1.1 </span></span> <span id="x1-32900024.1.1"></span><span class="cmss-10x-x-109">The expected value in poker</span></h3>
<p><span class="cmss-10x-x-109">One more example</span> <span id="dx1-329001"></span><span class="cmss-10x-x-109">before we move on. I was a mediocre no-limit Texas hold‚Äôem player a while ago, and the first time I heard about the expected value was years before I studied probability theory.</span></p>
<p><span class="cmss-10x-x-109">According to the rules of Texas hold‚Äôem, each player holds two cards on their own, while five more shared cards are dealt. The shared cards are available for everyone, and the player with the strongest hand wins.</span></p>
<p><span class="cmssi-10x-x-109">Figure¬†</span><a href="#"><span class="cmssi-10x-x-109">20.1</span></a> <span class="cmss-10x-x-109">shows how the table looks before the last card (the river) is revealed.</span></p>
<div class="minipage">
<p><img src="../media/file1913.png" width="484" alt="PIC"/> <span id="x1-329002r1"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.1: The poker table before the river card</span> </span>
</div>
<p><span class="cmss-10x-x-109">There is money in the pot to be won, but to see the river, you have to call the opponent‚Äôs bet. The question is, should you? Expected value to the rescue.</span></p>
<p><span class="cmss-10x-x-109">Let‚Äôs build a probabilistic model. We would win the pot with certain river cards but lose with all the others. If </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">represents our winnings, then</span></p>
<div class="math-display">
<img src="../media/file1914.png" class="math-display" alt=" #winning cards P(X = pot) = ---------------, #remaining cards P (X = ‚àí bet) = --#losing-cards-. #remaining cards "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the expected value is</span></p>
<img src="../media/file1915.png" class="math-display" width="450" alt="ùîº[X] = pot‚ãÖP (X = pot )‚àí bet‚ãÖP (X = ‚àí bet) #winning cards #losing cards = pot‚ãÖ ----------------‚àí bet‚ãÖ----------------. #remaining cards #remaining cards "/>

<p><span class="cmss-10x-x-109">When is the expected value positive? With some algebra, we obtain that </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]<span class="cmmi-10x-x-109">/span&gt;0 <span class="cmss-10x-x-109">if and only if</span> </span></p>
<div class="math-display">
<img src="../media/file1916.png" class="math-display" alt="#winning-cards bet- #losing cards &gt; pot, "/>
</div>
<p><span class="cmss-10x-x-109">which is called </span><span class="cmssi-10x-x-109">positive pot odds</span><span class="cmss-10x-x-109">. If this is satisfied, making the bet is the right call. You might lose a hand with positive pot odds, but in the long term, your winnings will be positive.</span></p>
<p><span class="cmss-10x-x-109">Of course, pot</span> <span id="dx1-329003"></span><span class="cmss-10x-x-109">odds are extremely hard to determine in practice. For instance, you don‚Äôt know</span> <span id="dx1-329004"></span><span class="cmss-10x-x-109">what others hold, and counting the cards that would win the pot for you is not possible unless you have a good read on the opponents. Poker is much more than just math. Good players choose their bet specifically to throw off their opponents‚Äô pot odds.</span></p>
<p><span class="cmss-10x-x-109">Now that we understand the idea behind the expected value, let‚Äôs move on to the general case!</span></p>
</section>
</section>
<section id="continuous-random-variables" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_295"><span class="titlemark"><span class="cmss-10x-x-109">20.2 </span></span> <span id="x1-33000024.2"></span><span class="cmss-10x-x-109">Continuous random variables</span></h2>
<p><span class="cmss-10x-x-109">So far, we</span> <span id="dx1-330001"></span><span class="cmss-10x-x-109">have only defined the expected value for discrete random variables. As </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">describes the average value of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">in the long run, it should exist for continuous random variables as well.</span></p>
<p><span class="cmss-10x-x-109">The interpretation of the expected value was simple: outcome times probability, summed over all potential values. However, there is a snag with continuous random variables: we don‚Äôt have such a mass distribution, as the probabilities of individual outcomes are zero: </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span>) = 0<span class="cmss-10x-x-109">. Moreover, we can‚Äôt sum uncountably many values.</span></p>
<p><span class="cmss-10x-x-109">What can we do?</span></p>
<p><span class="cmss-10x-x-109">Wishful thinking. This is one of the most powerful techniques in mathematics, and I am not joking.</span></p>
<p><span class="cmss-10x-x-109">Here‚Äôs the plan. We‚Äôll pretend that the expected value of a continuous random variable is well-defined, and let our imagination run free. Say goodbye to mathematical precision, and allow our intuition to unfold. Instead of the probability of a given outcome, we can talk about </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">landing in a small interval. First, we divide up the set of real numbers into really small parts. To be more precise, let </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">‚Ä¶/span&gt;<span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">be a granular partition of the real line. If the partition is refined enough, we should have</span> </span></span></span></p>
<div class="math-display">
  <span>
    ùîº[X] ‚âà ‚àë<sub>k=1</sub><sup>n</sup> x<sub>k</sub> P(x<sub>k‚àí1</sub> ‚â§ X ‚â§ x<sub>k</sub>)
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(20.1)</span>
</div>

<p><span class="cmss-10x-x-109">The probabilities in (</span><a href="ch032.xhtml#continuous-random-variables"><span class="cmss-10x-x-109">20.1</span></a><span class="cmss-10x-x-109">) can be expressed in terms of the CDF:</span></p>
<img src="../media/file1917.png" class="math-display" alt=" n n ‚àë x P(x &lt;X ‚â§ X ) = ‚àë x (F (x ) ‚àí F (x )). k k‚àí1 k k X k X k‚àí1 k=1 k=1 " width="450"/>
<p><span class="cmss-10x-x-109">These increments remind us of the difference quotients. We don‚Äôt quite have these inside the sum, but with a ‚Äúfancy multiplication with one,‚Äù we can achieve this:</span></p>

<img src="../media/file1918.png" class="math-display" alt="‚àën ‚àën xk(FX (xk)‚àí FX (xk‚àí1)) = xk(xk ‚àí xk‚àí1)FX-(xk)‚àí-FX-(xk‚àí1). k=1 k=1 xk ‚àí xk‚àí 1 " width="450"/>

<p><span class="cmss-10x-x-109">If</span> <span id="dx1-330002"></span><span class="cmss-10x-x-109">the </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s are close to each other (and we can select them to be arbitrarily close), the difference quotients are close to the derivative of </span><span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">X</span></sub><span class="cmss-10x-x-109">, which is the density function </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub><span class="cmss-10x-x-109">. Thus,</span></p>
<img src="../media/file1919.png" class="math-display" alt="FX (xk)‚àí FX (xk‚àí1) ‚àën FX (xk)‚àí FX (xk‚àí1) ‚àën -----x-‚àí-x-------- ‚âà fX (xk ) xk(xk ‚àí xk‚àí1)-----x-‚àí-x-------- ‚âà xk(xk ‚àí xk ‚àí1)fX(xk). k k‚àí 1 k=1 k k‚àí 1 k=1 " width="650"/>
<p><span class="cmss-10x-x-109">This is a Riemann-sum, defined by (</span><a href="#"><span class="cmss-10x-x-109">14.7</span></a><span class="cmss-10x-x-109">)! Hence, the last sum is close to a Riemann-integral:</span></p>
<div class="math-display">
<img src="../media/file1920.png" class="math-display" alt=" n ‚à´ ‚àë ‚àû xk(xk ‚àí xk‚àí1)fX(xk) ‚âà ‚àí‚àû xfX (x)dx. k=1 "/>
</div>
<p><span class="cmss-10x-x-109">Although we were not exactly precise in our argument, all of the above can be made mathematically correct. (But we are not going to do it here, as it is not relevant to us.) Thus, we finally obtain the formula of the expected value for continuous random variables.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-330003r93"></span> <span class="cmbx-10x-x-109">Definition 93.</span> </span><span class="cmbx-10x-x-109">(The expected value of continuous random variables)</span></p>
<p>Let (Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) be a probability space, and <span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span>be a continuous random variable. The <span class="cmti-10x-x-109">expected value </span>of <span class="cmmi-10x-x-109">X </span>is defined by</p>
<div class="math-display">
<img src="../media/file1921.png" class="math-display" alt=" ‚à´ ‚àû ùîº [X ] := xfX(x)dx. ‚àí‚àû "/>
</div>
</div>
<p><span class="cmss-10x-x-109">As usual, let‚Äôs see some examples first.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmssi-10x-x-109">Expected value of the uniform distribution. </span><span class="cmss-10x-x-109">(See the definition of the uniform distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-uniform-distribution2"><span class="cmssi-10x-x-109">19.3.4</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Uniform(<span class="cmmi-10x-x-109">a,b</span>)<span class="cmss-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file1922.png" class="math-display" alt=" ‚à´ ‚àû 1 ùîº[X ] = x-----dx ‚àí ‚àû b‚à´‚àí a --1-- b = b ‚àí a xdx [ a ]x=b = ---1---x2 2(b ‚àí a) x=a a + b = --2--, "/>
</div>
<p><span class="cmss-10x-x-109">which is the midpoint of the interval</span> [<span class="cmmi-10x-x-109">a,b</span>]<span class="cmss-10x-x-109">, where the</span> Uniform(<span class="cmmi-10x-x-109">a,b</span>) <span class="cmss-10x-x-109">lives.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmssi-10x-x-109">Expected value of the exponential distribution. </span><span class="cmss-10x-x-109">(See the definition of the exponential distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-exponential-distribution"><span class="cmssi-10x-x-109">19.3.5</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> exp(<span class="cmmi-10x-x-109">Œª</span>)<span class="cmss-10x-x-109">. Then, we need to calculate the integral</span></p>
<div class="math-display">
<img src="../media/file1923.png" class="math-display" alt=" ‚à´ ‚àû ‚àí Œªx ùîº[X ] = xŒªe dx. 0 "/>
</div>
<p><span class="cmss-10x-x-109">We can do this via</span> <span id="dx1-330004"></span><span class="cmss-10x-x-109">integration by parts (</span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch022.xhtml#x1-238002r95"><span class="cmssi-10x-x-109">95</span></a><span class="cmss-10x-x-109">): by letting </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">x </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">g</span><sup><span class="cmsy-8">‚Ä≤</span></sup>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">Œªe</span><sup><span class="cmsy-8">‚àí</span><span class="cmmi-8">Œªx</span></sup><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file1924.png" class="math-display" alt=" ‚à´ ‚àû ùîº [X ] = xŒªe‚àíŒªxdx 0 [ ]x=‚àû ‚à´ ‚àû = ‚àí xe‚àíŒªx x=0 + e‚àí Œªxdx ‚óü-----‚óù=‚óú0-----‚óû 0 [ ]x=‚àû = ‚àí 1-e‚àíŒªx Œª x=0 1- = Œª. "/>
</div>
</section>
<section id="properties-of-the-expected-value" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_296"><span class="titlemark"><span class="cmss-10x-x-109">20.3 </span></span> <span id="x1-33100024.3"></span><span class="cmss-10x-x-109">Properties of the expected value</span></h2>
<p><span class="cmss-10x-x-109">As usual, the</span> <span id="dx1-331001"></span><span class="cmss-10x-x-109">expected value has several useful properties. Most importantly, the expected value is linear with respect to the random variable.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-331002r129"></span> <span class="cmbx-10x-x-109">Theorem 129.</span> </span><span class="cmbxti-10x-x-109">(Linearity of the expected value)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, and let </span><span class="cmmi-10x-x-109">X,Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí </span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be two random variables. Moreover, let </span><span class="cmmi-10x-x-109">a,b </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be two scalars. Then</span></p>
<div class="math-display">
<img src="../media/file1925.png" class="math-display" alt="ùîº[aX + bY ] = aùîº [X ]+ bùîº [Y ] "/>
</div>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<p><span class="cmss-10x-x-109">We are not going to prove this theorem here, but know that linearity is an essential tool. Do you recall the game that we used to introduce the expected value for discrete random variables? I toss a coin, and if it comes up heads, you win </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">1. Tails, you lose </span><span class="tcss-1095">$</span><span class="cmss-10x-x-109">2. If you think about it for a minute, this is the</span></p>
<div class="math-display">
<img src="../media/file1926.png" class="math-display" alt="X = 3‚ãÖBernoulli(1‚àï2)‚àí 2 "/>
</div>
<p><span class="cmss-10x-x-109">distribution, and as such,</span></p>
<div class="math-display">
<img src="../media/file1927.png" class="math-display" alt="ùîº[X] = ùîº[3‚ãÖBernoulli(1 ‚àï2)‚àí 2] = 3‚ãÖùîº [Bernoulli(1 ‚àï2)]‚àí 2 = 3‚ãÖ 1‚àí 2 2 1- = ‚àí 2. "/>
</div>
<p><span class="cmss-10x-x-109">Of course, linearity</span> <span id="dx1-331003"></span><span class="cmss-10x-x-109">goes way beyond this simple example. As you‚Äôve gotten used to this already, linearity is a crucial property in mathematics. We love linearity.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-331004r20"></span> <span class="cmbx-10x-x-109">Remark 20.</span> </span></p>
<p>Notice that <span class="cmti-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-331002r129"><span class="cmti-10x-x-109">129</span></a> did not say that <span class="cmmi-10x-x-109">X </span>and <span class="cmmi-10x-x-109">Y </span>have to be both discrete or both continuous. Even though we have only defined the expected value in such cases, there is a general definition that works for <span class="cmti-10x-x-109">all </span>random variables.</p>
<p>The snag is, it requires a familiarity with measure theory, falling way outside of our scope. Suffice to say, the theorem works as is.</p>
</div>
<p><span class="cmss-10x-x-109">If the expected value of a sum is the sum of the expected values, does the same apply to the product? Not in general, but fortunately, this works for independent random variables. (See </span><span class="cmssi-10x-x-109">Definition¬†</span><a href="ch031.xhtml#x1-305002r84"><span class="cmssi-10x-x-109">84</span></a> <span class="cmss-10x-x-109">for the definition of independent random variables.)</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-331005r130"></span> <span class="cmbx-10x-x-109">Theorem 130.</span> </span><span class="cmbxti-10x-x-109">(Expected value of the product of independent random variables)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, and let </span><span class="cmmi-10x-x-109">X,Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be two independent random variables.</span></p>
<p><span class="cmti-10x-x-109">Then</span></p>
<div class="math-display">
<img src="../media/file1928.png" class="math-display" alt="ùîº [XY ] = ùîº [X ]ùîº[Y] "/>
</div>
<p><span class="cmti-10x-x-109">holds.</span></p>
</div>
<p><span class="cmss-10x-x-109">This property is extremely useful, as we‚Äôll see in the next section, where we‚Äôll talk about variance and covariance.</span></p>
<p><span class="cmss-10x-x-109">One more property that‚Äôll help us to calculate the expected value of </span><span class="cmssi-10x-x-109">functions </span><span class="cmss-10x-x-109">of the random variable, such as </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">or</span> sin<span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">:</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-331006r131"></span> <span class="cmbx-10x-x-109">Theorem 131.</span> </span><span class="cmbxti-10x-x-109">(Law of the unconscious statistician)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí </span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be a random variable, and let </span><span class="cmmi-10x-x-109">g </span>: <span class="msbm-10x-x-109">‚Ñù </span><span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be an arbitrary function.</span></p>
<p><span class="cmti-10x-x-109">(a) If </span><span class="cmmi-10x-x-109">X </span><span class="cmti-10x-x-109">is discrete with possible values </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶</span><span class="cmti-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file1929.png" class="math-display" alt=" ‚àë ùîº[g(X)] = g(xn)P (X = xn). n "/>
</div>
<p><span class="cmti-10x-x-109">(b) If </span><span class="cmmi-10x-x-109">X </span><span class="cmti-10x-x-109">is continuous with the probability density function </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span>)<span class="cmti-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file1930.png" class="math-display" alt=" ‚à´ ‚àû ùîº[g(X )] = g(x)f (x)dx. ‚àí‚àû X "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Thus, calculating </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">2</span></sup>] <span class="cmss-10x-x-109">for a continuous random variable</span> <span id="dx1-331007"></span><span class="cmss-10x-x-109">can be done by simply taking</span></p>
<div class="math-display">
<img src="../media/file1931.png" class="math-display" alt=" ‚à´ ‚àû ùîº[X2] = x2fX (x)dx, ‚àí‚àû "/>
</div>
<p><span class="cmss-10x-x-109">which will be used all the time.</span></p>
</section>
<section id="variance" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_297"><span class="titlemark"><span class="cmss-10x-x-109">20.4 </span></span> <span id="x1-33200024.4"></span><span class="cmss-10x-x-109">Variance</span></h2>
<p><span class="cmss-10x-x-109">Plainly speaking, the</span><span id="dx1-332001"></span> <span class="cmss-10x-x-109">expected value measures the average value of the random variable. However, even though both</span> Uniform(<span class="cmsy-10x-x-109">‚àí</span>1<span class="cmmi-10x-x-109">,</span>1) <span class="cmss-10x-x-109">and</span> Uniform(<span class="cmsy-10x-x-109">‚àí</span>100<span class="cmmi-10x-x-109">,</span>100) <span class="cmss-10x-x-109">have zero expected value, the latter is much more spread out than the former. Thus, </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">is not a good descriptor of the random variable </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">To add one more layer, we measure the average deviation from the expected value. This is done via the </span><span class="cmssi-10x-x-109">variance </span><span class="cmss-10x-x-109">and the </span><span class="cmssi-10x-x-109">standard deviation</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-332002r94"></span> <span class="cmbx-10x-x-109">Definition 94.</span> </span><span class="cmbx-10x-x-109">(Variance and standard deviation)</span></p>
<p>Let (Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) be a probability space, let <span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span>be a random variable, and let <span class="cmmi-10x-x-109">Œº </span>= <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] be its expected value. The <span class="cmti-10x-x-109">variance </span>of <span class="cmmi-10x-x-109">X </span>is defined by</p>
<div class="math-display">
<img src="../media/file1932.png" class="math-display" alt=" [ 2] Var [X ] := ùîº (X ‚àí Œº ) , "/>
</div>
<p>while its <span class="cmti-10x-x-109">standard deviation </span>is defined by</p>
<div class="math-display">
<img src="../media/file1933.png" class="math-display" alt="Std[X] := ‚àòVar--[X-]. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Take note that in the literature, the expected value is often denoted by </span><span class="cmmi-10x-x-109">Œº</span><span class="cmss-10x-x-109">, while the standard deviation is denoted by </span><span class="cmmi-10x-x-109">œÉ</span><span class="cmss-10x-x-109">. Together, they form two of the most important descriptors of a random variable.</span></p>
<p><span class="cmssi-10x-x-109">Figure¬†</span><a href="#"><span class="cmssi-10x-x-109">20.2</span></a> <span class="cmss-10x-x-109">shows a visual interpretation of the mean and standard deviation in the case of a normal distribution. The mean shows the average value, while the standard deviation can be interpreted as the average deviation from the mean. (We‚Äôll talk about the normal distribution in detail later, so don‚Äôt worry if it is not yet familiar to you.)</span></p>
<div class="minipage">
<p><img src="../media/file1934.png" width="484" alt="PIC"/> <span id="x1-332003r2"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.2: Mean (</span><span class="cmmi-10x-x-109">Œº</span><span class="cmss-10x-x-109">) and standard deviation </span><img src="../media/file1935.png" class="math" alt="(œÉ) "/> <span class="cmss-10x-x-109">of the standard normal distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">The usual method of calculating variance is not taking the expected</span><span id="dx1-332004"></span> <span class="cmss-10x-x-109">value of</span> (<span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">Œº</span>)<sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, but taking the expected value of </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">and subtracting </span><span class="cmmi-10x-x-109">Œº</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">from it. This is shown by the following proposition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-332005r5"></span> <span class="cmbx-10x-x-109">Proposition 5.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, and let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be a random variable.</span></p>
<p><span class="cmti-10x-x-109">Then</span></p>
<div class="math-display">
<img src="../media/file1936.png" class="math-display" alt="Var[X] = ùîº[X2 ]‚àí ùîº[X ]2. "/>
</div>
</div>
<div id="tcolobox-321" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">Œº </span>= <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]<span class="cmss-10x-x-109">. Because of the linearity of the expected value, we have</span></p>
<div class="math-display">
<img src="../media/file1937.png" class="math-display" alt="Var[X ] = ùîº[(X ‚àí Œº)2] = ùîº[X2 ‚àí 2ŒºX + Œº2] = ùîº[X ]2 ‚àí 2Œºùîº [X ]+ Œº2 2 2 2 = ùîº[X ] ‚àí 2Œº + Œº = ùîº[X2 ]‚àí Œº2 = ùîº[X2 ]‚àí ùîº[X ]2, "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Is the variance linear as well? No, but there are some important identities regarding</span> <span id="dx1-332006"></span><span class="cmss-10x-x-109">scalar multiplication and addition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-332007r132"></span> <span class="cmbx-10x-x-109">Theorem 132.</span> </span><span class="cmbxti-10x-x-109">(Variance and the linear operations)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, and let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be a random variable.</span></p>
<p><span class="cmti-10x-x-109">(a) Let </span><span class="cmmi-10x-x-109">a </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be an arbitrary constant. Then</span></p>
<div class="math-dispay">
<img src="../media/file1938.png" width="150" class="math-display" alt=" 2 Var[aX ] = a Var[X ]. "/>
</div>
<p><span class="cmti-10x-x-109">(b) Let </span><span class="cmmi-10x-x-109">Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be a random variable that is independent from </span><span class="cmmi-10x-x-109">X</span><span class="cmti-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file1939.png" class="math-display" alt="Var[X + Y ] = Var[X ]+ Var[Y]. "/>
</div>
</div>
<div id="tcolobox-322" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. (a) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">Œº</span><sub><span class="cmmi-8">X</span></sub> = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]<span class="cmss-10x-x-109">. Then we have</span></p>
<div class="math-display">
<img src="../media/file1940.png" class="math-display" alt=" [ ] [ ] Var[aX ] = ùîº (aX ‚àí a ŒºX)2 = ùîº a2(X ‚àí ŒºX )2 [ ] = a2ùîº (X ‚àí ŒºX )2 = a2Var[X ], "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show. </span><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">Œº</span><sub><span class="cmmi-8">Y</span></sub> = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">Y</span> ]<span class="cmss-10x-x-109">. Then, due to the linearity of the expected value, we have</span></p>
<img src="../media/file1941.png" class="math-display" alt=" [ 2] Var[X + Y ] = ùîº (X + Y ‚àí (ŒºX + ŒºY )) [ 2] = ùîº ((X ‚àí ŒºX ) + (Y ‚àí ŒºY)) = ùîº [(X ‚àí ŒºX)2]+ 2ùîº[(X ‚àí ŒºX )(Y ‚àí ŒºY)] + ùîº[(Y ‚àí ŒºY )2]. " width="450"/>
<p><span class="cmss-10x-x-109">Now, as </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">are independent, </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">XY</span> ] = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]<span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">Y</span> ]<span class="cmss-10x-x-109">. Thus, due to the linearity of the expected value,</span></p>
<img src="../media/file1942.png" class="math-display" alt=" [ ] [ ] ùîº (X ‚àí ŒºX )(Y ‚àí ŒºY) = ùîº XY ‚àí X ŒºY ‚àí ŒºX Y + ŒºX ŒºY = ùîº[XY ] ‚àí ùîº[X Œº ]‚àí ùîº [Œº Y ]+ Œº Œº [ ] [ ] [Y ] X [ ] X Y = ùîº X ùîº Y ‚àí ùîº X ŒºY ‚àí ŒºX ùîº Y + ŒºX ŒºY = ŒºXŒºY ‚àí ŒºXŒºY ‚àí ŒºXŒºY + ŒºXŒºY = 0. " width="450"/>

<p><span class="cmss-10x-x-109">Thus, continuing the first calculation,</span></p>

<img src="../media/file1943.png" class="math-display" alt="Var[X + Y ] = ùîº [(X ‚àí Œº )2]+ 2ùîº[(X ‚àí Œº )(Y ‚àí Œº )] + ùîº[(Y ‚àí Œº )2] X X Y Y = ùîº [(X ‚àí ŒºX)2]+ ùîº[(Y ‚àí ŒºY )2], " width="450"/>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<section id="covariance-and-correlation" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_298"><span class="titlemark"><span class="cmss-10x-x-109">20.4.1 </span></span> <span id="x1-33300024.4.1"></span><span class="cmss-10x-x-109">Covariance and correlation</span></h3>
<p><span class="cmss-10x-x-109">Expected value and variance measure a random variable in isolation. However, in real problems, we need to discover relations between separate</span> <span id="dx1-333001"></span><span class="cmss-10x-x-109">measurements. Say, </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">describes the price of a given real estate, while </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">measures its size. These are certainly related, but one does not</span> <span id="dx1-333002"></span><span class="cmss-10x-x-109">determine the other. For instance, the location might be a differentiator between the prices.</span></p>
<p><span class="cmss-10x-x-109">The simplest statistical way of measuring similarity is the </span><span class="cmssi-10x-x-109">covariance </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">correlation</span><span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-333003r95"></span> <span class="cmbx-10x-x-109">Definition 95.</span> </span><span class="cmbx-10x-x-109">(Covariance and correlation)</span></p>
<p>Let (Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) be a probability space, let <span class="cmmi-10x-x-109">X,Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí </span><span class="msbm-10x-x-109">‚Ñù </span>be two random variables, and let <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmmi-8">X</span></sub> = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]<span class="cmmi-10x-x-109">,Œº</span><sub><span class="cmmi-8">Y</span></sub> = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">Y</span> ] be their expected values and <span class="cmmi-10x-x-109">œÉ</span><sub><span class="cmmi-8">X</span></sub> = Std[<span class="cmmi-10x-x-109">X</span>]<span class="cmmi-10x-x-109">,œÉ</span><sub><span class="cmmi-8">Y</span></sub> = Std[<span class="cmmi-10x-x-109">Y</span> ] their standard deviations.</p>
<p><span class="cmti-10x-x-109">(a) </span>The <span class="cmti-10x-x-109">covariance </span>of <span class="cmmi-10x-x-109">X </span>and <span class="cmmi-10x-x-109">Y </span>is defined by</p>
<div class="math-display">
<img src="../media/file1944.png" class="math-display" alt=" [ ] Cov [X,Y ] := ùîº (X ‚àí ŒºX )(Y ‚àí ŒºY ). "/>
</div>
<p><span class="cmti-10x-x-109">(b) </span>The <span class="cmti-10x-x-109">correlation </span>of <span class="cmmi-10x-x-109">X </span>and <span class="cmmi-10x-x-109">Y </span>is defined by</p>
<div class="math-display">
<img src="../media/file1945.png" class="math-display" alt=" Cov[X, Y] Corr [X, Y ] := ---------. œÉXœÉY "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Similarly to variance, the definition of covariance can be simplified to provide an easier way of calculating its exact value.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-333004r6"></span> <span class="cmbx-10x-x-109">Proposition 6.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, let </span><span class="cmmi-10x-x-109">X,Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí </span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be two random variables, and let </span><span class="cmmi-10x-x-109">Œº</span><sub><span class="cmmi-8">X</span></sub> = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]<span class="cmmi-10x-x-109">,Œº</span><sub><span class="cmmi-8">Y</span></sub> = <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">Y</span> ] <span class="cmti-10x-x-109">be their expected values.</span></p>
<p><span class="cmti-10x-x-109">Then</span></p>
<div class="math-display">
<img src="../media/file1946.png" class="math-display" alt="Cov[X, Y] = ùîº[XY ]‚àí ŒºX ŒºY . "/>
</div>
</div>
<div id="tcolobox-323" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This is just a simple calculation. According to the definition, we have</span></p>
<div class="math-display">
<img src="../media/file1947.png" class="math-display" alt=" [ ] Cov[X, Y] = ùîº (X ‚àí ŒºX )(Y ‚àí ŒºY) [ ] = ùîº XY ‚àí X ŒºY ‚àí ŒºX Y + ŒºX ŒºY = ùîº[XY ] ‚àí ùîº[X Œº ]‚àí ùîº[Œº Y ]+ Œº Œº [ ] [ ]Y X[ ] X Y = ùîº XY ‚àí ùîº X ŒºY ‚àí ŒºX ùîº Y + ŒºX ŒºY [ ] = ùîº XY ‚àí ŒºX ŒºY ‚àí ŒºX ŒºY + ŒºX ŒºY = ùîº[XY ]‚àí ŒºX ŒºY , "/>
</div>
<p><span class="cmss-10x-x-109">which is</span> <span id="dx1-333005"></span><span class="cmss-10x-x-109">what we</span> <span id="dx1-333006"></span><span class="cmss-10x-x-109">had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">One of the most important properties of covariance and correlation is that they are zero for independent random variables.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-333007r133"></span> <span class="cmbx-10x-x-109">Theorem 133.</span> </span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space, and let </span><span class="cmmi-10x-x-109">X,Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be two independent random variables.</span></p>
<p><span class="cmti-10x-x-109">Then,</span> Cov[<span class="cmmi-10x-x-109">X,Y</span> ] = 0<span class="cmti-10x-x-109">. (And consequently,</span> Corr[<span class="cmmi-10x-x-109">X,Y</span> ] = 0 <span class="cmti-10x-x-109">as well.)</span></p>
</div>
<p><span class="cmss-10x-x-109">The proof follows straight from the definition and </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-331005r130"><span class="cmssi-10x-x-109">130</span></a><span class="cmss-10x-x-109">, so this is left as an exercise for you.</span></p>
<p><span class="cmss-10x-x-109">Take note, as this is extra important: independence implies zero covariance, but zero covariance </span><span class="cmssi-10x-x-109">does not </span><span class="cmss-10x-x-109">imply independence. Here is an example.</span></p>
<p><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">be a discrete random variable with the probability mass function</span></p>
<div class="math-display">
<img src="../media/file1948.png" class="math-display" alt=" 1 P(X = ‚àí 1) = P (X = 0) = P(X = 1) = 3, "/>
</div>
<p><span class="cmss-10x-x-109">and let </span><span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">The expected value of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is</span></p>
<img src="../media/file1949.png" class="math-display" alt="ùîº [X ] = (‚àí 1)‚ãÖP (X = ‚àí 1)+ 0 ‚ãÖP (X = 0)+ 1 ‚ãÖP (X = 1) = ‚àí 1-+ 0 + 1- 3 3 = 0, " width="450"/>
<p><span class="cmss-10x-x-109">while the law of the unconscious statistician (</span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-331006r131"><span class="cmssi-10x-x-109">131</span></a><span class="cmss-10x-x-109">) gives that</span></p>
<img src="../media/file1950.png" class="math-display" alt=" 2 ùîº[Y ] = ùîº[X ] = 1 ‚ãÖP(X = ‚àí 1) + 0‚ãÖP (X = 0) + 1‚ãÖP (X = 1 ) 1- 1- = 3 + 0+ 3 2 = -, 3 " width="450"/>
<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file1951.png" class="math-display" alt="ùîº[XY ] = ùîº[X3] = 0. "/>
</div>
<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file1952.png" class="math-display" alt="Cov [X,Y ] = ùîº [XY ]‚àí ùîº[X ]ùîº [Y ] 3 2 = ùîº [X ]‚àí ùîº [X ]ùîº[X ] 2- = 0 ‚àí 0‚ãÖ 3 = 0. "/>
</div>
<p><span class="cmss-10x-x-109">However, </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">are not independent, as </span><span class="cmmi-10x-x-109">Y </span>= <span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">2</span></sup> <span class="cmss-10x-x-109">is a function of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. (I shamelessly stole this example from a brilliant</span> <span id="dx1-333008"></span><span class="cmss-10x-x-109">Stack Overflow thread, which you should read here</span> <span id="dx1-333009"></span><span class="cmss-10x-x-109">for more on this question: </span> <a href="https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence" class="url"><span class="cmtt-10x-x-109">https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence</span></a><span class="cmss-10x-x-109">)</span></p>
<p><span class="cmss-10x-x-109">Do you recall that we interpreted the concept of probability as the relative frequency of occurrences? Now that we have the expected value under our belt, we can finally make this precise. Let‚Äôs look at the famous law of large numbers!</span></p>
</section>
</section>
<section id="the-law-of-large-numbers" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_299"><span class="titlemark"><span class="cmss-10x-x-109">20.5 </span></span> <span id="x1-33400024.5"></span><span class="cmss-10x-x-109">The law of large numbers</span></h2>
<p><span class="cmss-10x-x-109">We‚Äôll continue</span> <span id="dx1-334001"></span><span class="cmss-10x-x-109">our journey with a quite remarkable and famous result: the law of large numbers. You have probably already heard several faulty arguments invoking the law of large numbers. For instance, gamblers are often convinced that their bad luck will end soon because of said law. This is one of the most frequently misused mathematical terms, and we are here to clear that up.</span></p>
<p><span class="cmss-10x-x-109">We‚Äôll do this in two passes. First, we are going to see an intuitive interpretation, then add the technical but important mathematical details. I‚Äôll try to be gentle.</span></p>
<section id="tossing-coins-" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_300"><span class="titlemark"><span class="cmss-10x-x-109">20.5.1 </span></span> <span id="x1-33500024.5.1"></span><span class="cmss-10x-x-109">Tossing coins‚Ä¶</span></h3>
<p><span class="cmss-10x-x-109">First, let‚Äôs</span> <span id="dx1-335001"></span><span class="cmss-10x-x-109">toss some coins again. If we toss coins repeatedly, what is the relative frequency of heads in the long run?</span></p>
<p><span class="cmss-10x-x-109">We should have a pretty good guess already: the average number of heads should converge to </span><span class="cmmi-10x-x-109">P</span>(heads) = <span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">as well. Why? Because we saw this when studying the frequentist interpretation of probability in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch030.xhtml#how-to-interpret-probability"><span class="cmssi-10x-x-109">18.2.7</span></a><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Our simulation showed that the relative frequency of heads does indeed converge to the true probability. This time, we‚Äôll carry the simulation a bit further.</span></p>
<p><span class="cmss-10x-x-109">First, to formulate the problem, let‚Äôs introduce the independent random variables </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶ </span><span class="cmss-10x-x-109">that are distributed along</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> = 0 <span class="cmss-10x-x-109">if the toss results in tails, while </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> = 1 <span class="cmss-10x-x-109">if it is heads. We are interested in the long-term behavior of</span></p>
<div class="math-display">
<img src="../media/file1953.png" class="math-display" alt="-- X1-+-‚ãÖ‚ãÖ‚ãÖ+-Xn- Xn = n . "/>
</div>
<p><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">sample average</span><span class="cmss-10x-x-109">. We have already seen that the sample average gets closer and closer to </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">as </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">grows. Let‚Äôs see the simulation one more time, before we go any further. (The parameter </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">is selected to be </span>1<span class="cmmi-10x-x-109">‚àï</span>2 <span class="cmss-10x-x-109">for the sake of the example.)</span></p>
<div id="tcolobox-324" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import numpy as np 
from scipy.stats import bernoulli 
 
 
n_tosses = 1000 
idx = range(n_tosses) 
 
coin_tosses = [bernoulli.rvs(p=0.5) for _ in idx] 
coin_toss_averages = [np.mean(coin_tosses[:k+1]) for k in idx]</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">And here is the plot.</span></p>
<div id="tcolobox-325" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>import matplotlib.pyplot as plt 
 
 
with plt.style.context("/span&gt;seaborn-v0_8": 
    plt.figure(figsize=(10, 5)) 
    plt.title("/span&gt;Relative frequency of the coin tosses 
    plt.xlabel("/span&gt;Relative frequency 
    plt.ylabel("/span&gt;Number of tosses 
 
    # plotting the averages 
    plt.plot(range(n_tosses), coin_toss_averages, linewidth=3) # the averages 
 
    # plotting the true expected value 
    plt.plot([-100, n_tosses+100], [0.5, 0.5], c="/span&gt;k 
    plt.xlim(-10, n_tosses+10) 
    plt.ylim(0, 1) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1954.png" width="569" alt="PIC"/> <span id="x1-335028r3"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.3: Relative frequency of the coin tosses</span> </span>
</div>
<p><span class="cmss-10x-x-109">Nothing new so far. However, if you have a sharp eye, you might ask the</span><span id="dx1-335029"></span> <span class="cmss-10x-x-109">question: is this just an accident? After all, we are studying the average</span></p>
<div class="math-display">
<img src="../media/file1955.png" class="math-display" alt="-- X1-+-‚ãÖ‚ãÖ‚ãÖ+-Xn- Xn = n , "/>
</div>
<p><span class="cmss-10x-x-109">which is (almost) a binomially distributed random variable! To be more precise, if </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">‚àº</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">, then</span></p>
<div class="math-display">
<img src="../media/file1956.png" class="math-display" alt="-- Xn ‚àº 1Binomial(n,p). n "/>
</div>
<p><span class="cmss-10x-x-109">(We saw this earlier when discussing the sums of discrete random variables in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#sums-of-discrete-random-variables"><span class="cmssi-10x-x-109">19.2.7</span></a><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">At this point, it is far from guaranteed that this distribution will be concentrated around a single value. So, let‚Äôs do some more simulations. This time, we‚Äôll toss a coin a thousand times to see the distribution of the averages. Quite meta, I know.</span></p>
<div id="tcolobox-326" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>more_coin_tosses = bernoulli.rvs(p=0.5, size=(n_tosses, n_tosses)) 
more_coin_toss_averages = np.array([[np.mean(more_coin_tosses[i][:j+1]) for j in idx] 
                                     for i in idx])</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">We can visualize the distributions on histograms.</span></p>
<div id="tcolobox-327" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>with plt.style.context("/span&gt;seaborn-v0_8": 
    fig, axs = plt.subplots(1, 3, figsize=(12, 4), sharey=False) 
    fig.suptitle("/span&gt;The distribution of sample averages 
    for ax, i in zip(axs, [5, 100, 999]): 
        x = [k/i for k in range(i+1)] 
        y = more_coin_toss_averages[:, i] 
        ax.hist(y, bins=x) 
        ax.set_title(f/span&gt;n = {i}" 
 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1957.png" width="626" alt="PIC"/> <span id="x1-335043r4"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.4: Sample average distributions of coin tosses</span> </span>
</div>
<p><span class="cmss-10x-x-109">In other words, the probability of </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">falling far from </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">becomes</span> <span id="dx1-335044"></span><span class="cmss-10x-x-109">smaller and smaller. For any small </span><span class="cmmi-10x-x-109">ùúÄ</span><span class="cmss-10x-x-109">, we can formulate the probability of ‚Äú</span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">falling farther from </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">than </span><span class="cmmi-10x-x-109">ùúÄ</span><span class="cmss-10x-x-109">‚Äù as </span><span class="cmmi-10x-x-109">P</span>(<span class="cmsy-10x-x-109">|</span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">p</span><span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">ùúÄ</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Thus, mathematically speaking, our guess is</span></p>
<div class="math-display">
<img src="../media/file1958.png" class="math-display" alt=" -- nl‚Üíim‚àû P(|Xn ‚àí p| &gt;ùúÄ) = 0. "/>
</div>
<p><span class="cmss-10x-x-109">Again, is this just an accident, and were we just lucky to study an experiment where this is true? Would the same work for random variables other than Bernoulli ones? What will the sample averages converge to? (If they converge at all.)</span></p>
<p><span class="cmss-10x-x-109">We‚Äôll find out.</span></p>
</section>
<section id="rolling-dice-" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_301"><span class="titlemark"><span class="cmss-10x-x-109">20.5.2 </span></span> <span id="x1-33600024.5.2"></span><span class="cmss-10x-x-109">‚Ä¶rolling dice‚Ä¶</span></h3>
<p><span class="cmss-10x-x-109">Let‚Äôs play</span> <span id="dx1-336001"></span><span class="cmss-10x-x-109">dice. To keep things simple, we are interested in the average value of a roll in the long run. To build a proper probabilistic model, let‚Äôs introduce random variables!</span></p>
<p><span class="cmss-10x-x-109">A single roll is uniformly distributed on </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,‚Ä¶,</span>6<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, and each roll is independent from the others. So, let </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶ </span><span class="cmss-10x-x-109">be independent random variables, each distributed according to</span> Uniform(<span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,‚Ä¶,</span>6<span class="cmsy-10x-x-109">}</span>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">How does the sample average </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">behave? Simulation time. We‚Äôll randomly generate 1000 rolls, then explore how </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">behaves.</span></p>
<div id="tcolobox-328" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from scipy.stats import randint 
 
 
n_rolls = 1000 
idx = range(n_rolls) 
 
dice_rolls = [randint.rvs(low=1, high=7) for _ in idx] 
dice_roll_averages = [np.mean(dice_rolls[:k+1]) for k in idx]</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Again, to obtain a bit of an insight, we‚Äôll visualize the averages on a plot.</span></p>
<div id="tcolobox-329" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>with plt.style.context("/span&gt;seaborn-v0_8": 
    plt.figure(figsize=(10, 5)) 
    plt.title("/span&gt;Sample averages of rolling a six-sided dice 
 
    # plotting the averages 
    plt.plot(idx, dice_roll_averages, linewidth=3) # the averages 
 
    # plotting the true expected value 
    plt.plot([-100, n_rolls+100], [3.5, 3.5], c="/span&gt;k 
 
    plt.xlim(-10, n_rolls+10) 
    plt.ylim(0, 6) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1959.png" width="569" alt="PIC"/> <span id="x1-336023r5"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.5: Sample averages of rolling a six-sided dice</span> </span>
</div>
<p><span class="cmss-10x-x-109">The first thing</span> <span id="dx1-336024"></span><span class="cmss-10x-x-109">to note is that these are suspiciously close to </span>3<span class="cmmi-10x-x-109">.</span>5<span class="cmss-10x-x-109">. This is </span><span class="cmssi-10x-x-109">not </span><span class="cmss-10x-x-109">a probability, but the expected value:</span></p>
<div class="math-display">
<img src="../media/file1960.png" class="math-display" alt="ùîº[X1 ] = ùîº[X2] = ‚ãÖ‚ãÖ‚ãÖ = 3.5. "/>
</div>
<p><span class="cmss-10x-x-109">For</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">distributed random variables, the expected value coincides with the probability </span><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">. However, this time, </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">does not have a nice and explicit distribution like in the case of coin tosses, where the sample averages were binomially distributed. So, let‚Äôs roll some more dice to estimate how </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is distributed.</span></p>
<div id="tcolobox-330" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>more_dice_rolls = randint.rvs(low=1, high=7, size=(n_rolls, n_rolls)) 
more_dice_roll_averages = np.array([[np.mean(more_dice_rolls[i][:j+1]) for j in idx] 
                                     for i in idx])</code></pre>
</div>
</div>
<div id="tcolobox-331" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>with plt.style.context("/span&gt;seaborn-v0_8": 
    fig, axs = plt.subplots(1, 3, figsize=(12, 4), sharey=False) 
    fig.suptitle("/span&gt;The distribution of sample averages 
    for ax, i in zip(axs, [5, 100, 999]): 
        x = [6*k/i for k in range(i+1)] 
        y = more_dice_roll_averages[:, i] 
        ax.hist(y, bins=x) 
        ax.set_title(f/span&gt;n = {i}" 
 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file1961.png" width="569" alt="PIC"/> <span id="x1-336038r6"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.6: Sample average distributions of dice rolls</span> </span>
</div>
<p><span class="cmss-10x-x-109">It seems like, once more, the distribution of </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is concentrated around </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub>]<span class="cmss-10x-x-109">. Our intuition tells us that this is not</span> <span id="dx1-336039"></span><span class="cmss-10x-x-109">an accident; that this phenomenon is true for a wide range of random variables.</span></p>
<p><span class="cmss-10x-x-109">Let me spoil the surprise: this is indeed the case, and we‚Äôll see this now.</span></p>
</section>
<section id="and-all-the-rest" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_302"><span class="titlemark"><span class="cmss-10x-x-109">20.5.3 </span></span> <span id="x1-33700024.5.3"></span><span class="cmss-10x-x-109">‚Ä¶and all the rest</span></h3>
<p><span class="cmss-10x-x-109">This time, let </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶ </span><span class="cmss-10x-x-109">be a sequence of independent and identically distributed (i.i.d.) random variables. Not coin tosses, not</span><span id="dx1-337001"></span> <span class="cmss-10x-x-109">dice rolls, but any distribution. We saw that the sample average </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">seems to converge to the joint expected value of the </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s:</span></p>
<div class="math-disply">
<img src="../media/file1962.png" width="150" class="math-display" alt="‚Ä≤‚Ä≤- ‚Ä≤‚Ä≤ Xn ‚Üí ùîº[X1] "/>
</div>
<p><span class="cmss-10x-x-109">Note the quotation marks: </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is not a number but a random variable. Thus, we can‚Äôt (yet) speak about convergence.</span></p>
<p><span class="cmss-10x-x-109">In mathematically precise terms, what we saw previously is that for large enough </span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">-s, the sample average </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is highly unlikely to fall far from the joint expected value </span><span class="cmmi-10x-x-109">Œº </span>= <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub>]<span class="cmss-10x-x-109">; that is,</span></p>
<div class="math-display">
  <span>
    lim<sub>n‚Üí‚àû</sub> P(|<span style="text-decoration: overline;">X<sub>n</sub></span> ‚àí Œº| &gt; ùúÄ) = 0
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(20.2)</span>
</div>

<p><span class="cmss-10x-x-109">holds for all </span><span class="cmmi-10x-x-109">ùúÄ/span&gt;0<span class="cmss-10x-x-109">.</span> </span></p>
<p><span class="cmss-10x-x-109">The limit (</span><a href="ch032.xhtml#and-all-the-rest"><span class="cmss-10x-x-109">20.2</span></a><span class="cmss-10x-x-109">) seems hard to prove right now even in the simple case of coin tossing. There, </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àº</span><img src="../media/file1963.png" width="10" data-align="middle" alt="1n"/> Binomial(<span class="cmmi-10x-x-109">n,p</span>)<span class="cmss-10x-x-109">, thus</span></p>

<img src="../media/file1964.png" width="450" class="math-display" alt=" ‚åän(p+ ùúÄ)‚åã ( ) -- ‚àë n k n‚àík P(|Xn ‚àí Œº | &gt;ùúÄ) = 1 ‚àí k p (1 ‚àí p) , k=‚åän(p‚àí ùúÄ)‚åã "/>

<p><span class="cmss-10x-x-109">where the symbol </span><span class="cmsy-10x-x-109">‚åä</span><span class="cmmi-10x-x-109">x</span><span class="cmsy-10x-x-109">‚åã </span><span class="cmss-10x-x-109">denotes the largest integer that is smaller than </span><span class="cmmi-10x-x-109">x</span><span class="cmss-10x-x-109">. This does not look friendly at all. (I leave the verification as an exercise.)</span></p>
<p><span class="cmss-10x-x-109">Thus, our plan is</span> <span id="dx1-337002"></span><span class="cmss-10x-x-109">the following.</span></p>
<ol>
<li><span id="x1-337004x1"><span class="cmss-10x-x-109">Find a way to estimate </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">j</span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">Œºj/span&gt;<span class="cmmi-10x-x-109">ùúÄ</span>) <span class="cmss-10x-x-109">in a way that is independent from the distribution of the </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s.</span> </span></span></li>
<li><span id="x1-337006x2"><span class="cmss-10x-x-109">Use the upper estimate to show</span> lim<sub><span class="cmmi-8">n</span><span class="cmsy-8">‚Üí‚àû</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">j</span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">Œºj/span&gt;<span class="cmmi-10x-x-109">ùúÄ</span>) = 0<span class="cmss-10x-x-109">.</span></span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Let‚Äôs go.</span></p>
</section>
<section id="the-weak-law-of-large-numbers" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_303"><span class="titlemark"><span class="cmss-10x-x-109">20.5.4 </span></span> <span id="x1-33800024.5.4"></span><span class="cmss-10x-x-109">The weak law of large numbers</span></h3>
<p><span class="cmss-10x-x-109">First, the</span> <span id="dx1-338001"></span><span class="cmss-10x-x-109">upper estimates. There are two general inequalities that‚Äôll help us to deal with </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">j</span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">Œºj </span><span class="cmsy-10x-x-109">‚â•</span><span class="cmmi-10x-x-109">ùúÄ</span>)<span class="cmss-10x-x-109">.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-338002r134"></span> <span class="cmbx-10x-x-109">Theorem 134.</span> </span><span class="cmbxti-10x-x-109">(Markov‚Äôs inequality)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí </span>[0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">‚àû</span>) <span class="cmti-10x-x-109">be a nonnegative random variable. Then</span></p>
<div class="math-dislay">
<img src="../media/file1965.png" width="150" class="math-display" alt=" ùîº[X ] P (X ‚â• t) ‚â§ ----- t "/>
</div>
<p><span class="cmti-10x-x-109">holds for any </span><span class="cmmi-10x-x-109">t </span><span class="cmsy-10x-x-109">‚àà </span>(0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">‚àû</span>)<span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-332" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">We have to separate the discrete and the continuous cases. The proofs are almost identical, so I‚Äôll only do the discrete case here, while the continuous is left for you as an exercise to test your understanding.</span></p>
<p><span class="cmss-10x-x-109">So, let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí {</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">be a discrete random variable (where </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">‚â• </span>0 <span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">), and </span><span class="cmmi-10x-x-109">t </span><span class="cmsy-10x-x-109">‚àà </span>(0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">‚àû</span>) <span class="cmss-10x-x-109">be an arbitrary positive real number.</span></p>
<p><span class="cmss-10x-x-109">Then</span></p>
<span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>]
</div>
</div>
<p>= <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">‚àû</span></sup><span class="cmmi-10x-x-109">x</span> <sub><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)</p>
<p>= <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">:</span><span class="cmmi-8">x</span><sub><span class="cmmi-6">k</span></sub><span class="cmmi-8">/span&gt;<span class="cmmi-8">t</span></span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>) + <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">:</span><span class="cmmi-8">x</span><sub><span class="cmmi-6">k</span></sub><span class="cmsy-8">‚â•</span><span class="cmmi-8">t</span></sub><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmmi-10x-x-109">,</span></sub></p>
<p><span class="cmss-10x-x-109">where the sum </span><span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">:</span><span class="cmmi-8">x</span><sub><span class="cmmi-6">k</span></sub><span class="cmmi-8">/span&gt;<span class="cmmi-8">t</span></span> <span class="cmss-10x-x-109">only accounts for </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-s with </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">t</span><span class="cmss-10x-x-109">, and similarly, </span><span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span></sub> : <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">‚â•</span><span class="cmmi-10x-x-109">t </span><span class="cmss-10x-x-109">only accounts for </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">-s with </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">‚â•</span><span class="cmmi-10x-x-109">t</span><span class="cmss-10x-x-109">.</span> </span></sub></p>
<p><span class="cmss-10x-x-109">As the </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">-s are nonnegative by assumption, we can estimate </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">from below by omitting one of them. Thus,</span></p>
<div class="math-dispay">
<img src="../media/file1966.png" width="450" class="math-display" alt=" ‚àë ‚àë ùîº[X ] = xkP (X = xk )+ xkP (X = xk) k:x &lt; k:x ‚â•t ‚àëk k ‚â• xkP (X = xk ) k:xk‚â•t ‚àë ‚â• t P (X = xk ) k:xk‚â•t = tP(X ‚â• t), "/>
</div>
<p><span class="cmss-10x-x-109">from which Markov‚Äôs inequality</span></p>
<div class="math-dispay">
<img src="../media/file1967.png" width="150" class="math-display" alt="P (X ‚â• t) ‚â§ ùîº[X-] t "/>
</div>
<p><span class="cmss-10x-x-109">follows.</span></p>
<p><span class="cmss-10x-x-109">The law of large numbers is only one step away from Markov‚Äôs inequality. This last step is so useful that it deserves to be its own theorem. Meet the famous inequality of Chebyshev.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-338003r135"></span> <span class="cmbx-10x-x-109">Theorem 135.</span> </span><span class="cmbxti-10x-x-109">(Chebyshev‚Äôs inequality)</span></p>
<p><span class="cmti-10x-x-109">Let </span>(Œ©<span class="cmmi-10x-x-109">,</span>Œ£<span class="cmmi-10x-x-109">,P</span>) <span class="cmti-10x-x-109">be a probability space and let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmti-10x-x-109">be a random variable with finite variance </span><span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup> = Var[<span class="cmmi-10x-x-109">X</span>]<span class="cmmi-10x-x-109">/span&gt;<span class="cmsy-10x-x-109">‚àû</span><span class="cmti-10x-x-109">and expected value </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] = <span class="cmmi-10x-x-109">Œº</span><span class="cmti-10x-x-109">.</span> </span></p>
<p><span class="cmti-10x-x-109">Then</span></p>
<div class="math-displa">
<img src="../media/file1968.png" width="150" class="math-display" alt=" œÉ2 P(|X ‚àí Œº| ‚â• t) ‚â§ -t2- "/>
</div>
<p><span class="cmti-10x-x-109">holds for all </span><span class="cmmi-10x-x-109">t </span><span class="cmsy-10x-x-109">‚àà </span>(0<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">‚àû</span>)<span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-333" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As </span><span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">Œº</span><span class="cmsy-10x-x-109">| </span><span class="cmss-10x-x-109">is a nonnegative random variable, we can apply </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-338002r134"><span class="cmssi-10x-x-109">134</span></a> <span class="cmss-10x-x-109">to obtain</span></p>
<div class="math-display">
<img src="../media/file1969.png" class="math-display" alt="P (|X ‚àí Œº| ‚â• t) = P(|X ‚àí Œº|2 ‚â• t2) 2 ‚â§ ùîº[|X--‚àí-Œº|-]. t2 "/>
</div>
<p><span class="cmss-10x-x-109">However, as </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">Œº</span><span class="cmsy-10x-x-109">|</span><sup><span class="cmr-8">2</span></sup>] = Var[<span class="cmmi-10x-x-109">X</span>] = <span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-display">
<img src="../media/file1970.png" class="math-display" alt=" 2 2 P(|X ‚àí Œº | ‚â• t) ‚â§ ùîº-[|X-‚àí2Œº-|] = œÉ2 t t "/>
</div>
<p><span class="cmss-10x-x-109">which is what we had to show.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">And with that, we are ready to precisely formulate and prove the law of large numbers. After all this setup, the (weak) law of large numbers is just a small step away. Here it is in its full glory.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-338004r136"></span> <span class="cmbx-10x-x-109">Theorem 136.</span> </span><span class="cmbxti-10x-x-109">(The weak law of large numbers)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶ </span><span class="cmti-10x-x-109">be a sequence of independent and identically distributed random variables with finite expected value </span><span class="cmmi-10x-x-109">Œº </span>= <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub>] <span class="cmti-10x-x-109">and variance </span><span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup> = Var[<span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub>]<span class="cmti-10x-x-109">, and let</span></p>
<div class="math-display">
<img src="../media/file1971.png" class="math-display" alt="-- X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn Xn = ------------- n "/>
</div>
<p><span class="cmti-10x-x-109">be their sample average. Then</span></p>
<div class="math-display">
<img src="../media/file1972.png" class="math-display" alt="lim P (|X- ‚àí Œº| ‚â• ùúÄ) = 0 n‚Üí ‚àû n "/>
</div>
<p><span class="cmti-10x-x-109">holds for any </span><span class="cmmi-10x-x-109">ùúÄ/span&gt;0<span class="cmti-10x-x-109">.</span> </span></p>
</div>
<div id="tcolobox-334" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">As the </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">-s are independent, the variance of the sample average is</span></p>
<div class="math-display">
<img src="../media/file1973.png" class="math-display" alt=" -- [ X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn ] Var[Xn ] = Var ------------- n = -1-Var[X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn] n2 = -1-(Var[X ]+ ‚ãÖ‚ãÖ‚ãÖ+ Var[X ]) n2 1 n n œÉ2 œÉ2 = -n2- = n-. "/>
</div>
<p><span class="cmss-10x-x-109">Now, by using Chebyshev‚Äôs inequality from </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-338003r135"><span class="cmssi-10x-x-109">135</span></a><span class="cmss-10x-x-109">, we obtain</span></p>
<div class="math-display">
<img src="../media/file1974.png" class="math-display" alt=" -- -- Var[Xn-] -œÉ2- P (|Xn ‚àí Œº | ‚â• ùúÄ) ‚â§ ùúÄ2 = n ùúÄ2. "/>
</div>
<p><span class="cmss-10x-x-109">Thus,</span></p>
<div class="math-display">
<img src="../media/file1975.png" class="math-display" alt="0 ‚â§ lim P(|Xn ‚àí Œº | ‚â• ùúÄ) n‚Üí ‚àû -œÉ2- ‚â§ nl‚Üíim‚àû nùúÄ2 = 0, "/>
</div>
<p><span class="cmss-10x-x-109">hence</span></p>
<div class="math-display">
<img src="../media/file1976.png" class="math-display" alt=" -- nli‚Üím‚àû P(|Xn ‚àí Œº | ‚â• ùúÄ) = 0, "/>
</div>
<p><span class="cmss-10x-x-109">which is what we needed to show.</span></p>
</div>
</div>
<p><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-338004r136"><span class="cmssi-10x-x-109">136</span></a> <span class="cmss-10x-x-109">is not all that can be said about the sample averages. There is a stronger result, showing that the sample averages do in fact converge to the mean with probability </span>1<span class="cmss-10x-x-109">.</span></p>
</section>
<section id="the-strong-law-of-large-numbers" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_304"><span class="titlemark"><span class="cmss-10x-x-109">20.5.5 </span></span> <span id="x1-33900024.5.5"></span><span class="cmss-10x-x-109">The strong law of large numbers</span></h3>
<p><span class="cmss-10x-x-109">Why is </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-338004r136"><span class="cmssi-10x-x-109">136</span></a> <span class="cmss-10x-x-109">called the ‚Äúweak‚Äù law? Think</span> <span id="dx1-339001"></span><span class="cmss-10x-x-109">about the statement</span></p>
<div class="math-display">
  <span>
    lim<sub>n‚Üí‚àû</sub> P(|<span style="text-decoration: overline;">X<sub>n</sub></span> ‚àí Œº| ‚â• ùúÄ) = 0
  </span>
  <span class="math-label" style="float: right; margin-left: 1em;">(20.3)</span>
</div>

<p><span class="cmss-10x-x-109">for a moment. For a given </span><span class="cmmi-10x-x-109">œâ </span><span class="cmsy-10x-x-109">‚àà </span>Œ©<span class="cmss-10x-x-109">, this doesn‚Äôt tell us </span><span class="cmssi-10x-x-109">anything </span><span class="cmss-10x-x-109">about the convergence of a concrete sample average</span></p>
<div class="math-display">
<img src="../media/file1977.png" class="math-display" alt="X- (œâ) = X1(œâ-)+-‚ãÖ‚ãÖ‚ãÖ+-Xn-(œâ-), n n "/>
</div>
<p><span class="cmss-10x-x-109">it just tells us that in a probabilistic sense, </span><span class="overline"><span class="cmmi-10x-x-109">X</span></span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is concentrated around the joint expected value </span><span class="cmmi-10x-x-109">Œº</span><span class="cmss-10x-x-109">. In a sense, (</span><a href="ch032.xhtml#the-strong-law-of-large-numbers"><span class="cmss-10x-x-109">20.3</span></a><span class="cmss-10x-x-109">) is a weaker version of</span></p>
<div class="math-display">
<img src="../media/file1978.png" class="math-display" alt="P( lim X- = Œº ) = 1, n‚Üí ‚àû n "/>
</div>
<p><span class="cmss-10x-x-109">hence the terminology </span><span class="cmssi-10x-x-109">weak </span><span class="cmss-10x-x-109">law of large numbers.</span></p>
<p><span class="cmss-10x-x-109">Do we have a stronger result than </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-338004r136"><span class="cmssi-10x-x-109">136</span></a><span class="cmss-10x-x-109">? Yes, we do.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-339002r137"></span> <span class="cmbx-10x-x-109">Theorem 137.</span> </span><span class="cmbxti-10x-x-109">(The strong law of large numbers)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶ </span><span class="cmti-10x-x-109">be a sequence of independent and identically distributed random variables with finite expected value </span><span class="cmmi-10x-x-109">Œº </span>= <span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub>] <span class="cmti-10x-x-109">and variance </span><span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup> = Var[<span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub>]<span class="cmti-10x-x-109">, and let</span></p>
<div class="math-disply">
<img src="../media/file1979.png" width="150" class="math-display" alt="-- X + ‚ãÖ‚ãÖ‚ãÖ+ X Xn = -1---------n- n "/>
</div>
<p><span class="cmti-10x-x-109">be their sample average. Then</span></p>
<div class="math-disply">
<img src="../media/file1980.png" width="150" class="math-display" alt="P( lim Xn = Œº ) = 1. n‚Üí ‚àû "/>
</div>
</div>
<p><span class="cmss-10x-x-109">We are not going to prove this, just know that the sample average will converge to the mean with probability one.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-339003r21"></span> <span class="cmbx-10x-x-109">Remark 21.</span> </span><span class="cmbx-10x-x-109">(Convergence of random variables)</span></p>
<p>What we have seen in the weak and strong laws of large numbers are not unique to sample averages. Similar phenomena can be observed in other cases, thus, these types of convergences have their own exact definitions.</p>
<p>If <span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶</span> is a sequence of random variables, we say that</p>
<p><span class="cmti-10x-x-109">(a) </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">n</span></sub> converges <span class="cmti-10x-x-109">in probability </span>towards <span class="cmmi-10x-x-109">X </span>if</p>
<div class="math-display">
<img src="../media/file1981.png" class="math-display" alt=" lim P(|Xn ‚àí X | ‚â• ùúÄ) = 0 n‚Üí ‚àû "/>
</div>
<p>for all <span class="cmmi-10x-x-109">ùúÄ/span&gt;0. Convergence in probability is denoted by <span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">n</span></sub><img src="../media/file1982.png" alt="‚àíP‚Üí" width="15"/><span class="cmmi-10x-x-109">X</span>. </span></p>
<p><span class="cmti-10x-x-109">(b) </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">n</span></sub> converges <span class="cmti-10x-x-109">almost surely </span>towards <span class="cmmi-10x-x-109">X </span>if</p>
<div class="math-display">
<img src="../media/file1983.png" class="math-display" alt="P( lim Xn = X ) = 1 n‚Üí ‚àû "/>
</div>
<p>holds. Almost sure convergence is denoted by <span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">n</span></sub><img src="../media/file1984.png" alt="‚àía‚àí.‚Üí s." width="15"/><span class="cmmi-10x-x-109">X</span>.</p>
<p>Thus, the weak and strong laws of large numbers state that in certain cases, the sample averages converge to the expected value both in probability and almost surely.</p>
</div>
</section>
</section>
<section id="information-theory" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_305"><span class="titlemark"><span class="cmss-10x-x-109">20.6 </span></span> <span id="x1-34000024.6"></span><span class="cmss-10x-x-109">Information theory</span></h2>
<p><span class="cmss-10x-x-109">If you have</span> <span id="dx1-340001"></span><span class="cmss-10x-x-109">already trained a machine learning model in your practice, chances are you are already familiar with the mean-squared error</span></p>
<div class="math-display">
<img src="../media/file1985.png" class="math-display" alt=" n 1-‚àë 2 n MSE (x,y) = n (f(xi)‚àí yi), x, y ‚àà ‚Ñù , i=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">f </span>: <span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">n</span></sup> <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">represents our model, </span><span class="cmbx-10x-x-109">x </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the vector of one-dimensional observations, and </span><span class="cmbx-10x-x-109">y </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the ground truth. After learning all about the expected value, this sum should be familiar: if we assume a probabilistic viewpoint and let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">be the random variables describing the data, then the mean-squared error can be written as the expected value</span></p>
<div class="math-display">
<img src="../media/file1986.png" class="math-display" alt=" [ ] MSE (x, y) = ùîº (f(X )‚àí Y )2. "/>
</div>
<p><span class="cmss-10x-x-109">However, the mean-squared error is not suitable for classification problems. For instance, if the task is to classify the object of an image, the output is a discrete probability distribution for each sample. In this situation, we could</span> <span id="dx1-340002"></span><span class="cmss-10x-x-109">use the so-called </span><span class="cmssi-10x-x-109">cross-entropy</span><span class="cmss-10x-x-109">, defined by</span></p>
<div class="math-display">
<img src="../media/file1987.png" class="math-display" alt=" n H [p,q] = ‚àí ‚àë p logq i i i=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmbx-10x-x-109">p </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">denotes the one-hot encoded vector of the class</span> <span id="dx1-340003"></span><span class="cmss-10x-x-109">label for a single data sample, and </span><span class="cmbx-10x-x-109">q </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">n</span></sup> <span class="cmss-10x-x-109">is the class label prediction, forming a probability distribution. (One-hot encoding is the process where we represent a finite set of possible class labels, such as </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">a,b,c</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">as zero-one vectors, like</span></p>
<div class="math-display">
<img src="../media/file1988.png" class="math-display" alt="a ‚Üê ‚Üí (1,0,0), b ‚Üê ‚Üí (0,1,0), c ‚Üê ‚Üí (0,0,1). "/>
</div>
<p><span class="cmss-10x-x-109">We do this because it‚Äôs easier to work with vectors and matrices than with strings.)</span></p>
<p><span class="cmss-10x-x-109">Not that surprisingly, </span><span class="cmmi-10x-x-109">H</span>[<span class="cmbx-10x-x-109">p</span><span class="cmmi-10x-x-109">,</span><span class="cmbx-10x-x-109">q</span>] <span class="cmss-10x-x-109">is also an expected value, but it‚Äôs much more than that: it quantifies the information content of the distribution </span><span class="cmbx-10x-x-109">q </span><span class="cmss-10x-x-109">compared to the ground truth distribution </span><span class="cmbx-10x-x-109">q</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">But what is information in a mathematical sense? Let‚Äôs dive in.</span></p>
<section id="guess-the-number" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_306"><span class="titlemark"><span class="cmss-10x-x-109">20.6.1 </span></span> <span id="x1-34100024.6.1"></span><span class="cmss-10x-x-109">Guess the number</span></h3>
<p><span class="cmss-10x-x-109">Let‚Äôs start with a simple game. I have thought of an integer between </span>0 <span class="cmss-10x-x-109">and </span>7<span class="cmss-10x-x-109">, and</span> <span id="dx1-341001"></span><span class="cmss-10x-x-109">your job is to find out which one by asking yes-no questions.</span></p>
<div class="minipage">
<p><img src="../media/file1989.png" width="284" alt="PIC"/> <span id="x1-341002r7"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.7: Which number am I thinking of?</span> </span>
</div>
<p><span class="cmss-10x-x-109">One possible strategy is to guess the numbers one by one. In other words, the sequence of your questions are:</span></p>
<ul>
<li><span class="cmss-10x-x-109">Is it 0?</span></li>
<li><span class="cmss-10x-x-109">Is it 1?</span></li>
<li><img src="../media/file1990.png" class="vdots" width="5" alt=".. . "/></li>
<li><span class="cmss-10x-x-109">Is it 7?</span></li>
</ul>
<p><span class="cmss-10x-x-109">Although this strategy works, it is not an effective one. Why? Consider the average number of questions. Let the random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">denote the number I have picked. As </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is uniformly distributed ‚Äî that is, </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>) = 1<span class="cmmi-10x-x-109">‚àï</span>8 <span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">k </span>= 0<span class="cmmi-10x-x-109">,‚Ä¶,</span>7 <span class="cmss-10x-x-109">‚Äî the probability of asking exactly </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">questions is</span></p>
<div class="math-display">
<img src="../media/file1991.png" class="math-display" alt="P(#questions = k) = P(X = k ‚àí 1) = 1 8 "/>
</div>
<p><span class="cmss-10x-x-109">as well. Thus, the</span> <span id="dx1-341003"></span><span class="cmss-10x-x-109">number of questions needed is also uniformly distributed on </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,‚Ä¶,</span>8<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, thus</span></p>
<div class="math-display">
<img src="../media/file1992.png" class="math-display" alt=" ‚àë8 ùîº[#questions] = kP (#questions = k ) k=1 8 = 1-‚àë k 8 k=1 = 1-8‚ãÖ9-= 9-, 8 2 2 "/>
</div>
<p><span class="cmss-10x-x-109">where we have used that </span><span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup> = <img src="../media/file1993.png" class="frac" data-align="middle" alt="n(n+1) 2"/><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Can we do better than this? Yes. In the previous sequential strategy, each question has a small chance of hitting, and a large chance of eliminating, only one potential candidate. It‚Äôs easy to see that the best would be to eliminate half the search space with each question.</span></p>
<p><span class="cmss-10x-x-109">Say, the number I thought of is </span>2<span class="cmss-10x-x-109">. By asking </span><span class="cmssi-10x-x-109">‚Äúis the number larger than </span>3<span class="cmssi-10x-x-109">‚Äù?</span><span class="cmss-10x-x-109">, the answer trims out four of the candidates.</span></p>
<div class="minipage">
<p><img src="../media/file1994.png" width="484" alt="PIC"/> <span id="x1-341004r8"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.8: The search space after the question is the number larger than 3?</span> </span>
</div>
<p><span class="cmss-10x-x-109">Each subsequent question cuts the remaining possibilities in half. In the case of </span><span class="cmmi-10x-x-109">X </span>= 2<span class="cmss-10x-x-109">, the three questions are the following:</span></p>
<ul>
<li><span class="cmss-10x-x-109">Is </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>4<span class="cmss-10x-x-109">? </span><span class="cmssi-10x-x-109">(no)</span></li>
<li><span class="cmss-10x-x-109">Is </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>2<span class="cmss-10x-x-109">? </span><span class="cmssi-10x-x-109">(yes)</span></li>
<li><span class="cmss-10x-x-109">Is </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>3<span class="cmss-10x-x-109">? </span><span class="cmssi-10x-x-109">(no)</span></li>
</ul>
<p><span class="cmss-10x-x-109">This is the</span> <span id="dx1-341005"></span><span class="cmss-10x-x-109">so-called </span><span class="cmssi-10x-x-109">binary search</span><span class="cmss-10x-x-109">, illustrated by </span><span class="cmssi-10x-x-109">Figure¬†</span><a href="#"><span class="cmssi-10x-x-109">20.9</span></a><span class="cmss-10x-x-109">.</span></p>
<div class="minipage">
<p><img src="../media/file1995.png" width="484" alt="PIC"/> <span id="x1-341006r9"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.9: Figuring out the answer with binary search</span> </span>
</div>
<p><span class="cmss-10x-x-109">If we write down</span> <span id="dx1-341007"></span><span class="cmss-10x-x-109">the answers for our three consecutive questions (</span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>4<span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>2<span class="cmss-10x-x-109">, </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>3<span class="cmss-10x-x-109">) as a zero-one sequence, we obtain </span>010<span class="cmss-10x-x-109">. If this looks familiar, it‚Äôs not an accident: </span>010 <span class="cmss-10x-x-109">is </span>2 <span class="cmss-10x-x-109">in binary. In fact, all of the answers can be coded using their binary form:</span></p>
<div class="math-display">
<img src="../media/file1996.png" class="math-display" alt="0 = 0002, 1 = 0012, 2 = 0102, 3 = 0112 4 = 1002, 5 = 1012, 6 = 1102, 7 = 1112. "/>
</div>
<p><span class="cmss-10x-x-109">Thus, we can reformulate our three questions:</span></p>
<ol>
<li><span id="x1-341009x1"><span class="cmss-10x-x-109">Is </span>1 <span class="cmss-10x-x-109">the 1st digit of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">in binary?</span></span></li>
<li><span id="x1-341011x2"><span class="cmss-10x-x-109">Is </span>1 <span class="cmss-10x-x-109">the 2nd digit of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">in binary?</span></span></li>
<li><span id="x1-341013x3"><span class="cmss-10x-x-109">Is </span>1 <span class="cmss-10x-x-109">the 3rd digit of </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">in binary?</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">As the above example shows, guessing the number is equivalent to finding the binary representation of the objects to be guessed. Each digit represents exactly one </span><span class="cmssi-10x-x-109">bit </span><span class="cmss-10x-x-109">of information. (In this case, the representation is the actual binary form.) Binary codings have an additional perk: we no longer have to sequentially go through the questions, we can ask them simultaneously. From now on, instead of questions, we‚Äôll talk about binary representations (codings) and their bits.</span></p>
<p><span class="cmss-10x-x-109">Notice that the number of bits is the same for each outcome of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. Thus, as this strategy always requires three bits, their average number is three as well:</span></p>
<div class="math-display">
<img src="../media/file1997.png" class="math-display" alt=" 7 7 ùîº [#bits] = ‚àë 3‚ãÖP (X = k ) = ‚àë 3‚ãÖ 1-= 3. 8 k=0 k=0 "/>
</div>
<p><span class="cmss-10x-x-109">Can we do better than the three questions on average? No.¬†I invite you to come up</span> <span id="dx1-341014"></span><span class="cmss-10x-x-109">with your arguments, but we‚Äôll see this later.</span></p>
<p><span class="cmss-10x-x-109">Where does the number three in the above come from? In general, if we have </span>2<sup><span class="cmmi-8">k</span></sup> <span class="cmss-10x-x-109">possible choices, then</span> log <sub><span class="cmr-8">2</span></sub>2<sup><span class="cmmi-8">k</span></sup> = <span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">questions will be enough to find the answer. (As each question cuts the set of possible answers in half.) In other words, we have</span></p>
<div class="math-display">
<img src="../media/file1998.png" class="math-display" alt=" ‚àë7 ùîº[#bits] = P (X = k )log2 23 k=0 7 = ‚àë P (X = k )log P(X = k)‚àí1. 2 k=0 "/>
</div>
<p><span class="cmss-10x-x-109">Thus, the value</span> log <sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is the number of bits needed to represent </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">in our coding. In other words,</span></p>
<div class="math-display">
<img src="../media/file1999.png" class="math-display" alt="ùîº[#bits] = ùîº[log2P (X = k)‚àí1]. "/>
</div>
<p><span class="cmss-10x-x-109">Let‚Äôs get a bit ahead of ourselves: this is the famous </span><span class="cmssi-10x-x-109">entropy </span><span class="cmss-10x-x-109">of the random variable </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, and the quantity</span> log <sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">is the so-called </span><span class="cmssi-10x-x-109">information content </span><span class="cmss-10x-x-109">of the event </span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">However, at this point, these concepts are quite unclear. What does</span> log <sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">have to do with information? Why can‚Äôt we represent </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">better than</span> log <sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> <span class="cmss-10x-x-109">bits? We‚Äôll see the answers soon.</span></p>
</section>
<section id="guess-the-number-electric-boogaloo" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_307"><span class="titlemark"><span class="cmss-10x-x-109">20.6.2 </span></span> <span id="x1-34200024.6.2"></span><span class="cmss-10x-x-109">Guess the number 2: Electric Boogaloo</span></h3>
<p><span class="cmss-10x-x-109">Let‚Äôs play the</span> <span id="dx1-342001"></span><span class="cmss-10x-x-109">guessing game again but with a twist this time. Now, I have picked a number from </span><span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, and you have to guess which one. The catch is, I am twice as likely to select </span>0 <span class="cmss-10x-x-109">than the others.</span></p>
<p><span class="cmss-10x-x-109">In probabilistic terms, if </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">denotes the number I picked, then </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= 0) = 1<span class="cmmi-10x-x-109">‚àï</span>2<span class="cmss-10x-x-109">, while </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= 1) = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= 2) = 1<span class="cmmi-10x-x-109">‚àï</span>4<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">What is the best strategy? There are two key facts to recall:</span></p>
<ul>
<li><span class="cmss-10x-x-109">good questions cut the search space in half,</span></li>
<li><span class="cmss-10x-x-109">and asking questions is equivalent to finding a binary encoding of the outcomes.</span></li>
</ul>
<p><span class="cmss-10x-x-109">However, as we are looking for the encoding that is optimal </span><span class="cmssi-10x-x-109">on average</span><span class="cmss-10x-x-109">, cutting the search space in half with each digit is </span><span class="cmssi-10x-x-109">not </span><span class="cmss-10x-x-109">meant in a numeric way. Rather, in a probabilistic one. Thus, if </span>0 <span class="cmss-10x-x-109">is indeed twice as likely, representing </span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>2 <span class="cmss-10x-x-109">by</span></p>

<img width="75" src="../media/file2000.png" class="math-display" alt="0 ‚àº 0, 1 ‚àº 01, 2 ‚àº 10, "/>

<p><span class="cmss-10x-x-109">the average number of bits is</span></p>
<img src="../media/file2001.png" width="450" class="math-display" alt="ùîº[#bits] = P (X = 0) ‚ãÖ1+ P (X = 1) ‚ãÖ2+ P (X = 2) ‚ãÖ2 = P (X = 0) log22 + P (X = 1)log24 + P (X = 2)log24 2 ‚àë ‚àí1 = P(X = k)log2P (X = k) k=0 = 3-. 2 "/>

<p><span class="cmss-10x-x-109">Once more, we have arrived at the familiar logarithmic formula. We are</span><span id="dx1-342002"></span> <span class="cmss-10x-x-109">one step closer to grasping the meaning of the mysterious quantity</span> log <sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup><span class="cmss-10x-x-109">. The smaller it is, the more questions we need; equivalently, the more bits we need to represent </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">within our encoding to avoid information loss.</span></p>
<p><span class="cmss-10x-x-109">So, what are these mysterious quantities exactly?</span></p>
</section>
<section id="information-and-entropy" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_308"><span class="titlemark"><span class="cmss-10x-x-109">20.6.3 </span></span> <span id="x1-34300024.6.3"></span><span class="cmss-10x-x-109">Information and entropy</span></h3>
<p><span class="cmss-10x-x-109">It is time to</span> <span id="dx1-343001"></span><span class="cmss-10x-x-109">formulate the general problem. Suppose that our random variable </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">assumes a number from the set </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,‚Ä¶,N</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">, each with probability </span><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub> = <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">k</span>)<span class="cmss-10x-x-109">. Upon repeatedly observing </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, what is the average information content of our observations?</span></p>
<p><span class="cmss-10x-x-109">According to what we‚Äôve learned, we are looking for the quantity</span></p>
<div class="math-display">
<img src="../media/file2002.png" class="math-display" alt=" ‚àëN ùîº [I (X )] = ‚àí p logp , k=1 k k "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">I </span>: <span class="msbm-10x-x-109">‚Ñï </span><span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">denotes the information </span><span class="cmmi-10x-x-109">I</span>(<span class="cmmi-10x-x-109">k</span>) = <span class="cmsy-10x-x-109">‚àí</span>log <span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">. Previously, we have seen two special cases where </span><span class="cmmi-10x-x-109">I</span>(<span class="cmmi-10x-x-109">k</span>) <span class="cmss-10x-x-109">is the average number of questions needed to guess </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">. (Equivalently, the information is the average number of bits in </span><span class="cmmi-10x-x-109">k </span><span class="cmss-10x-x-109">using the optimal encoding.)</span></p>
<p><span class="cmss-10x-x-109">What is the information in general?</span></p>
<p><span class="cmss-10x-x-109">Let‚Äôs look for </span><span class="cmmi-10x-x-109">I </span><span class="cmss-10x-x-109">as an unknown function of the probabilities: </span><span class="cmmi-10x-x-109">I</span>(<span class="cmmi-10x-x-109">x</span>) = <span class="cmmi-10x-x-109">f</span><span class="big">(</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span>)<span class="big">)</span><span class="cmss-10x-x-109">. What can </span><span class="cmmi-10x-x-109">f </span><span class="cmss-10x-x-109">be? There are two key properties of that‚Äôll lead us to the answer. First, the more probable an event is, the less information content there is. (Recall the previous example, where the most probable outcome required the least amount of bits in our binary representation.)</span></p>
<p><span class="cmss-10x-x-109">Second, as a</span> <span id="dx1-343002"></span><span class="cmss-10x-x-109">function of the probabilities, the information is additive: </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">pq</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>) + <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">q</span>)<span class="cmss-10x-x-109">. Why? Suppose that I have picked two numbers, independently from each other, and now you have to guess those two. You can do this sequentially, applying the optimal strategy to the first one, then the second one.</span></p>
<p><span class="cmss-10x-x-109">In mathematical terms, </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">is</span></p>
<ul>
<li><span class="cmss-10x-x-109">continuous,</span></li>
<li><span class="cmss-10x-x-109">strictly increasing, that is, </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>)<span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">q</span>) <span class="cmss-10x-x-109">for any </span><span class="cmmi-10x-x-109">p/span&gt;<span class="cmmi-10x-x-109">q</span><span class="cmss-10x-x-109">,</span> </span></span></li>
<li><span class="cmss-10x-x-109">and additive, that is, </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">pq</span>) = <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>) + <span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">q</span>) <span class="cmss-10x-x-109">for any </span><span class="cmmi-10x-x-109">p,q</span><span class="cmss-10x-x-109">.</span></li>
</ul>
<p><span class="cmss-10x-x-109">I‚Äôll spare you the mathematical details, but with a bit of calculus magic, we can confidently conclude that the only option is </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>) = <span class="cmsy-10x-x-109">‚àí</span>log <sub><span class="cmmi-8">a</span></sub><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">a/span&gt;1<span class="cmss-10x-x-109">. Seemingly, information depends on the base, but as</span> </span></p>
<div class="math-dispay">
<img src="../media/file2005.png" width="150" class="math-display" alt="log x = logax-, b logab "/>
</div>
<p><span class="cmss-10x-x-109">the choice of base only influences the information and entropy up to a multiplicative scaling factor. Thus, using the natural logarithm is the simplest choice.</span></p>
<p><span class="cmss-10x-x-109">So, here is the formal definition at last.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-343003r96"></span> <span class="cmbx-10x-x-109">Definition 96.</span> </span><span class="cmbx-10x-x-109">(Information)</span></p>
<p>Let <span class="cmmi-10x-x-109">X </span>be a discrete random variable with probability mass function <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span></sub>.</p>
<p>The information of the event <span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub> is defined by</p>
<div class="math-display">
<img src="../media/file2006.png" class="math-display" alt="I(xk) := ‚àí logP (X = xk ) = logP (X = xk )‚àí 1. "/>
</div>
</div>
<p><span class="cmss-10x-x-109">(Note that whenever the base of</span> log <span class="cmss-10x-x-109">is not indicated, we are using the natural base </span><span class="cmmi-10x-x-109">e</span><span class="cmss-10x-x-109">.) To emphasize the dependency of the information on </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, we‚Äôll sometimes explicitly denote the connection by </span><span class="cmmi-10x-x-109">I</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Armed with the notion of information, we are ready to define entropy, the average amount of information per observation. This quantity is named after Claude Shannon, who essentially founded information theory in his epic paper </span><span class="cmssi-10x-x-109">‚ÄúA Mathematical Theory of Communication.‚Äù</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-343004r97"></span> <span class="cmbx-10x-x-109">Definition 97.</span> </span><span class="cmbx-10x-x-109">(Shannon entropy)</span></p>
<p>Let <span class="cmmi-10x-x-109">X </span>be a discrete random variable with probability mass function <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmsy-10x-x-109">}</span><sub><span class="cmmi-8">k</span></sub>.</p>
<p>The entropy of <span class="cmmi-10x-x-109">X </span>is defined by</p>
<div class="math-display">
<img src="../media/file2007.png" class="math-display" alt="H[X ] := ùîº[I(X )] ‚àë‚àû = ‚àí P(X = xk)logP (X = xk). k=1 "/>
</div>
</div>
<p><span class="cmss-10x-x-109">Even though </span><span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">is called the </span><span class="cmssi-10x-x-109">Shannon entropy</span><span class="cmss-10x-x-109">, we‚Äôll</span><span id="dx1-343005"></span> <span class="cmss-10x-x-109">just simply</span> <span id="dx1-343006"></span><span class="cmss-10x-x-109">refer to it as </span><span class="cmssi-10x-x-109">entropy</span><span class="cmss-10x-x-109">, unless an explicit distinction is needed.</span></p>
<p><span class="cmss-10x-x-109">One of the first things we can notice is that </span><span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmsy-10x-x-109">‚â• </span>0<span class="cmss-10x-x-109">. This is shown in the following proposition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-343007r7"></span> <span class="cmbx-10x-x-109">Proposition 7.</span> </span><span class="cmbx-10x-x-109">(The nonnegativity of entropy)</span></p>
<p>Let <span class="cmmi-10x-x-109">X </span>be an arbitrary discrete random variable. Then <span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmsy-10x-x-109">‚â• </span>0.</p>
</div>
<div id="tcolobox-335" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-content">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">By definition,</span></p>
<div class="math-display">
<img src="../media/file2008.png" class="math-display" alt=" ‚àë H [X ] = P(X = xk)logP (X = xk)‚àí1. k "/>
</div>
<p><span class="cmss-10x-x-109">First, suppose that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<span class="cmmi-10x-x-109">‚â†</span>0 <span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">. Then, as </span>0 <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>) <span class="cmsy-10x-x-109">‚â§ </span>1<span class="cmss-10x-x-109">, the information is nonnegative:</span> log <span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> <span class="cmsy-10x-x-109">‚â• </span>0<span class="cmss-10x-x-109">. Hence, as all terms in the defining sum are nonnegative, </span><span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">is nonnegative as well.</span></p>
<p><span class="cmss-10x-x-109">If </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>) = 0 <span class="cmss-10x-x-109">for some </span><span class="cmmi-10x-x-109">k</span><span class="cmss-10x-x-109">, then, as</span> lim<sub><span class="cmmi-8">x</span><span class="cmsy-8">‚Üí</span><span class="cmr-8">0+</span></sub><span class="cmmi-10x-x-109">x</span>log <span class="cmmi-10x-x-109">x </span>= 0<span class="cmss-10x-x-109">, the expression </span>0 <span class="cmsy-10x-x-109">‚ãÖ</span> log 0 <span class="cmss-10x-x-109">is taken to be </span>0<span class="cmss-10x-x-109">. Thus, </span><span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">is still nonnegative.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">Computing the entropy in practice is hard, as we have to evaluate sums that involve logarithms. However, there are a few special cases that shed some much needed light on the concept of entropy. Let‚Äôs look at them!</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmssi-10x-x-109">The discrete uniform distribution. </span><span class="cmss-10x-x-109">(See the definition of the discrete uniform distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-uniform-distribution"><span class="cmssi-10x-x-109">19.2.4</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Uniform(<span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,‚Ä¶,n</span><span class="cmsy-10x-x-109">}</span>)<span class="cmss-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file2009.png" class="math-display" alt=" ‚àën H [X ] = ‚àí -1log 1- k=1n n ‚àën = 1logn k=1 n = log n. "/>
</div>
<p><span class="cmss-10x-x-109">By now, we have an intuitive understanding of entropy as the average amount of information per observation. Take a wild guess: how does the entropy of the uniform distribution compare amongst all other distributions concentrated on </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,‚Ä¶,n</span><span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">? Is it above or below average? Is it perhaps minimal or maximal?</span></p>
<p><span class="cmss-10x-x-109">We‚Äôll reveal the answer by the end of this chapter, but take a minute to ponder this question before moving on to the next example.</span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmssi-10x-x-109">The single-point distribution. </span><span class="cmss-10x-x-109">(See the definition of the single-point distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-singlepoint-distribution"><span class="cmssi-10x-x-109">19.2.5</span></a><span class="cmss-10x-x-109">) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span><span class="cmmi-10x-x-109">Œ¥</span>(<span class="cmmi-10x-x-109">a</span>)<span class="cmss-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file2010.png" class="math-display" alt="H [X ] = ‚àí 1 ‚ãÖlog 1 = 0. "/>
</div>
<p><span class="cmss-10x-x-109">In other</span> <span id="dx1-343008"></span><span class="cmss-10x-x-109">words, as the event </span><span class="cmmi-10x-x-109">X </span>= <span class="cmmi-10x-x-109">a </span><span class="cmss-10x-x-109">is certain, no information is gained upon observing </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. Now think back to the previous example. As </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span><span class="cmmi-10x-x-109">Œ¥</span>(<span class="cmmi-10x-x-109">k</span>) <span class="cmss-10x-x-109">is concentrated on </span><span class="cmsy-10x-x-109">{</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,‚Ä¶,n</span><span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">for all </span><span class="cmmi-10x-x-109">k </span>= 1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,‚Ä¶,n</span><span class="cmss-10x-x-109">, give the previous question one more thought.</span></p>
<p><span class="cmss-10x-x-109">Let‚Äôs see a partial answer in the next example.</span></p>
<p><span class="cmssbx-10x-x-109">Example 3. </span><span class="cmssi-10x-x-109">The Bernoulli distribution. </span><span class="cmss-10x-x-109">(See the definition of the Bernoulli distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-bernoulli-distribution"><span class="cmssi-10x-x-109">19.2.1</span></a><span class="cmss-10x-x-109">). Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">. Then, it is easy to see that</span></p>
<div class="math-display">
<img src="../media/file2011.png" class="math-display" alt="H [X ] = ‚àí plogp ‚àí (1‚àí p)log(1‚àí p). "/>
</div>
<p><span class="cmss-10x-x-109">Which value of </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">maximizes the entropy? To find the maxima of </span><span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>]<span class="cmss-10x-x-109">, we can turn to the derivatives. (Recall how the derivative and second derivative can be used for optimization, as claimed by </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch021.xhtml#x1-214004r87"><span class="cmssi-10x-x-109">87</span></a><span class="cmss-10x-x-109">.)</span></p>
<p><span class="cmss-10x-x-109">Thus, let </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>) = <span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] = <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">p</span>log <span class="cmmi-10x-x-109">p </span><span class="cmsy-10x-x-109">‚àí </span>(1 <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">p</span>)log(1 <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">. Then,</span></p>
<div class="math-display">
<img src="../media/file2012.png" class="math-display" alt="f‚Ä≤(p) = ‚àí logp + log(1‚àí p) = log 1-‚àí-p, p f‚Ä≤‚Ä≤(p) = ‚àí 1-‚àí--1--. p 1 ‚àí p "/>
</div>
<p><span class="cmss-10x-x-109">By solving </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">‚Ä≤</span></sup>(<span class="cmmi-10x-x-109">p</span>) = 0<span class="cmss-10x-x-109">, we obtain that </span><span class="cmmi-10x-x-109">p </span>= 1<span class="cmmi-10x-x-109">‚àï</span>2<span class="cmss-10x-x-109">, which is the only potential extrema of </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">. As </span><span class="cmmi-10x-x-109">f</span><sup><span class="cmsy-8">‚Ä≤‚Ä≤</span></sup>(1<span class="cmmi-10x-x-109">‚àï</span>2) = <span class="cmsy-10x-x-109">‚àí</span>4<span class="cmmi-10x-x-109">/span&gt;0<span class="cmss-10x-x-109">, we see that </span><span class="cmmi-10x-x-109">p </span>= 1<span class="cmmi-10x-x-109">‚àï</span>2 <span class="cmss-10x-x-109">is indeed a local maximum. Let‚Äôs plot </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">to obtain a visual confirmation as well.</span> </span></p>
<div id="tcolobox-336" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def bernoulli_entropy(p): 
    return -p*np.log(p) - (1 - p)*np.log(1 - p) 
 
X = np.linspace(0.001, 0.999, 100) 
y = bernoulli_entropy(X) 
with plt.style.context(‚Äôseaborn-v0_8‚Äô): 
    plt.figure(figsize=(8, 8)) 
    plt.xlabel("/span&gt;p 
    plt.ylabel("/span&gt;H[X]" 
    plt.title("/span&gt;The entropy of Bernoulli(p) 
    plt.plot(X, y) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file2013.png" width="499" alt="PIC"/> <span id="x1-343021r10"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.10: The entropy of the Bernoulli distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">For </span><span class="cmmi-10x-x-109">p </span>= 1<span class="cmmi-10x-x-109">‚àï</span>2<span class="cmss-10x-x-109">, that is, where the entropy of</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">is maximal, we have a uniform distribution on</span><span id="dx1-343022"></span> <span class="cmss-10x-x-109">the two-element set </span><span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmsy-10x-x-109">}</span><span class="cmss-10x-x-109">. On the other hand, for </span><span class="cmmi-10x-x-109">p </span>= 0 <span class="cmss-10x-x-109">or </span><span class="cmmi-10x-x-109">p </span>= 1<span class="cmss-10x-x-109">, where the entropy is minimal,</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">is a single-point distribution.</span></p>
<p><span class="cmss-10x-x-109">As every random variable on </span><span class="cmsy-10x-x-109">{</span>0<span class="cmmi-10x-x-109">,</span>1<span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">is Bernoulli-distributed, we seem to have a partial answer to our question: the uniform distribution maximizes entropy, while single-point ones minimize it.</span></p>
<p><span class="cmss-10x-x-109">As the following theorem indicates, this is true in general as well.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-343023r138"></span> <span class="cmbx-10x-x-109">Theorem 138.</span> </span><span class="cmbxti-10x-x-109">(The uniform distribution and maximal entropy)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">E </span>= <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub><span class="cmsy-10x-x-109">}</span><span class="cmti-10x-x-109">be a finite set, and let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="cmmi-10x-x-109">E </span><span class="cmti-10x-x-109">be a random variable that assumes values in </span><span class="cmmi-10x-x-109">E</span><span class="cmti-10x-x-109">. Then,</span></p>
<div class="math-display">
<img src="../media/file2014.png" class="math-display" alt="H [X] ‚â§ H [Uniform (E)], "/>
</div>
<p><span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">H</span>[<span class="cmmi-10x-x-109">X</span>] = <span class="cmmi-10x-x-109">H</span>[Uniform(<span class="cmmi-10x-x-109">E</span>)] <span class="cmti-10x-x-109">if and only if </span><span class="cmmi-10x-x-109">X </span><span class="cmti-10x-x-109">is uniformly distributed on </span><span class="cmmi-10x-x-109">E</span><span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">We are not going to show this here, but there are several proofs out there. For instance, Bishop‚Äôs classic </span><span class="cmssi-10x-x-109">Pattern Recognition and Machine Learning </span><span class="cmss-10x-x-109">uses the Lagrange multiplier method to explicitly find the maximum of the multivariable function </span><span class="cmmi-10x-x-109">f</span>(<span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,p</span><sub><span class="cmmi-8">n</span></sub>) = <span class="cmsy-10x-x-109">‚àí</span><span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub> log <span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">k</span></sub><span class="cmss-10x-x-109">; feel free to check it out for the details.</span></p>
<p><span class="cmss-10x-x-109">What if we</span> <span id="dx1-343024"></span><span class="cmss-10x-x-109">don‚Äôt restrict our discrete random variable to a finite set? In that case, the Shannon entropy has no upper limit. In the problem set of this chapter, you‚Äôll see that the entropy of the geometric distribution is</span></p>
<div class="math-display">
<img src="../media/file2015.png" class="math-display" alt="H [Geo (p)] = ‚àí plogp-+-(1‚àí-p)-log(1-‚àí-p). p "/>
</div>
<p><span class="cmss-10x-x-109">It is easy to see that</span> lim<sub><span class="cmmi-8">p</span><span class="cmsy-8">‚Üí</span><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">H</span>[Geo(<span class="cmmi-10x-x-109">p</span>)] = <span class="cmsy-10x-x-109">‚àû</span><span class="cmss-10x-x-109">. Let‚Äôs plot this!</span></p>
<div id="tcolobox-337" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>def geom_entropy(p): 
    return -(p*np.log(p) + (1 - p)*np.log(1 - p))/p 
 
X = np.linspace(1e-16, 1-1e-16, 1000) 
y = geom_entropy(X) 
 
 
with plt.style.context(‚Äôseaborn-v0_8‚Äô): 
    plt.figure(figsize=(8, 8)) 
    plt.xlabel("/span&gt;p 
    plt.ylabel("/span&gt;H[X]" 
    plt.title("/span&gt;The entropy of Geo(p) 
    plt.plot(X, y) 
    plt.show()</code></pre>
</div>
</div>
<div class="minipage">
<p><img src="../media/file2016.png" width="284" alt="PIC"/> <span id="x1-343039r11"></span></p>
<span class="id"><span class="cmss-10x-x-109">Figure¬†20.11: The entropy of the geometric distribution</span> </span>
</div>
<p><span class="cmss-10x-x-109">Thus, the</span> <span id="dx1-343040"></span><span class="cmss-10x-x-109">Shannon entropy can assume any nonnegative value.</span></p>
</section>
<section id="differential-entropy" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_309"><span class="titlemark"><span class="cmss-10x-x-109">20.6.4 </span></span> <span id="x1-34400024.6.4"></span><span class="cmss-10x-x-109">Differential entropy</span></h3>
<p><span class="cmss-10x-x-109">So far, we</span> <span id="dx1-344001"></span><span class="cmss-10x-x-109">have only defined the</span> <span id="dx1-344002"></span><span class="cmss-10x-x-109">entropy for discrete random variables.</span></p>
<p><span class="cmss-10x-x-109">Does it translate to continuous ones as well? Yes. The formula </span><span class="msbm-10x-x-109">ùîº</span>[ <span class="cmsy-10x-x-109">‚àí</span> log <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub>(<span class="cmmi-10x-x-109">X</span>)] <span class="cmss-10x-x-109">can be directly applied for continuous random variables, yielding the so-called </span><span class="cmssi-10x-x-109">differential entropy</span><span class="cmss-10x-x-109">. Here is the formal definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-344003r98"></span> <span class="cmbx-10x-x-109">Definition 98.</span> </span><span class="cmbx-10x-x-109">(Differential entropy)</span></p>
<p>Let <span class="cmmi-10x-x-109">X </span>be a continuous random variable. The <span class="cmti-10x-x-109">differential entropy </span>of <span class="cmmi-10x-x-109">X </span>is defined by the formula</p>
<div class="math-display">
<img src="../media/file2017.png" class="math-display" alt=" ‚à´ ‚àû H [X ] := ‚àí f (x )log f (x)dx, ‚àí‚àû X X "/>
</div>
<p>where <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub> denotes the probability density function of <span class="cmmi-10x-x-109">X</span>.</p>
</div>
<p><span class="cmss-10x-x-109">Now comes the surprise. Can we derive the formula from the Shannon entropy? We are going to approach the problem like we did when we defined the expected value for continuous random variables in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch032.xhtml#continuous-random-variables"><span class="cmssi-10x-x-109">20.2</span></a><span class="cmss-10x-x-109">: approximate the continuous random variable with a discrete one, then see where the Shannon entropy converges.</span></p>
<p><span class="cmss-10x-x-109">Thus, let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">be a continuous random variable, and let</span> [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmsy-10x-x-109">‚äÜ</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">be a (large) interval, so large that </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X</span><img src="../media/file2018.png" width="8" class="notin" alt="‚àà‚àï"/>[<span class="cmmi-10x-x-109">a,b</span>]) <span class="cmss-10x-x-109">is extremely small. We‚Äôll subdivide</span> [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">into </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">equal parts by</span></p>
<div class="math-display">
<img src="../media/file2019.png" class="math-display" alt=" k(b‚àí a) xk = a+ -------, k = 0,1,...,n, n "/>
</div>
<p><span class="cmss-10x-x-109">and define the</span> <span id="dx1-344004"></span><span class="cmss-10x-x-109">approximating random variable </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sup> <span class="cmss-10x-x-109">by</span></p>

<img src="../media/file2020.png" width="450" class="math-display" alt=" ( (n) |{ xk if x ‚àà (xk ‚àí1,xk] for some k = 1,2,...,n, X (œâ ) := | ( 0 otherwise. "/>

<p><span class="cmss-10x-x-109">This way, the</span> <span id="dx1-344005"></span><span class="cmss-10x-x-109">entropy of </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sup> <span class="cmss-10x-x-109">is given by</span></p>
<div class="math-display">
<img src="../media/file2021.png" class="math-display" alt=" (n) ‚àën (n) (n) H [X ] = ‚àí P (X = xk)logP (X = xk). k=1 "/>
</div>
<p><span class="cmss-10x-x-109">However, due to how we defined </span><span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sup><span class="cmss-10x-x-109">,</span></p>
<div class="math-display">
<img src="../media/file2022.png" class="math-display" alt=" ‚à´ (n) xk P(X = xk) = P(xk‚àí 1 &lt;X ‚â§ Xk ) = x fX (x)dx, k‚àí1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span></sub> <span class="cmss-10x-x-109">is the density function of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. Now, the mean value theorem for definite integrals (</span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch022.xhtml#x1-235008r93"><span class="cmssi-10x-x-109">93</span></a><span class="cmss-10x-x-109">) gives that there is a </span><span class="cmmi-10x-x-109">Œæ</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">‚àà </span>[<span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmmi-8">k</span></sub>] <span class="cmss-10x-x-109">such that</span></p>
<div class="math-display">
<img src="../media/file2023.png" class="math-display" alt="‚à´ xk f (Œæ ) fX (x)dx = (xk ‚àí xk‚àí 1)fX (Œæk) = -X---k-, xk‚àí1 n "/>
</div>
<p><span class="cmss-10x-x-109">thus, in conclusion,</span></p>
<div class="math-display">
<img src="../media/file2024.png" class="math-display" alt=" (n) fX-(Œæk) P (X = xk ) = n . "/>
</div>
<p><span class="cmss-10x-x-109">(Recall that as the partition </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">/span&gt;<span class="cmmi-10x-x-109">‚Ä¶/span&gt;<span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">is equidistant, </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub> <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sub> = 1<span class="cmmi-10x-x-109">‚àïn</span><span class="cmss-10x-x-109">.)</span> </span></span></span></p>
<p><span class="cmss-10x-x-109">Now, using </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sup> = <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">k</span></sub>) = <img src="../media/file2025.png" width="25" data-align="middle" alt="fX(Œæk) n"/><span class="cmss-10x-x-109">, we obtain</span></p>

<img src="../media/file2026.png" width="450" class="math-display" alt=" ‚àën H[X (n)] = ‚àí P (X (n) = xk )log P(X (n) = xk) k=1 n = ‚àí ‚àë fX-(Œæk)-log fX-(Œæk)- n n k=n1 n = ‚àí ‚àë fX-(Œæk)-log f (Œæ )+ log n‚àë fX(Œæk). n X k n k=1 k=1 "/>

<p><span class="cmss-10x-x-109">Both of these</span> <span id="dx1-344006"></span><span class="cmss-10x-x-109">terms are Riemann-sums, approximating the integral of the functions inside. If </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">is large, and the interval</span> [<span class="cmmi-10x-x-109">a,b</span>] <span class="cmss-10x-x-109">is big enough, then</span></p>

<img src="../media/file2027.png" width="450" class="math-display" alt=" n ‚à´ ‚àí ‚àë fX-(Œæk)logf (Œæ ) ‚âà ‚àí ‚àû f (x)logf (x)dx = h[X ], n X k ‚àí‚àû X X k=1 "/>

<p><span class="cmss-10x-x-109">and</span></p>
<div class="math-display">
<img src="../media/file2028.png" class="math-display" alt=" n ‚à´ ‚àû ‚àë fX-(Œæk)-‚âà f (x)dx = 1, n ‚àí ‚àû X k=1 "/>
</div>
<p><span class="cmss-10x-x-109">implying</span></p>
<div class="math-display">
<img src="../media/file2029.png" class="math-display" alt="H [X (n)] ‚âà h [X ]+ logn. "/>
</div>
<p><span class="cmss-10x-x-109">This is</span> <span id="dx1-344007"></span><span class="cmss-10x-x-109">quite surprising, as one would expect </span>H[<span class="cmmi-10x-x-109">X</span><sup><span class="cmr-8">(</span><span class="cmmi-8">n</span><span class="cmr-8">)</span></sup>] <span class="cmss-10x-x-109">to converge towards </span><span class="cmmi-10x-x-109">h</span>(<span class="cmmi-10x-x-109">X</span>)<span class="cmss-10x-x-109">. This is not the case. In fact,</span></p>
<div class="math-display">
<img src="../media/file2030.png" class="math-display" alt=" lim (H [X (n)]‚àí log n) = h[X ] n‚Üí ‚àû "/>
</div>
<p><span class="cmss-10x-x-109">holds.</span></p>
<p><span class="cmss-10x-x-109">It‚Äôs time for the examples.</span></p>
<p><span class="cmssbx-10x-x-109">Example 1. </span><span class="cmssi-10x-x-109">The uniform distribution. </span><span class="cmss-10x-x-109">(See the definition of the uniform distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-uniform-distribution2"><span class="cmssi-10x-x-109">19.3.4</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Uniform(<span class="cmmi-10x-x-109">a,b</span>)<span class="cmss-10x-x-109">. Then,</span></p>
<div class="math-display">
<img src="../media/file2031.png" class="math-display" alt=" ‚à´ b -1--- --1-- h[X ] = ‚àí b‚àí a log b‚àí a dx a = log(b‚àí a), "/>
</div>
<p><span class="cmss-10x-x-109">which is similar to the discrete uniform case. However, there is one notable difference: </span><span class="cmmi-10x-x-109">h</span>(<span class="cmmi-10x-x-109">X</span>) <span class="cmss-10x-x-109">is negative when </span><span class="cmmi-10x-x-109">b</span><span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">a/span&gt;1<span class="cmss-10x-x-109">. This is in stark contrast with the Shannon entropy, which is always nonnegative.</span> </span></p>
<p><span class="cmssbx-10x-x-109">Example 2. </span><span class="cmssi-10x-x-109">The normal distribution. </span><span class="cmss-10x-x-109">(See the definition of the normal distribution in </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-normal-distribution"><span class="cmssi-10x-x-109">19.3.6</span></a><span class="cmss-10x-x-109">.) Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àºùí©</span>(<span class="cmmi-10x-x-109">Œº,œÉ</span><sup><span class="cmr-8">2</span></sup>)<span class="cmss-10x-x-109">. Then,</span></p>

<img src="../media/file2032.png" width="550" class="math-display" alt=" ‚à´ ‚àû 1 (x‚àíŒº)2 ( 1 (x‚àíŒº)2) h[X ] = ‚àí -‚àö---e‚àí 2œÉ2 log -‚àö----e‚àí 2œÉ2 dx ‚àí ‚àû œÉ 2œÄ‚à´ œÉ 2œÄ ‚à´ ( ‚àö ---) ‚àû --1---‚àí (x‚àí2œÉŒº2)2- ‚àû (x‚àí-Œº-)2---1---‚àí (x‚àí2ŒºœÉ)22 = log œÉ 2œÄ ‚àí‚àû œÉ‚àö 2œÄe dx+ ‚àí ‚àû 2œÉ2 œÉ‚àö 2œÄe dx ‚óü--------‚óù‚óú--------‚óû ‚óü------------‚óù‚óú------------‚óû =1 = 21œÉ2Var[X ]= 12 1 ( 2 ) = 2- 1+ log(œÉ 2œÄ ) . "/>

<p><span class="cmss-10x-x-109">Depending on</span> <span id="dx1-344008"></span><span class="cmss-10x-x-109">the value of </span><span class="cmmi-10x-x-109">œÉ</span><span class="cmss-10x-x-109">, the value of </span><span class="cmmi-10x-x-109">h</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">can be negative here as well.</span></p>
<p><span class="cmss-10x-x-109">Previously, we</span> <span id="dx1-344009"></span><span class="cmss-10x-x-109">have seen that for discrete distributions on a given finite set, the uniform distribution maximizes entropy, as </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-343023r138"><span class="cmssi-10x-x-109">138</span></a> <span class="cmss-10x-x-109">claims.</span></p>
<p><span class="cmss-10x-x-109">What is the analogue of </span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-343023r138"><span class="cmssi-10x-x-109">138</span></a> <span class="cmss-10x-x-109">for continuous distributions? Take a wild guess. If we let </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">be any continuous distribution, then, as we have seen,</span></p>
<div class="math-display">
<img src="../media/file2033.png" class="math-display" alt="h[Uniform (a,b)] = log(b‚àí a), "/>
</div>
<p><span class="cmss-10x-x-109">which can assume any real number. Similarly to the discrete case, we have to make restrictions; this time, we‚Äôll fix the variance. Here is</span><span id="dx1-344010"></span> <span class="cmss-10x-x-109">the result.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-344011r139"></span> <span class="cmbx-10x-x-109">Theorem 139.</span> </span><span class="cmbxti-10x-x-109">(Maximizing the differential entropy)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmti-10x-x-109">be a continuous random variable with variance </span><span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup><span class="cmti-10x-x-109">. Then</span></p>
<div class="math-display">
<img src="../media/file2034.png" class="math-display" alt="h[X ] ‚â§ h [ùí© (0,œÉ2)], "/>
</div>
<p><span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">h</span>[<span class="cmmi-10x-x-109">X</span>] = <span class="cmmi-10x-x-109">h</span>[<span class="cmsy-10x-x-109">ùí©</span>(0<span class="cmmi-10x-x-109">,œÉ</span><sup><span class="cmr-8">2</span></sup>)] <span class="cmti-10x-x-109">if and only if </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àºùí©</span>(<span class="cmmi-10x-x-109">Œº,œÉ</span><sup><span class="cmr-8">2</span></sup>)<span class="cmti-10x-x-109">.</span></p>
</div>
<p><span class="cmss-10x-x-109">Again, we are not going to prove this. You can check Bishop‚Äôs </span><span class="cmssi-10x-x-109">Pattern Recognition and Machine Learning </span><span class="cmss-10x-x-109">for more details.</span></p>
</section>
</section>
<section id="the-maximum-likelihood-estimation" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_310"><span class="titlemark"><span class="cmss-10x-x-109">20.7 </span></span> <span id="x1-34500024.7"></span><span class="cmss-10x-x-109">The Maximum Likelihood Estimation</span></h2>
<p><span class="cmss-10x-x-109">I am an evangelist for simple ideas. Stop me any time you want, but</span><span id="dx1-345001"></span> <span class="cmss-10x-x-109">whichever field I was in, I‚Äôve always been able to find a small set of mind-numbingly simple ideas making the entire shebang work. (Not that you could interrupt me, as this is a book. Joke‚Äôs on you!)</span></p>
<p><span class="cmss-10x-x-109">Let me give you a concrete example that‚Äôs on my mind. What do you think enabled the rise of deep learning, including neural networks with billions of parameters? Three ideas as simple as ABC:</span></p>
<ul>
<li><span class="cmss-10x-x-109">that you can optimize the loss function by going against its gradient (no matter the number of parameters),</span></li>
<li><span class="cmss-10x-x-109">that you can efficiently compute the gradient with a clever application of the chain rule and matrix multiplication,</span></li>
<li><span class="cmss-10x-x-109">and that we can perform matrix operations blazingly fast on a GPU.</span></li>
</ul>
<p><span class="cmss-10x-x-109">Sure, there‚Äôs a great tower of work built upon these ideas, but these three lie at the very foundation of machine learning today. Ultimately, these enable you to converse with large language models. To have your car cruise around town while you read the newspaper. To predict the exact shape of massive amino-acid chains called proteins, responsible for building up every living thing. (Including you.)</span></p>
<p><span class="cmss-10x-x-109">Gradient descent, backpropagation, and high-performance linear algebra are on the practical side of the metaphorical machine learning coin. If we conjure up a parametric model, we can throw some extremely powerful tools at it.</span></p>
<p><span class="cmss-10x-x-109">But where do our models come from?</span></p>
<p><span class="cmss-10x-x-109">As I‚Äôve said, there is a small set of key ideas that go a long way. We are about to meet one: the maximum likelihood estimation.</span></p>
<section id="probabilistic-modeling-" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_311"><span class="titlemark"><span class="cmss-10x-x-109">20.7.1 </span></span> <span id="x1-34600024.7.1"></span><span class="cmss-10x-x-109">Probabilistic modeling 101</span></h3>
<p><span class="cmss-10x-x-109">As a self-proclaimed evangelist of simple ideas, I‚Äôll start with a</span><span id="dx1-346001"></span> <span class="cmssi-10x-x-109">simple </span><span class="cmss-10x-x-109">example to illustrate a </span><span class="cmssi-10x-x-109">simple </span><span class="cmss-10x-x-109">idea.</span></p>
<p><span class="cmss-10x-x-109">Pick up a coin and toss it a few times, recording each outcome. The question is, once more, simple: what‚Äôs the probability of heads? We can‚Äôt just immediately assume </span><span class="cmmi-10x-x-109">p </span>= 1<span class="cmmi-10x-x-109">‚àï</span>2<span class="cmss-10x-x-109">, that is, a fair coin. For instance, one side of our coin could be coated with lead, resulting in a bias. To find out, let‚Äôs perform some statistics. (Rolling up my sleeves, throwing down my gloves.)</span></p>
<p><span class="cmss-10x-x-109">Mathematically speaking, we can model coin tosses with the Bernoulli distribution (</span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-bernoulli-distribution"><span class="cmssi-10x-x-109">19.2.1</span></a><span class="cmss-10x-x-109">):</span></p>
<div class="math-display">
<img src="../media/file2035.png" class="math-display" alt="P(X = 1) = p, P(X = 0) = 1 ‚àí p, "/>
</div>
<p><span class="cmss-10x-x-109">where</span></p>
<ul>
<li><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">is the random variable representing the outcome of a single toss,</span></li>
<li><span class="cmmi-10x-x-109">X </span>= 1 <span class="cmss-10x-x-109">for heads and </span><span class="cmmi-10x-x-109">X </span>= 0 <span class="cmss-10x-x-109">for tails,</span></li>
<li><span class="cmss-10x-x-109">and </span><span class="cmmi-10x-x-109">p </span><span class="cmsy-10x-x-109">‚àà </span>[0<span class="cmmi-10x-x-109">,</span>1] <span class="cmss-10x-x-109">is the probability of heads.</span></li>
</ul>
<p><span class="cmss-10x-x-109">That‚Äôs just the model. We‚Äôre here to estimate the parameter </span><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">, and this is what we have statistics for.</span></p>
<p><span class="cmss-10x-x-109">Tossing up the coin </span><span class="cmmi-10x-x-109">n </span><span class="cmss-10x-x-109">times yields the zero-one sequence </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,x</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, where each </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is a realization of a Bernoulli-distributed random variable </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">‚àº</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">, independent of each other.</span></p>
<p><span class="cmss-10x-x-109">As we saw</span> <span id="dx1-346002"></span><span class="cmss-10x-x-109">previously when discussing the law of large numbers (</span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch032.xhtml#x1-339002r137"><span class="cmssi-10x-x-109">137</span></a><span class="cmss-10x-x-109">), one natural idea is to compute the sample mean to estimate </span><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">, which is coincidentally the expected value of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">. To move beyond empirical estimates, let‚Äôs leverage that, this time, we have a probabilistic model.</span></p>
<p><span class="cmss-10x-x-109">The key question is this: which parameter </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">is the most likely to produce our sample?</span></p>
<p><span class="cmss-10x-x-109">In the language of probability, this question is answered by maximizing the </span><span class="cmssi-10x-x-109">likelihood </span><span class="cmss-10x-x-109">function</span></p>
<div class="math-display">
<img src="../media/file2036.png" class="math-display" alt=" n ‚àè LLH (p;x1,...,xn) = P(Xi = xi | p), i=1 "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">‚à£</span><span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">represents the probability of observing </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">given a fixed parameter </span><span class="cmmi-10x-x-109">p</span><span class="cmss-10x-x-109">. The larger the</span> LLH(<span class="cmmi-10x-x-109">p</span>;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub>)<span class="cmss-10x-x-109">, the more likely the parameter </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">is. In other words, our estimate of </span><span class="cmmi-10x-x-109">p </span><span class="cmss-10x-x-109">is going to be</span></p>
<div class="math-display">
<img src="../media/file2037.png" class="math-display" alt="ÀÜp = argmaxp ‚àà[0,1]LLH (p;x1,...,xn). "/>
</div>
<p><span class="cmss-10x-x-109">Let‚Äôs find it.</span></p>
<p><span class="cmss-10x-x-109">In our concrete case, </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">‚à£</span><span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">can be written as</span></p>
<div class="math-display">
<img src="../media/file2038.png" class="math-display" alt=" (| {p if xi = 1, P(Xi = xi | p) = | (1 ‚àí p if xi = 0. "/>
</div>
<p><span class="cmss-10x-x-109">Algebra doesn‚Äôt welcome if-else type functions, so with a clever mathematical trick, we write </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub><span class="cmsy-10x-x-109">‚à£</span><span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">as</span></p>
<div class="math-display">
<img src="../media/file2039.png" class="math-display" alt=" x 1‚àíx P(Xi = xi | p) = p i(1‚àí p ) i, "/>
</div>
<p><span class="cmss-10x-x-109">making the likelihood function be</span></p>
<div class="math-display">
<img src="../media/file2040.png" class="math-display" alt=" ‚àèn LLH (p;x1,...,xn) = pxi(1 ‚àí p)1‚àíxi. i=1 "/>
</div>
<p><span class="cmss-10x-x-109">(We‚Äôll often write</span> LLH(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">to minimize notational complexity.)</span></p>
<p><span class="cmss-10x-x-109">This is still not easy to optimize, as it is composed of the product of exponential functions. So, here‚Äôs another mathematical trick: take the logarithm to turn the product into a sum.</span></p>
<p><span class="cmss-10x-x-109">As the logarithm is increasing, it won‚Äôt change the optima, so we‚Äôre good to go:</span></p>
<div class="math-display">
<img src="../media/file2041.png" class="math-display" alt=" ‚àèn log LLH (p) = log pxi(1‚àí p )1‚àíxi i=1 ‚àën [ ] = log pxi(1 ‚àí p)1‚àíxi i=1 ‚àën [ ] = logpxi + log(1‚àí p)1‚àíxi i=1 ‚àën ‚àën = logp xi + log(1 ‚àí p) (1‚àí xi). i=1 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Trust me, this is much better. According</span> <span id="dx1-346003"></span><span class="cmss-10x-x-109">to the second derivative test (</span><span class="cmssi-10x-x-109">Theorem¬†</span><a href="ch021.xhtml#x1-214004r87"><span class="cmssi-10x-x-109">87</span></a><span class="cmss-10x-x-109">), we can find the maxima by</span></p>
<ol>
<li><span id="x1-346005x1"><span class="cmss-10x-x-109">solving</span> <img src="../media/file2042.png" width="15" data-align="middle" alt="ddp"/> log LLH(<span class="cmmi-10x-x-109">p</span>) = 0 <span class="cmss-10x-x-109">to find the critical point</span> pÃÇ <span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-346007x2"><span class="cmss-10x-x-109">then showing that</span> pÃÇ <span class="cmss-10x-x-109">is a maximum because</span> <img src="../media/file2045.png" width="15" data-align="middle" alt="-d2- dp2"/> log LLH(<span class="cmmi-10x-x-109">p</span>)<span class="cmmi-10x-x-109">/span&gt;0<span class="cmss-10x-x-109">.</span></span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Let‚Äôs get to it.</span></p>
<p><span class="cmss-10x-x-109">As</span> <img src="../media/file2046.png" width="15" data-align="middle" alt="ddp"/> log <span class="cmmi-10x-x-109">p </span>= <img src="../media/file2047.png" width="8" data-align="middle" alt="1p"/> <span class="cmss-10x-x-109">and</span> <img src="../media/file2048.png" width="15" data-align="middle" alt="ddp-"/> log(1 <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">p</span>) = <span class="cmsy-10x-x-109">‚àí</span><img src="../media/file2049.png" width="15" data-align="middle" alt="11‚àíp"/><span class="cmss-10x-x-109">, we have</span></p>
<div class="math-dispay">
<img src="../media/file2050.png" width="450" class="math-display" alt="d 1 ‚àën 1 ‚àën --logLLH (p;x1,...,xn) = -- xi ‚àí ----- (1‚àí xi). dp p i=1 1‚àí p i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Solving</span> <img src="../media/file2051.png" width="15" data-align="middle" alt="ddp"/> log LLH(<span class="cmmi-10x-x-109">p</span>;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub>) = 0 <span class="cmss-10x-x-109">yields a single solution</span></p>
<div class="math-display">
<img src="../media/file2052.png" class="math-display" alt=" 1 ‚àën ÀÜp = -- xi. n i=1 "/>
</div>
<p><span class="cmss-10x-x-109">(Pick up a pen and paper and calculate the solution yourself.) Regarding the second derivative, we have</span></p>
<img src="../media/file2053.png" class="math-display" alt="d2 1 ‚àën 1 ‚àën --2 logLLH (p) = ‚àí -2 xi ‚àí-------2 (1‚àí xi), dp p i=1 (1 ‚àí p) i=1 " width="450"/>
<p><span class="cmss-10x-x-109">which is uniformly negative. Thus,</span> pÃÇ = <img src="../media/file2055.png" width="10" data-align="middle" alt="1n"/> <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=1</span></sub><sup><span class="cmmi-8">n</span></sup><span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub> <span class="cmss-10x-x-109">is indeed a (local) maximum. Yay!</span></p>
<p><span class="cmss-10x-x-109">In this case, the maximum likelihood estimate is identical to the sample mean. Trust me, this is one of the rare exceptions. Think of it as validating the sample mean: we‚Äôve obtained the same estimate through different trains of thought, so it must be good.</span></p>
</section>
<section id="modeling-heights" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_312"><span class="titlemark"><span class="cmss-10x-x-109">20.7.2 </span></span> <span id="x1-34700024.7.2"></span><span class="cmss-10x-x-109">Modeling heights</span></h3>
<p><span class="cmss-10x-x-109">Let‚Äôs</span> <span id="dx1-347001"></span><span class="cmss-10x-x-109">continue with another example. The coin-tossing example demonstrated the discrete case. It‚Äôs time to move into the continuous domain!</span></p>
<p><span class="cmss-10x-x-109">This time, we are measuring the heights of a high school class, and we want to build a probabilistic model of it. A natural idea is to assume the heights to come from a normal distribution </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àºùí©</span>(<span class="cmmi-10x-x-109">Œº,œÉ</span><sup><span class="cmr-8">2</span></sup>)<span class="cmss-10x-x-109">. (Check </span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-normal-distribution"><span class="cmssi-10x-x-109">19.3.6</span></a> <span class="cmss-10x-x-109">for the normal distribution.)</span></p>
<p><span class="cmss-10x-x-109">Our job is to estimate the expected value </span><span class="cmmi-10x-x-109">Œº </span><span class="cmss-10x-x-109">and the variance </span><span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">. Let‚Äôs go, maximum likelihood!</span></p>
<p><span class="cmss-10x-x-109">To make the problem mathematically precise, we have the measurements </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub><span class="cmss-10x-x-109">, coming from independent and identically distributed random variables </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> <span class="cmsy-10x-x-109">‚àºùí©</span>(<span class="cmmi-10x-x-109">Œº,œÉ</span><sup><span class="cmr-8">2</span></sup>)<span class="cmss-10x-x-109">. However, there‚Äôs a snag: as our random variables are continuous,</span></p>
<img src="../media/file2056.png" width="450" class="math-display" alt=" ‚àèn P (X1 = x1,...,Xn = xn | Œº, œÉ2) = P (Xi = xi | Œº, œÉ2) = 0. i=1 "/>
<p><span class="cmss-10x-x-109">(As all terms of the product are zero.) How can we define the likelihood function, then? No worries: even though we don‚Äôt have a mass function, we have density! Thus, the likelihood function defined by</span></p>
<div class="math-display">
<img src="../media/file2057.png" class="math-display" alt=" ‚àèn LLH (Œº, œÉ;x1,...,xn) = fX (xi) i=1 i n 2 = ‚àè -‚àö1--e‚àí (xi2‚àíœÉŒº2), i=1 œÉ 2œÄ "/>
</div>
<p><span class="cmss-10x-x-109">where </span><span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">X</span><sub><span class="cmmi-6">i</span></sub></sub>(<span class="cmmi-10x-x-109">x</span>) <span class="cmss-10x-x-109">is the probability density function of </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Let‚Äôs maximize it. The idea is similar: take the logarithm, find the critical points, then use the second derivative test. Here we go:</span></p>
<div class="math-display">
<img src="../media/file2058.png" class="math-display" alt=" 1 1 ‚àën 2 log LLH (Œº,œÉ) = nlog -‚àö----‚àí œÉ2- (xi ‚àí Œº) . œÉ 2œÄ i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Just the usual business from now on. The derivatives are given by</span></p>
<div class="math-display">
<img src="../media/file2059.png" class="math-display" alt=" ‚àën ‚àÇ--log LLH (Œº,œÉ) = -2- (xi ‚àí Œº), ‚àÇŒº œÉ2 i=1 ‚àën ‚àÇ--log LLH (Œº,œÉ) = ‚àí n-‚àí -2- (xi ‚àí Œº)2. ‚àÇœÉ œÉ œÉ3 i=1 "/>
</div>
<p><span class="cmss-10x-x-109">With a bit of number-crunching (that you should attempt to carry out by yourself), we get that</span> ‚àÇŒº log LLH(<span class="cmmi-10x-x-109">Œº,œÉ</span>) = 0 <span class="cmss-10x-x-109">implies</span></p>
<div class="math-disply">
<img src="../media/file2061.png" width="150" class="math-display" alt=" -1‚àën Œº = n xi, i=1 "/>
</div>
<p><span class="cmss-10x-x-109">and</span> <img src="../media/file2062.png" width="15" data-align="middle" alt="-‚àÇ ‚àÇœÉ"/> log LLH(<span class="cmmi-10x-x-109">Œº,œÉ</span>) <span class="cmss-10x-x-109">implies</span></p>
<div class="math-display">
<img src="../media/file2063.png" class="math-display" alt=" ‚àën œÉ = -1 (xi ‚àí Œº )2. n i=1 "/>
</div>
<p><span class="cmss-10x-x-109">We won‚Äôt do</span> <span id="dx1-347002"></span><span class="cmss-10x-x-109">the second derivative test here, but trust me: it‚Äôs a maximum, leaving us with the estimates</span></p>
<div class="math-display">
<img src="../media/file2064.png" class="math-display" alt=" n ÀÜŒº = -1‚àë x, n i=1 i n ÀÜœÉ = -1‚àë (x ‚àí ŒºÀÜ)2. n i i=1 "/>
</div>
<p><span class="cmss-10x-x-109">Again, the sample mean and variance. Think of it this way: defaulting to the sample mean and variance is the simplest thing to do, yet even clever methods like the maximum likelihood estimation yield them as parameter estimates.</span></p>
<p><span class="cmss-10x-x-109">After working out the above two examples in detail, we are ready to abstract away the details and introduce the general problem.</span></p>
</section>
<section id="the-general-method" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_313"><span class="titlemark"><span class="cmss-10x-x-109">20.7.3 </span></span> <span id="x1-34800024.7.3"></span><span class="cmss-10x-x-109">The general method</span></h3>
<p><span class="cmss-10x-x-109">We‚Äôve seen</span> <span id="dx1-348001"></span><span class="cmss-10x-x-109">how</span> <span id="dx1-348002"></span><span class="cmss-10x-x-109">maximum likelihood estimation works. Now, it‚Äôs time to construct the abstract mathematical framework.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-348003r99"></span> <span class="cmbx-10x-x-109">Definition 99.</span> </span><span class="cmbx-10x-x-109">(The likelihood function)</span></p>
<p>Let <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub> be a probability distribution parametrized by the parameter <span class="cmmi-10x-x-109">ùúÉ </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">k</span></sup>, and let <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àà </span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">d</span></sup> be an independent realization of the probability distribution. (That is, the samples are coming from independent and identically distributed random variables <span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,X</span><sub><span class="cmmi-8">n</span></sub>, distributed according to <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub>.)</p>
<p>The <span class="cmti-10x-x-109">likelihood function </span>of <span class="cmmi-10x-x-109">ùúÉ </span>given the sample <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> is defined by</p>
<p><span class="cmti-10x-x-109">(a)</span></p>
<div class="math-display">
<img src="../media/file2065.png" class="math-display" alt=" ‚àèn LLH (ùúÉ;x1,...,xn) := P ùúÉ(Xi = xi) i=1 "/>
</div>
<p>if <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub> is discrete, and</p>
<p><span class="cmti-10x-x-109">(b)</span></p>
<div class="math-display">
<img src="../media/file2066.png" class="math-display" alt=" ‚àèn LLH (ùúÉ;x1,...,xn) := fùúÉ(xi) i=1 "/>
</div>
<p>if <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub> is continuous, where <span class="cmmi-10x-x-109">f</span><sub><span class="cmmi-8">ùúÉ</span></sub> is the probability density function of <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub>.</p>
</div>
<p><span class="cmss-10x-x-109">We‚Äôve already seen two examples of the likelihood function: for the Bernoulli-distribution</span> Bernoulli(<span class="cmmi-10x-x-109">p</span>)<span class="cmss-10x-x-109">, given by</span></p>
<div class="math-display">
<img src="../media/file2067.png" class="math-display" alt=" ‚àèn LLH (p;x1,...,xn) = pxi(1 ‚àí p)1‚àíxi, i=1 "/>
</div>
<p><span class="cmss-10x-x-109">and for the normal distribution </span><span class="cmsy-10x-x-109">ùí©</span>(<span class="cmmi-10x-x-109">Œº,œÉ</span>)<span class="cmss-10x-x-109">, given by</span></p>
<div class="math-display">
<img src="../media/file2068.png" class="math-display" alt=" n 2 LLH (Œº, œÉ;x ,...,x ) = ‚àè -‚àö1--e‚àí (xi2‚àíœÉŒº2). 1 n i=1 œÉ 2œÄ "/>
</div>
<p><span class="cmss-10x-x-109">Intuitively, the</span> <span id="dx1-348004"></span><span class="cmss-10x-x-109">likelihood</span> <span id="dx1-348005"></span><span class="cmss-10x-x-109">function</span> LLH(<span class="cmmi-10x-x-109">ùúÉ</span>;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub>) <span class="cmss-10x-x-109">expresses the probability of our observation </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">if the parameter </span><span class="cmmi-10x-x-109">ùúÉ </span><span class="cmss-10x-x-109">is indeed true. The maximum likelihood</span> <span id="dx1-348006"></span><span class="cmss-10x-x-109">estimate is the parameter</span> <img src="../media/file2069.png" width="8" alt="ÀÜùúÉ"/> <span class="cmss-10x-x-109">that maximizes this probability; that is, under which the observation is the most likely.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-348007r100"></span> <span class="cmbx-10x-x-109">Definition 100.</span> </span><span class="cmbx-10x-x-109">(The maximum likelihood estimate)</span></p>
<p>Let <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub> be a probability distribution parametrized by the parameter <span class="cmmi-10x-x-109">ùúÉ </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">k</span></sup>, and let <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àà </span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">d</span></sup> be an independent realization of the probability distribution.</p>
<p>The <span class="cmti-10x-x-109">maximum likelihood estimate </span>of <span class="cmmi-10x-x-109">ùúÉ </span>is given by</p>
<div class="math-display">
<img src="../media/file2070.png" class="math-display" alt="ÀÜùúÉ = argmax ùúÉ‚àà‚ÑùkLLH (ùúÉ;x1,...,xn ). "/>
</div>
</div>
<p><span class="cmss-10x-x-109">In both examples, we used the logarithm to turn the product into a sum. Use it once and it‚Äôs a trick; use it (at least) twice and it‚Äôs a method. Here‚Äôs the formal definition.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-348008r101"></span> <span class="cmbx-10x-x-109">Definition 101.</span> </span><span class="cmbx-10x-x-109">(The log-likelihood function)</span></p>
<p>Let <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">ùúÉ</span></sub> be a probability distribution parametrized by the parameter <span class="cmmi-10x-x-109">ùúÉ </span><span class="cmsy-10x-x-109">‚àà</span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">k</span></sup>, and let <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àà </span><span class="msbm-10x-x-109">‚Ñù</span><sup><span class="cmmi-8">d</span></sup> be <span id="dx1-348009"></span>an independent realization of the probability distribution.</p>
<p>The <span class="cmti-10x-x-109">log-likelihood function </span>of <span class="cmmi-10x-x-109">ùúÉ </span>given the sample <span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> is defined by</p>
<div class="math-display">
<img src="../media/file2071.png" class="math-display" alt="logLLH (ùúÉ;x1,...,xn), "/>
</div>
<p>where LLH(<span class="cmmi-10x-x-109">ùúÉ</span>;<span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub>) is the likelihood function.</p>
</div>
<p><span class="cmss-10x-x-109">In a</span> <span id="dx1-348010"></span><span class="cmss-10x-x-109">classical</span> <span id="dx1-348011"></span><span class="cmss-10x-x-109">statistical setting, the maximum likelihood estimation is done via</span></p>
<ol>
<li><span id="x1-348013x1"><span class="cmss-10x-x-109">pulling a parametric probabilistic model out from the mathematician‚Äôs hat,</span></span></li>
<li><span id="x1-348015x2"><span class="cmss-10x-x-109">massaging the (log-)likelihood function until we obtain an analytically manageable form,</span></span></li>
<li><span id="x1-348017x3"><span class="cmss-10x-x-109">and solving </span><span class="cmsy-10x-x-109">‚àá</span>LLH = 0 <span class="cmss-10x-x-109">(or </span><span class="cmsy-10x-x-109">‚àá</span>log LLH = 0<span class="cmss-10x-x-109">) to obtain the parameter estimate.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">Statistics can be extremely powerful under specific circumstances, but let‚Äôs face it: the above method has quite a few weaknesses. First, constructing a tractable probabilistic model is a challenging task, burdened by the experts‚Äô inherent bias. (It‚Äôs no accident that I indirectly compared the modeling process to pulling a rabbit out of a hat.) Moreover, the more complex the model, the more complex the likelihood function is. Which, in turn, increases the complexity of our optimization problem.</span></p>
<p><span class="cmss-10x-x-109">Why did we spend quite a few pages learning this ancient technique, then?</span></p>
<p><span class="cmss-10x-x-109">Because its idea is fundamental in machine learning, we‚Äôll arrive at (somewhere near the) state of the art by breaking down its barriers one by one. Is modeling hard? Let‚Äôs construct a function with BILLIONS of parameters that‚Äôll do the job. Is optimization computationally intensive? Fear not. We have clusters of GPUs at our disposal.</span></p>
</section>
<section id="the-german-tank-problem" class="level4 subsectionHead">
<h3 class="subsectionHead" id="sigil_toc_id_314"><span class="titlemark"><span class="cmss-10x-x-109">20.7.4 </span></span> <span id="x1-34900024.7.4"></span><span class="cmss-10x-x-109">The German tank problem</span></h3>
<p><span class="cmss-10x-x-109">One more</span> <span id="dx1-349001"></span><span class="cmss-10x-x-109">example</span> <span id="dx1-349002"></span><span class="cmss-10x-x-109">before finishing up, straight from World War II. Imagine you are an Allied intelligence officer tasked to estimate the size of a German armored division. (That is, to guess the number of tanks.)</span></p>
<p><span class="cmss-10x-x-109">There was no satellite imagery back in the day, so there‚Äôs only a little to go on, except for a tiny piece of information: the serial numbers of the enemy‚Äôs destroyed tanks. What can we do with these?</span></p>
<p><span class="cmss-10x-x-109">Without detailed knowledge of the manufacturing process, we can assume that the tanks are labeled sequentially as they roll out from the factory. We also don‚Äôt know how the tanks are distributed between the battlefields.</span></p>
<p><span class="cmss-10x-x-109">These two pieces of knowledge (or lack of knowledge, to be more precise) translate to a simple probabilistic model: encountering an enemy tank is the same as drawing from the distribution</span> Uniform(<span class="cmmi-10x-x-109">N</span>)<span class="cmss-10x-x-109">, where </span><span class="cmmi-10x-x-109">N </span><span class="cmss-10x-x-109">is the total number of tanks. Thus, if </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmss-10x-x-109">are the serial numbers of destroyed tanks, we can use the maximum likelihood method to estimate </span><span class="cmmi-10x-x-109">N</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Let‚Äôs do it. The likelihood function for the discrete uniform distribution</span> Uniform(<span class="cmmi-10x-x-109">N</span>) <span class="cmss-10x-x-109">is given by</span></p>
<div class="math-display">
<img src="../media/file2072.png" class="math-display" alt=" ‚àèn LLH (N ) = P (Xi = xi), i=1 "/>
</div>
<p><span class="cmss-10x-x-109">where the probability </span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">X</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">x</span><sub><span class="cmmi-8">i</span></sub>) <span class="cmss-10x-x-109">has a quite peculiar form:</span></p>
<div class="math-display">
<img src="../media/file2073.png" class="math-display" alt=" ( |{ -1 P (X = x ) = N if xi ‚àà {1,...,N }, i i |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">Keeping in mind that </span><span class="cmmi-10x-x-109">x</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,‚Ä¶,x</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚â§</span><span class="cmmi-10x-x-109">N </span><span class="cmss-10x-x-109">(as no observed serial number can be larger than the total number of tanks), we have</span></p>
<div class="math-display">
<img src="../media/file2074.png" class="math-display" alt=" ( |{ -1- LLH (N ) = Nn if N &lt;max {x1,...,xn}, |( 0 otherwise. "/>
</div>
<p><span class="cmss-10x-x-109">Ponder on this a minute: the larger the </span><span class="cmmi-10x-x-109">N</span><span class="cmss-10x-x-109">, the smaller the</span> LLH(<span class="cmmi-10x-x-109">N</span>) <span class="cmss-10x-x-109">is. Thus, the maximum likelihood estimate is the smallest</span> <span id="dx1-349003"></span><span class="cmss-10x-x-109">possible choice</span></p>
<div class="math-display">
<img src="../media/file2075.png" class="math-display" alt="NÀÜ= max {x1,...,xn }. "/>
</div>
<p><span class="cmss-10x-x-109">In other</span> <span id="dx1-349004"></span><span class="cmss-10x-x-109">words, our guess about the number of tanks is the largest serial number we‚Äôve encountered.</span></p>
<p><span class="cmss-10x-x-109">What do you think about this estimate? I won‚Äôt lie; I am not a big fan. The German tank problem highlights the importance of modeling assumptions in statistics. The final estimate</span> <img src="../media/file2076.png" width="8" alt="ÀÜN"/> <span class="cmss-10x-x-109">is the outcome of our choice of</span> Uniform(<span class="cmmi-10x-x-109">N</span>)<span class="cmss-10x-x-109">. Common wisdom in machine learning is ‚Äúgarbage in, garbage out.‚Äù It is true for modeling as well.</span></p>
</section>
</section>
<section id="summary19" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_315"><span class="titlemark"><span class="cmss-10x-x-109">20.8 </span></span> <span id="x1-35000024.8"></span><span class="cmss-10x-x-109">Summary</span></h2>
<p><span class="cmss-10x-x-109">In this chapter, we have learned about the concept of the expected value. Mathematically speaking, the expected value is defined by</span></p>
<div class="math-display">
<img src="../media/file2077.png" class="math-display" alt=" ‚àë ùîº[X ] = xkP (X = xk) k "/>
</div>
<p><span class="cmss-10x-x-109">for discrete random variables and</span></p>
<div class="math-display">
<img src="../media/file2078.png" class="math-display" alt=" ‚à´ ‚àû ùîº[X ] = xfX(x)dx ‚àí‚àû "/>
</div>
<p><span class="cmss-10x-x-109">for continuous ones. Although these formulas involve possibly infinite sums and integrals, the underlying meaning is simple: </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmss-10x-x-109">represents the average outcome of </span><span class="cmmi-10x-x-109">X</span><span class="cmss-10x-x-109">, weighted by the underlying probability distribution.</span></p>
<p><span class="cmss-10x-x-109">According to the law of large numbers, the expected value also describes a long-term average: if the independent and identically distributed random variables </span><span class="cmmi-10x-x-109">X</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,X</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,‚Ä¶ </span><span class="cmss-10x-x-109">describe the outcomes of a repeated experiment ‚Äî say, betting a hand in poker ‚Äî then the sample average converges to the joint expected value, that is,</span></p>
<div class="math-display">
<img src="../media/file2079.png" class="math-display" alt=" 1 ‚àën lim -- Xi = ùîº[X1 ] n‚Üí ‚àû n i=1 "/>
</div>
<p><span class="cmss-10x-x-109">holds with probability </span>1<span class="cmss-10x-x-109">. In a sense, the law of large numbers allows you to glimpse into the future and see what happens if you make the same choice. In the case of poker, if you only make bets with a positive expected value, you‚Äôll win in the long run.</span></p>
<p><span class="cmss-10x-x-109">In machine learning, the LLN also plays an essential role. Check out the mean-squared error</span></p>
<div class="math-display">
<img src="../media/file2080.png" class="math-display" alt=" -1‚àën 2 n MSE (x,y ) = n (f(xi)‚àí yi) , x,y ‚àà ‚Ñù i=1 "/>
</div>
<p><span class="cmss-10x-x-109">once more. If the number of samples (</span><span class="cmmi-10x-x-109">n</span><span class="cmss-10x-x-109">) is in the millions, computing the gradient of this sum is not feasible. However, the mean-squared error is the sample average of the prediction errors; thus, it‚Äôs enough to sample a smaller amount. This is the core principle behind stochastic gradients, an idea that makes machine learning on a large scale feasible.</span></p>
<p><span class="cmss-10x-x-109">With this chapter, our journey comes to a close. Still, there‚Äôs so much to learn; I could probably write this book until the end of time. Sadly, we have to stop somewhere. Now, instead of giving a summary of all that‚Äôs in the book, let‚Äôs talk about the most important message: learning never ends.</span></p>
<p><span class="cmss-10x-x-109">It‚Äôs a spiral that you continue to ascend, meeting familiar landscapes from higher and higher vantage points. If you keep going, you‚Äôll know what I‚Äôm talking about.</span></p>
<p><span class="cmss-10x-x-109">If you lead an intellectually challenging life, you‚Äôll also find that knowledge is like keeping a dozen leaky cups full of water. If your focus shifts from one, it‚Äôll empty faster than you think. In other words, you‚Äôll lose it if you don‚Äôt use it. This is completely normal. The good news is, if you already have a good foundation, refilling the cup can be done quickly. Sometimes, simply glancing at a page from a book you read long ago can do the trick.</span></p>
<p><span class="cmss-10x-x-109">This is how I know that it‚Äôs not goodbye. If you have found the book useful and continue down the rabbit hole that is machine learning, we‚Äôll meet again with probability one. You just have to keep going.</span></p>
</section>
<section id="problems18" class="level3 sectionHead">
<h2 class="sectionHead" id="sigil_toc_id_316"><span class="titlemark"><span class="cmss-10x-x-109">20.9 </span></span> <span id="x1-35100024.9"></span><span class="cmss-10x-x-109">Problems</span></h2>
<p><span class="cmssbx-10x-x-109">Problem 1. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X,Y </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">be two random variables.</span></p>
<p><span class="cmssi-10x-x-109">(a) </span><span class="cmss-10x-x-109">Show that if </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â• </span>0<span class="cmss-10x-x-109">, then </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmsy-10x-x-109">‚â• </span>0<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssi-10x-x-109">(b) </span><span class="cmss-10x-x-109">Show that if </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚â•</span><span class="cmmi-10x-x-109">Y </span><span class="cmss-10x-x-109">, then </span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">X</span>] <span class="cmsy-10x-x-109">‚â•</span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">Y</span> ]<span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 2. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span>: Œ© <span class="cmsy-10x-x-109">‚Üí</span><span class="msbm-10x-x-109">‚Ñù </span><span class="cmss-10x-x-109">be a random variable. Show that if</span> Var[<span class="cmmi-10x-x-109">X</span>] = 0<span class="cmss-10x-x-109">, then </span><span class="cmmi-10x-x-109">X </span><span class="cmss-10x-x-109">assumes only a single value. (That is, the set </span><span class="cmmi-10x-x-109">X</span>(Œ©) = <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">X</span>(<span class="cmmi-10x-x-109">œâ</span>) : <span class="cmmi-10x-x-109">œâ </span><span class="cmsy-10x-x-109">‚àà </span>Œ©<span class="cmsy-10x-x-109">} </span><span class="cmss-10x-x-109">has only a single element.)</span></p>
<p><span class="cmssbx-10x-x-109">Problem 3. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> Geo(<span class="cmmi-10x-x-109">p</span>) <span class="cmss-10x-x-109">be a geometrically distributed (</span><span class="cmssi-10x-x-109">Section¬†</span><a href="ch031.xhtml#the-geometric-distribution"><span class="cmssi-10x-x-109">19.2.3</span></a><span class="cmss-10x-x-109">) discrete random variable. Show that</span></p>
<div class="math-display">
<img src="../media/file2081.png" class="math-display" alt="H [X] = ‚àí plogp-+-(1‚àí-p)log(1‚àí-p)-. p "/>
</div>
<p><span class="cmssi-10x-x-109">Hint: </span><span class="cmss-10x-x-109">Use that for any </span><span class="cmmi-10x-x-109">q </span><span class="cmsy-10x-x-109">‚àà </span>(0<span class="cmmi-10x-x-109">,</span>1)<span class="cmss-10x-x-109">, </span><span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">k</span><span class="cmr-8">=1</span></sub><sup><span class="cmsy-8">‚àû</span></sup><span class="cmmi-10x-x-109">kq</span><sup><span class="cmmi-8">k</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> = (1 <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">q</span>)<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">2</span></sup><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmssbx-10x-x-109">Problem 4. </span><span class="cmss-10x-x-109">Let </span><span class="cmmi-10x-x-109">X </span><span class="cmsy-10x-x-109">‚àº</span> exp(<span class="cmmi-10x-x-109">Œª</span>) <span class="cmss-10x-x-109">be an exponentially distributed continuous random variable. Show that</span></p>
<div class="math-display">
<img src="../media/file2082.png" class="math-display" alt="h [X ] = 1 ‚àí logŒª. "/>
</div>
<p><span class="cmssbx-10x-x-109">Problem 5. </span><span class="cmss-10x-x-109">Find the maximum likelihood estimation for the </span><span class="cmmi-10x-x-109">Œª </span><span class="cmss-10x-x-109">parameter of the exponential distribution.</span></p>
</section>
<section id="join-our-community-on-discord20" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc" id="sigil_toc_id_317"><span id="x1-352000"></span><span class="cmss-10x-x-109">Join our community on Discord</span></h2>
<p><span class="cmss-10x-x-109">Read this book alongside other users, Machine Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community.</span> <a href="https://packt.link/math" class="url"><span class="cmtt-10x-x-109">https://packt.link/math</span></a></p>
<p><img src="../media/file1.png" width="85" alt="PIC"/></p>
</section>
</section>
</body>
</html>