- en: '20'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Expected Value
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, we learned about probability distributions, the objects
    that represent probabilistic models as sequences or functions. After all, there
    is the entire field of calculus to help us deal with functions, so they open up
    a wide array of mathematical tools.
  prefs: []
  type: TYPE_NORMAL
- en: However, we might not need all the information available. Sometimes, simple
    descriptive statistics such as mean, variance, or median suffice. Even in machine
    learning, loss functions are given in terms of them. For instance, the famous
    mean-squared error
  prefs: []
  type: TYPE_NORMAL
- en: '![ n MSE (x,y ) =-1âˆ‘ (f(x )âˆ’ y )2, x,y âˆˆ â„n n i i i=1 ](img/file1893.png)'
  prefs: []
  type: TYPE_IMG
- en: is the variance of the prediction error. Deep down, these familiar quantities
    are rooted in probability theory, and weâ€™ll devote this chapter to learning about
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 20.1 Discrete random variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s play a simple game. I toss a coin, and if it comes up heads, you win $1
    . If it is tails, you lose $2 .
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we were dealing with questions like the probability of winning.
    Say, for the coin toss, whether you win or lose, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![P(heads) = P(tails) = 1\. 2 ](img/file1894.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite the equal chances of winning and losing, should you play this game?
    Letâ€™s find out.
  prefs: []
  type: TYPE_NORMAL
- en: After n rounds, your earnings can be calculated by the number of heads times
    $1 minus the number of tails times $2 . If we divide total earnings by n, we obtain
    your average winnings per round. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![ total-winnings- your average winnings = n 1â‹…#heads âˆ’ 2â‹…#tails = --------------------
    n = 1â‹… #heads âˆ’ 2â‹… #tails, n n ](img/file1895.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where #heads and #tails denote the number of heads and tails respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall the frequentist interpretation of probability from SectionÂ [18.2.7](ch030.xhtml#how-to-interpret-probability)?
    According to our intuition, we should have
  prefs: []
  type: TYPE_NORMAL
- en: '![ lim #heads- = P(heads) = 1, nâ†’ âˆ n 2 #tails- 1- lniâ†’mâˆ n = P(tails) = 2\.
    ](img/file1896.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that if you play long enough, your average winnings per round is
  prefs: []
  type: TYPE_NORMAL
- en: '![your average winnings = 1â‹…P (heads)âˆ’ 2 â‹…P(tails) = âˆ’ 1\. 2 ](img/file1897.png)'
  prefs: []
  type: TYPE_IMG
- en: So, as you are losing half a dollar per round on average, you definitely shouldnâ€™t
    play this game.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s formalize this argument with a random variable. Say, if X describes your
    winnings per round, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 P(X = 1) = P (X = âˆ’ 2) =-, 2 ](img/file1898.png)'
  prefs: []
  type: TYPE_IMG
- en: so the average winnings can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![average value of X = 1 â‹…P(X = 1)âˆ’ 2 â‹…P(X = âˆ’ 2) 1 = âˆ’ -. 2 ](img/file1899.png)'
  prefs: []
  type: TYPE_IMG
- en: With a bit of a pattern matching, we find that for a general discrete random
    variable X, the formula looks like
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘ average value of X = (value)â‹…P (X = value). value ](img/file1900.png)'
  prefs: []
  type: TYPE_IMG
- en: And from this, the definition of expected value is born.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 92\. (The expected value of discrete random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and X : Î© â†’{x[1],x[2],â€¦} be a discrete
    random variable. The expected value of X is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼ [X ] := âˆ‘ x P (X = x ). k k k ](img/file1901.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that if X assumes finitely many values, the sum only contains a finite
    number of terms.)
  prefs: []
  type: TYPE_NORMAL
- en: In English, the expected value describes the average value of a random variable
    in the long run. The expected value is also called the mean and is often denoted
    by Î¼. Instead of using random variables, weâ€™ll often use the expected value symbol
    by plugging in distributions, like ğ”¼[Bernoulli(p)]. Although this is mathematically
    not precise, 1) it is simpler in certain cases, 2) and the expected value only
    depends on the distribution anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s time for examples.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. Expected value of the Bernoulli distribution. (See the definition
    of the Bernoulli distribution in SectionÂ [19.2.1](ch031.xhtml#the-bernoulli-distribution).)
    Let X âˆ¼ Bernoulli(p). Its expected value is quite simple to compute, as
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[X] = 0â‹…P (X = 0 )+ 1â‹…P (X = 1) = = 0â‹…(1 âˆ’ p)+ 1 â‹…p = p. ](img/file1902.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Weâ€™ve seen this before: the introductory example with the simple game is the
    transformed Bernoulli distribution 3 â‹… Bernoulli(1âˆ•2) âˆ’ 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. Expected value of the binomial distribution. (See the definition
    of the binomial distribution in SectionÂ [19.2.2](ch031.xhtml#the-binomial-distribution).)
    Let X âˆ¼ Binomial(n,p). Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘n ğ”¼[X] = kP (X = k ) k=0 âˆ‘n (n ) = k pk(1 âˆ’ p)nâˆ’k k=0 k âˆ‘n = k----n!---
    pk(1 âˆ’ p)nâˆ’k. k=0 k!(n âˆ’ k)! ](img/file1903.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plan is the following: absorb that k with the fraction ![--n!--- k!(nâˆ’k)!](img/file1904.png),
    and adjust the sum such that its terms form the probability mass function for
    Binomial(n âˆ’ 1,p). As n âˆ’k = (n âˆ’ 1) âˆ’ (k âˆ’ 1), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘n ğ”¼[X ] = k ---n!----pk(1âˆ’ p)nâˆ’k k=0 k!(n âˆ’ k)! âˆ‘n = np ---------(n-âˆ’-1)!--------pkâˆ’1(1
    âˆ’ p)(nâˆ’1)âˆ’(kâˆ’1) k=1(k âˆ’ 1)!((n âˆ’ 1)âˆ’ (k âˆ’ 1))! nâˆ’1 = np âˆ‘ ---(n-âˆ’-1)!--pk(1âˆ’ p)(nâˆ’1âˆ’k)
    k!(n âˆ’ 1âˆ’ k)! k=0 nâˆ‘âˆ’1 = np P (Binomial(nâˆ’ 1,p) = k) k=0 = np. ](img/file1905.png)'
  prefs: []
  type: TYPE_IMG
- en: This computation might not look like the simplest, but once you get familiar
    with the trick, itâ€™ll be like second nature for you.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. Expected value of the geometric distribution. (See the definition
    of the geometric distribution in SectionÂ [19.2.3](ch031.xhtml#the-geometric-distribution).)
    Let X âˆ¼ Geo(p). We need to calculate
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘âˆ ğ”¼[X ] = k (1 âˆ’ p)kâˆ’1p. k=1 ](img/file1906.png)'
  prefs: []
  type: TYPE_IMG
- en: Do you remember the geometric series ([19.2](#))? This is almost it, except
    for the k term, which throws a monkey wrench into our gears. To fix that, weâ€™ll
    use another magic trick. Recall that
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 âˆ‘âˆ ----- = xk. 1 âˆ’ x k=0 ](img/file1907.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we are going to differentiate the geometric series, thus obtaining
  prefs: []
  type: TYPE_NORMAL
- en: '![ d 1 d âˆ‘âˆ k dx-1-âˆ’-x = dx- x k=0 âˆ‘âˆ d k = dx-x k=0 âˆ‘âˆ kâˆ’ 1 = kx , k=1 ](img/file1908.png)'
  prefs: []
  type: TYPE_IMG
- en: where we used the linearity of the derivative and the pleasant analytic properties
    of the geometric series. Mathematicians would scream upon the sight of switching
    the derivative and the infinite sum, but donâ€™t worry, everything here is correct
    as is. (Mathematicians are really afraid of interchanging limits. Mind you, for
    a good reason!)
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand,
  prefs: []
  type: TYPE_NORMAL
- en: '![d---1--- ---1---- dx 1âˆ’ x = (1 âˆ’ x)2, ](img/file1909.png)'
  prefs: []
  type: TYPE_IMG
- en: thus
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ âˆ‘ kâˆ’1 ---1---- kx = (1âˆ’ x )2\. k=1 ](img/file1910.png)'
  prefs: []
  type: TYPE_IMG
- en: Combining all of these, we finally have
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘âˆ ğ”¼[X ] = k (1 âˆ’ p)kâˆ’1p k=1 âˆâˆ‘ = p k(1âˆ’ p)kâˆ’1 k=1 1 1 = p-2 = -. p p ](img/file1911.png)'
  prefs: []
  type: TYPE_IMG
- en: Example 4\. Expected value of the constant random variable. Let c âˆˆâ„ be an arbitrary
    constant, and let X be the random variable that assumes the value c everywhere.
    As X is a discrete random variable, its expected value is simply
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[X ] = câ‹…P (X = c) = c. ](img/file1912.png)'
  prefs: []
  type: TYPE_IMG
- en: I know, this example looks silly, but it can be quite useful. When it is clear,
    we abuse the notation by denoting the constant c as the random variable itself.
  prefs: []
  type: TYPE_NORMAL
- en: 20.1.1 The expected value in poker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One more example before we move on. I was a mediocre no-limit Texas holdâ€™em
    player a while ago, and the first time I heard about the expected value was years
    before I studied probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: According to the rules of Texas holdâ€™em, each player holds two cards on their
    own, while five more shared cards are dealt. The shared cards are available for
    everyone, and the player with the strongest hand wins.
  prefs: []
  type: TYPE_NORMAL
- en: FigureÂ [20.1](#) shows how the table looks before the last card (the river)
    is revealed.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1913.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.1: The poker table before the river card'
  prefs: []
  type: TYPE_NORMAL
- en: There is money in the pot to be won, but to see the river, you have to call
    the opponentâ€™s bet. The question is, should you? Expected value to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s build a probabilistic model. We would win the pot with certain river cards
    but lose with all the others. If X represents our winnings, then
  prefs: []
  type: TYPE_NORMAL
- en: '![ #winning cards P(X = pot) = ---------------, #remaining cards P (X = âˆ’ bet)
    = --#losing-cards-. #remaining cards ](img/file1914.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the expected value is
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[X] = potâ‹…P (X = pot )âˆ’ betâ‹…P (X = âˆ’ bet) #winning cards #losing cards =
    potâ‹… ----------------âˆ’ betâ‹…----------------. #remaining cards #remaining cards
    ](img/file1915.png)'
  prefs: []
  type: TYPE_IMG
- en: When is the expected value positive? With some algebra, we obtain that ğ”¼[X]/span>0
    if and only if
  prefs: []
  type: TYPE_NORMAL
- en: '![#winning-cards bet- #losing cards > pot, ](img/file1916.png)'
  prefs: []
  type: TYPE_IMG
- en: which is called positive pot odds. If this is satisfied, making the bet is the
    right call. You might lose a hand with positive pot odds, but in the long term,
    your winnings will be positive.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, pot odds are extremely hard to determine in practice. For instance,
    you donâ€™t know what others hold, and counting the cards that would win the pot
    for you is not possible unless you have a good read on the opponents. Poker is
    much more than just math. Good players choose their bet specifically to throw
    off their opponentsâ€™ pot odds.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the idea behind the expected value, letâ€™s move on to
    the general case!
  prefs: []
  type: TYPE_NORMAL
- en: 20.2 Continuous random variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have only defined the expected value for discrete random variables.
    As ğ”¼[X] describes the average value of X in the long run, it should exist for
    continuous random variables as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interpretation of the expected value was simple: outcome times probability,
    summed over all potential values. However, there is a snag with continuous random
    variables: we donâ€™t have such a mass distribution, as the probabilities of individual
    outcomes are zero: P(X = x) = 0\. Moreover, we canâ€™t sum uncountably many values.'
  prefs: []
  type: TYPE_NORMAL
- en: What can we do?
  prefs: []
  type: TYPE_NORMAL
- en: Wishful thinking. This is one of the most powerful techniques in mathematics,
    and I am not joking.
  prefs: []
  type: TYPE_NORMAL
- en: Hereâ€™s the plan. Weâ€™ll pretend that the expected value of a continuous random
    variable is well-defined, and let our imagination run free. Say goodbye to mathematical
    precision, and allow our intuition to unfold. Instead of the probability of a
    given outcome, we can talk about X landing in a small interval. First, we divide
    up the set of real numbers into really small parts. To be more precise, let x[0]/span>x[1]/span>â€¦/span>x[n]
    be a granular partition of the real line. If the partition is refined enough,
    we should have
  prefs: []
  type: TYPE_NORMAL
- en: ğ”¼[X] â‰ˆ âˆ‘[k=1]^n x[k] P(x[kâˆ’1] â‰¤ X â‰¤ x[k]) (20.1)
  prefs: []
  type: TYPE_NORMAL
- en: 'The probabilities in ([20.1](ch032.xhtml#continuous-random-variables)) can
    be expressed in terms of the CDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n n âˆ‘ x P(x <X â‰¤ X ) = âˆ‘ x (F (x ) âˆ’ F (x )). k kâˆ’1 k k X k X kâˆ’1 k=1 k=1
    ](img/file1917.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These increments remind us of the difference quotients. We donâ€™t quite have
    these inside the sum, but with a â€œfancy multiplication with one,â€ we can achieve
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![âˆ‘n âˆ‘n xk(FX (xk)âˆ’ FX (xkâˆ’1)) = xk(xk âˆ’ xkâˆ’1)FX-(xk)âˆ’-FX-(xkâˆ’1). k=1 k=1 xk
    âˆ’ xkâˆ’ 1 ](img/file1918.png)'
  prefs: []
  type: TYPE_IMG
- en: If the x[i]-s are close to each other (and we can select them to be arbitrarily
    close), the difference quotients are close to the derivative of F[X], which is
    the density function f[X]. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![FX (xk)âˆ’ FX (xkâˆ’1) âˆ‘n FX (xk)âˆ’ FX (xkâˆ’1) âˆ‘n -----x-âˆ’-x-------- â‰ˆ fX (xk )
    xk(xk âˆ’ xkâˆ’1)-----x-âˆ’-x-------- â‰ˆ xk(xk âˆ’ xk âˆ’1)fX(xk). k kâˆ’ 1 k=1 k kâˆ’ 1 k=1
    ](img/file1919.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a Riemann-sum, defined by ([14.7](#))! Hence, the last sum is close
    to a Riemann-integral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n âˆ« âˆ‘ âˆ xk(xk âˆ’ xkâˆ’1)fX(xk) â‰ˆ âˆ’âˆ xfX (x)dx. k=1 ](img/file1920.png)'
  prefs: []
  type: TYPE_IMG
- en: Although we were not exactly precise in our argument, all of the above can be
    made mathematically correct. (But we are not going to do it here, as it is not
    relevant to us.) Thus, we finally obtain the formula of the expected value for
    continuous random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 93\. (The expected value of continuous random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and X : Î© â†’â„ be a continuous random variable.
    The expected value of X is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ ğ”¼ [X ] := xfX(x)dx. âˆ’âˆ ](img/file1921.png)'
  prefs: []
  type: TYPE_IMG
- en: As usual, letâ€™s see some examples first.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. Expected value of the uniform distribution. (See the definition
    of the uniform distribution in SectionÂ [19.3.4](ch031.xhtml#the-uniform-distribution2).)
    Let X âˆ¼ Uniform(a,b). Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ 1 ğ”¼[X ] = x-----dx âˆ’ âˆ bâˆ«âˆ’ a --1-- b = b âˆ’ a xdx [ a ]x=b = ---1---x2
    2(b âˆ’ a) x=a a + b = --2--, ](img/file1922.png)'
  prefs: []
  type: TYPE_IMG
- en: which is the midpoint of the interval [a,b], where the Uniform(a,b) lives.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. Expected value of the exponential distribution. (See the definition
    of the exponential distribution in SectionÂ [19.3.5](ch031.xhtml#the-exponential-distribution).)
    Let X âˆ¼ exp(Î»). Then, we need to calculate the integral
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ âˆ’ Î»x ğ”¼[X ] = xÎ»e dx. 0 ](img/file1923.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can do this via integration by parts (TheoremÂ [95](ch022.xhtml#x1-238002r95)):
    by letting f(x) = x and g^â€²(x) = Î»e^(âˆ’Î»x), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ ğ”¼ [X ] = xÎ»eâˆ’Î»xdx 0 [ ]x=âˆ âˆ« âˆ = âˆ’ xeâˆ’Î»x x=0 + eâˆ’ Î»xdx â—Ÿ-----â—=â—œ0-----â—
    0 [ ]x=âˆ = âˆ’ 1-eâˆ’Î»x Î» x=0 1- = Î». ](img/file1924.png)'
  prefs: []
  type: TYPE_IMG
- en: 20.3 Properties of the expected value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, the expected value has several useful properties. Most importantly,
    the expected value is linear with respect to the random variable.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 129\. (Linearity of the expected value)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and let X,Y : Î© â†’ â„ be two random variables.
    Moreover, let a,b âˆˆâ„ be two scalars. Then'
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[aX + bY ] = ağ”¼ [X ]+ bğ”¼ [Y ] ](img/file1925.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to prove this theorem here, but know that linearity is an essential
    tool. Do you recall the game that we used to introduce the expected value for
    discrete random variables? I toss a coin, and if it comes up heads, you win $1\.
    Tails, you lose $2\. If you think about it for a minute, this is the
  prefs: []
  type: TYPE_NORMAL
- en: '![X = 3â‹…Bernoulli(1âˆ•2)âˆ’ 2 ](img/file1926.png)'
  prefs: []
  type: TYPE_IMG
- en: distribution, and as such,
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[X] = ğ”¼[3â‹…Bernoulli(1 âˆ•2)âˆ’ 2] = 3â‹…ğ”¼ [Bernoulli(1 âˆ•2)]âˆ’ 2 = 3â‹… 1âˆ’ 2 2 1-
    = âˆ’ 2\. ](img/file1927.png)'
  prefs: []
  type: TYPE_IMG
- en: Of course, linearity goes way beyond this simple example. As youâ€™ve gotten used
    to this already, linearity is a crucial property in mathematics. We love linearity.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 20\.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that TheoremÂ [129](ch032.xhtml#x1-331002r129) did not say that X and
    Y have to be both discrete or both continuous. Even though we have only defined
    the expected value in such cases, there is a general definition that works for
    all random variables.
  prefs: []
  type: TYPE_NORMAL
- en: The snag is, it requires a familiarity with measure theory, falling way outside
    of our scope. Suffice to say, the theorem works as is.
  prefs: []
  type: TYPE_NORMAL
- en: If the expected value of a sum is the sum of the expected values, does the same
    apply to the product? Not in general, but fortunately, this works for independent
    random variables. (See DefinitionÂ [84](ch031.xhtml#x1-305002r84) for the definition
    of independent random variables.)
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 130\. (Expected value of the product of independent random variables)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and let X,Y : Î© â†’â„ be two independent random
    variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼ [XY ] = ğ”¼ [X ]ğ”¼[Y] ](img/file1928.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: This property is extremely useful, as weâ€™ll see in the next section, where weâ€™ll
    talk about variance and covariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more property thatâ€™ll help us to calculate the expected value of functions
    of the random variable, such as XÂ² or sinX:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 131\. (Law of the unconscious statistician)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, let X : Î© â†’ â„ be a random variable, and
    let g : â„ â†’â„ be an arbitrary function.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) If X is discrete with possible values x[1],x[2],â€¦, then
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘ ğ”¼[g(X)] = g(xn)P (X = xn). n ](img/file1929.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) If X is continuous with the probability density function f[X](x), then
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ ğ”¼[g(X )] = g(x)f (x)dx. âˆ’âˆ X ](img/file1930.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, calculating ğ”¼[XÂ²] for a continuous random variable can be done by simply
    taking
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ ğ”¼[X2] = x2fX (x)dx, âˆ’âˆ ](img/file1931.png)'
  prefs: []
  type: TYPE_IMG
- en: which will be used all the time.
  prefs: []
  type: TYPE_NORMAL
- en: 20.4 Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Plainly speaking, the expected value measures the average value of the random
    variable. However, even though both Uniform(âˆ’1,1) and Uniform(âˆ’100,100) have zero
    expected value, the latter is much more spread out than the former. Thus, ğ”¼[X]
    is not a good descriptor of the random variable X.
  prefs: []
  type: TYPE_NORMAL
- en: To add one more layer, we measure the average deviation from the expected value.
    This is done via the variance and the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 94\. (Variance and standard deviation)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, let X : Î© â†’â„ be a random variable, and
    let Î¼ = ğ”¼[X] be its expected value. The variance of X is defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ 2] Var [X ] := ğ”¼ (X âˆ’ Î¼ ) , ](img/file1932.png)'
  prefs: []
  type: TYPE_IMG
- en: while its standard deviation is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![Std[X] := âˆ˜Var--[X-]. ](img/file1933.png)'
  prefs: []
  type: TYPE_IMG
- en: Take note that in the literature, the expected value is often denoted by Î¼,
    while the standard deviation is denoted by Ïƒ. Together, they form two of the most
    important descriptors of a random variable.
  prefs: []
  type: TYPE_NORMAL
- en: FigureÂ [20.2](#) shows a visual interpretation of the mean and standard deviation
    in the case of a normal distribution. The mean shows the average value, while
    the standard deviation can be interpreted as the average deviation from the mean.
    (Weâ€™ll talk about the normal distribution in detail later, so donâ€™t worry if it
    is not yet familiar to you.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1934.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.2: Mean (Î¼) and standard deviation ![(Ïƒ) ](img/file1935.png) of the
    standard normal distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The usual method of calculating variance is not taking the expected value of
    (X âˆ’Î¼)Â², but taking the expected value of XÂ² and subtracting Î¼Â² from it. This
    is shown by the following proposition.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 5\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and let X : Î© â†’â„ be a random variable.'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '![Var[X] = ğ”¼[X2 ]âˆ’ ğ”¼[X ]2\. ](img/file1936.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. Let Î¼ = ğ”¼[X]. Because of the linearity of the expected value, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Var[X ] = ğ”¼[(X âˆ’ Î¼)2] = ğ”¼[X2 âˆ’ 2Î¼X + Î¼2] = ğ”¼[X ]2 âˆ’ 2Î¼ğ”¼ [X ]+ Î¼2 2 2 2 =
    ğ”¼[X ] âˆ’ 2Î¼ + Î¼ = ğ”¼[X2 ]âˆ’ Î¼2 = ğ”¼[X2 ]âˆ’ ğ”¼[X ]2, ](img/file1937.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: Is the variance linear as well? No, but there are some important identities
    regarding scalar multiplication and addition.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 132\. (Variance and the linear operations)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and let X : Î© â†’â„ be a random variable.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Let a âˆˆâ„ be an arbitrary constant. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ 2 Var[aX ] = a Var[X ]. ](img/file1938.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Let Y : Î© â†’â„ be a random variable that is independent from X. Then'
  prefs: []
  type: TYPE_NORMAL
- en: '![Var[X + Y ] = Var[X ]+ Var[Y]. ](img/file1939.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. (a) Let Î¼[X] = ğ”¼[X]. Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ ] [ ] Var[aX ] = ğ”¼ (aX âˆ’ a Î¼X)2 = ğ”¼ a2(X âˆ’ Î¼X )2 [ ] = a2ğ”¼ (X âˆ’ Î¼X )2
    = a2Var[X ], ](img/file1940.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show. (b) Let Î¼[Y] = ğ”¼[Y ]. Then, due to the linearity
    of the expected value, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ 2] Var[X + Y ] = ğ”¼ (X + Y âˆ’ (Î¼X + Î¼Y )) [ 2] = ğ”¼ ((X âˆ’ Î¼X ) + (Y âˆ’ Î¼Y))
    = ğ”¼ [(X âˆ’ Î¼X)2]+ 2ğ”¼[(X âˆ’ Î¼X )(Y âˆ’ Î¼Y)] + ğ”¼[(Y âˆ’ Î¼Y )2]. ](img/file1941.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, as X and Y are independent, ğ”¼[XY ] = ğ”¼[X]ğ”¼[Y ]. Thus, due to the linearity
    of the expected value,
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ ] [ ] ğ”¼ (X âˆ’ Î¼X )(Y âˆ’ Î¼Y) = ğ”¼ XY âˆ’ X Î¼Y âˆ’ Î¼X Y + Î¼X Î¼Y = ğ”¼[XY ] âˆ’ ğ”¼[X
    Î¼ ]âˆ’ ğ”¼ [Î¼ Y ]+ Î¼ Î¼ [ ] [ ] [Y ] X [ ] X Y = ğ”¼ X ğ”¼ Y âˆ’ ğ”¼ X Î¼Y âˆ’ Î¼X ğ”¼ Y + Î¼X Î¼Y
    = Î¼XÎ¼Y âˆ’ Î¼XÎ¼Y âˆ’ Î¼XÎ¼Y + Î¼XÎ¼Y = 0\. ](img/file1942.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, continuing the first calculation,
  prefs: []
  type: TYPE_NORMAL
- en: '![Var[X + Y ] = ğ”¼ [(X âˆ’ Î¼ )2]+ 2ğ”¼[(X âˆ’ Î¼ )(Y âˆ’ Î¼ )] + ğ”¼[(Y âˆ’ Î¼ )2] X X Y Y
    = ğ”¼ [(X âˆ’ Î¼X)2]+ ğ”¼[(Y âˆ’ Î¼Y )2], ](img/file1943.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: 20.4.1 Covariance and correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Expected value and variance measure a random variable in isolation. However,
    in real problems, we need to discover relations between separate measurements.
    Say, X describes the price of a given real estate, while Y measures its size.
    These are certainly related, but one does not determine the other. For instance,
    the location might be a differentiator between the prices.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest statistical way of measuring similarity is the covariance and correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 95\. (Covariance and correlation)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, let X,Y : Î© â†’ â„ be two random variables,
    and let Î¼[X] = ğ”¼[X],Î¼[Y] = ğ”¼[Y ] be their expected values and Ïƒ[X] = Std[X],Ïƒ[Y]
    = Std[Y ] their standard deviations.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) The covariance of X and Y is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ ] Cov [X,Y ] := ğ”¼ (X âˆ’ Î¼X )(Y âˆ’ Î¼Y ). ](img/file1944.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The correlation of X and Y is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ Cov[X, Y] Corr [X, Y ] := ---------. ÏƒXÏƒY ](img/file1945.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly to variance, the definition of covariance can be simplified to provide
    an easier way of calculating its exact value.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 6\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, let X,Y : Î© â†’ â„ be two random variables,
    and let Î¼[X] = ğ”¼[X],Î¼[Y] = ğ”¼[Y ] be their expected values.'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '![Cov[X, Y] = ğ”¼[XY ]âˆ’ Î¼X Î¼Y . ](img/file1946.png)'
  prefs: []
  type: TYPE_IMG
- en: Proof. This is just a simple calculation. According to the definition, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ ] Cov[X, Y] = ğ”¼ (X âˆ’ Î¼X )(Y âˆ’ Î¼Y) [ ] = ğ”¼ XY âˆ’ X Î¼Y âˆ’ Î¼X Y + Î¼X Î¼Y = ğ”¼[XY
    ] âˆ’ ğ”¼[X Î¼ ]âˆ’ ğ”¼[Î¼ Y ]+ Î¼ Î¼ [ ] [ ]Y X[ ] X Y = ğ”¼ XY âˆ’ ğ”¼ X Î¼Y âˆ’ Î¼X ğ”¼ Y + Î¼X Î¼Y [
    ] = ğ”¼ XY âˆ’ Î¼X Î¼Y âˆ’ Î¼X Î¼Y + Î¼X Î¼Y = ğ”¼[XY ]âˆ’ Î¼X Î¼Y , ](img/file1947.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important properties of covariance and correlation is that they
    are zero for independent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 133\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space, and let X,Y : Î© â†’â„ be two independent random
    variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, Cov[X,Y ] = 0\. (And consequently, Corr[X,Y ] = 0 as well.)
  prefs: []
  type: TYPE_NORMAL
- en: The proof follows straight from the definition and TheoremÂ [130](ch032.xhtml#x1-331005r130),
    so this is left as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take note, as this is extra important: independence implies zero covariance,
    but zero covariance does not imply independence. Here is an example.'
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a discrete random variable with the probability mass function
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 P(X = âˆ’ 1) = P (X = 0) = P(X = 1) = 3, ](img/file1948.png)'
  prefs: []
  type: TYPE_IMG
- en: and let Y = XÂ².
  prefs: []
  type: TYPE_NORMAL
- en: The expected value of X is
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼ [X ] = (âˆ’ 1)â‹…P (X = âˆ’ 1)+ 0 â‹…P (X = 0)+ 1 â‹…P (X = 1) = âˆ’ 1-+ 0 + 1- 3 3
    = 0, ](img/file1949.png)'
  prefs: []
  type: TYPE_IMG
- en: while the law of the unconscious statistician (TheoremÂ [131](ch032.xhtml#x1-331006r131))
    gives that
  prefs: []
  type: TYPE_NORMAL
- en: '![ 2 ğ”¼[Y ] = ğ”¼[X ] = 1 â‹…P(X = âˆ’ 1) + 0â‹…P (X = 0) + 1â‹…P (X = 1 ) 1- 1- = 3 +
    0+ 3 2 = -, 3 ](img/file1950.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[XY ] = ğ”¼[X3] = 0\. ](img/file1951.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![Cov [X,Y ] = ğ”¼ [XY ]âˆ’ ğ”¼[X ]ğ”¼ [Y ] 3 2 = ğ”¼ [X ]âˆ’ ğ”¼ [X ]ğ”¼[X ] 2- = 0 âˆ’ 0â‹… 3
    = 0\. ](img/file1952.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, X and Y are not independent, as Y = XÂ² is a function of X. (I shamelessly
    stole this example from a brilliant Stack Overflow thread, which you should read
    here for more on this question: [https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence](https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence))'
  prefs: []
  type: TYPE_NORMAL
- en: Do you recall that we interpreted the concept of probability as the relative
    frequency of occurrences? Now that we have the expected value under our belt,
    we can finally make this precise. Letâ€™s look at the famous law of large numbers!
  prefs: []
  type: TYPE_NORMAL
- en: 20.5 The law of large numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Weâ€™ll continue our journey with a quite remarkable and famous result: the law
    of large numbers. You have probably already heard several faulty arguments invoking
    the law of large numbers. For instance, gamblers are often convinced that their
    bad luck will end soon because of said law. This is one of the most frequently
    misused mathematical terms, and we are here to clear that up.'
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll do this in two passes. First, we are going to see an intuitive interpretation,
    then add the technical but important mathematical details. Iâ€™ll try to be gentle.
  prefs: []
  type: TYPE_NORMAL
- en: 20.5.1 Tossing coinsâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, letâ€™s toss some coins again. If we toss coins repeatedly, what is the
    relative frequency of heads in the long run?
  prefs: []
  type: TYPE_NORMAL
- en: 'We should have a pretty good guess already: the average number of heads should
    converge to P(heads) = p as well. Why? Because we saw this when studying the frequentist
    interpretation of probability in SectionÂ [18.2.7](ch030.xhtml#how-to-interpret-probability).'
  prefs: []
  type: TYPE_NORMAL
- en: Our simulation showed that the relative frequency of heads does indeed converge
    to the true probability. This time, weâ€™ll carry the simulation a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: First, to formulate the problem, letâ€™s introduce the independent random variables
    X[1],X[2],â€¦ that are distributed along Bernoulli(p), where X[i] = 0 if the toss
    results in tails, while X[i] = 1 if it is heads. We are interested in the long-term
    behavior of
  prefs: []
  type: TYPE_NORMAL
- en: '![-- X1-+-â‹…â‹…â‹…+-Xn- Xn = n . ](img/file1953.png)'
  prefs: []
  type: TYPE_IMG
- en: X[n] is called the sample average. We have already seen that the sample average
    gets closer and closer to p as n grows. Letâ€™s see the simulation one more time,
    before we go any further. (The parameter p is selected to be 1âˆ•2 for the sake
    of the example.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And here is the plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1954.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.3: Relative frequency of the coin tosses'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nothing new so far. However, if you have a sharp eye, you might ask the question:
    is this just an accident? After all, we are studying the average'
  prefs: []
  type: TYPE_NORMAL
- en: '![-- X1-+-â‹…â‹…â‹…+-Xn- Xn = n , ](img/file1955.png)'
  prefs: []
  type: TYPE_IMG
- en: which is (almost) a binomially distributed random variable! To be more precise,
    if X[i] âˆ¼ Bernoulli(p), then
  prefs: []
  type: TYPE_NORMAL
- en: '![-- Xn âˆ¼ 1Binomial(n,p). n ](img/file1956.png)'
  prefs: []
  type: TYPE_IMG
- en: (We saw this earlier when discussing the sums of discrete random variables in
    SectionÂ [19.2.7](ch031.xhtml#sums-of-discrete-random-variables).)
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is far from guaranteed that this distribution will be concentrated
    around a single value. So, letâ€™s do some more simulations. This time, weâ€™ll toss
    a coin a thousand times to see the distribution of the averages. Quite meta, I
    know.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can visualize the distributions on histograms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1957.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.4: Sample average distributions of coin tosses'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the probability of X[n] falling far from p becomes smaller and
    smaller. For any small ğœ€, we can formulate the probability of â€œX[n] falling farther
    from p than ğœ€â€ as P(|X[n] âˆ’p|ğœ€).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, mathematically speaking, our guess is
  prefs: []
  type: TYPE_NORMAL
- en: '![ -- nlâ†’imâˆ P(|Xn âˆ’ p| >ğœ€) = 0\. ](img/file1958.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, is this just an accident, and were we just lucky to study an experiment
    where this is true? Would the same work for random variables other than Bernoulli
    ones? What will the sample averages converge to? (If they converge at all.)
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll find out.
  prefs: []
  type: TYPE_NORMAL
- en: 20.5.2 â€¦rolling diceâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Letâ€™s play dice. To keep things simple, we are interested in the average value
    of a roll in the long run. To build a proper probabilistic model, letâ€™s introduce
    random variables!
  prefs: []
  type: TYPE_NORMAL
- en: A single roll is uniformly distributed on {1,2,â€¦,6}, and each roll is independent
    from the others. So, let X[1],X[2],â€¦ be independent random variables, each distributed
    according to Uniform({1,2,â€¦,6}).
  prefs: []
  type: TYPE_NORMAL
- en: How does the sample average X[n] behave? Simulation time. Weâ€™ll randomly generate
    1000 rolls, then explore how X[n] behaves.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Again, to obtain a bit of an insight, weâ€™ll visualize the averages on a plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1959.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.5: Sample averages of rolling a six-sided dice'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to note is that these are suspiciously close to 3.5\. This
    is not a probability, but the expected value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[X1 ] = ğ”¼[X2] = â‹…â‹…â‹… = 3.5\. ](img/file1960.png)'
  prefs: []
  type: TYPE_IMG
- en: For Bernoulli(p) distributed random variables, the expected value coincides
    with the probability p. However, this time, X[n] does not have a nice and explicit
    distribution like in the case of coin tosses, where the sample averages were binomially
    distributed. So, letâ€™s roll some more dice to estimate how X[n] is distributed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file1961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.6: Sample average distributions of dice rolls'
  prefs: []
  type: TYPE_NORMAL
- en: It seems like, once more, the distribution of X[n] is concentrated around ğ”¼[X[1]].
    Our intuition tells us that this is not an accident; that this phenomenon is true
    for a wide range of random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me spoil the surprise: this is indeed the case, and weâ€™ll see this now.'
  prefs: []
  type: TYPE_NORMAL
- en: 20.5.3 â€¦and all the rest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This time, let X[1],X[2],â€¦ be a sequence of independent and identically distributed
    (i.i.d.) random variables. Not coin tosses, not dice rolls, but any distribution.
    We saw that the sample average X[n] seems to converge to the joint expected value
    of the X[i]-s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![â€²â€²- â€²â€² Xn â†’ ğ”¼[X1] ](img/file1962.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note the quotation marks: X[n] is not a number but a random variable. Thus,
    we canâ€™t (yet) speak about convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: In mathematically precise terms, what we saw previously is that for large enough
    n-s, the sample average X[n] is highly unlikely to fall far from the joint expected
    value Î¼ = ğ”¼[X[1]]; that is,
  prefs: []
  type: TYPE_NORMAL
- en: lim[nâ†’âˆ] P(|X[n] âˆ’ Î¼| > ğœ€) = 0 (20.2)
  prefs: []
  type: TYPE_NORMAL
- en: holds for all ğœ€/span>0\.
  prefs: []
  type: TYPE_NORMAL
- en: The limit ([20.2](ch032.xhtml#and-all-the-rest)) seems hard to prove right now
    even in the simple case of coin tossing. There, X[n] âˆ¼![1n](img/file1963.png)
    Binomial(n,p), thus
  prefs: []
  type: TYPE_NORMAL
- en: '![ âŒŠn(p+ ğœ€)âŒ‹ ( ) -- âˆ‘ n k nâˆ’k P(|Xn âˆ’ Î¼ | >ğœ€) = 1 âˆ’ k p (1 âˆ’ p) , k=âŒŠn(pâˆ’ ğœ€)âŒ‹
    ](img/file1964.png)'
  prefs: []
  type: TYPE_IMG
- en: where the symbol âŒŠxâŒ‹ denotes the largest integer that is smaller than x. This
    does not look friendly at all. (I leave the verification as an exercise.)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, our plan is the following.
  prefs: []
  type: TYPE_NORMAL
- en: Find a way to estimate P(jX[n]âˆ’Î¼j/span>ğœ€) in a way that is independent from
    the distribution of the X[i]-s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the upper estimate to show lim[nâ†’âˆ]P(jX[n] âˆ’Î¼j/span>ğœ€) = 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Letâ€™s go.
  prefs: []
  type: TYPE_NORMAL
- en: 20.5.4 The weak law of large numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, the upper estimates. There are two general inequalities thatâ€™ll help
    us to deal with P(jX[n] âˆ’Î¼j â‰¥ğœ€).
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 134\. (Markovâ€™s inequality)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space and let X : Î© â†’ [0,âˆ) be a nonnegative random
    variable. Then'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ğ”¼[X ] P (X â‰¥ t) â‰¤ ----- t ](img/file1965.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for any t âˆˆ (0,âˆ).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. We have to separate the discrete and the continuous cases. The proofs
    are almost identical, so Iâ€™ll only do the discrete case here, while the continuous
    is left for you as an exercise to test your understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let X : Î© â†’ {x[1],x[2],â€¦} be a discrete random variable (where x[k] â‰¥ 0
    for all k), and t âˆˆ (0,âˆ) be an arbitrary positive real number.'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: ğ”¼[X]
  prefs: []
  type: TYPE_NORMAL
- en: = âˆ‘ [k=1]^âˆx [k]P(X = x[k])
  prefs: []
  type: TYPE_NORMAL
- en: = âˆ‘ [k:x[k]/span>tx[k]P(X = x[k]) + âˆ‘ [k:x[k]â‰¥t]x[k]P(X = x[k]),]
  prefs: []
  type: TYPE_NORMAL
- en: 'where the sum âˆ‘ [k:x[k]/span>t only accounts for k-s with x[k]/span>t, and
    similarly, âˆ‘ [k] : x[k] â‰¥t only accounts for k-s with x[k] â‰¥t.]'
  prefs: []
  type: TYPE_NORMAL
- en: As the x[k]-s are nonnegative by assumption, we can estimate ğ”¼[X] from below
    by omitting one of them. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘ âˆ‘ ğ”¼[X ] = xkP (X = xk )+ xkP (X = xk) k:x < k:x â‰¥t âˆ‘k k â‰¥ xkP (X = xk
    ) k:xkâ‰¥t âˆ‘ â‰¥ t P (X = xk ) k:xkâ‰¥t = tP(X â‰¥ t), ](img/file1966.png)'
  prefs: []
  type: TYPE_IMG
- en: from which Markovâ€™s inequality
  prefs: []
  type: TYPE_NORMAL
- en: '![P (X â‰¥ t) â‰¤ ğ”¼[X-] t ](img/file1967.png)'
  prefs: []
  type: TYPE_IMG
- en: follows.
  prefs: []
  type: TYPE_NORMAL
- en: The law of large numbers is only one step away from Markovâ€™s inequality. This
    last step is so useful that it deserves to be its own theorem. Meet the famous
    inequality of Chebyshev.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 135\. (Chebyshevâ€™s inequality)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let (Î©,Î£,P) be a probability space and let X : Î© â†’â„ be a random variable with
    finite variance ÏƒÂ² = Var[X]/span>âˆand expected value ğ”¼[X] = Î¼.'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ Ïƒ2 P(|X âˆ’ Î¼| â‰¥ t) â‰¤ -t2- ](img/file1968.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for all t âˆˆ (0,âˆ).
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As |X âˆ’Î¼| is a nonnegative random variable, we can apply TheoremÂ [134](ch032.xhtml#x1-338002r134)
    to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![P (|X âˆ’ Î¼| â‰¥ t) = P(|X âˆ’ Î¼|2 â‰¥ t2) 2 â‰¤ ğ”¼[|X--âˆ’-Î¼|-]. t2 ](img/file1969.png)'
  prefs: []
  type: TYPE_IMG
- en: However, as ğ”¼[|X âˆ’Î¼|Â²] = Var[X] = ÏƒÂ², we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ 2 2 P(|X âˆ’ Î¼ | â‰¥ t) â‰¤ ğ”¼-[|X-âˆ’2Î¼-|] = Ïƒ2 t t ](img/file1970.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we had to show.
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we are ready to precisely formulate and prove the law of large
    numbers. After all this setup, the (weak) law of large numbers is just a small
    step away. Here it is in its full glory.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 136\. (The weak law of large numbers)
  prefs: []
  type: TYPE_NORMAL
- en: Let X[1],X[2],â€¦ be a sequence of independent and identically distributed random
    variables with finite expected value Î¼ = ğ”¼[X[1]] and variance ÏƒÂ² = Var[X[1]],
    and let
  prefs: []
  type: TYPE_NORMAL
- en: '![-- X1 + â‹…â‹…â‹…+ Xn Xn = ------------- n ](img/file1971.png)'
  prefs: []
  type: TYPE_IMG
- en: be their sample average. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![lim P (|X- âˆ’ Î¼| â‰¥ ğœ€) = 0 nâ†’ âˆ n ](img/file1972.png)'
  prefs: []
  type: TYPE_IMG
- en: holds for any ğœ€/span>0\.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. As the X[i]-s are independent, the variance of the sample average is
  prefs: []
  type: TYPE_NORMAL
- en: '![ -- [ X1 + â‹…â‹…â‹…+ Xn ] Var[Xn ] = Var ------------- n = -1-Var[X1 + â‹…â‹…â‹…+ Xn]
    n2 = -1-(Var[X ]+ â‹…â‹…â‹…+ Var[X ]) n2 1 n n Ïƒ2 Ïƒ2 = -n2- = n-. ](img/file1973.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, by using Chebyshevâ€™s inequality from TheoremÂ [135](ch032.xhtml#x1-338003r135),
    we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ -- -- Var[Xn-] -Ïƒ2- P (|Xn âˆ’ Î¼ | â‰¥ ğœ€) â‰¤ ğœ€2 = n ğœ€2\. ](img/file1974.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![0 â‰¤ lim P(|Xn âˆ’ Î¼ | â‰¥ ğœ€) nâ†’ âˆ -Ïƒ2- â‰¤ nlâ†’imâˆ nğœ€2 = 0, ](img/file1975.png)'
  prefs: []
  type: TYPE_IMG
- en: hence
  prefs: []
  type: TYPE_NORMAL
- en: '![ -- nliâ†’mâˆ P(|Xn âˆ’ Î¼ | â‰¥ ğœ€) = 0, ](img/file1976.png)'
  prefs: []
  type: TYPE_IMG
- en: which is what we needed to show.
  prefs: []
  type: TYPE_NORMAL
- en: TheoremÂ [136](ch032.xhtml#x1-338004r136) is not all that can be said about the
    sample averages. There is a stronger result, showing that the sample averages
    do in fact converge to the mean with probability 1.
  prefs: []
  type: TYPE_NORMAL
- en: 20.5.5 The strong law of large numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Why is TheoremÂ [136](ch032.xhtml#x1-338004r136) called the â€œweakâ€ law? Think
    about the statement
  prefs: []
  type: TYPE_NORMAL
- en: lim[nâ†’âˆ] P(|X[n] âˆ’ Î¼| â‰¥ ğœ€) = 0 (20.3)
  prefs: []
  type: TYPE_NORMAL
- en: for a moment. For a given Ï‰ âˆˆ Î©, this doesnâ€™t tell us anything about the convergence
    of a concrete sample average
  prefs: []
  type: TYPE_NORMAL
- en: '![X- (Ï‰) = X1(Ï‰-)+-â‹…â‹…â‹…+-Xn-(Ï‰-), n n ](img/file1977.png)'
  prefs: []
  type: TYPE_IMG
- en: it just tells us that in a probabilistic sense, X[n] is concentrated around
    the joint expected value Î¼. In a sense, ([20.3](ch032.xhtml#the-strong-law-of-large-numbers))
    is a weaker version of
  prefs: []
  type: TYPE_NORMAL
- en: '![P( lim X- = Î¼ ) = 1, nâ†’ âˆ n ](img/file1978.png)'
  prefs: []
  type: TYPE_IMG
- en: hence the terminology weak law of large numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Do we have a stronger result than TheoremÂ [136](ch032.xhtml#x1-338004r136)?
    Yes, we do.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 137\. (The strong law of large numbers)
  prefs: []
  type: TYPE_NORMAL
- en: Let X[1],X[2],â€¦ be a sequence of independent and identically distributed random
    variables with finite expected value Î¼ = ğ”¼[X[1]] and variance ÏƒÂ² = Var[X[1]],
    and let
  prefs: []
  type: TYPE_NORMAL
- en: '![-- X + â‹…â‹…â‹…+ X Xn = -1---------n- n ](img/file1979.png)'
  prefs: []
  type: TYPE_IMG
- en: be their sample average. Then
  prefs: []
  type: TYPE_NORMAL
- en: '![P( lim Xn = Î¼ ) = 1\. nâ†’ âˆ ](img/file1980.png)'
  prefs: []
  type: TYPE_IMG
- en: We are not going to prove this, just know that the sample average will converge
    to the mean with probability one.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 21\. (Convergence of random variables)
  prefs: []
  type: TYPE_NORMAL
- en: What we have seen in the weak and strong laws of large numbers are not unique
    to sample averages. Similar phenomena can be observed in other cases, thus, these
    types of convergences have their own exact definitions.
  prefs: []
  type: TYPE_NORMAL
- en: If X[1],X[2],â€¦ is a sequence of random variables, we say that
  prefs: []
  type: TYPE_NORMAL
- en: (a) X[n] converges in probability towards X if
  prefs: []
  type: TYPE_NORMAL
- en: '![ lim P(|Xn âˆ’ X | â‰¥ ğœ€) = 0 nâ†’ âˆ ](img/file1981.png)'
  prefs: []
  type: TYPE_IMG
- en: for all ğœ€/span>0\. Convergence in probability is denoted by X[n]![âˆ’Pâ†’](img/file1982.png)X.
  prefs: []
  type: TYPE_NORMAL
- en: (b) X[n] converges almost surely towards X if
  prefs: []
  type: TYPE_NORMAL
- en: '![P( lim Xn = X ) = 1 nâ†’ âˆ ](img/file1983.png)'
  prefs: []
  type: TYPE_IMG
- en: holds. Almost sure convergence is denoted by X[n]![âˆ’aâˆ’.â†’ s.](img/file1984.png)X.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the weak and strong laws of large numbers state that in certain cases,
    the sample averages converge to the expected value both in probability and almost
    surely.
  prefs: []
  type: TYPE_NORMAL
- en: 20.6 Information theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have already trained a machine learning model in your practice, chances
    are you are already familiar with the mean-squared error
  prefs: []
  type: TYPE_NORMAL
- en: '![ n 1-âˆ‘ 2 n MSE (x,y) = n (f(xi)âˆ’ yi), x, y âˆˆ â„ , i=1 ](img/file1985.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where f : â„^n â†’â„ represents our model, x âˆˆâ„^n is the vector of one-dimensional
    observations, and y âˆˆâ„^n is the ground truth. After learning all about the expected
    value, this sum should be familiar: if we assume a probabilistic viewpoint and
    let X and Y be the random variables describing the data, then the mean-squared
    error can be written as the expected value'
  prefs: []
  type: TYPE_NORMAL
- en: '![ [ ] MSE (x, y) = ğ”¼ (f(X )âˆ’ Y )2\. ](img/file1986.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the mean-squared error is not suitable for classification problems.
    For instance, if the task is to classify the object of an image, the output is
    a discrete probability distribution for each sample. In this situation, we could
    use the so-called cross-entropy, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n H [p,q] = âˆ’ âˆ‘ p logq i i i=1 ](img/file1987.png)'
  prefs: []
  type: TYPE_IMG
- en: where p âˆˆâ„^n denotes the one-hot encoded vector of the class label for a single
    data sample, and q âˆˆâ„^n is the class label prediction, forming a probability distribution.
    (One-hot encoding is the process where we represent a finite set of possible class
    labels, such as {a,b,c} as zero-one vectors, like
  prefs: []
  type: TYPE_NORMAL
- en: '![a â† â†’ (1,0,0), b â† â†’ (0,1,0), c â† â†’ (0,0,1). ](img/file1988.png)'
  prefs: []
  type: TYPE_IMG
- en: We do this because itâ€™s easier to work with vectors and matrices than with strings.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Not that surprisingly, H[p,q] is also an expected value, but itâ€™s much more
    than that: it quantifies the information content of the distribution q compared
    to the ground truth distribution q.'
  prefs: []
  type: TYPE_NORMAL
- en: But what is information in a mathematical sense? Letâ€™s dive in.
  prefs: []
  type: TYPE_NORMAL
- en: 20.6.1 Guess the number
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Letâ€™s start with a simple game. I have thought of an integer between 0 and 7,
    and your job is to find out which one by asking yes-no questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1989.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.7: Which number am I thinking of?'
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible strategy is to guess the numbers one by one. In other words, the
    sequence of your questions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Is it 0?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it 1?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![.. . ](img/file1990.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Is it 7?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this strategy works, it is not an effective one. Why? Consider the
    average number of questions. Let the random variable X denote the number I have
    picked. As X is uniformly distributed â€” that is, P(X = k) = 1âˆ•8 for all k = 0,â€¦,7
    â€” the probability of asking exactly k questions is
  prefs: []
  type: TYPE_NORMAL
- en: '![P(#questions = k) = P(X = k âˆ’ 1) = 1 8 ](img/file1991.png)'
  prefs: []
  type: TYPE_IMG
- en: as well. Thus, the number of questions needed is also uniformly distributed
    on {1,â€¦,8}, thus
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘8 ğ”¼[#questions] = kP (#questions = k ) k=1 8 = 1-âˆ‘ k 8 k=1 = 1-8â‹…9-= 9-,
    8 2 2 ](img/file1992.png)'
  prefs: []
  type: TYPE_IMG
- en: where we have used that âˆ‘ [k=1]^n = ![n(n+1) 2](img/file1993.png).
  prefs: []
  type: TYPE_NORMAL
- en: Can we do better than this? Yes. In the previous sequential strategy, each question
    has a small chance of hitting, and a large chance of eliminating, only one potential
    candidate. Itâ€™s easy to see that the best would be to eliminate half the search
    space with each question.
  prefs: []
  type: TYPE_NORMAL
- en: Say, the number I thought of is 2\. By asking â€œis the number larger than 3â€?,
    the answer trims out four of the candidates.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1994.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.8: The search space after the question is the number larger than
    3?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each subsequent question cuts the remaining possibilities in half. In the case
    of X = 2, the three questions are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Is X â‰¥ 4? (no)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is X â‰¥ 2? (yes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is X â‰¥ 3? (no)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the so-called binary search, illustrated by FigureÂ [20.9](#).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1995.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.9: Figuring out the answer with binary search'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we write down the answers for our three consecutive questions (X â‰¥ 4, X
    â‰¥ 2, X â‰¥ 3) as a zero-one sequence, we obtain 010\. If this looks familiar, itâ€™s
    not an accident: 010 is 2 in binary. In fact, all of the answers can be coded
    using their binary form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![0 = 0002, 1 = 0012, 2 = 0102, 3 = 0112 4 = 1002, 5 = 1012, 6 = 1102, 7 =
    1112\. ](img/file1996.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can reformulate our three questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is 1 the 1st digit of X in binary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is 1 the 2nd digit of X in binary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is 1 the 3rd digit of X in binary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As the above example shows, guessing the number is equivalent to finding the
    binary representation of the objects to be guessed. Each digit represents exactly
    one bit of information. (In this case, the representation is the actual binary
    form.) Binary codings have an additional perk: we no longer have to sequentially
    go through the questions, we can ask them simultaneously. From now on, instead
    of questions, weâ€™ll talk about binary representations (codings) and their bits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the number of bits is the same for each outcome of X. Thus, as
    this strategy always requires three bits, their average number is three as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 7 7 ğ”¼ [#bits] = âˆ‘ 3â‹…P (X = k ) = âˆ‘ 3â‹… 1-= 3\. 8 k=0 k=0 ](img/file1997.png)'
  prefs: []
  type: TYPE_IMG
- en: Can we do better than the three questions on average? No.Â I invite you to come
    up with your arguments, but weâ€™ll see this later.
  prefs: []
  type: TYPE_NORMAL
- en: Where does the number three in the above come from? In general, if we have 2^k
    possible choices, then log [2]2^k = k questions will be enough to find the answer.
    (As each question cuts the set of possible answers in half.) In other words, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘7 ğ”¼[#bits] = P (X = k )log2 23 k=0 7 = âˆ‘ P (X = k )log P(X = k)âˆ’1\. 2 k=0
    ](img/file1998.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the value log [2]P(X = k)^(âˆ’1) is the number of bits needed to represent
    k in our coding. In other words,
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[#bits] = ğ”¼[log2P (X = k)âˆ’1]. ](img/file1999.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Letâ€™s get a bit ahead of ourselves: this is the famous entropy of the random
    variable X, and the quantity log [2]P(X = k)^(âˆ’1) is the so-called information
    content of the event X = k.'
  prefs: []
  type: TYPE_NORMAL
- en: However, at this point, these concepts are quite unclear. What does log [2]P(X
    = k)^(âˆ’1) have to do with information? Why canâ€™t we represent k better than log
    [2]P(X = k)^(âˆ’1) bits? Weâ€™ll see the answers soon.
  prefs: []
  type: TYPE_NORMAL
- en: '20.6.2 Guess the number 2: Electric Boogaloo'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Letâ€™s play the guessing game again but with a twist this time. Now, I have picked
    a number from {0,1,2}, and you have to guess which one. The catch is, I am twice
    as likely to select 0 than the others.
  prefs: []
  type: TYPE_NORMAL
- en: In probabilistic terms, if X denotes the number I picked, then P(X = 0) = 1âˆ•2,
    while P(X = 1) = P(X = 2) = 1âˆ•4.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the best strategy? There are two key facts to recall:'
  prefs: []
  type: TYPE_NORMAL
- en: good questions cut the search space in half,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and asking questions is equivalent to finding a binary encoding of the outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, as we are looking for the encoding that is optimal on average, cutting
    the search space in half with each digit is not meant in a numeric way. Rather,
    in a probabilistic one. Thus, if 0 is indeed twice as likely, representing 0,1,2
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![0 âˆ¼ 0, 1 âˆ¼ 01, 2 âˆ¼ 10, ](img/file2000.png)'
  prefs: []
  type: TYPE_IMG
- en: the average number of bits is
  prefs: []
  type: TYPE_NORMAL
- en: '![ğ”¼[#bits] = P (X = 0) â‹…1+ P (X = 1) â‹…2+ P (X = 2) â‹…2 = P (X = 0) log22 + P
    (X = 1)log24 + P (X = 2)log24 2 âˆ‘ âˆ’1 = P(X = k)log2P (X = k) k=0 = 3-. 2 ](img/file2001.png)'
  prefs: []
  type: TYPE_IMG
- en: Once more, we have arrived at the familiar logarithmic formula. We are one step
    closer to grasping the meaning of the mysterious quantity log [2]P(X = k)^(âˆ’1).
    The smaller it is, the more questions we need; equivalently, the more bits we
    need to represent k within our encoding to avoid information loss.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are these mysterious quantities exactly?
  prefs: []
  type: TYPE_NORMAL
- en: 20.6.3 Information and entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is time to formulate the general problem. Suppose that our random variable
    X assumes a number from the set {1,2,â€¦,N}, each with probability p[k] = P(X =
    k). Upon repeatedly observing X, what is the average information content of our
    observations?
  prefs: []
  type: TYPE_NORMAL
- en: According to what weâ€™ve learned, we are looking for the quantity
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘N ğ”¼ [I (X )] = âˆ’ p logp , k=1 k k ](img/file2002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where I : â„• â†’â„ denotes the information I(k) = âˆ’log p[k]. Previously, we have
    seen two special cases where I(k) is the average number of questions needed to
    guess k. (Equivalently, the information is the average number of bits in k using
    the optimal encoding.)'
  prefs: []
  type: TYPE_NORMAL
- en: What is the information in general?
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s look for I as an unknown function of the probabilities: I(x) = f(P(X
    = x)). What can f be? There are two key properties of thatâ€™ll lead us to the answer.
    First, the more probable an event is, the less information content there is. (Recall
    the previous example, where the most probable outcome required the least amount
    of bits in our binary representation.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, as a function of the probabilities, the information is additive: f(pq)
    = f(p) + f(q). Why? Suppose that I have picked two numbers, independently from
    each other, and now you have to guess those two. You can do this sequentially,
    applying the optimal strategy to the first one, then the second one.'
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, f(p) is
  prefs: []
  type: TYPE_NORMAL
- en: continuous,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strictly increasing, that is, f(p)/span>f(q) for any p/span>q,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and additive, that is, f(pq) = f(p) + f(q) for any p,q.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iâ€™ll spare you the mathematical details, but with a bit of calculus magic, we
    can confidently conclude that the only option is f(p) = âˆ’log [a]p, where a/span>1\.
    Seemingly, information depends on the base, but as
  prefs: []
  type: TYPE_NORMAL
- en: '![log x = logax-, b logab ](img/file2005.png)'
  prefs: []
  type: TYPE_IMG
- en: the choice of base only influences the information and entropy up to a multiplicative
    scaling factor. Thus, using the natural logarithm is the simplest choice.
  prefs: []
  type: TYPE_NORMAL
- en: So, here is the formal definition at last.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 96\. (Information)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a discrete random variable with probability mass function {P(X = x[k])}[k].
  prefs: []
  type: TYPE_NORMAL
- en: The information of the event X = x[k] is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![I(xk) := âˆ’ logP (X = xk ) = logP (X = xk )âˆ’ 1\. ](img/file2006.png)'
  prefs: []
  type: TYPE_IMG
- en: (Note that whenever the base of log is not indicated, we are using the natural
    base e.) To emphasize the dependency of the information on X, weâ€™ll sometimes
    explicitly denote the connection by I[X](x[k]).
  prefs: []
  type: TYPE_NORMAL
- en: Armed with the notion of information, we are ready to define entropy, the average
    amount of information per observation. This quantity is named after Claude Shannon,
    who essentially founded information theory in his epic paper â€œA Mathematical Theory
    of Communication.â€
  prefs: []
  type: TYPE_NORMAL
- en: Definition 97\. (Shannon entropy)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a discrete random variable with probability mass function {P(X = x[k])}[k].
  prefs: []
  type: TYPE_NORMAL
- en: The entropy of X is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![H[X ] := ğ”¼[I(X )] âˆ‘âˆ = âˆ’ P(X = xk)logP (X = xk). k=1 ](img/file2007.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though H[X] is called the Shannon entropy, weâ€™ll just simply refer to it
    as entropy, unless an explicit distinction is needed.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first things we can notice is that H[X] â‰¥ 0\. This is shown in the
    following proposition.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 7\. (The nonnegativity of entropy)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be an arbitrary discrete random variable. Then H[X] â‰¥ 0.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. By definition,
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘ H [X ] = P(X = xk)logP (X = xk)âˆ’1\. k ](img/file2008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, suppose that P(X = x[k])â‰ 0 for all k. Then, as 0 P(X = x[k]) â‰¤ 1, the
    information is nonnegative: log P(X = x[k])^(âˆ’1) â‰¥ 0\. Hence, as all terms in
    the defining sum are nonnegative, H[X] is nonnegative as well.'
  prefs: []
  type: TYPE_NORMAL
- en: If P(X = x[k]) = 0 for some k, then, as lim[xâ†’0+]xlog x = 0, the expression
    0 â‹… log 0 is taken to be 0\. Thus, H[X] is still nonnegative.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the entropy in practice is hard, as we have to evaluate sums that
    involve logarithms. However, there are a few special cases that shed some much
    needed light on the concept of entropy. Letâ€™s look at them!
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. The discrete uniform distribution. (See the definition of the discrete
    uniform distribution in SectionÂ [19.2.4](ch031.xhtml#the-uniform-distribution).)
    Let X âˆ¼ Uniform({1,â€¦,n}). Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘n H [X ] = âˆ’ -1log 1- k=1n n âˆ‘n = 1logn k=1 n = log n. ](img/file2009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By now, we have an intuitive understanding of entropy as the average amount
    of information per observation. Take a wild guess: how does the entropy of the
    uniform distribution compare amongst all other distributions concentrated on {1,2,â€¦,n}?
    Is it above or below average? Is it perhaps minimal or maximal?'
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll reveal the answer by the end of this chapter, but take a minute to ponder
    this question before moving on to the next example.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. The single-point distribution. (See the definition of the single-point
    distribution in SectionÂ [19.2.5](ch031.xhtml#the-singlepoint-distribution)) Let
    X âˆ¼Î´(a). Then
  prefs: []
  type: TYPE_NORMAL
- en: '![H [X ] = âˆ’ 1 â‹…log 1 = 0\. ](img/file2010.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, as the event X = a is certain, no information is gained upon
    observing X. Now think back to the previous example. As X âˆ¼Î´(k) is concentrated
    on {1,2,â€¦,n} for all k = 1,2,â€¦,n, give the previous question one more thought.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s see a partial answer in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3\. The Bernoulli distribution. (See the definition of the Bernoulli
    distribution in SectionÂ [19.2.1](ch031.xhtml#the-bernoulli-distribution)). Let
    X âˆ¼ Bernoulli(p). Then, it is easy to see that
  prefs: []
  type: TYPE_NORMAL
- en: '![H [X ] = âˆ’ plogp âˆ’ (1âˆ’ p)log(1âˆ’ p). ](img/file2011.png)'
  prefs: []
  type: TYPE_IMG
- en: Which value of p maximizes the entropy? To find the maxima of H[X], we can turn
    to the derivatives. (Recall how the derivative and second derivative can be used
    for optimization, as claimed by TheoremÂ [87](ch021.xhtml#x1-214004r87).)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, let f(p) = H[X] = âˆ’plog p âˆ’ (1 âˆ’p)log(1 âˆ’p). Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![fâ€²(p) = âˆ’ logp + log(1âˆ’ p) = log 1-âˆ’-p, p fâ€²â€²(p) = âˆ’ 1-âˆ’--1--. p 1 âˆ’ p ](img/file2012.png)'
  prefs: []
  type: TYPE_IMG
- en: By solving f^â€²(p) = 0, we obtain that p = 1âˆ•2, which is the only potential extrema
    of f(p). As f^(â€²â€²)(1âˆ•2) = âˆ’4/span>0, we see that p = 1âˆ•2 is indeed a local maximum.
    Letâ€™s plot f(p) to obtain a visual confirmation as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file2013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.10: The entropy of the Bernoulli distribution'
  prefs: []
  type: TYPE_NORMAL
- en: For p = 1âˆ•2, that is, where the entropy of Bernoulli(p) is maximal, we have
    a uniform distribution on the two-element set {0,1}. On the other hand, for p
    = 0 or p = 1, where the entropy is minimal, Bernoulli(p) is a single-point distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As every random variable on {0,1} is Bernoulli-distributed, we seem to have
    a partial answer to our question: the uniform distribution maximizes entropy,
    while single-point ones minimize it.'
  prefs: []
  type: TYPE_NORMAL
- en: As the following theorem indicates, this is true in general as well.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 138\. (The uniform distribution and maximal entropy)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let E = {x[1],â€¦,x[n]}be a finite set, and let X : Î© â†’E be a random variable
    that assumes values in E. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: '![H [X] â‰¤ H [Uniform (E)], ](img/file2014.png)'
  prefs: []
  type: TYPE_IMG
- en: and H[X] = H[Uniform(E)] if and only if X is uniformly distributed on E.
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to show this here, but there are several proofs out there.
    For instance, Bishopâ€™s classic Pattern Recognition and Machine Learning uses the
    Lagrange multiplier method to explicitly find the maximum of the multivariable
    function f(p[1],â€¦,p[n]) = âˆ’âˆ‘ [k=1]^np[k] log p[k]; feel free to check it out for
    the details.
  prefs: []
  type: TYPE_NORMAL
- en: What if we donâ€™t restrict our discrete random variable to a finite set? In that
    case, the Shannon entropy has no upper limit. In the problem set of this chapter,
    youâ€™ll see that the entropy of the geometric distribution is
  prefs: []
  type: TYPE_NORMAL
- en: '![H [Geo (p)] = âˆ’ plogp-+-(1âˆ’-p)-log(1-âˆ’-p). p ](img/file2015.png)'
  prefs: []
  type: TYPE_IMG
- en: It is easy to see that lim[pâ†’0]H[Geo(p)] = âˆ. Letâ€™s plot this!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file2016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 20.11: The entropy of the geometric distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the Shannon entropy can assume any nonnegative value.
  prefs: []
  type: TYPE_NORMAL
- en: 20.6.4 Differential entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have only defined the entropy for discrete random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Does it translate to continuous ones as well? Yes. The formula ğ”¼[ âˆ’ log f[X](X)]
    can be directly applied for continuous random variables, yielding the so-called
    differential entropy. Here is the formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 98\. (Differential entropy)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a continuous random variable. The differential entropy of X is defined
    by the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ H [X ] := âˆ’ f (x )log f (x)dx, âˆ’âˆ X X ](img/file2017.png)'
  prefs: []
  type: TYPE_IMG
- en: where f[X] denotes the probability density function of X.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the surprise. Can we derive the formula from the Shannon entropy?
    We are going to approach the problem like we did when we defined the expected
    value for continuous random variables in SectionÂ [20.2](ch032.xhtml#continuous-random-variables):
    approximate the continuous random variable with a discrete one, then see where
    the Shannon entropy converges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, let X : Î© â†’â„ be a continuous random variable, and let [a,b] âŠ†â„ be a (large)
    interval, so large that P(X![âˆˆâˆ•](img/file2018.png)[a,b]) is extremely small. Weâ€™ll
    subdivide [a,b] into n equal parts by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ k(bâˆ’ a) xk = a+ -------, k = 0,1,...,n, n ](img/file2019.png)'
  prefs: []
  type: TYPE_IMG
- en: and define the approximating random variable X^((n)) by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( (n) |{ xk if x âˆˆ (xk âˆ’1,xk] for some k = 1,2,...,n, X (Ï‰ ) := | ( 0 otherwise.
    ](img/file2020.png)'
  prefs: []
  type: TYPE_IMG
- en: This way, the entropy of X^((n)) is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ (n) âˆ‘n (n) (n) H [X ] = âˆ’ P (X = xk)logP (X = xk). k=1 ](img/file2021.png)'
  prefs: []
  type: TYPE_IMG
- en: However, due to how we defined X^((n)),
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« (n) xk P(X = xk) = P(xkâˆ’ 1 <X â‰¤ Xk ) = x fX (x)dx, kâˆ’1 ](img/file2022.png)'
  prefs: []
  type: TYPE_IMG
- en: where f[X] is the density function of X. Now, the mean value theorem for definite
    integrals (TheoremÂ [93](ch022.xhtml#x1-235008r93)) gives that there is a Î¾[k]
    âˆˆ [x[kâˆ’1],x[k]] such that
  prefs: []
  type: TYPE_NORMAL
- en: '![âˆ« xk f (Î¾ ) fX (x)dx = (xk âˆ’ xkâˆ’ 1)fX (Î¾k) = -X---k-, xkâˆ’1 n ](img/file2023.png)'
  prefs: []
  type: TYPE_IMG
- en: thus, in conclusion,
  prefs: []
  type: TYPE_NORMAL
- en: '![ (n) fX-(Î¾k) P (X = xk ) = n . ](img/file2024.png)'
  prefs: []
  type: TYPE_IMG
- en: (Recall that as the partition x[0]/span>x[1]/span>â€¦/span>x[n] is equidistant,
    x[k] âˆ’x[kâˆ’1] = 1âˆ•n.)
  prefs: []
  type: TYPE_NORMAL
- en: Now, using P(X^((n)) = x[k]) = ![fX(Î¾k) n](img/file2025.png), we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘n H[X (n)] = âˆ’ P (X (n) = xk )log P(X (n) = xk) k=1 n = âˆ’ âˆ‘ fX-(Î¾k)-log
    fX-(Î¾k)- n n k=n1 n = âˆ’ âˆ‘ fX-(Î¾k)-log f (Î¾ )+ log nâˆ‘ fX(Î¾k). n X k n k=1 k=1 ](img/file2026.png)'
  prefs: []
  type: TYPE_IMG
- en: Both of these terms are Riemann-sums, approximating the integral of the functions
    inside. If n is large, and the interval [a,b] is big enough, then
  prefs: []
  type: TYPE_NORMAL
- en: '![ n âˆ« âˆ’ âˆ‘ fX-(Î¾k)logf (Î¾ ) â‰ˆ âˆ’ âˆ f (x)logf (x)dx = h[X ], n X k âˆ’âˆ X X k=1
    ](img/file2027.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![ n âˆ« âˆ âˆ‘ fX-(Î¾k)-â‰ˆ f (x)dx = 1, n âˆ’ âˆ X k=1 ](img/file2028.png)'
  prefs: []
  type: TYPE_IMG
- en: implying
  prefs: []
  type: TYPE_NORMAL
- en: '![H [X (n)] â‰ˆ h [X ]+ logn. ](img/file2029.png)'
  prefs: []
  type: TYPE_IMG
- en: This is quite surprising, as one would expect H[X^((n))] to converge towards
    h(X). This is not the case. In fact,
  prefs: []
  type: TYPE_NORMAL
- en: '![ lim (H [X (n)]âˆ’ log n) = h[X ] nâ†’ âˆ ](img/file2030.png)'
  prefs: []
  type: TYPE_IMG
- en: holds.
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s time for the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. The uniform distribution. (See the definition of the uniform distribution
    in SectionÂ [19.3.4](ch031.xhtml#the-uniform-distribution2).) Let X âˆ¼ Uniform(a,b).
    Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« b -1--- --1-- h[X ] = âˆ’ bâˆ’ a log bâˆ’ a dx a = log(bâˆ’ a), ](img/file2031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which is similar to the discrete uniform case. However, there is one notable
    difference: h(X) is negative when bâˆ’a/span>1\. This is in stark contrast with
    the Shannon entropy, which is always nonnegative.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. The normal distribution. (See the definition of the normal distribution
    in SectionÂ [19.3.6](ch031.xhtml#the-normal-distribution).) Let X âˆ¼ğ’©(Î¼,ÏƒÂ²). Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ 1 (xâˆ’Î¼)2 ( 1 (xâˆ’Î¼)2) h[X ] = âˆ’ -âˆš---eâˆ’ 2Ïƒ2 log -âˆš----eâˆ’ 2Ïƒ2 dx âˆ’ âˆ Ïƒ
    2Ï€âˆ« Ïƒ 2Ï€ âˆ« ( âˆš ---) âˆ --1---âˆ’ (xâˆ’2ÏƒÎ¼2)2- âˆ (xâˆ’-Î¼-)2---1---âˆ’ (xâˆ’2Î¼Ïƒ)22 = log Ïƒ
    2Ï€ âˆ’âˆ Ïƒâˆš 2Ï€e dx+ âˆ’ âˆ 2Ïƒ2 Ïƒâˆš 2Ï€e dx â—Ÿ--------â—â—œ--------â— â—Ÿ------------â—â—œ------------â—
    =1 = 21Ïƒ2Var[X ]= 12 1 ( 2 ) = 2- 1+ log(Ïƒ 2Ï€ ) . ](img/file2032.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the value of Ïƒ, the value of h[X] can be negative here as well.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we have seen that for discrete distributions on a given finite set,
    the uniform distribution maximizes entropy, as TheoremÂ [138](ch032.xhtml#x1-343023r138)
    claims.
  prefs: []
  type: TYPE_NORMAL
- en: What is the analogue of TheoremÂ [138](ch032.xhtml#x1-343023r138) for continuous
    distributions? Take a wild guess. If we let X be any continuous distribution,
    then, as we have seen,
  prefs: []
  type: TYPE_NORMAL
- en: '![h[Uniform (a,b)] = log(bâˆ’ a), ](img/file2033.png)'
  prefs: []
  type: TYPE_IMG
- en: which can assume any real number. Similarly to the discrete case, we have to
    make restrictions; this time, weâ€™ll fix the variance. Here is the result.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 139\. (Maximizing the differential entropy)
  prefs: []
  type: TYPE_NORMAL
- en: Let X be a continuous random variable with variance ÏƒÂ². Then
  prefs: []
  type: TYPE_NORMAL
- en: '![h[X ] â‰¤ h [ğ’© (0,Ïƒ2)], ](img/file2034.png)'
  prefs: []
  type: TYPE_IMG
- en: and h[X] = h[ğ’©(0,ÏƒÂ²)] if and only if X âˆ¼ğ’©(Î¼,ÏƒÂ²).
  prefs: []
  type: TYPE_NORMAL
- en: Again, we are not going to prove this. You can check Bishopâ€™s Pattern Recognition
    and Machine Learning for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 20.7 The Maximum Likelihood Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am an evangelist for simple ideas. Stop me any time you want, but whichever
    field I was in, Iâ€™ve always been able to find a small set of mind-numbingly simple
    ideas making the entire shebang work. (Not that you could interrupt me, as this
    is a book. Jokeâ€™s on you!)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me give you a concrete example thatâ€™s on my mind. What do you think enabled
    the rise of deep learning, including neural networks with billions of parameters?
    Three ideas as simple as ABC:'
  prefs: []
  type: TYPE_NORMAL
- en: that you can optimize the loss function by going against its gradient (no matter
    the number of parameters),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: that you can efficiently compute the gradient with a clever application of the
    chain rule and matrix multiplication,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and that we can perform matrix operations blazingly fast on a GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sure, thereâ€™s a great tower of work built upon these ideas, but these three
    lie at the very foundation of machine learning today. Ultimately, these enable
    you to converse with large language models. To have your car cruise around town
    while you read the newspaper. To predict the exact shape of massive amino-acid
    chains called proteins, responsible for building up every living thing. (Including
    you.)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent, backpropagation, and high-performance linear algebra are on
    the practical side of the metaphorical machine learning coin. If we conjure up
    a parametric model, we can throw some extremely powerful tools at it.
  prefs: []
  type: TYPE_NORMAL
- en: But where do our models come from?
  prefs: []
  type: TYPE_NORMAL
- en: 'As Iâ€™ve said, there is a small set of key ideas that go a long way. We are
    about to meet one: the maximum likelihood estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: 20.7.1 Probabilistic modeling 101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a self-proclaimed evangelist of simple ideas, Iâ€™ll start with a simple example
    to illustrate a simple idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick up a coin and toss it a few times, recording each outcome. The question
    is, once more, simple: whatâ€™s the probability of heads? We canâ€™t just immediately
    assume p = 1âˆ•2, that is, a fair coin. For instance, one side of our coin could
    be coated with lead, resulting in a bias. To find out, letâ€™s perform some statistics.
    (Rolling up my sleeves, throwing down my gloves.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically speaking, we can model coin tosses with the Bernoulli distribution
    (SectionÂ [19.2.1](ch031.xhtml#the-bernoulli-distribution)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![P(X = 1) = p, P(X = 0) = 1 âˆ’ p, ](img/file2035.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: X is the random variable representing the outcome of a single toss,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X = 1 for heads and X = 0 for tails,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and p âˆˆ [0,1] is the probability of heads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thatâ€™s just the model. Weâ€™re here to estimate the parameter p, and this is what
    we have statistics for.
  prefs: []
  type: TYPE_NORMAL
- en: Tossing up the coin n times yields the zero-one sequence x[1],x[2],â€¦,x[n], where
    each x[i] is a realization of a Bernoulli-distributed random variable X[i] âˆ¼ Bernoulli(p),
    independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw previously when discussing the law of large numbers (TheoremÂ [137](ch032.xhtml#x1-339002r137)),
    one natural idea is to compute the sample mean to estimate p, which is coincidentally
    the expected value of X. To move beyond empirical estimates, letâ€™s leverage that,
    this time, we have a probabilistic model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key question is this: which parameter p is the most likely to produce our
    sample?'
  prefs: []
  type: TYPE_NORMAL
- en: In the language of probability, this question is answered by maximizing the
    likelihood function
  prefs: []
  type: TYPE_NORMAL
- en: '![ n âˆ LLH (p;x1,...,xn) = P(Xi = xi | p), i=1 ](img/file2036.png)'
  prefs: []
  type: TYPE_IMG
- en: where P(X[i] = x[i]âˆ£p) represents the probability of observing x[i] given a
    fixed parameter p. The larger the LLH(p;x[1],â€¦,x[n]), the more likely the parameter
    p is. In other words, our estimate of p is going to be
  prefs: []
  type: TYPE_NORMAL
- en: '![Ë†p = argmaxp âˆˆ[0,1]LLH (p;x1,...,xn). ](img/file2037.png)'
  prefs: []
  type: TYPE_IMG
- en: Letâ€™s find it.
  prefs: []
  type: TYPE_NORMAL
- en: In our concrete case, P(X[i] = x[i]âˆ£p) can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ (| {p if xi = 1, P(Xi = xi | p) = | (1 âˆ’ p if xi = 0\. ](img/file2038.png)'
  prefs: []
  type: TYPE_IMG
- en: Algebra doesnâ€™t welcome if-else type functions, so with a clever mathematical
    trick, we write P(X[i] = x[i]âˆ£p) as
  prefs: []
  type: TYPE_NORMAL
- en: '![ x 1âˆ’x P(Xi = xi | p) = p i(1âˆ’ p ) i, ](img/file2039.png)'
  prefs: []
  type: TYPE_IMG
- en: making the likelihood function be
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn LLH (p;x1,...,xn) = pxi(1 âˆ’ p)1âˆ’xi. i=1 ](img/file2040.png)'
  prefs: []
  type: TYPE_IMG
- en: (Weâ€™ll often write LLH(p) to minimize notational complexity.)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is still not easy to optimize, as it is composed of the product of exponential
    functions. So, hereâ€™s another mathematical trick: take the logarithm to turn the
    product into a sum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the logarithm is increasing, it wonâ€™t change the optima, so weâ€™re good to
    go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn log LLH (p) = log pxi(1âˆ’ p )1âˆ’xi i=1 âˆ‘n [ ] = log pxi(1 âˆ’ p)1âˆ’xi i=1
    âˆ‘n [ ] = logpxi + log(1âˆ’ p)1âˆ’xi i=1 âˆ‘n âˆ‘n = logp xi + log(1 âˆ’ p) (1âˆ’ xi). i=1
    i=1 ](img/file2041.png)'
  prefs: []
  type: TYPE_IMG
- en: Trust me, this is much better. According to the second derivative test (TheoremÂ [87](ch021.xhtml#x1-214004r87)),
    we can find the maxima by
  prefs: []
  type: TYPE_NORMAL
- en: solving ![ddp](img/file2042.png) log LLH(p) = 0 to find the critical point pÌ‚
    ,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: then showing that pÌ‚ is a maximum because ![-d2- dp2](img/file2045.png) log
    LLH(p)/span>0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Letâ€™s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: As ![ddp](img/file2046.png) log p = ![1p](img/file2047.png) and ![ddp-](img/file2048.png)
    log(1 âˆ’p) = âˆ’![11âˆ’p](img/file2049.png), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![d 1 âˆ‘n 1 âˆ‘n --logLLH (p;x1,...,xn) = -- xi âˆ’ ----- (1âˆ’ xi). dp p i=1 1âˆ’ p
    i=1 ](img/file2050.png)'
  prefs: []
  type: TYPE_IMG
- en: Solving ![ddp](img/file2051.png) log LLH(p;x[1],â€¦,x[n]) = 0 yields a single
    solution
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 âˆ‘n Ë†p = -- xi. n i=1 ](img/file2052.png)'
  prefs: []
  type: TYPE_IMG
- en: (Pick up a pen and paper and calculate the solution yourself.) Regarding the
    second derivative, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![d2 1 âˆ‘n 1 âˆ‘n --2 logLLH (p) = âˆ’ -2 xi âˆ’-------2 (1âˆ’ xi), dp p i=1 (1 âˆ’ p)
    i=1 ](img/file2053.png)'
  prefs: []
  type: TYPE_IMG
- en: which is uniformly negative. Thus, pÌ‚ = ![1n](img/file2055.png) âˆ‘ [i=1]^nx[i]
    is indeed a (local) maximum. Yay!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the maximum likelihood estimate is identical to the sample mean.
    Trust me, this is one of the rare exceptions. Think of it as validating the sample
    mean: weâ€™ve obtained the same estimate through different trains of thought, so
    it must be good.'
  prefs: []
  type: TYPE_NORMAL
- en: 20.7.2 Modeling heights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Letâ€™s continue with another example. The coin-tossing example demonstrated the
    discrete case. Itâ€™s time to move into the continuous domain!
  prefs: []
  type: TYPE_NORMAL
- en: This time, we are measuring the heights of a high school class, and we want
    to build a probabilistic model of it. A natural idea is to assume the heights
    to come from a normal distribution X âˆ¼ğ’©(Î¼,ÏƒÂ²). (Check SectionÂ [19.3.6](ch031.xhtml#the-normal-distribution)
    for the normal distribution.)
  prefs: []
  type: TYPE_NORMAL
- en: Our job is to estimate the expected value Î¼ and the variance ÏƒÂ². Letâ€™s go, maximum
    likelihood!
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the problem mathematically precise, we have the measurements x[1],â€¦,x[n],
    coming from independent and identically distributed random variables X[i] âˆ¼ğ’©(Î¼,ÏƒÂ²).
    However, thereâ€™s a snag: as our random variables are continuous,'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn P (X1 = x1,...,Xn = xn | Î¼, Ïƒ2) = P (Xi = xi | Î¼, Ïƒ2) = 0\. i=1 ](img/file2056.png)'
  prefs: []
  type: TYPE_IMG
- en: '(As all terms of the product are zero.) How can we define the likelihood function,
    then? No worries: even though we donâ€™t have a mass function, we have density!
    Thus, the likelihood function defined by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn LLH (Î¼, Ïƒ;x1,...,xn) = fX (xi) i=1 i n 2 = âˆ -âˆš1--eâˆ’ (xi2âˆ’ÏƒÎ¼2), i=1 Ïƒ
    2Ï€ ](img/file2057.png)'
  prefs: []
  type: TYPE_IMG
- en: where f[X[i]](x) is the probability density function of X[i].
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s maximize it. The idea is similar: take the logarithm, find the critical
    points, then use the second derivative test. Here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 1 âˆ‘n 2 log LLH (Î¼,Ïƒ) = nlog -âˆš----âˆ’ Ïƒ2- (xi âˆ’ Î¼) . Ïƒ 2Ï€ i=1 ](img/file2058.png)'
  prefs: []
  type: TYPE_IMG
- en: Just the usual business from now on. The derivatives are given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘n âˆ‚--log LLH (Î¼,Ïƒ) = -2- (xi âˆ’ Î¼), âˆ‚Î¼ Ïƒ2 i=1 âˆ‘n âˆ‚--log LLH (Î¼,Ïƒ) = âˆ’ n-âˆ’
    -2- (xi âˆ’ Î¼)2\. âˆ‚Ïƒ Ïƒ Ïƒ3 i=1 ](img/file2059.png)'
  prefs: []
  type: TYPE_IMG
- en: With a bit of number-crunching (that you should attempt to carry out by yourself),
    we get that âˆ‚Î¼ log LLH(Î¼,Ïƒ) = 0 implies
  prefs: []
  type: TYPE_NORMAL
- en: '![ -1âˆ‘n Î¼ = n xi, i=1 ](img/file2061.png)'
  prefs: []
  type: TYPE_IMG
- en: and ![-âˆ‚ âˆ‚Ïƒ](img/file2062.png) log LLH(Î¼,Ïƒ) implies
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘n Ïƒ = -1 (xi âˆ’ Î¼ )2\. n i=1 ](img/file2063.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We wonâ€™t do the second derivative test here, but trust me: itâ€™s a maximum,
    leaving us with the estimates'
  prefs: []
  type: TYPE_NORMAL
- en: '![ n Ë†Î¼ = -1âˆ‘ x, n i=1 i n Ë†Ïƒ = -1âˆ‘ (x âˆ’ Î¼Ë†)2\. n i i=1 ](img/file2064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, the sample mean and variance. Think of it this way: defaulting to the
    sample mean and variance is the simplest thing to do, yet even clever methods
    like the maximum likelihood estimation yield them as parameter estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: After working out the above two examples in detail, we are ready to abstract
    away the details and introduce the general problem.
  prefs: []
  type: TYPE_NORMAL
- en: 20.7.3 The general method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Weâ€™ve seen how maximum likelihood estimation works. Now, itâ€™s time to construct
    the abstract mathematical framework.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 99\. (The likelihood function)
  prefs: []
  type: TYPE_NORMAL
- en: Let P[ğœƒ] be a probability distribution parametrized by the parameter ğœƒ âˆˆâ„^k,
    and let x[1],â€¦,x[n] âˆˆ â„^d be an independent realization of the probability distribution.
    (That is, the samples are coming from independent and identically distributed
    random variables X[1],â€¦,X[n], distributed according to P[ğœƒ].)
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood function of ğœƒ given the sample x[1],â€¦,x[n] is defined by
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn LLH (ğœƒ;x1,...,xn) := P ğœƒ(Xi = xi) i=1 ](img/file2065.png)'
  prefs: []
  type: TYPE_IMG
- en: if P[ğœƒ] is discrete, and
  prefs: []
  type: TYPE_NORMAL
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn LLH (ğœƒ;x1,...,xn) := fğœƒ(xi) i=1 ](img/file2066.png)'
  prefs: []
  type: TYPE_IMG
- en: if P[ğœƒ] is continuous, where f[ğœƒ] is the probability density function of P[ğœƒ].
  prefs: []
  type: TYPE_NORMAL
- en: 'Weâ€™ve already seen two examples of the likelihood function: for the Bernoulli-distribution
    Bernoulli(p), given by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn LLH (p;x1,...,xn) = pxi(1 âˆ’ p)1âˆ’xi, i=1 ](img/file2067.png)'
  prefs: []
  type: TYPE_IMG
- en: and for the normal distribution ğ’©(Î¼,Ïƒ), given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n 2 LLH (Î¼, Ïƒ;x ,...,x ) = âˆ -âˆš1--eâˆ’ (xi2âˆ’ÏƒÎ¼2). 1 n i=1 Ïƒ 2Ï€ ](img/file2068.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, the likelihood function LLH(ğœƒ;x[1],â€¦,x[n]) expresses the probability
    of our observation x[1],â€¦,x[n] if the parameter ğœƒ is indeed true. The maximum
    likelihood estimate is the parameter ![Ë†ğœƒ](img/file2069.png) that maximizes this
    probability; that is, under which the observation is the most likely.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 100\. (The maximum likelihood estimate)
  prefs: []
  type: TYPE_NORMAL
- en: Let P[ğœƒ] be a probability distribution parametrized by the parameter ğœƒ âˆˆâ„^k,
    and let x[1],â€¦,x[n] âˆˆ â„^d be an independent realization of the probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The maximum likelihood estimate of ğœƒ is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![Ë†ğœƒ = argmax ğœƒâˆˆâ„kLLH (ğœƒ;x1,...,xn ). ](img/file2070.png)'
  prefs: []
  type: TYPE_IMG
- en: In both examples, we used the logarithm to turn the product into a sum. Use
    it once and itâ€™s a trick; use it (at least) twice and itâ€™s a method. Hereâ€™s the
    formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 101\. (The log-likelihood function)
  prefs: []
  type: TYPE_NORMAL
- en: Let P[ğœƒ] be a probability distribution parametrized by the parameter ğœƒ âˆˆâ„^k,
    and let x[1],â€¦,x[n] âˆˆ â„^d be an independent realization of the probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The log-likelihood function of ğœƒ given the sample x[1],â€¦,x[n] is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![logLLH (ğœƒ;x1,...,xn), ](img/file2071.png)'
  prefs: []
  type: TYPE_IMG
- en: where LLH(ğœƒ;x[1],â€¦,x[n]) is the likelihood function.
  prefs: []
  type: TYPE_NORMAL
- en: In a classical statistical setting, the maximum likelihood estimation is done
    via
  prefs: []
  type: TYPE_NORMAL
- en: pulling a parametric probabilistic model out from the mathematicianâ€™s hat,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: massaging the (log-)likelihood function until we obtain an analytically manageable
    form,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and solving âˆ‡LLH = 0 (or âˆ‡log LLH = 0) to obtain the parameter estimate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics can be extremely powerful under specific circumstances, but letâ€™s
    face it: the above method has quite a few weaknesses. First, constructing a tractable
    probabilistic model is a challenging task, burdened by the expertsâ€™ inherent bias.
    (Itâ€™s no accident that I indirectly compared the modeling process to pulling a
    rabbit out of a hat.) Moreover, the more complex the model, the more complex the
    likelihood function is. Which, in turn, increases the complexity of our optimization
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Why did we spend quite a few pages learning this ancient technique, then?
  prefs: []
  type: TYPE_NORMAL
- en: Because its idea is fundamental in machine learning, weâ€™ll arrive at (somewhere
    near the) state of the art by breaking down its barriers one by one. Is modeling
    hard? Letâ€™s construct a function with BILLIONS of parameters thatâ€™ll do the job.
    Is optimization computationally intensive? Fear not. We have clusters of GPUs
    at our disposal.
  prefs: []
  type: TYPE_NORMAL
- en: 20.7.4 The German tank problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One more example before finishing up, straight from World War II. Imagine you
    are an Allied intelligence officer tasked to estimate the size of a German armored
    division. (That is, to guess the number of tanks.)
  prefs: []
  type: TYPE_NORMAL
- en: 'There was no satellite imagery back in the day, so thereâ€™s only a little to
    go on, except for a tiny piece of information: the serial numbers of the enemyâ€™s
    destroyed tanks. What can we do with these?'
  prefs: []
  type: TYPE_NORMAL
- en: Without detailed knowledge of the manufacturing process, we can assume that
    the tanks are labeled sequentially as they roll out from the factory. We also
    donâ€™t know how the tanks are distributed between the battlefields.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two pieces of knowledge (or lack of knowledge, to be more precise) translate
    to a simple probabilistic model: encountering an enemy tank is the same as drawing
    from the distribution Uniform(N), where N is the total number of tanks. Thus,
    if x[1],â€¦,x[n] are the serial numbers of destroyed tanks, we can use the maximum
    likelihood method to estimate N.'
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s do it. The likelihood function for the discrete uniform distribution Uniform(N)
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆn LLH (N ) = P (Xi = xi), i=1 ](img/file2072.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where the probability P(X[i] = x[i]) has a quite peculiar form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ -1 P (X = x ) = N if xi âˆˆ {1,...,N }, i i |( 0 otherwise. ](img/file2073.png)'
  prefs: []
  type: TYPE_IMG
- en: Keeping in mind that x[1],â€¦,x[n] â‰¤N (as no observed serial number can be larger
    than the total number of tanks), we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( |{ -1- LLH (N ) = Nn if N <max {x1,...,xn}, |( 0 otherwise. ](img/file2074.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Ponder on this a minute: the larger the N, the smaller the LLH(N) is. Thus,
    the maximum likelihood estimate is the smallest possible choice'
  prefs: []
  type: TYPE_NORMAL
- en: '![NË†= max {x1,...,xn }. ](img/file2075.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, our guess about the number of tanks is the largest serial number
    weâ€™ve encountered.
  prefs: []
  type: TYPE_NORMAL
- en: What do you think about this estimate? I wonâ€™t lie; I am not a big fan. The
    German tank problem highlights the importance of modeling assumptions in statistics.
    The final estimate ![Ë†N](img/file2076.png) is the outcome of our choice of Uniform(N).
    Common wisdom in machine learning is â€œgarbage in, garbage out.â€ It is true for
    modeling as well.
  prefs: []
  type: TYPE_NORMAL
- en: 20.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the concept of the expected value. Mathematically
    speaking, the expected value is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ‘ ğ”¼[X ] = xkP (X = xk) k ](img/file2077.png)'
  prefs: []
  type: TYPE_IMG
- en: for discrete random variables and
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« âˆ ğ”¼[X ] = xfX(x)dx âˆ’âˆ ](img/file2078.png)'
  prefs: []
  type: TYPE_IMG
- en: 'for continuous ones. Although these formulas involve possibly infinite sums
    and integrals, the underlying meaning is simple: ğ”¼[X] represents the average outcome
    of X, weighted by the underlying probability distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the law of large numbers, the expected value also describes a
    long-term average: if the independent and identically distributed random variables
    X[1],X[2],â€¦ describe the outcomes of a repeated experiment â€” say, betting a hand
    in poker â€” then the sample average converges to the joint expected value, that
    is,'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 âˆ‘n lim -- Xi = ğ”¼[X1 ] nâ†’ âˆ n i=1 ](img/file2079.png)'
  prefs: []
  type: TYPE_IMG
- en: holds with probability 1\. In a sense, the law of large numbers allows you to
    glimpse into the future and see what happens if you make the same choice. In the
    case of poker, if you only make bets with a positive expected value, youâ€™ll win
    in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, the LLN also plays an essential role. Check out the mean-squared
    error
  prefs: []
  type: TYPE_NORMAL
- en: '![ -1âˆ‘n 2 n MSE (x,y ) = n (f(xi)âˆ’ yi) , x,y âˆˆ â„ i=1 ](img/file2080.png)'
  prefs: []
  type: TYPE_IMG
- en: once more. If the number of samples (n) is in the millions, computing the gradient
    of this sum is not feasible. However, the mean-squared error is the sample average
    of the prediction errors; thus, itâ€™s enough to sample a smaller amount. This is
    the core principle behind stochastic gradients, an idea that makes machine learning
    on a large scale feasible.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this chapter, our journey comes to a close. Still, thereâ€™s so much to
    learn; I could probably write this book until the end of time. Sadly, we have
    to stop somewhere. Now, instead of giving a summary of all thatâ€™s in the book,
    letâ€™s talk about the most important message: learning never ends.'
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s a spiral that you continue to ascend, meeting familiar landscapes from
    higher and higher vantage points. If you keep going, youâ€™ll know what Iâ€™m talking
    about.
  prefs: []
  type: TYPE_NORMAL
- en: If you lead an intellectually challenging life, youâ€™ll also find that knowledge
    is like keeping a dozen leaky cups full of water. If your focus shifts from one,
    itâ€™ll empty faster than you think. In other words, youâ€™ll lose it if you donâ€™t
    use it. This is completely normal. The good news is, if you already have a good
    foundation, refilling the cup can be done quickly. Sometimes, simply glancing
    at a page from a book you read long ago can do the trick.
  prefs: []
  type: TYPE_NORMAL
- en: This is how I know that itâ€™s not goodbye. If you have found the book useful
    and continue down the rabbit hole that is machine learning, weâ€™ll meet again with
    probability one. You just have to keep going.
  prefs: []
  type: TYPE_NORMAL
- en: 20.9 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Problem 1\. Let X,Y : Î© â†’â„ be two random variables.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Show that if X â‰¥ 0, then ğ”¼[X] â‰¥ 0.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that if X â‰¥Y , then ğ”¼[X] â‰¥ğ”¼[Y ].
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 2\. Let X : Î© â†’â„ be a random variable. Show that if Var[X] = 0, then
    X assumes only a single value. (That is, the set X(Î©) = {X(Ï‰) : Ï‰ âˆˆ Î©} has only
    a single element.)'
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Let X âˆ¼ Geo(p) be a geometrically distributed (SectionÂ [19.2.3](ch031.xhtml#the-geometric-distribution))
    discrete random variable. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![H [X] = âˆ’ plogp-+-(1âˆ’-p)log(1âˆ’-p)-. p ](img/file2081.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hint: Use that for any q âˆˆ (0,1), âˆ‘ [k=1]^âˆkq^(kâˆ’1) = (1 âˆ’q)^(âˆ’2).'
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let X âˆ¼ exp(Î») be an exponentially distributed continuous random
    variable. Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![h [X ] = 1 âˆ’ logÎ». ](img/file2082.png)'
  prefs: []
  type: TYPE_IMG
- en: Problem 5\. Find the maximum likelihood estimation for the Î» parameter of the
    exponential distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
