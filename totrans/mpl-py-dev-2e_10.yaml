- en: Integrating Data Visualization into the Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now come to the concluding chapter of this book. Throughout the course
    of this book, you have mastered the techniques to create and customize static
    and animated plots using real-world data in different formats scraped from the
    web. To wrap up, we will start a mini-project in this chapter to combine the skills
    of data analytics with the visualization techniques you've learned. We will demonstrate
    how to integrate visualization techniques in your current workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the era of big data, machine learning becomes fundamental to ease analytic
    work by replacing huge amounts of manual curation with automatic prediction. Yet,
    before we enter model building, **Exploratory Data Analysis** (**EDA**) is always
    essential to get a good grasp of what the data is like. Constant review during
    the optimization process also helps improve our training strategy and results.
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional data typically requires special processing techniques to be
    visualized intuitively. Statistical methods such as **Principle Component Analysis**
    (**PCA**) and **t-Distributed Stochastic Neighbor Embedding** (**t-SNE**) are
    important skills in reducing the dimension of data for effective visualization.
  prefs: []
  type: TYPE_NORMAL
- en: As a showcase, we will demonstrate the use of various visualization techniques
    in a workflow involving recognizing handwritten digits using a **Convolutional
    Neural Network** (**CNN**).
  prefs: []
  type: TYPE_NORMAL
- en: One important note is that we do not intend to illustrate all the mathematics
    and machine learning approaches in detail in this chapter. Our goal is to visualize
    some of the processes in between. Hopefully, readers will appreciate the importance
    of exploring processes such as the `loss` function when training a CNN, or visualizing
    the dimension reduction results with different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall the MNIST dataset we briefly touched upon in [Chapter 04](456b0dc2-84d5-40f9-bf63-1ab4635cbac8.xhtml), *Advanced
    Matplotlib*. It contains 70,000 images of handwritten digits, often used in data
    mining tutorials as *Machine Learning 101*. We will continue using a similar image
    dataset of handwritten digits for our project in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We are almost certain that you had already heard about the popular keywords—deep
    learning or machine learning in general—before starting with this course. That's
    why we are choosing it as our showcase. As detailed concepts in machine learning,
    such as **hyperparameter tuning** to optimize performance, are beyond the scope
    of this book, we will not go into them. But we will cover the model training part
    in a cookbook style. We will focus on how visualization helps our workflow. For
    those of you interested in the details of machine learning, we recommend exploring
    further resources that are largely available online.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing sample images from the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data cleaning and EDA are indispensable components of data science. Before we
    begin analyzing our data, it is important to understand some basic properties
    of what we have input. The dataset we are using comprises standardized images
    with regular shapes and normalized pixel values. The features are simple, thin
    lines. Our goal is straightforward as well, to recognize digits from images. Yet,
    in many cases of real-world practice, the problems can be more complicated; the
    data we collect is going to be raw and often much more heterogeneous. Before tackling
    the problem, it is usually worth the time to sample a small amount of input data
    for inspection. Imagine training a model to recognize Ramen just to get you drooling
    ;). You will probably take a look at some images to decide what features make
    a good input sample to exemplify the presence of the bowl. Besides the initial
    preparatory phase, during model building taking out some of the mislabeled samples
    to examine may also help us devise strategies for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In case you wonder where the Ramen idea comes from, a data scientist named Kenji
    Doi created a model to recognize in which restaurant branch a bowl of Ramen was made.
    You may read more on the Google Cloud Big Data and Machine Learning Blog post
    on [https://cloud.google.com/blog/big-data/2018/03/automl-vision-in-action-from-ramen-to-branded-goods](https://cloud.google.com/blog/big-data/2018/03/automl-vision-in-action-from-ramen-to-branded-goods).
  prefs: []
  type: TYPE_NORMAL
- en: Importing the UCI ML handwritten digits dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we will be using the MNIST dataset as in [Chapter 04](https://cdp.packtpub.com/matplotlib_for_python_developers__second_edition/wp-admin/post.php?post=338&action=edit#post_73), *Advanced
    Matplotlib* (since we will be demonstrating visualization along with model building
    in machine learning), we will take a shortcut to speed up the training process.
    Instead of using the 60,000 images with 28x28 pixels each, we will import another
    similar dataset of 8x8-pixel images from the scikit-learn package.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is obtained from the University of California, Irvine Machine Learning
    Repository, found at [http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits](http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits).
    It is a preprocessed version of images of digits written by 43 people, with 30
    and 13 contributing to the training and testing set respectively. The preprocessing
    method is described in M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick,
    J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint
    Recognition System, NISTIR 5469, 1994.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to import the dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To begin, let''s store our dataset into a variable. We will call it `digits`
    and reuse it throughout the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what is loaded by the `load_digits()` function by printing out the
    variable `digits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The type of `digits` is `<class 'sklearn.utils.Bunch'>`, which is specific to
    loading sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the output for `print(digits)` is somewhat long, we will show its beginning
    and end in two screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/913f538a-09d1-4fb8-99c3-78856b08da5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the tail of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60f31ffd-78ae-406b-837e-c8148acb223c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that there are five members within the class `digits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''data''`: Pixel values flattened into 1D NumPy arrays'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''target''`: A NumPy array of identity labels of each element in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''target_names''`: A list of unique labels existing in the dataset—integers
    0-9 in this case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''images''`: Pixel values reshaped into 2D NumPy arrays in the dimension of
    images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''DESCR''`: Description of the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides having smaller image dimensions than MNIST, this dataset also has far
    fewer images. So, how many are there in total? We get the dimensions of a NumPy
    array in a tuple of dimensions by `nd.shape`, where `nd` is the array. Hence,
    to inquire about the shape of `digits.image`, we call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We get `(1797, 8, 8)` as result.
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder why the number is so peculiar. If you have particularly sharp
    eyes, you might have seen that there are 5,620 instances in the description. In
    fact, the description is retrieved from the archive web page. The data we have
    loaded is actually the testing portion of the full dataset. You may also download
    the plain text equivalent from [http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes](http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in getting the full MNIST dataset, scikit-learn also
    offers an API to fetch it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from sklearn.datasets import fetch_mldata mnist = fetch_mldata(''MNIST original'',
    data_home=custom_data_home)`'
  prefs: []
  type: TYPE_NORMAL
- en: Plotting sample images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know more about the background of our input data, let's plot out
    some sample images.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting one sample each of digits 0-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get a feel for what the images look like, we want to extract one image of
    each digit for inspection. For this, we need to find out where the digits are.
    We can do so by tracking down the right indices (the data labels) with `digits.target` then
    we write a few lines of code to call `10` images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]` is returned as output, which
    means the first `10` samples happen to be in numerical order. Is it by chance
    or is the data ordered? We need to check as the sample distribution can impact
    our training performance.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the randomness of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because showing all 1,797 data points will make the plot too dense for any
    meaningful interpretation, we will plot the first `200` data points to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we get a scatter plot of the sample distribution. Not quite random, is
    it? The 0-9 digits are ordered and repeated three times. We also see a repetition
    of patterns from around the *125^(th)* sample. The structure of the data hints
    at randomization before our training of machine learning model later. For now,
    we will first take it as-is and continue with our image inspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/276c26f9-b086-4787-b2b8-ea51b1116162.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting the 10 digits in subplots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will align the images in a grid of two rows and five columns. We first set
    the rectangular canvas by `plt.figure()`. For each sample image of a digit, we
    define the axes with the `plt.subplot()` function, and then call `imshow()` to
    show the arrays of color values as images. Recall that the colormap `''gray_r''`
    plots values in grayscale from white to black with values from zero to maximum.
    As there is no need for *x* and *y* tick labels to show the scales, we will remove
    them by passing an empty list to `plt.xticks()` and `plt.yticks()` to remove the
    clutter. Here is the code snippet to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the following figure, the images are a bit blurry to human
    eyes. But, believe it or not, there are enough details for the algorithm to extract
    features and differentiate between each digit. We will observe this together along
    with our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d98d2ebf-0141-4136-b077-9844f371e7b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring the data nature by the t-SNE method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After visualizing a few images and glimpsing of how the samples are distributed,
    we will go deeper into our EDA.
  prefs: []
  type: TYPE_NORMAL
- en: Each pixel comes with an intensity value, which makes 64 variables for each
    8x8 image. The human brain is not good at intuitively perceiving dimensions higher
    than three. For high-dimensional data, we need more effective visual aids.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction methods, such as the commonly used PCA and t-SNE, reduce
    the number of input variables under consideration, while retaining most of the
    useful information. As a result, the visualization of data becomes more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will focus our discussion on the t-SNE method by
    using the scikit-learn library in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding t-Distributed stochastic neighbor embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The t-SNE method was proposed by van der Maaten and Hinton in 2008 in the publication
    *Visualizing Data using t-SNE*. It is a nonlinear dimension reduction method that
    aims to effectively visualize high-dimensional data. t-SNE is based on probability
    distributions with random walk on neighborhood graphs to find the structure within
    the data. The mathematical details of t-SNE are beyond the scope of this book,
    and readers are advised to read the paper for more details.
  prefs: []
  type: TYPE_NORMAL
- en: In short, t-SNE is a way to capture non-linear relationships in a high-dimensional
    data. This is particularly useful when we are trying to extract features from
    a high-dimensional matrix such as image processing, biological data, and network
    information. It enables us to reduce high-dimensional data to two or three dimensions;
    one interesting feature of t-SNE is that it is stochastic, indicating that the
    final results it shows each time will be different, but still they are all equally
    correct. Therefore, in order to get the best performance in t-SNE dimension reduction,
    it is advisable to first perform PCA dimension reduction on the big dataset, and
    then incorporate the PCA dimensions into t-SNE for subsequent dimension reduction.
    Thus, you get more consistent and replicable results.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the t-SNE method from scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will implement the t-SNE method by loading the `TSNE` function from scikit-learn,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few hypervariables that the user has to set upon running t-SNE,
    which include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''init''` : Initialization of embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''method''`: `barnes_hut` or exact'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''perplexity''`: Default `30`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''n_iter''`: Default `1000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''n_components''`: Default `2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going into the mathematical details of individual hypervariables would be a
    chapter on its own, but we do have suggestions on what the parameters should be
    in general. For `init`, it is recommended to use `'pca'` with the reason given
    before. For method, `barnes_hut` will be faster and gives very similar results
    if the provided dataset is not highly similar intrinsically. For perplexity, it
    reflects on the focus in teasing out local and global substructures of the data.
    `n_iter` indicates the number of iterations that you will run through the algorithm,
    and `n_components = 2` indicates that the final outcome is a two-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: To track the time use for rounds of experiments, we can use the cell magic `%%timeit`
    in the Jupyter notebook to track the time needed for a cell to run.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing a t-SNE plot for our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first reorder the data points according to the handwritten numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`y` will become `array([0, 0, 0, ..., 9, 9, 9])`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the t-SNE transformation can take minutes to compute on a regular
    laptop, and the `tSNE` command can be simply run as follows. We will first try
    running t-SNE with `250` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s draw a scatter plot to see how the data cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the clusters are not well separated at `250` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d7d2b79-8a06-4de2-b6ac-6eb559ade1c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now try running with `2000` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen from the following screenshot, the samples appear as `10` distinct
    blots of clusters. By running `2000` iterations, we have obtained far more satisfying
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5ae44f2-a6cf-4e9f-ad15-0c5598b077d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a CNN to recognize digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following section, we will use Keras. Keras is a Python library for neural
    networks and provides a high-level interface to TensorFlow libraries. We do not
    intend to give a complete tutorial on Keras or CNN, but we want to show how we
    can use Matplotlib to visualize the `loss` function, `accuracy`, and outliers
    of the results.
  prefs: []
  type: TYPE_NORMAL
- en: Readers who are not familiar with machine learning should be able to go through
    the logic of the remaining chapter and hopefully understand why visualizing the
    `loss` function, `accuracy`, and outliers of the results is important in fine-tuning
    the CNN model.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a snippet of code for the CNN; the most important part is the evaluation
    section after this!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating prediction results with visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have specified the callbacks that store the loss and accuracy information
    for each epoch to be saved as the variable `history`. We can retrieve this data
    from the dictionary `history.history`. Let''s check out the dictionary `keys`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will output `dict_keys(['loss', 'acc'])`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will plot out the `loss` function and `accuracy` along epochs in line
    graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon training, we can say the `loss` function is decreasing, accompanied by
    an increase in accuracy, which is something we are delighted to see. Here is the
    first graph showing the `loss` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f14a248-9cf2-47c6-ae6f-b4077351bcbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next graph shows the changes in **Accuracy** across **Epoch**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65bc06af-bb45-435a-a020-0dfaca639b24.png)'
  prefs: []
  type: TYPE_IMG
- en: From these screenshots, we can observe a general trend for decreasing loss and
    increasing accuracy with each epoch along the training process, with alternating
    ups and downs. We can observe whether the final accuracy or the learning rate
    is desirable and optimize the model where necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the prediction performance for each digit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first revert the labels from one-hot format back to lists of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will extract the indices of mislabeled images, and use them to retrieve
    the corresponding true and predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows, with NumPy arrays of the indices, true and predicted
    labels of the array of mislabeled images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s count how many samples are mislabeled for each digit. We will store
    the counts into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will plot a bar chart of the ratio of mislabeled samples for each digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a bar chart showing the ratio of each digit mislabeled by
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4337625d-7d76-4b64-98f2-9a661335fe80.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding figure, we see that the digit 8 is the most easily mis-recognized
    digit by our model. Let's find out why.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting falsely predicted images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to what we did at the beginning of this chapter, we will draw the digit
    images out. This time, we pick out the mislabeled ones because they are the ones
    we''re concerned about. We will again pick 10 images and put them in a grid of
    subplots. We write the true label in green at the bottom as `xlabel` and the false
    label predicted in `red` as the `title` at the top for each image in a subplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let's see how the images look. Does the handwriting look more like the true
    or falsely predicted label to you?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15efdaaa-3fc6-484a-8780-8f1cd93f3501.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that, for some images, it is quite difficult to identify the
    true label at the 8x8 resolution even with the naked eye, such as the number **4**
    in the middle of the bottom row. However, the leftmost number **4** on the same
    row should be legible enough for humans to recognize. From here, we can estimate
    the maximum possible improvement in accuracy by additional training and optimizing
    the model. This will guide our decision on whether it is worth while to expend
    further effort to improve our model, or what kind of training data to get or generate
    next to obtain better results.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, notice that the training and testing datasets generally contain samples
    from different distributions. It is left for an exercise for you to repeat the
    process by downloading the actual training dataset from UCI ML, using the larger
    MNIST dataset (by downloading it via Keras, or even scraping or creating your
    own).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You have now completed this chapter as well as the whole book.
    In this chapter, we integrated various data visualization techniques along with
    an analytic project workflow, from the initial inspection and exploratory analysis
    of data, to model building and evaluation. Give yourself a huge round of applause,
    and get ready to leap forward into the journey of data science!
  prefs: []
  type: TYPE_NORMAL
