["```py\n    rownames(installed.packages()) \n    ```", "```py\n    install.packages(\"Rserve\") \n    ```", "```py\n    Fatal error: you must specify '--save', '--no-save' or '--vanilla' \n    ```", "```py\n    SCRIPT_REAL(\" \n    x <- lm(.arg1 ~ .arg2 + .arg3 + .arg4) \n    x$fitted\",\n    SUM(Profit), COUNT(Quantity), SUM(Sales), AVG(Discount)\n    ) \n    ```", "```py\n    SUM(Profit)/Profit_Expected - 1 \n    ```", "```py\n    SCRIPT_REAL(\" \n    x <- lm(.arg1 ~ .arg2 + .arg3 + .arg4) \n    x$residuals\", \n    SUM(Profit),COUNT(Quantity), SUM(Sales), AVG(Discount)) \n    ```", "```py\n    SCRIPT_INT(\"\n    m <- cbind(.arg1, .arg2); \n    kmeans(m,4,nstart=5)$cluster\",\n    AVG( [Life Expectancy Female] ), AVG([Life Expectancy Male])) \n    ```", "```py\n    SCRIPT_REAL(\"\n    x <- .arg1;\n    y <- .arg2[1];\n    m <- c(1:y)/y;\n    n <- length(x);\n    z <- c(1:n); for (i in c(1:n)) z[i] <- 0;\n    for (j in c(1:y)) for (i in c(1:n)) z[i] <- if (x[i] <= quantile(x,m)[j] && z[i] == 0 ) j else z[i];\n    z;\"\n    , SUM(Sales), [Number of quantiles]) \n    ```", "```py\n    RANK_PERCENTILE(SUM([Sales])) < [Select Percentile Top Range] \n    AND\n    RANK_PERCENTILE(SUM([Sales])) > [Select Percentile Bottom Range] \n    ```", "```py\n    !pip install tabpy \n    ```", "```py\n    pip install tabpy \n    ```", "```py\n    SCRIPT_REAL(\"\n    from numpy import random as rd \n    return rd.random(_arg1[0]).tolist()\",\n    SIZE()\n    ) \n    ```", "```py\n    SCRIPT_REAL(\"\n    from numpy import random as rd \n    mu, sigma = 0, 1\n    return (rd.normal(mu, sigma, _arg1[0])).tolist()\n    \",\n    SIZE()\n    ) \n    ```", "```py\nSCRIPT_REAL(\"\nfrom numpy import random as rd \nmu, sigma = _arg2, _arg3\nreturn (rd.normal(mu, sigma, _arg1[0])).tolist()\n\",\nSIZE(), [mu], [sigma]\n) \n```", "```py\n    SCRIPT_REAL(\"\n    from nltk.sentiment import SentimentIntensityAnalyzer \n    text = _arg1\n    scores = []\n    sid = SentimentIntensityAnalyzer() \n    for word in text:\n        ss = sid.polarity_scores(word) \n    scores.append(ss['compound'])\n    return scores \"\n    ,ATTR([headline_text])) \n    ```", "```py\n    IIF ([Sentiment Scores] >= 0, 'Positivity', 'Negativity') \n    ```", "```py\n    from tabpy.tabpy_tools.client import Client\n    client = Client('http://localhost:9004/')\n    def common_words(_arg1, _arg2):\n\n        #load all required libraries\n        import pandas as pd\n        import nltk\n        from nltk.corpus import stopwords\n        from nltk.tokenize import word_tokenize\n        from nltk.probability import FreqDist\n        from nltk.sentiment import SentimentIntensityAnalyzer \n        df = pd.read_csv('abcnews-date-text.csv')\n        df['publish_date'] = pd.to_datetime(df['publish_date'], format='%Y%m%d')\n\n        #convert the Tableau input date to a readable format\n        _arg1 = pd.to_datetime(_arg1, format='%Y-%m-%d')[0]\n        _arg2 = pd.to_datetime(_arg2, format='%Y-%m-%d')[0]\n        df = df.set_index('publish_date')\n        #empty lists to be filled\n        scores = []\n        filtered_sentence = []\n\n        # Select the rows between two dates\n        in_range_df = df[df.index.isin(pd.date_range(_arg1, _arg2))]\n        #loading english stop words\n        stop_words = set(stopwords.words('english'))\n        text = in_range_df['headline_text']\n        #most used words\n        for row in range(0, len(in_range_df)):\n            text =  in_range_df['headline_text'][row]\n            word_tokens = word_tokenize(text)\n            for w in word_tokens:\n                if w not in stop_words:\n                    filtered_sentence.append(w)\n        #calculate the frequency of each word\n        fdist = FreqDist(filtered_sentence)\n        #extract the N most common words\n        most_common_words = fdist.most_common(3)\n        return str(most_common_words)\n    client.deploy('common_words', common_words,'Retrieves the most common words', override = True) \n    ```", "```py\n    MODEL_EXTENSION_STR(\"common_words\", \"_arg1, _arg2\", [Start], [End]) \n    ```", "```py\n    from tabpy.tabpy_tools.client import Client\n    client = Client('http://localhost:9004/') \n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    from sklearn import ensemble\n    from sklearn.model_selection import train_test_split\n    df = pd.read_csv(\"Diabetes.csv\", sep = ',')\n    df = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age', 'Outcome']] \n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(['Outcome'], axis = 'columns'),df['Outcome'], test_size = 0.2) \n    ```", "```py\n    gb = ensemble.GradientBoostingClassifier()\n    gb.fit(X_train, y_train)\n    gbscore = gb.score(X_test, y_test)\n    print('gradient boost: ' + str(round(gbscore*100,2))) \n    ```", "```py\n    def diabetes_predictor(_arg1, _arg2, _arg3, _arg4, _arg5,_arg6,_arg7):\n        import pandas as pd\n        row = {'Pregnancies': _arg1, 'Glucose': _arg2, 'BloodPressure':\n               _arg3, 'SkinThickness': _arg4, 'Insulin': _arg5, 'BMI':_arg6,\n               'Age': _arg7}\n        test_data = pd.DataFrame(data = row,index=[0])\n        predprob_diabetes = gb.predict_proba(test_data)\n        return [probability[1] for probability in predprob_diabetes] \n    ```", "```py\n    client.deploy('diabetes_predictor', diabetes_predictor,'Predicts the chances of a Pima female having diabetes', override = True) \n    ```", "```py\n    SCRIPT_REAL(\"\n    return tabpy.query('diabetes_predictor',_arg1,_arg2,_arg3,_arg4,_arg5,_arg6,\n    _arg7)['response']\",\n    [Pregnancies], [Glucose], [BloodPressure], [SkinThickness], [Insulin], \n    [BMI],[Age]\n    ) \n    ```", "```py\n    MODEL_EXTENSION_REAL(\"diabetes_predictor\", \"_arg1, _arg2, _arg3, _arg4, _arg5, _arg6, _arg7\", \n    [Pregnancies], [Glucose], [BloodPressure], [SkinThickness], [Insulin], [BMI],[Age]\n    ) \n    ```", "```py\n    tabpy.query('ttest', _arg1, _arg2)['response'] \n    ```"]