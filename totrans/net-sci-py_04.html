<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer063" class="IMG---Figure">
			<h1 id="_idParaDest-107" class="chapter-number"><a id="_idTextAnchor158"/>4</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor159"/>NLP and Network Synergy</h1>
			<p>In the previous chapters, we discussed <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), network analysis, and the tools used in the Python programming language for both. We also discussed non-programmatic tools for doing <span class="No-Break">network analysis.</span></p>
			<p>In this chapter, we are going to put all of that knowledge to work. I hope to explain the power and insights that can be unveiled by combining NLP and network analysis, which is the theme of this book. In later chapters, we will continue with this theme, but we’ll also discuss other tools for working with NLP and networks, such as unsupervised and supervised machine learning. This chapter will demonstrate techniques for determining who or what a piece of text is <span class="No-Break">talking about.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Why are we learning about NLP in a <span class="No-Break">network book?</span></li>
				<li>Asking questions to tell <span class="No-Break">a story</span></li>
				<li>Introducing <span class="No-Break">web scraping</span></li>
				<li>Choosing between libraries, APIs, and <span class="No-Break">source data</span></li>
				<li>Using the Natural Language Toolkit (NLTK) library for part-of-speech (<span class="No-Break">PoS) tagging</span></li>
				<li>Using spaCy for PoS tagging and named-entity <span class="No-Break">recognition (NER)</span></li>
				<li>Converting entity lists into <span class="No-Break">network data</span></li>
				<li>Converting network data <span class="No-Break">into networks</span></li>
				<li>Doing a network visualization <span class="No-Break">spot check</span></li>
			</ul>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor160"/>Technical requirements</h1>
			<p>In this chapter, we will be using several different Python libraries. The <strong class="source-inline">pip install</strong> command is listed in each section for installing each library, so just follow along and do the installations as needed. If you run into installation problems, there is usually an answer on Stack Overflow. Google <span class="No-Break">the error!</span></p>
			<p>Before we start, I would like to explain one thing so that the number of libraries we are using doesn’t seem so overwhelming. It is the reason why we use each library <span class="No-Break">that matters.</span></p>
			<p>For most of this book, we will be doing one of three things: network analysis, network visualization, or using network data for machine learning (also known <span class="No-Break">as GraphML).</span></p>
			<p>Anytime we have network data, we will be using <strong class="source-inline">NetworkX</strong> to <span class="No-Break">use it.</span></p>
			<p>Anytime we are doing analysis, we will probably be <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break">.</span></p>
			<p>The relationship looks <span class="No-Break">like this:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Network</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">NetworkX</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Analysis</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">pandas</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Visualization</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">scikit-network</strong></span></li>
				<li><strong class="bold">ML</strong>: <strong class="source-inline">scikit-learn</strong> and <span class="No-Break"><strong class="source-inline">Karate Club</strong></span></li>
			</ul>
			<p>Look at the first words. If I want to do network analysis, then I’ll be using <strong class="source-inline">NetworkX</strong> and <strong class="source-inline">pandas</strong>. If I want to do network visualization, I will be using <strong class="source-inline">NetworkX</strong> and <strong class="source-inline">scikit-network</strong>. If I want to do machine learning using network data, I will be using <strong class="source-inline">NetworkX</strong>, <strong class="source-inline">scikit-learn</strong>, and possibly <span class="No-Break"><strong class="source-inline">Karate Club</strong></span><span class="No-Break">.</span></p>
			<p>These are the core libraries that will be used in <span class="No-Break">this chapter.</span></p>
			<p>Also, <em class="italic">you must keep the code for the </em><strong class="source-inline">draw_graph()</strong><em class="italic"> function handy</em>, as you will be using it throughout this book. That code is a bit gnarly because, at the time of writing, it needs to be. Unfortunately, <strong class="source-inline">NetworkX</strong> is not great for network visualization, and <strong class="source-inline">scikit-network</strong> is not great for network construction or analysis, so I use both libraries together for visualization, and that works well. I am hoping this will not be the case for later editions of this book and that visualization will be improved and simplified <span class="No-Break">over time.</span></p>
			<p>All of the necessary code can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Network-Science-with-Python"><span class="No-Break">https://github.com/PacktPublishing/Network-Science-with-Python</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor161"/>Why are we learning about NLP in a network book?</h1>
			<p>I briefly answered <a id="_idIndexMarker247"/>this question in the introduction of the first chapter, but it is worth repeating in more detail. Many people who work on text analysis are aware of sentiment analysis and text classification. <strong class="bold">Text classification</strong> is<a id="_idIndexMarker248"/> the ability to predict whether a piece of text can be classified as something. For example, let’s take <span class="No-Break">this string:</span></p>
			<p>“<em class="italic">How are you </em><span class="No-Break"><em class="italic">doing today?</em></span><span class="No-Break">”</span></p>
			<p>What can we tell about this string? Is it a question or a statement? It’s a question. Who is being asked this question? You are. Is there a positive, negative, or neutral emotion tied to the question? It looks neutral to me. Let’s try <span class="No-Break">another string.</span></p>
			<p>“<em class="italic">Jack and Jill went up the hill, but Jack fell down because he is </em><span class="No-Break"><em class="italic">an idiot.</em></span><span class="No-Break">”</span></p>
			<p>This is a statement about Jack and Jill, and Jack is being called an idiot, which is not a very nice thing to say. However, it seems like the insult was written jokingly, so it is unclear whether the author was angry or laughing when it was written. I can confirm that I was laughing when I wrote it, so there is a positive emotion behind it, but text classification would struggle to pick up on that. Let’s <span class="No-Break">try another.</span></p>
			<p>“<em class="italic">I have never been so angry in my entire life as I am </em><span class="No-Break"><em class="italic">right now!</em></span><span class="No-Break">”</span></p>
			<p>The author is expressing a very strong negative sentiment. Sentiment analysis and text classification would easily pick up <span class="No-Break">on that.</span></p>
			<p><strong class="bold">Sentiment analysis</strong> is a <a id="_idIndexMarker249"/>set of techniques that use algorithms to automatically detect emotions that are embedded into a piece of text or transcribed recording. Text classification uses the same algorithms, but the goal is not to identify an emotion but rather a theme. For instance, text classification can detect whether there is abusive language <span class="No-Break">in text.</span></p>
			<p>This is not a chapter about sentiment analysis or text classification. The purpose of this chapter is to explain how to automatically extract the entities (people, places, things, and more) that exist in text so that you can identify and study the social network that is being described in the text. However, text classification and sentiment analysis can be coupled with these techniques to provide additional context about social networks, or for building specialized social networks, such as <span class="No-Break">friendship networks.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor162"/>Asking questions to tell a story</h1>
			<p>I approach my work from a storytelling perspective; I let my story dictate the work, not the other way around. For instance, if I am starting a project, I’ll ponder or even write down a series of who, what, where, when, why, and <span class="No-Break">how questions:</span></p>
			<ul>
				<li>What data do we have? Is <span class="No-Break">it enough?</span></li>
				<li>Where do we <span class="No-Break">get more?</span></li>
				<li>How do we <span class="No-Break">get more?</span></li>
				<li>What blockers prevent us from <span class="No-Break">getting more?</span></li>
				<li>How often will we <span class="No-Break">need more?</span></li>
			</ul>
			<p>But this is a different kind of project. We want to gain deeper insights into a piece of text than we might gather through reading. Even after reading a whole book, most people can’t memorize the relationships that are described in the text, and it would likely be a faulty recollection anyway. But we should have questions such <span class="No-Break">as these:</span></p>
			<ul>
				<li>Who is mentioned in <span class="No-Break">the text?</span></li>
				<li>Who do <span class="No-Break">they know?</span></li>
				<li>Who are <span class="No-Break">their adversaries?</span></li>
				<li>What are the themes of <span class="No-Break">this text?</span></li>
				<li>What emotions <span class="No-Break">are present?</span></li>
				<li>What places <span class="No-Break">are mentioned?</span></li>
				<li>When did this <span class="No-Break">take place?</span></li>
			</ul>
			<p>It is important to come up with a solid set of questions before starting any kind of analysis. More questions will come to you the deeper <span class="No-Break">you go.</span></p>
			<p>This chapter will give you the tools to automatically investigate all of these questions other than the ones about themes and adversaries. The knowledge from this chapter, coupled with an understanding of text classification and sentiment analysis, will give you the ability to answer <em class="italic">all</em> these questions. That is the “why” of this chapter. We want to automatically extract people, places, and maybe even some things that are mentioned. Most of the time, I only want people and places, as I want to study the <span class="No-Break">social network.</span></p>
			<p>However, it is important to explain that this is useful for accompanying text classification and sentiment analysis. For example, a positive Amazon or Yelp review is of little use if you do not know what is <span class="No-Break">being reviewed.</span></p>
			<p>Before we can do any work to uncover the relationships that exist in text, we need to get some text. For practice, we have a few options. We could load it using a Python library such as NLTK, we could harvest it using the Twitter (or another social network) library, or we could scrape it ourselves. Even scraping has options, but I am only going to explain one way: using Python’s <strong class="source-inline">BeautifulSoup</strong> library. Just know that there are options, but I love the flexibility <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">BeautifulSoup</strong></span><span class="No-Break">.</span></p>
			<p>In this chapter, the demos will use text scraped off of the internet, and you can make slight changes to the code for your own web <span class="No-Break">scraping needs.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor163"/>Introducing web scraping</h1>
			<p>First, what<a id="_idIndexMarker250"/> even is <strong class="bold">web scraping</strong>, and who can do it? Anyone with any programming skill can do scraping using several different programming languages, but we will do this with Python. Web scraping is the action of harvesting content from web resources so that you may use the data in your products and software. You can use scraping to pull information that a website hasn’t exposed as a data feed or through an API. But one warning: do not scrape too aggressively; otherwise, you could knock down a web server through an accidental <strong class="bold">denial-of-service</strong> (<strong class="bold">DoS</strong>) attack. Just get what you need as often as you need it. Go slow. Don’t be greedy <span class="No-Break">or selfish.</span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor164"/>Introducing BeautifulSoup</h2>
			<p><strong class="bold">BeautifulSoup</strong> is a <a id="_idIndexMarker251"/>powerful Python library for scraping anything that you have access to online. I frequently use this to harvest story URLs from news websites, and then I scrape each of these URLs for their text content. I typically do not want the actual HTML, CSS, or JavaScript, so I render the web page and then scrape <span class="No-Break">the content.</span></p>
			<p><strong class="source-inline">BeautifulSoup</strong> is an important Python library to know about if you are going to be doing web scraping. There<a id="_idIndexMarker252"/> are other options for web scraping with Python, but <strong class="source-inline">BeautifulSoup</strong> is <span class="No-Break">commonly used.</span></p>
			<p>There is no better way to explain <strong class="source-inline">BeautifulSoup</strong> than to see it in action, so let’s get <span class="No-Break">to work!</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor165"/>Loading and scraping data with BeautifulSoup</h2>
			<p>In this hands-on <a id="_idIndexMarker253"/>demonstration, we will see three different ways of loading and scraping data. They are all useful independently, but they can also be used together <span class="No-Break">in ways.</span></p>
			<p>First, the <a id="_idIndexMarker254"/>easiest approach is to use a library that can get exactly what you want in one shot, with minimal cleanup. Several libraries such as <strong class="source-inline">pandas</strong>, <strong class="source-inline">Wikipedia</strong>, and <strong class="source-inline">NLTK</strong> have ways to load data, so let’s start with them. As this book is primarily about extracting relationships from text and then analyzing them, we want text data. I will demonstrate a few approaches, and then describe the pros and cons <span class="No-Break">of each.</span></p>
			<h3>Python library – Wikipedia</h3>
			<p>There<a id="_idIndexMarker255"/> is a powerful library for pulling data from Wikipedia, also called <strong class="source-inline">Wikipedia</strong>. It can be installed by running the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pip install wikipedia</pre>
			<p>Once it has been installed, you may import it into your code <span class="No-Break">like this:</span></p>
			<pre class="source-code">
import wikipedia as wiki</pre>
			<p>Once it has been imported, you have access to Wikipedia programmatically, allowing you to search for and use any content you are curious about. As we will be doing a lot of social network analysis while working through this book, let’s see what <strong class="source-inline">Wikipedia</strong> has on <span class="No-Break">the subject:</span></p>
			<pre class="source-code">
search_string = 'Social Network Analysis'
page = wiki.page(search_string)
content = page.content
content[0:680]</pre>
			<p>The last line, <strong class="source-inline">content[0:680]</strong>, just shows what is inside the <strong class="source-inline">content</strong> string, up to the 680<span class="superscript">th</span> character, which is the end of the sentence shown in the following code. There is a lot<a id="_idIndexMarker256"/> more past 680. I chose to cut it off for <span class="No-Break">this demonstration:</span></p>
			<pre class="source-code">
'Social network analysis (SNA) is the process of investigating social structures  through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them.  Examples of social structures commonly visualized through social network analysis include social media networks, memes spread, information circulation, friendship and acquaintance networks, business networks, knowledge networks, difficult working relationships, social networks, collaboration graphs, kinship, disease transmission, and sexual relationships.'</pre>
			<p>With a few lines of code, we were able to pull text data directly from Wikipedia. Can we see what links exist on that Wikipedia page? Yes, <span class="No-Break">we can!</span></p>
			<pre class="source-code">
links = page.links
links[0:10]</pre>
			<p>I’m using a square bracket, <strong class="source-inline">[</strong>, to select only the first 10 entries <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">links</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
['Actor-network theory',
 'Adjacency list',
 'Adjacency matrix',
 'Adolescent cliques',
 'Agent-based model',
 'Algorithm',
 'Alpha centrality',
 'Anatol Rapoport',
 'Anthropology',
 'ArXiv (identifier)']</pre>
			<p>If we are only<a id="_idIndexMarker257"/> in the A-links after choosing only the first 10 links, I think it’s safe to say that there are probably many more. There’s quite a lot to learn about social network analysis on Wikipedia! That was so simple! Let’s move on to another option for easy scraping: the <span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break"> library.</span></p>
			<h3>Python library – pandas</h3>
			<p>If you are interested in <a id="_idIndexMarker258"/>using Python for data science, you will inevitably learn about and use <strong class="source-inline">pandas</strong>. This library is powerful and versatile for working with data. For this demonstration, I will use it for pulling tabular data from web pages, but it can do much more. If you are interested in data science, learn as much as you can about <strong class="source-inline">pandas</strong>, and get comfortable <span class="No-Break">using it.</span></p>
			<p><strong class="source-inline">pandas</strong> can be <a id="_idIndexMarker259"/>useful for pulling tabular data from web pages, but it is much less useful for raw text. If you want to load text from Wikipedia, you should use the <strong class="source-inline">Wikipedia</strong> library shown previously. If you want to pull text off other web pages, you should use the <strong class="source-inline">Requests</strong> and <strong class="source-inline">BeautifulSoup</strong> <span class="No-Break">libraries together.</span></p>
			<p>Let’s use <strong class="source-inline">pandas</strong> to pull some tabular data from Wikipedia. First, let’s try to scrape the same Wikipedia page using <strong class="source-inline">pandas</strong> and see what we get. If you have installed Jupyter on your computer, you likely already have <strong class="source-inline">pandas</strong> installed, so, let’s jump straight to <span class="No-Break">the code:</span></p>
			<ol>
				<li>Start by <span class="No-Break">importing </span><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break">:</span><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
url = 'https://en.wikipedia.org/wiki/Social_network_analysis'</pre><pre class="source-code">
data = pd.read_html(url)</pre><pre class="source-code">
type(data)</pre></li>
			</ol>
			<p>Here, we imported the <strong class="source-inline">pandas</strong> library and gave it a shorter name, and then used <strong class="source-inline">Pandas</strong> to read the same Wikipedia page on social network analysis. What did we get back? What does <strong class="source-inline">type(data)</strong> show? I would expect a <span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break"> DataFrame.</span></p>
			<ol>
				<li value="2">Enter the <a id="_idIndexMarker260"/>following code. Just type <strong class="source-inline">data</strong> and run <span class="No-Break">the code:</span><pre class="source-code">
data</pre></li>
			</ol>
			<p>You should see that we get a list back. In <strong class="source-inline">pandas</strong>, if you do a <strong class="source-inline">read</strong> operation, you will usually get a DataFrame back, so why did we get a list? This happened because there are multiple data tables on this page, so <strong class="source-inline">pandas</strong> has returned a Python list of all of <span class="No-Break">the tables.</span></p>
			<ol>
				<li value="3">Let’s check out the elements of <span class="No-Break">the list:</span><pre class="source-code">
data[0]</pre></li>
			</ol>
			<p>This should give us a <strong class="source-inline">pandas</strong> DataFrame of the first table from a <span class="No-Break">Wikipedia page:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B17105_04_001.jpg" alt="Figure 4.1 – pandas DataFrame of the first element of Wikipedia data" width="615" height="577"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – pandas DataFrame of the first element of Wikipedia data</p>
			<p>This data is already looking a bit problematic. Why are we getting a bunch of text inside <a id="_idIndexMarker261"/>of a table? Why does the last row look like a bunch of code? What does the next table <span class="No-Break">look like?</span></p>
			<pre class="source-code">
data[1]</pre>
			<p>This will give us a <strong class="source-inline">pandas</strong> DataFrame of the second table from the same Wikipedia page. Please be aware that Wikipedia pages are occasionally edited, so your results might <span class="No-Break">be different:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17105_04_002.jpg" alt="Figure 4.2 – pandas DataFrame of the second element of Wikipedia data" width="663" height="1147"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – pandas DataFrame of the second element of Wikipedia data</p>
			<p>Gross. This <a id="_idIndexMarker262"/>looks even worse. I say that because it appears that we have some very short strings that look like web page sections. This doesn’t look very useful. Remember, <strong class="source-inline">pandas</strong> is great for loading tabular data, but not great on tables of text data, which Wikipedia uses. We can already see that this is less useful than the results we very easily captured using the <span class="No-Break">Wikipedia library.</span></p>
			<ol>
				<li value="4">Let’s try a<a id="_idIndexMarker263"/> page that has useful data in a table. This page contains tabular data about crime <span class="No-Break">in Oregon:</span><pre class="source-code">
url = 'https://en.wikipedia.org/wiki/Crime_in_Oregon'</pre><pre class="source-code">
data = pd.read_html(url)</pre><pre class="source-code">
df = data[1]</pre><pre class="source-code">
df.tail()</pre></li>
			</ol>
			<p>This will show us the last five rows of a <strong class="source-inline">Pandas</strong> DataFrame containing Oregon <span class="No-Break">crime data:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17105_04_003.jpg" alt="Figure 4.3 – Pandas DataFrame of tabular numeric Wikipedia data" width="1650" height="305"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Pandas DataFrame of tabular numeric Wikipedia data</p>
			<p>Wow, this looks like useful data. However, the data does not show anything more recent than 2009, which is quite a while ago. Maybe there is a better dataset out there that we should use instead. Regardless, this shows that <strong class="source-inline">Pandas</strong> can easily pull tabular data off the web. However, there are a few things to keep <span class="No-Break">in mind.</span></p>
			<p>First, if you use scraped data in your projects, know that you are putting yourself at the mercy of the website administrators. If they decided to throw away the data table or rename or reorder the columns, that may break your application if it reads directly from Wikipedia. You can protect yourself against this by keeping a local copy of the data when you do a scrape, as well as including error handling in your code to watch <span class="No-Break">for exceptions.</span></p>
			<p>Be prepared for the worst. When you are scraping, build in any error-checking that you need as well as ways of knowing when your scrapers are no longer able to pull data. Let’s move on to the next approach – <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">NLTK</strong></span><span class="No-Break">.</span></p>
			<h3>Python library – NLTK</h3>
			<p>Let’s get straight <span class="No-Break">to it:</span></p>
			<ol>
				<li value="1">First, NLTK <a id="_idIndexMarker264"/>does not come installed with Jupyter, so you will need to install it. You can do so with the <span class="No-Break">following command:</span><pre class="source-code">
pip install nltk</pre></li>
				<li>Python’s <strong class="bold">NLTK</strong> library can easily pull data from Project Gutenberg, which is a library of over 60,000 freely available books. Let’s see what we <span class="No-Break">can get:</span><pre class="source-code">
from nltk.corpus import gutenberg</pre><pre class="source-code">
gutenberg.fileids()</pre><pre class="source-code">
…</pre><pre class="source-code">
['austen-emma.txt',</pre><pre class="source-code">
 'austen-persuasion.txt',</pre><pre class="source-code">
 'austen-sense.txt',</pre><pre class="source-code">
 'bible-kjv.txt',</pre><pre class="source-code">
 'blake-poems.txt',</pre><pre class="source-code">
 'bryant-stories.txt',</pre><pre class="source-code">
 'burgess-busterbrown.txt',</pre><pre class="source-code">
 'carroll-alice.txt',</pre><pre class="source-code">
 'chesterton-ball.txt',</pre><pre class="source-code">
 'chesterton-brown.txt',</pre><pre class="source-code">
 'chesterton-thursday.txt',</pre><pre class="source-code">
 'edgeworth-parents.txt',</pre><pre class="source-code">
 'melville-moby_dick.txt',</pre><pre class="source-code">
 'milton-paradise.txt',</pre><pre class="source-code">
 'shakespeare-caesar.txt',</pre><pre class="source-code">
 'shakespeare-hamlet.txt',</pre><pre class="source-code">
 'shakespeare-macbeth.txt',</pre><pre class="source-code">
 'whitman-leaves.txt']</pre></li>
			</ol>
			<p>As you<a id="_idIndexMarker265"/> can easily see, this is far fewer than 60,000 results, so we are limited in what we can get using this approach, but it is still useful data for <span class="No-Break">practicing NLP.</span></p>
			<ol>
				<li value="3">Let’s see what is inside the <span class="No-Break"><strong class="source-inline">blake-poems.txt</strong></span><span class="No-Break"> file:</span><pre class="source-code">
file = 'blake-poems.txt'</pre><pre class="source-code">
data = gutenberg.raw(file)</pre><pre class="source-code">
data[0:600]</pre><pre class="source-code">
…</pre><pre class="source-code">
'[Poems by William Blake 1789]\n\n \nSONGS OF INNOCENCE AND OF EXPERIENCE\nand THE BOOK of THEL\n\n\n SONGS OF INNOCENCE\n \n \n INTRODUCTION\n \n Piping down the valleys wild,\n   Piping songs of pleasant glee,\n On a cloud I saw a child,\n   And he laughing said to me:\n \n "Pipe a song about a Lamb!"\n   So I piped with merry cheer.\n "Piper, pipe that song again;"\n   So I piped: he wept to hear.\n \n "Drop thy pipe, thy happy pipe;\n   Sing thy songs of happy cheer:!"\n So I sang the same again,\n   While he wept with joy to hear.\n \n "Piper, sit thee down and write\n   In a book, that all may read."\n So he vanish\'d'</pre></li>
			</ol>
			<p>We can load the entire file. It is messy in that it contains line breaks and other formatting, but we will clean that out. What if we want one of the other books from the full library of 60,000 books that are not on this list? Are we out of luck? Browsing the website, I can see that I can read the plain text of one of my favorite books, Franz Kafka’s <em class="italic">The Metamorphosis</em>, at <a href="https://www.gutenberg.org/files/5200/5200-0.txt">https://www.gutenberg.org/files/5200/5200-0.txt</a>. Let’s try to get that data, but this time, we will use Python’s <span class="No-Break">Requests library.</span></p>
			<h3>Python library – Requests</h3>
			<p>The <strong class="bold">Requests</strong> library <a id="_idIndexMarker266"/>comes pre-installed with Python, so there should be nothing to do but import it. Requests is used to scrape raw text off the web, but it can do more. Please research the library to learn about <span class="No-Break">its capabilities.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If you use this approach, please note that you should not do this aggressively. It is okay to load one book at a time like this, but if you attempt to download too many books at once, or aggressively crawl all books available on Project Gutenberg, you will very likely end up getting your IP address <span class="No-Break">temporarily blocked.</span></p>
			<p>For our <a id="_idIndexMarker267"/>demonstration, let’s import the library and then scrape the raw text from Gutenberg’s offering of <span class="No-Break"><em class="italic">The Metamorphosis</em></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import requests
url = 'https://www.gutenberg.org/files/5200/5200-0.txt'
data = requests.get(url).text
data
…
'ï»¿The Project Gutenberg eBook of Metamorphosis, by Franz Kafka\r\n\r\nThis eBook is for the use of anyone anywhere in the United States and\r\nmost other parts of the world at no cost and with almost no restrictions\r\nwhatsoever. You may copy it, give it away or re-use it under the terms\r\nof the Project Gutenberg License included with this eBook or online at\r\nwww.gutenberg.org. If you are not located in the United States, you\r\nwill have to check the laws of the country where you are located before\r\nusing this eBook.\r\n\r\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\r\n**     Please follow the copyright guidelines in this file.     *\r\n\r\nTitle: Metamorphosis\r\n\r\nAuthor: Franz Kafka\r\n\r\nTranslator: David Wyllie\r\n\r\nRelease Date: May 13, 2002 [eBook #5200]\r\n[Most recently updated: May 20, 2012]\r\n\r\nLanguage: English\r\n\r\nCharacter set encoding: UTF-8\r\n\r\nCopyright (C) 2002 by David Wyllie.\r\n\r\n*** START OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\r\n\r\n\r\n\r\n\r\nMetamorphosis\r\n\r\nby Franz Kafka\r\n\r\nTranslated by David Wyllie\r\n\r\n\r\n\r\n\r\nI\r\n\r\n\r\nOne morning, when Gregor Samsa woke from troubled dreams, he found\r\nhimself transformed in his bed into a horrible vermin…'</pre>
			<p>I chopped <a id="_idIndexMarker268"/>off all text after “<strong class="source-inline">vermin</strong>” just to briefly show what is in the data. Just like that, we’ve got the full text of the book. As was the case with NLTK, the data is full of formatting and other characters, so this data needs to be cleaned before it will be useful. Cleaning is a very important part of everything that I will explain in this book. Let’s keep moving; I will show some ways to clean text during <span class="No-Break">these demonstrations.</span></p>
			<p>I’ve shown how easy it is to pull text from Gutenberg or Wikipedia. But these two do not even begin to scratch the surface of what is available for scraping on the internet. <strong class="source-inline">pandas</strong> can read tabular data from any web page, but that is limited. Most content on the web is not perfectly formatted or clean. What if we want to set up scrapers to harvest text and content from various news websites that we are interested in? NLTK won’t be able to help us get that data, and Pandas will be limited in what it can return. What are our options? We saw that the Requests library was able to pull another Gutenberg book that wasn’t available through NLTK. Can we similarly use requests to pull HTML from websites? Let’s try getting some news from <span class="No-Break">Okinawa, Japan!</span></p>
			<pre class="source-code">
url = 'http://english.ryukyushimpo.jp/'
data = requests.get(url).text
data
'&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;\r\n&lt;html  dir="ltr" lang="en-US"&gt;\r\n\r\n&lt;!-- BEGIN html head --&gt;\r\n&lt;head profile="http://gmpg.org/xfn/11"&gt;\r\n&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /&gt;\r\n&lt;title&gt;Ryukyu Shimpo - Okinawa, Japanese newspaper, local news&lt;/title&gt;…'</pre>
			<p>There we<a id="_idIndexMarker269"/> go! We’ve just loaded the raw HTML from the given URL, and we could do that for any publicly accessible web page. However, if you thought the Gutenberg data was messy, look at this! Do we have any hope in the world of using this, let alone building automation to parse HTML and mine useful data? Amazingly, the answer is yes, and we can thank the <strong class="source-inline">BeautifulSoup</strong> Python library and other scraping libraries for that. They have truly opened up a world of data for us. Let’s see what we can get out of this <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">BeautifulSoup</strong></span><span class="No-Break">.</span></p>
			<h3>Python library – BeautifulSoup</h3>
			<p>First, <strong class="source-inline">BeautifulSoup</strong> is <a id="_idIndexMarker270"/>used along with the <strong class="source-inline">Requests</strong> library. Requests comes pre-installed with Python, but <strong class="source-inline">BeautifulSoup</strong> does not, so you will need to install it. You can do so with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pip install beautifulsoup4</pre>
			<p>There’s a lot you <a id="_idIndexMarker271"/>can do with <strong class="source-inline">BeautifulSoup</strong>, so please go explore the library. But what would it take to extract all of the links from the Okinawa News URL? This is how you can do <span class="No-Break">exactly that:</span></p>
			<pre class="source-code">
from bs4 import BeautifulSoup
soup = BeautifulSoup(data, 'html.parser')
links = soup.find_all('a', href=True)
links
[&lt;a href="http://english.ryukyushimpo.jp"&gt;Home&lt;/a&gt;,
 &lt;a href="http://english.ryukyushimpo.jp"&gt;Ryukyu Shimpo – Okinawa, Japanese newspaper, local news&lt;/a&gt;,
 &lt;a href="http://english.ryukyushimpo.jp/special-feature-okinawa-holds-mass-protest-rally-against-us-base/"&gt;Special Feature: Okinawa holds mass protest rally against US base&lt;/a&gt;,
 &lt;a href="http://english.ryukyushimpo.jp/2021/09/03/34020/"&gt;Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"&lt;/a&gt;,
 &lt;a href="http://english.ryukyushimpo.jp/2021/09/03/34020/"&gt;&lt;img alt="Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"" class="medium" src="http://english.ryukyushimpo.jp/wp-content/uploads/2021/09/RS20210830G01268010100.jpg"/&gt; &lt;/a&gt;…]</pre>
			<p>That <a id="_idIndexMarker272"/>was easy! I am only showing the first several extracted links. The first line imported the library, the second line set <strong class="source-inline">BeautifulSoup</strong> up for parsing HTML, the third line looked for all links containing an <strong class="source-inline">href</strong> attribute, and then finally the last line displayed the links. How many links did <span class="No-Break">we harvest?</span></p>
			<pre class="source-code">
len(links)
…
277</pre>
			<p>Your result might be different, as the page may have been edited after this book <span class="No-Break">was written.</span></p>
			<p>277 links were harvested in less than a second! Let’s see whether we can clean these up and just extract the URLs. Let’s not worry about the link text. We should also convert this into a list of URLs rather than a list of <strong class="source-inline">&lt;a&gt;</strong> <span class="No-Break">HTML tags:</span></p>
			<pre class="source-code">
urls = [link.get('href') for link in links]
urls
…
['http://english.ryukyushimpo.jp',
 'http://english.ryukyushimpo.jp',
 'http://english.ryukyushimpo.jp/special-feature-okinawa-holds-mass-protest-rally-against-us-base/',
 'http://english.ryukyushimpo.jp/2021/09/03/34020/',
 'http://english.ryukyushimpo.jp/2021/09/03/34020/',
 'http://english.ryukyushimpo.jp/2021/09/03/34020/',
 'http://english.ryukyushimpo.jp/2021/09/03/34020/'…]</pre>
			<p>Here, we are<a id="_idIndexMarker273"/> using list comprehension and <strong class="source-inline">BeautifulSoup</strong> to extract the value stored in <strong class="source-inline">href</strong> for each of our harvested links. I can see that there is some duplication, so we should remove that before eventually storing the results. Let’s see whether we lost any of the former <span class="No-Break">277 links:</span></p>
			<pre class="source-code">
len(urls)
…
277</pre>
			<p>Perfect! Let’s take one of them and see whether we can extract the raw text from the page, with all HTML removed. Let’s try this URL that I <span class="No-Break">have hand-selected:</span></p>
			<pre class="source-code">
url = 'http://english.ryukyushimpo.jp/2021/09/03/34020/'
data = requests.get(url).text
soup = BeautifulSoup(data, 'html.parser')
soup.get_text()
…
"\n\n\n\n\nRyukyu Shimpo – Okinawa, Japanese newspaper, local news  » Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHome\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\nTuesdaySeptember 07,2021Ryukyu Shimpo – Okinawa, Japanese newspaper, local news\n\n\n\n\n\n\r\nTOPICS:Special Feature: Okinawa holds mass protest rally against US base\n\n\n\n\n\n\n\n\n\n\n\n\nHirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"…"</pre>
			<p>Done! We<a id="_idIndexMarker274"/> have captured pretty clean and usable text from a web page! This can be automated to constantly harvest links and text from any website of interest. Now, we have what we need to get to the fun stuff in this chapter: extracting entities from text, and then using entities to build social networks. It should be obvious by now that some cleanup will be needed for any of the options that we have explored, so let’s just make short work of that now. To get this perfect, you need to do more than I am about to do, but let’s at least make this <span class="No-Break">somewhat usable:</span></p>
			<pre class="source-code">
text = soup.get_text()
text[0:500]
…
'\n\n\n\n\nRyukyu Shimpo – Okinawa, Japanese newspaper, local news  » Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHome\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\nTuesdaySeptember 07,2021Ryukyu Shimpo – Okinawa, Japanese newspaper, local news\n\n\n\n\n\n\r\nTOPICS:Special Feature: Okinawa holds mass protest rally against US base\n\n\n\n\n\n\n\n\n\n\n\n\nHirokazu Ueyonabaru returns home t'</pre>
			<p>The first thing that stands out to me is the amount of text formatting and special characters that exist in this text. We have a few options. First, we could convert all of the line spaces into empty spaces. Let’s see what that <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
text = text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ').replace('\xa0', ' ')
text
…
"     Ryukyu Shimpo – Okinawa, Japanese newspaper, local news  » Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"                                                            Home    Search           TuesdaySeptember 07,2021Ryukyu Shimpo – Okinawa, Japanese newspaper, local news        TOPICS:Special Feature: Okinawa holds mass protest rally against US base             Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, "the cheers gave me power"   Tokyo Paralympic bronze medalist Hirokazu Ueyonabaru receiving congratulations from some young supporters at Naha Airport on August 30…"</pre>
			<p>If you scroll <a id="_idIndexMarker275"/>further down the text, you may see that the story ends at “<strong class="source-inline">Go to Japanese</strong>,” so let’s remove that as well as <span class="No-Break">everything after:</span></p>
			<pre class="source-code">
cutoff = text.index('Go to Japanese')
cutoff
…
1984</pre>
			<p>This shows that the cutoff string starts at the 1,984<span class="superscript">th</span> character. Let’s keep everything up to <span class="No-Break">the cutoff:</span></p>
			<pre class="source-code">
text = text[0:cutoff]</pre>
			<p>This has successfully removed the footer junk, but there is still some header junk to deal with, so let’s see whether we can remove that. This part is always tricky, and every website is unique in some way, but let’s remove everything before the story as an exercise. Looking closer, I can see that the story starts at the second occurrence of “<strong class="source-inline">Hirokazu Uevonabaru</strong>.” Let’s capture everything from that point and beyond. We will be using <strong class="source-inline">.rindex()</strong> instead of <strong class="source-inline">.index()</strong>to capture the last occurrence. This code is too specific for real-world use, but hopefully, you can see that you have options for cleaning <span class="No-Break">dirty data:</span></p>
			<pre class="source-code">
cutoff = text.rindex('Hirokazu Ueyonabaru')
text = text[cutoff:]</pre>
			<p>If you are not familiar with Python, that might look a bit strange to you. We are keeping everything after the beginning of the last occurrence of “<strong class="source-inline">Hirokazu Ueyonabaru</strong>,” which is where the<a id="_idIndexMarker276"/> story starts. How does it <span class="No-Break">look now?</span></p>
			<pre class="source-code">
text
…
'Hirokazu Ueyonabaru, 50, – SMBC Nikko Securities Inc. – who won the bronze medal in both the 400-meter and 1,500-meter men's T52 wheelchair race, returned to Okinawa the evening of August 30, landing at Naha airport. Seeing the people who came out to meet him, he said "It was a sigh of relief (to win a medal)" beaming a big smile. That morning he contended with press conferences in Tokyo before heading home. He showed off his two bronze medals, holding them up from his neck in the airport lobby, saying "I could not have done it without all the cheers from everyone…'</pre>
			<p>That looks just about perfect! There is always more cleaning that can be done, but this is good enough! When you first get started with any new scraping, you will need to clean, inspect, clean, inspect, clean, and inspect – gradually, you will stop finding obvious things to remove. You don’t want to cut too much. Just get the text to be usable – we will do additional cleaning at <span class="No-Break">later steps.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor166"/>Choosing between libraries, APIs, and source data</h1>
			<p>As part of this demonstration, I showed several ways to pull useful data off of the internet. I showed that several libraries have ways to load data directly but that there are limitations to what they have available. NLTK only offered a small portion of the complete Gutenberg book archive, so we had to use the <strong class="source-inline">Requests</strong> library to load <em class="italic">The Metamorphosis</em>. I also demonstrated that <strong class="source-inline">Requests</strong> accompanied by <strong class="source-inline">BeautifulSoup</strong> can easily harvest links and <span class="No-Break">raw text.</span></p>
			<p>Python libraries can also make loading data very easy when those libraries have data loading functionality as part of their library, but you are limited by what those libraries make available. If you just want some data to play with, with minimal cleanup, this may be ideal, but there will still be cleanup. You will not get away from that when working <span class="No-Break">with text.</span></p>
			<p>Other web resources expose their own APIs, which makes it pretty simple to load data after sending a request to them. Twitter does this. You authenticate using your API key, and then you can pull whatever data you want. This is a happy middle-ground between Python libraries and <span class="No-Break">web scraping.</span></p>
			<p>Finally, web scraping opens up the entire web to you. If you can access a web page, you can scrape it and use any text and data that it has made available. You have flexibility with web scraping, but it is more difficult, and the results require <span class="No-Break">more cleanup.</span></p>
			<p>I tend to approach my own scraping and data enrichment projects by making considerations in the <span class="No-Break">following order:</span></p>
			<ul>
				<li>Is there a Python library that will make it easy for me to load the data <span class="No-Break">I want?</span></li>
				<li>No? OK, is there an API that I can use to pull the data that <span class="No-Break">I want?</span></li>
				<li>No? OK, can I just scrape it using <strong class="source-inline">BeautifuSoup</strong>? Yes? Game on. <span class="No-Break">Let’s dance.</span></li>
			</ul>
			<p>Start simple and scale out in terms of complexity only as needed. Starting simple means starting with the simplest approach – in this case, Python libraries. If libraries won’t help, add a little complexity by seeing whether an API is available to help and whether it is affordable. If one is not available, then web scraping is the solution that you need, and there is no way around it, but you will get the data that <span class="No-Break">you need.</span></p>
			<p>Now that we have text, we are going to move into NLP. Specifically, we will be using <strong class="bold">PoS tagging and NER</strong> as two distinct ways to extract entities (people and things) from <span class="No-Break">raw text.</span></p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor167"/>Using NLTK for PoS tagging</h1>
			<p>In this<a id="_idIndexMarker277"/> section, I will explain how to do what is called PoS tagging using the NLTK Python library. NLTK is an older Python NLP library, but it is still very useful. There are also pros and cons when comparing NLTK with other Python NLP libraries, such as <strong class="source-inline">spaCy</strong>, so it doesn’t hurt to understand the pros and <a id="_idIndexMarker278"/>cons of each. However, during my coding for this demonstration, I realized just how much easier <strong class="source-inline">spaCy</strong> has made both PoS tagging as well as NER, so, if you want the easiest approach, feel free to just skip ahead to <strong class="source-inline">spaCy</strong>. I am still fond of <strong class="source-inline">NLTK</strong>, and in some ways, the library still feels more natural to me than <strong class="source-inline">spaCy</strong>, but that may simply be due to years of use. Anyway, I’d like to demonstrate <strong class="source-inline">PoS tagging</strong> with <strong class="source-inline">NLTK</strong>, and then I will demonstrate both <strong class="source-inline">PoS tagging</strong> and NER with <strong class="source-inline">spaCy</strong> in the <span class="No-Break">next section.</span></p>
			<p>PoS tagging is a process that takes text tokens and identifies the PoS that the token belongs to. Just as a review, a token is a single word. A token might <span class="No-Break">be </span><span class="No-Break"><em class="italic">apples</em></span><span class="No-Break">.</span></p>
			<p>With NLP, tokens<a id="_idIndexMarker279"/> are useful, but bigrams are often even more useful, and they can improve the results for text classification, sentiment analysis, and even unsupervised learning. A bigram is essentially two tokens – for example, <span class="No-Break">t</span><span class="No-Break"><em class="italic">wo tokens</em></span><span class="No-Break">.</span></p>
			<p>Let’s not<a id="_idIndexMarker280"/> overthink this. It’s just two tokens. What do you think a trigram is? That’s right – three tokens. For instance, if filler words were removed from some text before the trigrams were captured, you could have one for <em class="italic">green </em><span class="No-Break"><em class="italic">eggs ham</em></span><span class="No-Break">.</span></p>
			<p>There are many different <strong class="source-inline">pos_tags</strong>, and you can see the list <span class="No-Break">here: </span><a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"><span class="No-Break">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</span></a><span class="No-Break">.</span></p>
			<p>For the work that we are doing, we will only use the NLP features that we need, and <strong class="source-inline">PoS tagging</strong> and NER are two different approaches that are useful for identifying entities (people and things) that are being described in text. In the mentioned list, the ones that we want are NNP and NNPS, and in most cases, we’ll find NNP, <span class="No-Break">not NNPS.</span></p>
			<p>To explain what we are trying to do, we are going to follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li value="1">Get some text to <span class="No-Break">work with.</span></li>
				<li>Split the text <span class="No-Break">into sentences.</span></li>
				<li>Split each sentence <span class="No-Break">into tokens.</span></li>
				<li>Identify the PoS tag for <span class="No-Break">each token.</span></li>
				<li>Extract each token that is a <span class="No-Break">proper noun.</span></li>
			</ol>
			<p>A proper noun is the name of a person, place, or thing. I have been saying that we want to extract entities and define entities as people, places, or things, so the NNP tag will identify exactly what <span class="No-Break">we want:</span></p>
			<ol>
				<li value="1">Let’s get to work and get some <span class="No-Break">text data!</span><pre class="source-code">
url = 'https://www.gutenberg.org/files/5200/5200-0.txt'</pre><pre class="source-code">
text = requests.get(url).text</pre></li>
			</ol>
			<p>We used this code previously to load the entire text from Kafka’s book <span class="No-Break"><em class="italic">The Metamorphosis</em></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">There is a lot<a id="_idIndexMarker281"/> of junk in the header of this file, but the story starts at “<strong class="source-inline">One morning</strong>,” so let’s remove everything<a id="_idIndexMarker282"/> from before that. Feel free to explore the <strong class="source-inline">text</strong> variable as we go. I am leaving out repeatedly showing the data to <span class="No-Break">save space:</span><pre class="source-code">
cutoff = text.index('One morning')</pre><pre class="source-code">
text = text[cutoff:]</pre></li>
			</ol>
			<p>Here, we have identified the starting point of the phrase, <strong class="source-inline">One morning</strong>, and then removed everything up to that point. It’s all just header junk that we <span class="No-Break">don’t need.</span></p>
			<ol>
				<li value="3">Next, if you look at the bottom of the text, you can see that the story ends at, <strong class="source-inline">*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS</strong>, so let’s cut from that point onward <span class="No-Break">as well:</span><pre class="source-code">
cutoff = text.rindex('*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***')</pre><pre class="source-code">
text = text[:cutoff]</pre></li>
			</ol>
			<p>Look closely at the cutoff and you will see that the cutoff is in a different position from that used for removing the header. I am essentially saying, “<em class="italic">Give me everything up to the cutoff</em>.” How does the ending text <span class="No-Break">look now?</span></p>
			<pre class="source-code">
text[-500:]
…
'talking, Mr. and Mrs.\r\nSamsa were struck, almost simultaneously, with the thought of how their\r\ndaughter was blossoming into a well built and beautiful young lady.\r\nThey became quieter. Just from each otherâ\x80\x99s glance and almost without\r\nknowing it they agreed that it would soon be time to find a good man\r\nfor her. And, as if in confirmation of their new dreams and good\r\nintentions, as soon as they reached their destination Grete was the\r\nfirst to get up and stretch out her young body.\r\n\r\n\r\n\r\n\r\n'
We have successfully removed both header and footer junk. I can see that there are a lot of line breaks, so let's remove all of those as well.
<strong class="bold">text = text.replace('\r', ' ').replace('\n', ' ')</strong>
<strong class="bold">text</strong>
<strong class="bold">…</strong>
'One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections…'</pre>
			<p>Not bad. That’s a <a id="_idIndexMarker283"/>noticeable<a id="_idIndexMarker284"/> improvement, and we are getting closer to clean text. Apostrophes are also being mangled, being shown as <strong class="source-inline">â\x80\x99</strong>, so let’s <span class="No-Break">replace those:</span></p>
			<pre class="source-code">
<strong class="bold">text = text.replace('â\x80\x99', '\'').replace('â\x80\x9c', '"').replace('â\x80\x9d', '""')\</strong>
<strong class="bold">.replace('â\x80\x94', ' ')</strong>
<strong class="bold">print(text)</strong>
<strong class="bold">…</strong>
One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin. He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off  any moment. His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.    "What's happened to me?"" he thought…</pre>
			<ol>
				<li value="4">That is<a id="_idIndexMarker285"/> about perfect, so let’s convert <a id="_idIndexMarker286"/>these steps into a <span class="No-Break">reusable function:</span><pre class="source-code">
def get_data():</pre><pre class="source-code">
    url = 'https://www.gutenberg.org/files/5200/5200-0.txt'</pre><pre class="source-code">
    text = requests.get(url).text</pre><pre class="source-code">
    # strip header junk</pre><pre class="source-code">
    cutoff = text.index('One morning')</pre><pre class="source-code">
    text = text[cutoff:]</pre><pre class="source-code">
    # strip footer junk</pre><pre class="source-code">
    cutoff = text.rindex('*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***')</pre><pre class="source-code">
    text = text[:cutoff]</pre><pre class="source-code">
    # pre-processing to clean the text</pre><pre class="source-code">
    text = text.replace('\r', ' ').replace('\n', ' ')</pre><pre class="source-code">
    text = text.replace('â\x80\x99', '\'').replace('â\x80\x9c', '"')\</pre><pre class="source-code">
     .replace('â\x80\x9d', '""').replace('â\x80\x94', ' ')</pre><pre class="source-code">
    return text</pre></li>
				<li>We<a id="_idIndexMarker287"/> should have fairly clean text<a id="_idIndexMarker288"/> after running <span class="No-Break">this function:</span><pre class="source-code">
text = get_data()</pre><pre class="source-code">
text</pre><pre class="source-code">
…</pre><pre class="source-code">
'One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin. He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off  any moment. His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.    "What\'s happened to me?"" he thought'</pre></li>
			</ol>
			<p>Outstanding! We are now ready for the <span class="No-Break">next steps.</span></p>
			<p>Before we move on, I want to explain one thing: if you do <strong class="source-inline">PoS tagging</strong> on the complete text of any book or article, then the text will be treated as one massive piece of text, and you lose your opportunity to understand how entities relate and interact. All you will end up with is a giant entity list, which isn’t very helpful for our needs, but it can be useful if you just want to extract entities from a given piece <span class="No-Break">of text.</span></p>
			<p>For our purposes, the first thing you need to do is split the text into sentences, chapters, or some other desirable bucket. For simplicity, let’s do this by sentences. This can easily be done using NLTK’s <span class="No-Break">sentence tokenizer:</span></p>
			<pre class="source-code">
from nltk.tokenize import sent_tokenize
sentences = sent_tokenize(text)
sentences[0:5]
…
['One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.',
 'He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.',
 'The bedding was hardly able to cover it and seemed ready to slide off  any moment.',
 'His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.',
 '"What\'s happened to me?""']</pre>
			<p><em class="italic">Beautiful</em>! We have a <a id="_idIndexMarker289"/>list of sentences to<a id="_idIndexMarker290"/> work with. Next, we want to take each of these sentences and extract any mentioned entities. We want the NNP-tagged tokens. This part takes a little work, so I will walk you through it. If we just feed the sentences to NLTK’s <strong class="source-inline">pos_tag tool</strong>, it will <span class="No-Break">misclassify everything:</span></p>
			<pre class="source-code">
import nltk
nltk.pos_tag(sentences)
[('One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.',
  'NNP'),
 ('He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.',
  'NNP'),
 ('The bedding was hardly able to cover it and seemed ready to slide off  any moment.',
  'NNP'),
 ('His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.',
  'NNP'),
 ('"What\'s happened to me?""', 'NNP'),
 ('he thought.', 'NN')…]</pre>
			<p>Good effort, but <a id="_idIndexMarker291"/>that is not what we need. What we need to do is go through each sentence and identify the PoS tags, so let’s do this manually for a <span class="No-Break">single sentence:</span></p>
			<pre class="source-code">
sentence = sentences[0]
sentence
…
'One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.'</pre>
			<p>First, we <a id="_idIndexMarker292"/>need to tokenize the sentence. There are many different tokenizers in NLTK, with strengths and weaknesses. I have gotten comfortable with the casual tokenizer, so I’ll just use that. The casual tokenizer does well with casual text, but there are several other tokenizers available to <span class="No-Break">choose from:</span></p>
			<pre class="source-code">
from nltk.tokenize import casual_tokenize
tokens = casual_tokenize(sentence)
tokens
…
['One',
 'morning',
 ',',
 'when',
 'Gregor',
 'Samsa',
 'woke',
 'from',
 'troubled',
 'dreams',
 ',',
 'he',
 'found',
 'himself',
 'transformed',
 'in',
 'his',
 'bed',
 'into',
 'a',
 'horrible',
 'vermin',
 '.']</pre>
			<p>Great. Now, for <a id="_idIndexMarker293"/>each token, we can find its<a id="_idIndexMarker294"/> <span class="No-Break">corresponding </span><span class="No-Break"><strong class="source-inline">pos_tag</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
nltk.pos_tag(tokens)
…
[('One', 'CD'),
 ('morning', 'NN'),
 (',', ','),
 ('when', 'WRB'),
 ('Gregor', 'NNP'),
 ('Samsa', 'NNP'),
 ('woke', 'VBD'),
 ('from', 'IN'),
 ('troubled', 'JJ'),
 ('dreams', 'NNS'),
 (',', ','),
 ('he', 'PRP'),
 ('found', 'VBD'),
 ('himself', 'PRP'),
 ('transformed', 'VBN'),
 ('in', 'IN'),
 ('his', 'PRP$'),
 ('bed', 'NN'),
 ('into', 'IN'),
 ('a', 'DT'),
 ('horrible', 'JJ'),
 ('vermin', 'NN'),
 ('.', '.')]</pre>
			<p>That’s also <a id="_idIndexMarker295"/>perfect! We are interested in<a id="_idIndexMarker296"/> extracting the NNPs. Can you see the two tokens that we want to extract? That’s right, it’s Gregor Samsa. Let’s loop through these PoS tags and extract the <span class="No-Break">NNP tokens:</span></p>
			<pre class="source-code">
entities = []
for row in nltk.pos_tag(tokens):
    token = row[0]
    tag = row[1]
    if tag == 'NNP':
        entities.append(token)
entities
…
['Gregor', 'Samsa']</pre>
			<p>This is what we need. NER would hopefully identify these two results as one person, but once this is thrown into a graph, it’s very easy to correct. Let’s convert this into a function that will take a sentence and return the NNP tokens – <span class="No-Break">the entities:</span></p>
			<pre class="source-code">
def extract_entities(sentence):
    entities = []
    tokens = casual_tokenize(sentence)
    for row in nltk.pos_tag(tokens):
        token = row[0]
        tag = row[1]
        if tag == 'NNP':
            entities.append(token)
    return entities</pre>
			<p>That looks good. Let’s give it <span class="No-Break">a shot!</span></p>
			<pre class="source-code">
extract_entities(sentence)
…
['Gregor', 'Samsa']</pre>
			<p>Now, let’s <a id="_idIndexMarker297"/>be bold and try this out against every <a id="_idIndexMarker298"/>sentence in the <span class="No-Break">entire book:</span></p>
			<pre class="source-code">
entities = [extract_entities(sentence) for sentence in sentences]
entities
[['Gregor', 'Samsa'],
 [],
 [],
 [],
 ["What's"],
 [],
 [],
 [],
 ['Samsa'],
 [],
 ['Gregor'],
 [],
 [],
 [],
 [],
 ['Oh', 'God', '"', '"', "I've"],
 [],
 [],
 ['Hell']]</pre>
			<p>Just to make <a id="_idIndexMarker299"/>analysis a bit easier, let’s do <a id="_idIndexMarker300"/>two things: first, let’s replace those empty lists with <strong class="source-inline">None</strong>. Second, let’s throw all of this into a <span class="No-Break"><strong class="source-inline">Pandas</strong></span><span class="No-Break"> DataFrame:</span></p>
			<pre class="source-code">
def extract_entities(sentence):
    entities = []
    tokens = casual_tokenize(sentence)
    for row in nltk.pos_tag(tokens):
        token = row[0]
        tag = row[1]
        if tag == 'NNP':
            entities.append(token)
    if len(entities) &gt; 0:
        return entities
    else:
        return None
entities = [extract_entities(sentence) for sentence in sentences]
entities
[['Gregor', 'Samsa'],
 None,
 None,
 None,
 ["What's"],
 None,
 None,
 None,
 ['Samsa'],
 None,
 ['Gregor'],
 None,
 None,
 None,
 None,
 ['Oh', 'God', '"', '"', "I've"],
 None,
 None,
 ['Hell']]
import pandas as pd
df = pd.DataFrame({'sentence':sentences, 'entities':entities})
df.head(10)</pre>
			<p>This<a id="_idIndexMarker301"/> will<a id="_idIndexMarker302"/> give us a DataFrame of sentences and entities extracted <span class="No-Break">from sentences:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17105_04_004.jpg" alt="Figure 4.4 – Pandas DataFrame of sentence entities" width="832" height="574"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Pandas DataFrame of sentence entities</p>
			<p>That’s a good start. We can see that “<strong class="bold">What’s</strong>” has been incorrectly flagged by NLTK, but it’s normal <a id="_idIndexMarker303"/>for junk to get through when dealing with text. That’ll get cleaned up soon. For <a id="_idIndexMarker304"/>now, we want to be able to build a social network using this book, so let’s grab every entity list that contains two or more entities. We need at least two to identify <span class="No-Break">a relationship:</span></p>
			<pre class="source-code">
df = df.dropna()
df = df[df['entities'].apply(len) &gt; 1]
entities = df['entities'].to_list()
entities
[['Gregor', 'Samsa'],
 ['Oh', 'God', '"', '"', "I've"],
 ['"', '"'],
 ["I'd", "I'd"],
 ["I've", "I'll"],
 ['First', "I've"],
 ['God', 'Heaven'],
 ['Gregor', '"', '"', '"'],
 ['Gregor', "I'm"],
 ['Gregor', 'Gregor', '"', '"', '"'],
 ['Gregor', "I'm"],
 ['"', '"', 'Gregor', '"'],
 ['Seven', '"'],
 ["That'll", '"', '"'],
 ["They're", '"', '"', 'Gregor'],
 ["Gregor's", 'Gregor'],
 ['Yes', '"', 'Gregor'],
 ['Gregor', '"', '"'],
 ['Mr', 'Samsa', '"', '"']]</pre>
			<p>This looks <a id="_idIndexMarker305"/>pretty good <a id="_idIndexMarker306"/>other than some punctuation sneaking in. Let’s revisit the previous code and disregard any non-alphabetical characters. That way, <strong class="source-inline">Gregor's</strong> will become <strong class="source-inline">Gregor</strong>, <strong class="source-inline">I'd</strong> will become <strong class="source-inline">I</strong>, and so forth. That’ll make cleanup a <span class="No-Break">lot easier:</span></p>
			<pre class="source-code">
def extract_entities(sentence):
    entities = []
    tokens = casual_tokenize(sentence)
    for row in nltk.pos_tag(tokens):
        token = row[0]
        tag = row[1]
        if tag == 'NNP':
            if "'" in token:
                cutoff = token.index('\'')
                token = token[:cutoff]
            entities.append(token)
    if len(entities) &gt; 0:
        return entities
    else:
        return None
entities = [extract_entities(sentence) for sentence in sentences]
entities
[['Gregor', 'Samsa'],
 None,
 None,
 None,
 ['What'],
 None,
 None,
 None,
 ['Samsa']…]</pre>
			<p>Let’s put this<a id="_idIndexMarker307"/> back into a DataFrame and <a id="_idIndexMarker308"/>repeat our steps to see whether the entity list is <span class="No-Break">looking better:</span></p>
			<pre class="source-code">
<strong class="bold">df = pd.DataFrame({'sentence':sentences, 'entities':entities})</strong>
<strong class="bold">df = df.dropna()</strong>
<strong class="bold">df = df[df['entities'].apply(len) &gt; 1]</strong>
<strong class="bold">entities = df['entities'].to_list()</strong>
<strong class="bold">entities</strong>
[['Gregor', 'Samsa'],
 ['Oh', 'God', '"', '"', 'I'],
 ['"', '"'],
 ['I', 'I'],
 ['I', 'I'],
 ['First', 'I'],
 ['God', 'Heaven'],
 ['Gregor', '"', '"', '"'],
 ['Gregor', 'I'],
 ['Gregor', 'Gregor', '"', '"', '"'],
 ['Gregor', 'I'],
 ['"', '"', 'Gregor', '"'],
 ['Seven', '"'],
 ['That', '"', '"'],
 ['They', '"', '"', 'Gregor'],
 ['Gregor', 'Gregor'],
 ['Yes', '"', 'Gregor'],
 ['Gregor', '"', '"'],
 ['Mr', 'Samsa', '"', '"']]</pre>
			<p>This is <a id="_idIndexMarker309"/>getting better, but some double quotes<a id="_idIndexMarker310"/> are still in the data. Let’s just remove any punctuation and <span class="No-Break">anything after:</span></p>
			<pre class="source-code">
<strong class="bold">from string import punctuation</strong>
<strong class="bold">def extract_entities(sentence):</strong>
<strong class="bold">    entities = []</strong>
<strong class="bold">    tokens = casual_tokenize(sentence)</strong>
<strong class="bold">    </strong><strong class="bold">for row in nltk.pos_tag(tokens):</strong>
<strong class="bold">        token = row[0]</strong>
<strong class="bold">        tag = row[1]</strong>
<strong class="bold">        if tag == 'NNP':</strong>
<strong class="bold">            for p in punctuation:</strong>
<strong class="bold">                if p in token:</strong>
<strong class="bold">                    cutoff = token.index(p)</strong>
<strong class="bold">                    token = token[:cutoff]</strong>
<strong class="bold">            if len(token) &gt; 1:</strong>
<strong class="bold">                entities.append(token)</strong>
<strong class="bold">    </strong><strong class="bold">if len(entities) &gt; 0:</strong>
<strong class="bold">        return entities</strong>
<strong class="bold">    else:</strong>
<strong class="bold">        return None</strong>
<strong class="bold">entities = [extract_entities(sentence) for sentence in sentences]</strong>
<strong class="bold">df = pd.DataFrame({'sentence':sentences, 'entities':entities})</strong>
<strong class="bold">df = df.dropna()</strong>
<strong class="bold">df = df[df['entities'].apply(len) &gt; 1]</strong>
<strong class="bold">entities = df['entities'].to_list()</strong>
<strong class="bold">entities</strong>
<strong class="bold">…</strong>
[['Gregor', 'Samsa'],
 ['Oh', 'God'],
 ['God', 'Heaven'],
 ['Gregor', 'Gregor'],
 ['They', 'Gregor'],
 ['Gregor', 'Gregor'],
 ['Yes', 'Gregor'],
 ['Mr', 'Samsa'],
 ['He', 'Gregor'],
 ['Well', 'Mrs', 'Samsa'],
 ['No', 'Gregor'],
 ['Mr', 'Samsa'],
 ['Mr', 'Samsa'],
 ['Sir', 'Gregor'],
 ['Oh', 'God']]</pre>
			<p>This is good <a id="_idIndexMarker311"/>enough! We could add a little more<a id="_idIndexMarker312"/> logic to prevent the same token from appearing twice in a row, but we can very easily remove those from a network, so let’s refactor our code and <span class="No-Break">move on:</span></p>
			<pre class="source-code">
<strong class="bold">def get_book_entities():</strong>
<strong class="bold">    text = get_data()</strong>
<strong class="bold">    sentences = sent_tokenize(text)</strong>
<strong class="bold">    entities = [extract_entities(sentence) for sentence in sentences]</strong>
<strong class="bold">    df = pd.DataFrame({'sentence':sentences, 'entities':entities})</strong>
<strong class="bold">    df = df.dropna()</strong>
<strong class="bold">    df = df[df['entities'].apply(len) &gt; 1]</strong>
<strong class="bold">    entities = df['entities'].to_list()</strong>
<strong class="bold">    return entities</strong>
entities = get_book_entities()
entities[0:5]
…
[['Gregor', 'Samsa'],
 ['Oh', 'God'],
 ['God', 'Heaven'],
 ['Gregor', 'Gregor'],
 ['They', 'Gregor']]</pre>
			<p>This is <a id="_idIndexMarker313"/>excellent. At this point, no matter whether we do PoS tagging or NER, we <a id="_idIndexMarker314"/>want an entity list, and this is close enough. Next, we will do the same using spaCy, and you should be able to see how spaCy is much easier in some regards. However, there is more setup involved, as you need to install a language model to work with spaCy. There are pros and cons <span class="No-Break">to everything.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor168"/>Using spaCy for PoS tagging and NER</h1>
			<p>In this section, I am going<a id="_idIndexMarker315"/> to explain how to do what we have just done with NLTK, but this time using spaCy. I will also show you how to use NER as an often-superior <a id="_idIndexMarker316"/>alternative to PoS tagging for identifying and extracting entities. Before I started working on this chapter, I primarily used NLTK’s <strong class="source-inline">PoS tagging</strong> as the heart of my entity extraction, but since writing the code for this section and exploring a bit more, I have come to realize that spaCy has improved quite a bit, so I do think that what I am about to show you in this section is superior to what I previously did with NLTK. I do think that it is helpful to explain the usefulness of NLTK. Learn both and use what works best for you. But for entity extraction, I believe spaCy is superior in terms of ease of use and speed <span class="No-Break">of delivery.</span></p>
			<p>Previously, I wrote a function to load Franz Kafka’s book <em class="italic">The Metamorphosis</em>, so we will use that loader here as well, as it has no dependence on either NLTK or spaCy, and it can be easily modified to load any book from the Project <span class="No-Break">Gutenberg archive.</span></p>
			<p>To do anything with spaCy, the first thing you need to do is load the spaCy language model of choice. Before we can load it, we must first install it. You can do so by running the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
python -m spacy download en_core_web_md</pre>
			<p>There are <a id="_idIndexMarker317"/>several different models available, but <a id="_idIndexMarker318"/>the three that I use for English text are small, medium, and large. <strong class="source-inline">md</strong> stands for medium. You could swap that out for <strong class="source-inline">sm</strong> or <strong class="source-inline">lg</strong> to get the small or large models, respectively. You can find out more about spaCy’s models <span class="No-Break">here: </span><a href="https://spacy.io/usage/models"><span class="No-Break">https://spacy.io/usage/models</span></a><span class="No-Break">.</span></p>
			<p>Once the model has been installed, we can load it into our <span class="No-Break">Python scripts:</span></p>
			<pre class="source-code">
import spacy
nlp = spacy.load("en_core_web_md")</pre>
			<p>As mentioned previously, you could swap that <strong class="source-inline">md</strong> for <strong class="source-inline">sm</strong> or <strong class="source-inline">lg</strong>, depending on what model you want to use. The larger model requires more storage and memory for use. The smaller model requires less. Pick the one that works well enough for what you are doing. You may not need the large model. The medium and small ones work well, and the difference between models is <span class="No-Break">often unnoticeable.</span></p>
			<p>Next, we need some text, so let’s reuse the function that we <span class="No-Break">previously wrote:</span></p>
			<pre class="source-code">
<strong class="bold">def get_data():</strong>
<strong class="bold">    url = 'https://www.gutenberg.org/files/5200/5200-0.txt'</strong>
<strong class="bold">    </strong><strong class="bold">text = requests.get(url).text</strong>
<strong class="bold">    # strip header junk</strong>
<strong class="bold">    cutoff = text.index('One morning')</strong>
<strong class="bold">    text = text[cutoff:]</strong>
<strong class="bold">    # strip footer junk</strong>
<strong class="bold">    cutoff = text.rindex('*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***')</strong>
<strong class="bold">    text = text[:cutoff]</strong>
<strong class="bold">    # pre-processing to clean the text</strong>
<strong class="bold">    text = text.replace('\r', ' ').replace('\n', ' ')</strong>
<strong class="bold">    text = text.replace('â\x80\x99', '\'').replace('â\x80\x9c', '"').replace('â\x80\x9d', '""').replace('â\x80\x94', ' ')</strong>
<strong class="bold">    </strong><strong class="bold">return text</strong>
<strong class="bold">That looks good. We are loading The Metamorphosis, cleaning out header and footer junk, and then returning text that is clean enough for our purposes. Just to lay eyes on the text, let's call our function and inspect the returned text.</strong>
<strong class="bold">text = get_data()</strong>
<strong class="bold">text[0:279]</strong>
<strong class="bold">…</strong>
'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.'</pre>
			<p>There are a <a id="_idIndexMarker319"/>few extra spaces in the middle of sentences, but that won’t present any problem for us at all. Tokenization will clean that up <a id="_idIndexMarker320"/>without any additional effort on our part. We will get to that shortly. First, as done with NLTK, we need to split the text into sentences so that we can build a network based on the entities that are uncovered in each sentence. That is much easier to do using spaCy rather than NLTK, and here is how to <span class="No-Break">do it:</span></p>
			<pre class="source-code">
doc = nlp(text)
sentences = list(doc.sents)</pre>
			<p>The first line feeds the full text of <em class="italic">The Metamorphosis</em> into spaCy and uses our language model of choice, while the second line extracts the sentences that are in the text. Now, we<a id="_idIndexMarker321"/> should have a Python list of <a id="_idIndexMarker322"/>sentences. Let’s inspect the first six sentences in <span class="No-Break">our list:</span></p>
			<pre class="source-code">
for s in sentences[0:6]:
    print(s)
    print()
…
One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.
He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.
The bedding was hardly able to cover it and seemed ready to slide off  any moment.
His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.
"What's happened to me?"
" he thought.</pre>
			<p>Six probably seems like a strange number of sentences to inspect, but I wanted to show you something. Take a look at the last two sentences. SpaCy has successfully extracted the main character’s inner dialog as a standalone sentence, as well as created a separate sentence to complete the surrounding sentence. For our entity extraction, it wouldn’t present any problem at all if those sentences were combined, but I like this. It’s not a bug, it’s a feature, as we software engineers like <span class="No-Break">to say.</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor169"/>SpaCy PoS tagging</h2>
			<p>Now that we <a id="_idIndexMarker323"/>have our sentences, let’s use spaCy’s PoS tagging as pre-processing for <span class="No-Break">entity extraction:</span></p>
			<pre class="source-code">
for token in sentences[0]:
    print('{}: {}'.format(token.text, token.tag_))
…
One: CD
morning: NN
,: ,
when: WRB
Gregor: NNP
Samsa: NNP
woke: VBD
from: IN
troubled: JJ
dreams: NNS
,: ,
he: PRP
found: VBD
 :
himself: PRP
transformed: VBD
in: IN
his: PRP$
bed: NN
into: IN
a: DT
horrible: JJ
vermin: NN
.: .</pre>
			<p>Nice. What we<a id="_idIndexMarker324"/> want are the NNPs as these are proper nouns. You can see this if you use <strong class="source-inline">pos_</strong> instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">tag_</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
for token in sentences[0]:
    print('{}: {}'.format(token.text, token.pos_))
…
One: NUM
morning: NOUN
,: PUNCT
when: ADV
Gregor: PROPN
Samsa: PROPN
woke: VERB
from: ADP
troubled: ADJ
dreams: NOUN
,: PUNCT
he: PRON
found: VERB
 : SPACE
himself: PRON
transformed: VERB
in: ADP
his: ADJ
bed: NOUN
into: ADP
a: DET
horrible: ADJ
vermin: NOUN
.: PUNCT</pre>
			<p>Let’s add a <a id="_idIndexMarker325"/>little logic for extracting them. We need to do two things – we need a list to store the results, and we need some logic to extract <span class="No-Break">the NNPs:</span></p>
			<pre class="source-code">
entities = []
for token in sentences[0]:
    if token.tag_ == 'NNP':
        entities.append(token.text)
entities
…
['Gregor', 'Samsa']
Perfect! But this is only working on a single sentence. Let's do the same for all sentences.
entities = []
for sentence in sentences:
    sentence_entities = []
    for token in sentence:
        if token.tag_ == 'NNP':
            sentence_entities.append(token.text)
    entities.append(sentence_entities)
entities[0:10]
…
[['Gregor', 'Samsa'], [], [], [], [], [], [], [], [], ['Samsa']]</pre>
			<p>For NLTK, we <a id="_idIndexMarker326"/>created a function that would extract entities for a given sentence, but this way, I just did everything in one go, and it was quite simple. Let’s convert this into a function so that we can easily use this for other future work. Let’s also prevent empty lists from being returned inside the entity list, as we have no use <span class="No-Break">for those:</span></p>
			<pre class="source-code">
def extract_entities(text):
    doc = nlp(text)
    sentences = list(doc.sents)
    entities = []
    for sentence in sentences:
        sentence_entities = []
        for token in sentence:
            if token.tag_ == 'NNP':
                sentence_entities.append(token.text)
        if len(sentence_entities) &gt; 0:
            entities.append(sentence_entities)
    return entities</pre>
			<p>We should now have a clean entity list, with no empty inner <span class="No-Break">lists included:</span></p>
			<pre class="source-code">
extract_entities(text)
…
[['Gregor', 'Samsa'],
 ['Samsa'],
 ['Gregor'],
 ['God'],
 ['Travelling'],
 ['God', 'Heaven'],
 ['Gregor'],
 ['Gregor'],
 ['Gregor'],
 ['Gregor'],
 ['Gregor']…]</pre>
			<p>This looks a lot better<a id="_idIndexMarker327"/> than NLTK’s results, and with fewer steps. This is simple and elegant. I didn’t need to use <strong class="source-inline">Pandas</strong> for anything, drop empty rows, or clean out any punctuation that somehow slipped in. We can use this function on any text we have, after cleaning. You can use it before cleaning, but you’ll end up getting a bunch of junk entities, especially if you use it against scraped <span class="No-Break">web data.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor170"/>SpaCy NER</h2>
			<p>SpaCy’s NER is <a id="_idIndexMarker328"/>equally easy and simple. The difference between <strong class="source-inline">PoS tagging</strong> and NER is that NER goes a step further and identifies people, places, things, and more. For a detailed description of spaCy’s linguistic features, I heartily recommend Duygu Altinok’s book <em class="italic">Mastering spaCy</em>. To be concise, spaCy labels tokens as one of <em class="italic">18</em> different kinds of entities. In my opinion, that is a bit excessive, as <strong class="source-inline">MONEY</strong> is not an entity, but I just take what I want. Please check spaCy for the full list of entity types. What we want are entities that are labeled<a id="_idIndexMarker329"/> as <strong class="source-inline">PERSON</strong>, <strong class="source-inline">ORG</strong>, or <strong class="source-inline">GPE</strong>. <strong class="source-inline">ORG</strong> stands for organization, and <strong class="source-inline">GPE</strong> contains countries, cities, <span class="No-Break">and states.</span></p>
			<p>Let’s loop through all the tokens in the first sentence and see how this looks <span class="No-Break">in practice:</span></p>
			<pre class="source-code">
for token in sentences[0]:
    print('{}: {}'.format(token.text, token.ent_type_))
…
One: TIME
morning: TIME
,:
when:
Gregor: PERSON
Samsa: PERSON
woke:
from:
troubled:
dreams:
,:
he:
found:
 :
himself:
transformed:
in:
his:
bed:
into:
a:
horrible:
vermin:
.:</pre>
			<p>This works, but there is a slight problem: we want Gregor Samsa to appear as one entity, not as two. What we need to do is create a new spaCy doc and then loop through the doc’s <strong class="source-inline">ents</strong> rather than the individual tokens. In that regard, the NER approach is slightly different from <span class="No-Break"><strong class="source-inline">PoS tagging</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
doc = nlp(sentences[0].text)
for ent in doc.ents:
    print('{}: {}'.format(ent, ent.label_))
…
One morning: TIME
Gregor Samsa: PERSON</pre>
			<p>Perfect! Let’s do<a id="_idIndexMarker330"/> a few things: we will redo our previous entity extraction function, but this time using NER rather than <strong class="source-inline">PoS tagging</strong>, and then limit our entities to <strong class="source-inline">PERSON</strong>, <strong class="source-inline">ORG</strong>, and <strong class="source-inline">GPE</strong>. Please note that I am only adding entities if there is more than one in a sentence. We are looking to identify relationships between people, and you need at least two people to have <span class="No-Break">a relationship:</span></p>
			<pre class="source-code">
def extract_entities(text):
    doc = nlp(text)
    sentences = list(doc.sents)
    entities = []
    for sentence in sentences:
        sentence_entities = []
        sent_doc = nlp(sentence.text)
        for ent in sent_doc.ents:
            if ent.label_ in ['PERSON', 'ORG', 'GPE']:
                entity = ent.text.strip()
                if "'s" in entity:
                    cutoff = entity.index("'s")
                    entity = entity[:cutoff]
                if entity != '':
                    sentence_entities.append(entity)
        sentence_entities = list(set(sentence_entities))
        if len(sentence_entities) &gt; 1:
            entities.append(sentence_entities)
    return entities</pre>
			<p>I added a<a id="_idIndexMarker331"/> bit of code to remove any whitespace that has snuck in and also to remove duplicates from each <strong class="source-inline">sentence_entity</strong> list. I also removed any <strong class="source-inline">'s</strong> characters that appeared after a name – for example, <strong class="source-inline">Gregor's</strong> – so that it’d show up as <strong class="source-inline">Gregor</strong>. I could have cleaned this up in network cleanup, but this is a nice optimization. Let’s see how our results look. I named the entity list <strong class="source-inline">morph_entities</strong> for <strong class="source-inline">metaMORPHosis</strong> entities. I wanted a descriptive name, and this is the best I could come <span class="No-Break">up with:</span></p>
			<pre class="source-code">
morph_entities = extract_entities(text)
morph_entities
…
[['Gregor', 'Grete'],
 ['Gregor', 'Grete'],
 ['Grete', 'Gregor'],
 ['Gregor', 'Grete'],
 ['Grete', 'Gregor'],
 ['Grete', 'Gregor'],
 ['Grete', 'Gregor'],
 ['Samsa', 'Gregor'],
 ['Samsa', 'Gregor'],
 ['Samsa', 'Grete'],
 ['Samsa', 'Grete'],
 ['Samsa', 'Grete']]</pre>
			<p>That looks great! Wow, I haven’t read <em class="italic">The Metamorphosis</em> in many years and forgot how few characters were in <span class="No-Break">the story!</span></p>
			<p>SpaCy’s NER is<a id="_idIndexMarker332"/> done using a pre-trained deep learning model, and machine learning is never perfect. Please keep that in mind. There is always some cleanup. SpaCy allows you to customize their language models for your documents, which is useful if you work within a specialized domain, but I prefer to use spaCy’s models as general-purpose tools, as I deal with a huge variety of different text. I don’t want to customize spaCy for tweets, literature, disinformation, and news. I would prefer to just use it as is and clean up as needed. That has worked very well <span class="No-Break">for me.</span></p>
			<p>You should be able to see that there is a lot of duplication. Apparently, in <em class="italic">The Metamorphosis</em>, a lot of time is spent talking about Gregor. We will be able to remove those duplicates with a single line of <strong class="source-inline">NetworkX</strong> code later, so I’ll just leave them in rather than tweaking the function. Good enough is good enough. If you are working with massive amounts of data and paying for cloud storage, you should probably <span class="No-Break">fix inefficiencies.</span></p>
			<p>For the rest of this chapter, I’m going to use the NER results as our network data. We could just as easily use the <strong class="source-inline">pos_tag</strong> entities, but this is better, as NER can combine first name and last name. In our current entities, none of those came through, but they will with other text. This is just how <em class="italic">The Metamorphosis</em> was written. We’ll just clean that up as part of the <span class="No-Break">network creation.</span></p>
			<p>Just for a sanity check, let’s check the entities from <em class="italic">Alice’s Adventures </em><span class="No-Break"><em class="italic">in Wonderland</em></span><span class="No-Break">!</span></p>
			<pre class="source-code">
def get_data():
    url = 'https://www.gutenberg.org/files/11/11-0.txt'
    text = requests.get(url).text
    # strip header junk
    cutoff = text.index('Alice was beginning')
    text = text[cutoff:]
    # strip footer junk
    cutoff = text.rindex('THE END')
    text = text[:cutoff]
    # pre-processing to clean the text
    text = text.replace('\r', ' ').replace('\n', ' ')
    text = text.replace('â\x80\x99', '\'').replace('â\x80\x9c', '"').replace('â\x80\x9d', '""').replace('â\x80\x94', ' ')
    return text
text = get_data()
text[0:310]
…
'Alice was beginning to get very tired of sitting by her sister on the  bank, and of having nothing to do: once or twice she had peeped into  the book her sister was reading, but it had no pictures or  conversations in it, "and what is the use of a book,"" thought Alice  "without pictures or conversations?""  '</pre>
			<p>I agree with <span class="No-Break">you, Alice.</span></p>
			<p>I have tweaked<a id="_idIndexMarker333"/> the loading function to load the book <em class="italic">Alice’s Adventures in Wonderland</em> and chop off any header or footer text. That’s actually kind of funny. OFF WITH THEIR HEAD(er)S! Let’s try extracting entities. I expect this to be a bit messy, as we are working with fantasy characters, but let’s see what happens. Something by Jane Austen might give better results. <span class="No-Break">We’ll see!</span></p>
			<pre class="source-code">
alice_entities = extract_entities(text)
alice_entities[0:10]
…
[['Alice', 'Rabbit'],
 ['Alice', 'Longitude'],
 ['New Zealand', "Ma'am", 'Australia'],
 ['Fender', 'Alice'],
 ['Alice', 'Rabbit'],
 ['Mabel', 'Ada'],
 ['Rome', 'Paris', 'London'],
 ['Improve', 'Nile'],
 ['Alice', 'Mabel'],
 ['Alice', 'William the Conqueror']]</pre>
			<p>That is much better than I expected, but some junk has snuck in. We’ll use both of these entity lists for network creation and visualization. This is a good foundation for our next steps. Now<a id="_idIndexMarker334"/> that we have some pretty useful-looking entities, let’s work toward creating a Pandas DataFrame that we can load into a NetworkX graph! That’s what’s needed to convert an entity list into an actual <span class="No-Break">social network.</span></p>
			<p>That concludes our demonstration on using spaCy for both PoS tagging as well as NER. I hope you can see that although there was one additional dependency (the language model), the process of entity extraction was much simpler. Now, it is time to move on to what I think is the most exciting part: converting entity lists into network data, which is then used to create a social network, which we can visualize <span class="No-Break">and investigate.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor171"/>Converting entity lists into network data</h1>
			<p>Now that we<a id="_idIndexMarker335"/> have pretty clean entity data, it is time to convert it into a Pandas DataFrame that we can easily load into NetworkX for creating an actual social network graph. There’s a bit to unpack in that sentence, but this is <span class="No-Break">our workflow:</span></p>
			<ol>
				<li value="1"><span class="No-Break">Load text.</span></li>
				<li><span class="No-Break">Extract entities.</span></li>
				<li>Create <span class="No-Break">network data.</span></li>
				<li>Create a graph using <span class="No-Break">network data.</span></li>
				<li>Analyze <span class="No-Break">the graph.</span></li>
			</ol>
			<p>Again, I use the<a id="_idIndexMarker336"/> terms graph and network interchangeably. That does cause confusion, but I did not come up with the names. I prefer to say “network,” but then people think I am talking about computer networks, so then I have to remind them that I am talking about graphs, and they then think I am talking about bar charts. You just can’t win when it comes to explaining graphs and networks to those who are not familiar, and even I get confused when people start talking about networks and graphs. Do you mean nodes and edges, or do you mean TCP/IP and bar charts? <span class="No-Break">Oh well.</span></p>
			<p>For this next part, we do have choices in how we implement this, but I will explain my typical method. Look at the entities from <em class="italic">Alice’s Adventures </em><span class="No-Break"><em class="italic">in Wonderland</em></span><span class="No-Break">:</span></p>
			<pre class="source-code">
alice_entities[0:10]
…
[['Alice', 'Rabbit'],
 ['Alice', 'Longitude'],
 ['New Zealand', "Ma'am", 'Australia'],
 ['Fender', 'Alice'],
 ['Alice', 'Rabbit'],
 ['Mabel', 'Ada'],
 ['Rome', 'Paris', 'London'],
 ['Improve', 'Nile'],
 ['Alice', 'Mabel'],
 ['Alice', 'William the Conqueror']]</pre>
			<p>In most of these, there are only two entities in each inner list, but sometimes, there are three or more. What I usually do is consider the first entity as the source and any additional entities as targets. What does that look like in a sentence? Let’s take this sentence: “<em class="italic">Jack and Jill went up the hill to say hi to their friend Mark.</em>” If we converted that into entities, we would have <span class="No-Break">this list:</span></p>
			<pre class="source-code">
['Jack', 'Jill', 'Mark']</pre>
			<p>To implement<a id="_idIndexMarker337"/> my approach, I will take the first element of my list and add it to my sources list, and then take everything after the first element and add that to my target list. Here is what that looks like in code, but using entities <span class="No-Break">from </span><span class="No-Break"><em class="italic">Alice</em></span><span class="No-Break">:</span></p>
			<pre class="source-code">
final_sources = []
final_targets = []
for row in alice_entities:
    source = row[0]
    targets = row[1:]
    for target in targets:
        final_sources.append(source)
        final_targets.append(target)</pre>
			<p>Take a close look at the two lines that capture both <strong class="source-inline">source</strong> and <strong class="source-inline">targets</strong>. <strong class="source-inline">source</strong> is the first element of each entity list, and <strong class="source-inline">targets</strong> is everything after the first element of each entity list. Then, for each target, I add the source and target to <strong class="source-inline">final_sources</strong> and <strong class="source-inline">final_targets</strong>. I loop through <strong class="source-inline">targets</strong> because there can be one or more of them. There will never be more than one <strong class="source-inline">source</strong>, as it is the first element. This is important to understand because this procedure is crucial for how relationships are shown in the resulting social network. We could have used an alternative approach of linking each entity to the other, but I prefer my shown approach. Later lists may bridge any gaps if there is evidence of those relationships. How do our final <span class="No-Break">sources look?</span></p>
			<pre class="source-code">
final_sources[0:5]
…
['Alice', 'Alice', 'New Zealand', 'New Zealand', 'Fender']
Nice. How about our final targets?
final_targets[0:5]
…
['Rabbit', 'Longitude', "Ma'am", 'Australia', 'Alice']</pre>
			<p>Both look great. Remember, we used NER to capture people, places, and things, so this looks fine. Later, I will very easily drop a few of these sources and targets directly from the social network. This is good enough <span class="No-Break">for now.</span></p>
			<p>The <a id="_idIndexMarker338"/>approach of taking the first element and linking it to targets is something that I still regularly consider. Another approach would be to take every entity that appears in the same sentence and link them together. I prefer my approach, but you should consider <span class="No-Break">both options.</span></p>
			<p>The first entity interacts with other entities from the same sentence, but it is not always the case that all entities interact with each other. For instance, look at <span class="No-Break">this sentence:</span></p>
			<p>“<em class="italic">John went to see his good friend Aaron, and then he went to the park </em><span class="No-Break"><em class="italic">with Jake.</em></span><span class="No-Break">”</span></p>
			<p>This is what the entity list would <span class="No-Break">look like:</span></p>
			<pre class="source-code">
['John', 'Aaron', 'Jake']</pre>
			<p>In this example, Aaron may know Jake, but we can’t tell for certain based on this sentence. The hope is that if there is a relationship, that will be picked up eventually. Maybe in another sentence, such as <span class="No-Break">this one:</span></p>
			<p>“<em class="italic">Aaron and Jake went ice skating and then ate </em><span class="No-Break"><em class="italic">some pizza.</em></span><span class="No-Break">”</span></p>
			<p>After that sentence, there will be a definite connection. My preferred approach requires further evidence before <span class="No-Break">connecting entities.</span></p>
			<p>We now have code to take an entity list and create two lists: <strong class="source-inline">final_sources</strong> and <strong class="source-inline">final_targets</strong>, but this isn’t practical for feeding to NetworkX to create a graph. Let’s do two more things: use these two lists to create a Pandas DataFrame, and then create a reusable function that takes any entity list and returns <span class="No-Break">this DataFrame:</span></p>
			<pre class="source-code">
def get_network_data(entities):
    final_sources = []
    final_targets = []
    for row in entities:
        source = row[0]
        targets = row[1:]
        for target in targets:
            final_sources.append(source)
            final_targets.append(target)
    df = pd.DataFrame({'source':final_sources, 'target':final_targets})
    return df</pre>
			<p>That <a id="_idIndexMarker339"/>looks great. Let’s see it <span class="No-Break">in action!</span></p>
			<pre class="source-code">
alice_network_df = get_network_data(alice_entities)
alice_network_df.head()</pre>
			<p>This will display a DataFrame of network data consisting of source and target nodes. This is called an <span class="No-Break">edge list:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17105_04_005.jpg" alt="Figure 4.5 – Pandas DataFrame of Alice in Wonderland entity relationships" width="336" height="297"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Pandas DataFrame of Alice in Wonderland entity relationships</p>
			<p>Great. How does it do with our entities from <span class="No-Break"><em class="italic">The Metamorphosis</em></span><span class="No-Break">?</span></p>
			<pre class="source-code">
morph_network_df = get_network_data(morph_entities)
morph_network_df.head()</pre>
			<p>This will display the network edge list for <span class="No-Break"><em class="italic">The Metamorphosis</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17105_04_006.jpg" alt="Figure 4.6 – Pandas DataFrame of The Metamorphosis entity relationships" width="254" height="310"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Pandas DataFrame of The Metamorphosis entity relationships</p>
			<p>Perfect, and the function <span class="No-Break">is reusable!</span></p>
			<p>We are <a id="_idIndexMarker340"/>now ready to convert both of these into actual NetworkX graphs. This is where things get interesting, in my opinion. Everything we did previously was just pre-processing. Now, we get to play with networks and specifically social network analysis! After this chapter, we will primarily be learning about social network analysis and network science. There are some areas of NLP that I blew past, such as lemmatization and stemming, but I purposefully did so because they are less relevant to extracting entities than PoS tagging and NER. I recommend that you check out Duygu Altinok’s book <em class="italic">Mastering spaCy</em> if you want to go deeper into NLP. This is as far as we will go with NLP in this book because it is all <span class="No-Break">we need.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor172"/>Converting network data into networks</h1>
			<p>It is time to take <a id="_idIndexMarker341"/>our created network data and create two graphs, one for <em class="italic">Alice’s Adventures in Wonderland</em>, and another for <em class="italic">The Metamorphosis</em>. We aren’t going to dive deep into network analysis yet, as that is for later chapters. But let’s see how they look and see what <span class="No-Break">insights emerge.</span></p>
			<p>First, we need to import the NetworkX library, and <a id="_idIndexMarker342"/>then we need to create our graphs. This is extremely easy to do because we have created Pandas DataFrames, which NetworkX will use. This is the easiest way I have found of <span class="No-Break">creating graphs.</span></p>
			<p>First, if you haven’t done so yet, you need to install NetworkX. You can do so with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pip install networkx</pre>
			<p>Now that we have installed <a id="_idIndexMarker343"/>NetworkX, let’s create our <span class="No-Break">two networks:</span></p>
			<pre class="source-code">
import networkx as nx
G_alice = nx.from_pandas_edgelist(alice_network_df)
G_morph = nx.from_pandas_edgelist(morph_network_df)</pre>
			<p>It’s that easy. We <a id="_idIndexMarker344"/>have already done the difficult work in our text pre-processing. Did it work? Let’s peek into <span class="No-Break">each graph:</span></p>
			<pre class="source-code">
nx.info(G_alice)
…
'Graph with 68 nodes and 71 edges'
…
nx.info(G_morph)
…
'Graph with 3 nodes and 3 edges'</pre>
			<p>Nice! Already, we are gaining insights into the differences between both social networks. In a graph, a node is just a thing that has relationships with other nodes, typically. Nodes without any relationships are called isolates, but due to the way our graphs have been constructed, there will be no isolates. It’s not possible, as we looked for sentences with two or more entities. Try to take a mental picture of those two entities as dots with a line between them. That’s literally what a graph/network visualization looks like, except that there are typically many dots and many lines. The relationship that exists between two nodes is called an edge. You will need to understand the difference between nodes and edges to work <span class="No-Break">on graphs.</span></p>
			<p>Looking at the summary information about the <em class="italic">Alice</em> graph, we can see that there are 68 nodes (characters) and 71 edges (relationships between <span class="No-Break">those characters).</span></p>
			<p>Looking at the summary information about the network from <em class="italic">The Metamorphosis</em>, we can see that there are only three nodes (characters) and three edges (relationships between those characters. When visualized, this is going to be a really basic network to look at, so I am glad that we did <em class="italic">Alice</em> <span class="No-Break">as well.</span></p>
			<p>There are many other useful metrics and summaries tucked away inside NetworkX, and we will discuss those when we go over centralities, shortest paths, and other social network analysis and network <span class="No-Break">science topics.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor173"/>Doing a network visualization spot check</h1>
			<p>Let’s visualize these networks, take a brief look, and then complete <span class="No-Break">this chapter.</span></p>
			<p>Here are two <a id="_idIndexMarker345"/>visualization functions that I frequently use. In my opinion, <strong class="bold">sknetwork</strong> is<a id="_idIndexMarker346"/> superior to NetworkX for network visualization. The visualizations look better, and they render faster. The downside is that there is less customizability, so if you want flexibility, you will need to use NetworkX’s native visualizations. Since finding <strong class="source-inline">sknetwork</strong>, I have not looked back to NetworkX <span class="No-Break">for visualization.</span></p>
			<p>The first function converts a NetworkX graph into an adjacency matrix, which sknetwork uses to calculate <strong class="source-inline">PageRank</strong> (an importance score) and then to render the network as an SVG image. The second function uses the first function, but the goal is to visualize an <strong class="source-inline">ego_graph</strong>, which will be described later. In an ego graph, you explore the relationships that exist around a single node. The first function is <span class="No-Break">more general-purpose.</span></p>
			<p>Enough talk. This will be more understandable when you see <span class="No-Break">the results:</span></p>
			<pre class="source-code">
def draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5):
    import numpy as np
    from IPython.display import SVG
    from sknetwork.visualization import svg_graph
    from sknetwork.data import Bunch
    from sknetwork.ranking import PageRank
    adjacency = nx.to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight='weight', format='csr')
    names = np.array(list(G.nodes()))
    graph = Bunch()
    graph.adjacency = adjacency
    graph.names = np.array(names)
    pagerank = PageRank()
    scores = pagerank.fit_transform(adjacency)
    if show_names:
        image = svg_graph(graph.adjacency, font_size=font_size, node_size=node_size, names=graph.names, width=700, height=500, scores=scores, edge_width=edge_width)
    else:
        image = svg_graph(graph.adjacency, node_size=node_size, width=700, height=500, scores = scores, edge_width=edge_width)
    return SVG(image)</pre>
			<p>Next, let’s create a function for displaying <span class="No-Break">ego graphs.</span></p>
			<p>To be clear, having<a id="_idIndexMarker347"/> <strong class="source-inline">import</strong> statements inside a function is not ideal. It is best to keep import statements external to functions. However, in this case, it makes it easier to copy and paste into your various Jupyter or Colab notebooks, so I am making <span class="No-Break">an exception:</span></p>
			<pre class="source-code">
def draw_ego_graph(G, ego, center=True, k=0, show_names=True, edge_width=0.1, node_size=3, font_size=12):
    ego = nx.ego_graph(G, ego, center=center)
    ego = nx.k_core(ego, k)
    return draw_graph(ego, node_size=node_size, font_size=font_size, show_names=show_names, edge_width=edge_width)</pre>
			<p>Look closely at these two functions. Pick them apart and try to figure out what they are doing. To quickly complete this chapter, I’m going to show the results of using these functions. I have abstracted away the difficulty of visualizing these networks so that you can <span class="No-Break">do this:</span></p>
			<pre class="source-code">
draw_graph(G_alice)</pre>
			<p>As we are not<a id="_idIndexMarker348"/> passing any parameters to the function, this should display a very simple network visualization. It will look like a bunch of dots (nodes), with some dots connected to other dots by a <span class="No-Break">line (edge):</span></p>
			<p class="callout-heading">Important</p>
			<p class="callout">Please keep the <strong class="source-inline">draw_graph</strong> function handy. We will use it throughout <span class="No-Break">this book.</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17105_04_007.jpg" alt="Figure 4.7 – Rough social network of Alice’s Adventures in Wonderland" width="1450" height="1064"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Rough social network of Alice’s Adventures in Wonderland</p>
			<p>Well, that’s a bit unhelpful to look at, but this is intentional. I typically work with large networks, so I prefer<a id="_idIndexMarker349"/> to keep node names left out at first so that I can visually inspect the network. However, you can override the default values I am using. Let’s do that as well as decrease the line width a bit, increase the node size, and add <span class="No-Break">node names:</span></p>
			<pre class="source-code">
draw_graph(G_alice, edge_width=0.2, node_size=3, show_names=True)</pre>
			<p>This will draw our social network, <span class="No-Break">with labels!</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17105_04_008.jpg" alt="Figure 4.8 – Labeled social network of Alice’s Adventures in Wonderland" width="1535" height="1059"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Labeled social network of Alice’s Adventures in Wonderland</p>
			<p>The font size is a bit small and difficult to read, so let’s increase that and reduce <strong class="source-inline">node_size</strong> <span class="No-Break">by one:</span></p>
			<pre class="source-code">
draw_graph(G_alice, edge_width=0.2, node_size=2, show_names=True, font_size=12)</pre>
			<p>This creates the <span class="No-Break">following network:</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B17105_04_009.jpg" alt="Figure 4.9 – Finalized social network of Alice in Wonderland" width="1495" height="1065"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Finalized social network of Alice in Wonderland</p>
			<p>That is <a id="_idIndexMarker350"/>excellent. Consider what we have done. We have taken raw text from <em class="italic">Alice</em>, extracted all entities, and built the social network that is described in this book. This is so powerful, and it also opens the door for you to learn more about social network analysis and network science. For instance, would you rather analyze somebody else’s toy dataset, or would you rather investigate something you are interested in, such as your favorite book? I prefer to chase my <span class="No-Break">own curiosity.</span></p>
			<p>Let’s see what the ego graph looks like <span class="No-Break">around Alice!</span></p>
			<pre class="source-code">
draw_ego_graph(G_alice, 'Alice')</pre>
			<p>This gives us the <span class="No-Break">following network:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B17105_04_010.jpg" alt="Figure 4.10 – Alice ego graph" width="1431" height="948"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Alice ego graph</p>
			<p>What?! That’s incredible. We <a id="_idIndexMarker351"/>can see that some trash came through in the entity list, but we’ll learn how to clean that up in the next chapter. We can take this one step further. What if we want to take Alice out of her ego graph and just explore the relationships that exist around her? Is <span class="No-Break">that possible?</span></p>
			<pre class="source-code">
draw_ego_graph(G_alice, 'Alice', center=False)</pre>
			<p>This gives us the <span class="No-Break">following visualization:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B17105_04_011.jpg" alt="Figure 4.11 – Alice ego graph with dropped center" width="1467" height="900"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Alice ego graph with dropped center</p>
			<p>Too easy. But it’s difficult to analyze the clusters of groups that exist. After dropping the center, many<a id="_idIndexMarker352"/> nodes became isolates. If only there were a way to remove the isolates so that we could more easily see the groups. <span class="No-Break">OH, WAIT!</span></p>
			<pre class="source-code">
draw_ego_graph(G_alice, 'Alice', center=False, k=1)</pre>
			<p>We get the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B17105_04_012.jpg" alt="Figure 4.12 – Alice ego graph with dropped center and dropped isolates" width="1070" height="1016"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Alice ego graph with dropped center and dropped isolates</p>
			<p>Overall, the <em class="italic">Alice</em> social network looks pretty good. There’s some cleanup to do, but we can investigate <a id="_idIndexMarker353"/>relationships. What does the social network of <em class="italic">The Metamorphosis</em> look like? Remember, there are only three nodes and three edges. Even Alice’s ego graph is more complicated than the social network from <em class="italic">The Metamorphosis</em>. Let’s <span class="No-Break">visualize it!</span></p>
			<pre class="source-code">
draw_graph(G_morph, show_names=True, node_size=3, font_size=12)</pre>
			<p>This code produces the <span class="No-Break">following network:</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B17105_04_013.jpg" alt="Figure 4.13 – Labeled social network of The Metamorphosis" width="1432" height="976"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Labeled social network of The Metamorphosis</p>
			<p>Wait, but <a id="_idIndexMarker354"/>why are there six edges? I only see three. The reason is that <strong class="source-inline">sknetwork</strong> will draw multiple edges as a single edge. We do have options, such as increasing the line width according to the number of edges but let’s just look at the Pandas DataFrame to make sure my thinking <span class="No-Break">is correct:</span></p>
			<pre class="source-code">
morph_network_df</pre>
			<p>This gets us the <span class="No-Break">following DataFrame:</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B17105_04_014.jpg" alt="Figure 4.14 – Pandas DataFrame of network data for The Metamorphosis" width="231" height="594"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Pandas DataFrame of network data for The Metamorphosis</p>
			<p>What happens if we <span class="No-Break">drop duplicates?</span></p>
			<pre class="source-code">
morph_network_df.drop_duplicates()</pre>
			<p>We get the <span class="No-Break">following DataFrame:</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B17105_04_015.jpg" alt="Figure 4.15 – Pandas DataFrame of network data for The Metamorphosis (dropped duplicates)" width="237" height="197"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Pandas DataFrame of network data for The Metamorphosis (dropped duplicates)</p>
			<p>Aha! There<a id="_idIndexMarker355"/> is a relationship between Gregor and Grete, but the reverse is also true. One thing that I can see is that Samsa links to Gregor and Grete, but Grete does not link back to Samsa. Another way of saying this, which we will discuss in this book, is that directionality also matters. You can have a directed graph. In this case, I am just using an undirected graph, because relationships are often (but not <span class="No-Break">always) reciprocal.</span></p>
			<p>This marks the end of this demonstration. We originally set out to take raw text and use it to create a social network, and we easily accomplished our goals. Now, we have network data to play with. Now, this book is going to get <span class="No-Break">more interesting.</span></p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor174"/>Additional NLP and network considerations</h1>
			<p>This has been a marathon of a chapter. Please bear with me a little longer. I have a few final thoughts that I’d like to express, and then we can conclude <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor175"/>Data cleanup</h2>
			<p>First, if<a id="_idIndexMarker356"/> you work with language data, there will always be cleanup. Language is messy and difficult. If you are only comfortable working with pre-cleaned tabular data, this is going to feel very messy. I love that, as every project allows me to improve my techniques <span class="No-Break">and tactics.</span></p>
			<p>I showed two different approaches for extracting entities: PoS tagging and NER. Both approaches work very well, but consider which approach gets us closer to a clean and useful entity list the quickest and easiest. With <strong class="source-inline">PoS tagging</strong>, we get one token at a time. With NER, we very quickly get to entities, but the models occasionally misbehave or don’t catch everything, so there is always cleanup with this <span class="No-Break">as well.</span></p>
			<p>There is no silver bullet. I want to use whatever approach gets me as close to the goal as quickly as possible because cleanup is inevitable. The less correction I have to do, the quicker I am playing with and pulling insights out <span class="No-Break">of networks.</span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor176"/>Comparing PoS tagging and NER</h2>
			<p>PoS tagging <a id="_idIndexMarker357"/>can involve extra steps, but cleanup is often easier. On the other hand, NER can involve fewer steps, but you can get mangled results if you use it against scraped web text. There may be fewer steps for some things, but the cleanup may be daunting. I have seen spaCy’s NER false a lot on scraped web content. If you are dealing with web text, spend extra time on cleanup before feeding the data <span class="No-Break">to NER.</span></p>
			<p>Finally, slightly messy results are infinitely better than no results. This stuff is so useful for enriching datasets and extracting the “who, what, and where” parts of any piece <span class="No-Break">of text.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor177"/>Scraping considerations</h2>
			<p>There are<a id="_idIndexMarker358"/> also a few things to keep in mind when planning any scraping project. First, privacy. If you are scraping social media text, and you are extracting entities from someone else’s text, you are running surveillance on them. Ponder how you would feel if someone did the same to you. Further, if you store this data, you are storing personal data, and there may be legal considerations as well. To save yourself headaches, unless you work in government or law enforcement, it might be wise to just use these techniques against literature and news, until you have a plan for other types <span class="No-Break">of content.</span></p>
			<p>There are also ethical considerations. If you decide to use these techniques to build a surveillance engine, you should consider whether building this is an ethical thing to do. Consider whether it is ethical to run surveillance on <span class="No-Break">random strangers.</span></p>
			<p>Finally, scraping is like browsing a website, automatically, but scrapers can do damage. If you hit a website with a scraper a thousand times in a second, you could accidentally hit it with<a id="_idIndexMarker359"/> a <strong class="bold">DoS</strong> attack. Get what you need at the pace that you need it. If you are looping through all of the links on a website and then scraping them, add a 1-second delay before each scrape rather than hitting it a thousand times every second. You will be liable if you take down a web server, even <span class="No-Break">by accident.</span></p>
			<p>That was a lot of words just to say that unless you are using this for news or literature, be mindful of what you are doing. For news and literature, this can be revealing and may allow new technologies to be created. For other types of content, think about what you are doing before you jump into <span class="No-Break">the work.</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor178"/>Summary</h1>
			<p>In this chapter, we learned how to find and scrape raw text, convert it into an entity list, and then convert that entity list into an actual social network so that we can investigate revealed entities and relationships. Did we capture the <em class="italic">who</em>, <em class="italic">what</em>, and <em class="italic">where</em> of a piece of text? Absolutely. I hope you can now understand the usefulness of NLP and social network analysis when <span class="No-Break">used together.</span></p>
			<p>In this chapter, I showed several ways to get data. If you are not familiar with web scraping, then this might seem a bit overwhelming, but it’s not so bad once you get started. However, in the next chapter, I will show several easier ways to <span class="No-Break">get data.</span></p>
		</div>
	</div>
</div>
</body></html>