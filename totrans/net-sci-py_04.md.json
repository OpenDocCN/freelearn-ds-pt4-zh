["```py\npip install wikipedia\n```", "```py\nimport wikipedia as wiki\n```", "```py\nsearch_string = 'Social Network Analysis'\npage = wiki.page(search_string)\ncontent = page.content\ncontent[0:680]\n```", "```py\n'Social network analysis (SNA) is the process of investigating social structures  through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them.  Examples of social structures commonly visualized through social network analysis include social media networks, memes spread, information circulation, friendship and acquaintance networks, business networks, knowledge networks, difficult working relationships, social networks, collaboration graphs, kinship, disease transmission, and sexual relationships.'\n```", "```py\nlinks = page.links\nlinks[0:10]\n```", "```py\n['Actor-network theory',\n 'Adjacency list',\n 'Adjacency matrix',\n 'Adolescent cliques',\n 'Agent-based model',\n 'Algorithm',\n 'Alpha centrality',\n 'Anatol Rapoport',\n 'Anthropology',\n 'ArXiv (identifier)']\n```", "```py\n    import pandas as pd\n    ```", "```py\n    url = 'https://en.wikipedia.org/wiki/Social_network_analysis'\n    ```", "```py\n    data = pd.read_html(url)\n    ```", "```py\n    type(data)\n    ```", "```py\n    data\n    ```", "```py\n    data[0]\n    ```", "```py\ndata[1]\n```", "```py\n    url = 'https://en.wikipedia.org/wiki/Crime_in_Oregon'\n    ```", "```py\n    data = pd.read_html(url)\n    ```", "```py\n    df = data[1]\n    ```", "```py\n    df.tail()\n    ```", "```py\n    pip install nltk\n    ```", "```py\n    from nltk.corpus import gutenberg\n    ```", "```py\n    gutenberg.fileids()\n    ```", "```py\n    …\n    ```", "```py\n    ['austen-emma.txt',\n    ```", "```py\n     'austen-persuasion.txt',\n    ```", "```py\n     'austen-sense.txt',\n    ```", "```py\n     'bible-kjv.txt',\n    ```", "```py\n     'blake-poems.txt',\n    ```", "```py\n     'bryant-stories.txt',\n    ```", "```py\n     'burgess-busterbrown.txt',\n    ```", "```py\n     'carroll-alice.txt',\n    ```", "```py\n     'chesterton-ball.txt',\n    ```", "```py\n     'chesterton-brown.txt',\n    ```", "```py\n     'chesterton-thursday.txt',\n    ```", "```py\n     'edgeworth-parents.txt',\n    ```", "```py\n     'melville-moby_dick.txt',\n    ```", "```py\n     'milton-paradise.txt',\n    ```", "```py\n     'shakespeare-caesar.txt',\n    ```", "```py\n     'shakespeare-hamlet.txt',\n    ```", "```py\n     'shakespeare-macbeth.txt',\n    ```", "```py\n     'whitman-leaves.txt']\n    ```", "```py\n    file = 'blake-poems.txt'\n    ```", "```py\n    data = gutenberg.raw(file)\n    ```", "```py\n    data[0:600]\n    ```", "```py\n    …\n    ```", "```py\n    '[Poems by William Blake 1789]\\n\\n \\nSONGS OF INNOCENCE AND OF EXPERIENCE\\nand THE BOOK of THEL\\n\\n\\n SONGS OF INNOCENCE\\n \\n \\n INTRODUCTION\\n \\n Piping down the valleys wild,\\n   Piping songs of pleasant glee,\\n On a cloud I saw a child,\\n   And he laughing said to me:\\n \\n \"Pipe a song about a Lamb!\"\\n   So I piped with merry cheer.\\n \"Piper, pipe that song again;\"\\n   So I piped: he wept to hear.\\n \\n \"Drop thy pipe, thy happy pipe;\\n   Sing thy songs of happy cheer:!\"\\n So I sang the same again,\\n   While he wept with joy to hear.\\n \\n \"Piper, sit thee down and write\\n   In a book, that all may read.\"\\n So he vanish\\'d'\n    ```", "```py\nimport requests\nurl = 'https://www.gutenberg.org/files/5200/5200-0.txt'\ndata = requests.get(url).text\ndata\n…\n'ï»¿The Project Gutenberg eBook of Metamorphosis, by Franz Kafka\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org. If you are not located in the United States, you\\r\\nwill have to check the laws of the country where you are located before\\r\\nusing this eBook.\\r\\n\\r\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\r\\n**     Please follow the copyright guidelines in this file.     *\\r\\n\\r\\nTitle: Metamorphosis\\r\\n\\r\\nAuthor: Franz Kafka\\r\\n\\r\\nTranslator: David Wyllie\\r\\n\\r\\nRelease Date: May 13, 2002 [eBook #5200]\\r\\n[Most recently updated: May 20, 2012]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\nCopyright (C) 2002 by David Wyllie.\\r\\n\\r\\n*** START OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMetamorphosis\\r\\n\\r\\nby Franz Kafka\\r\\n\\r\\nTranslated by David Wyllie\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nI\\r\\n\\r\\n\\r\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed into a horrible vermin…'\n```", "```py\nurl = 'http://english.ryukyushimpo.jp/'\ndata = requests.get(url).text\ndata\n'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\r\\n<html  dir=\"ltr\" lang=\"en-US\">\\r\\n\\r\\n<!-- BEGIN html head -->\\r\\n<head profile=\"http://gmpg.org/xfn/11\">\\r\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\\r\\n<title>Ryukyu Shimpo - Okinawa, Japanese newspaper, local news</title>…'\n```", "```py\npip install beautifulsoup4\n```", "```py\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(data, 'html.parser')\nlinks = soup.find_all('a', href=True)\nlinks\n[<a href=\"http://english.ryukyushimpo.jp\">Home</a>,\n <a href=\"http://english.ryukyushimpo.jp\">Ryukyu Shimpo – Okinawa, Japanese newspaper, local news</a>,\n <a href=\"http://english.ryukyushimpo.jp/special-feature-okinawa-holds-mass-protest-rally-against-us-base/\">Special Feature: Okinawa holds mass protest rally against US base</a>,\n <a href=\"http://english.ryukyushimpo.jp/2021/09/03/34020/\">Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"</a>,\n <a href=\"http://english.ryukyushimpo.jp/2021/09/03/34020/\"><img alt=\"Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"\" class=\"medium\" src=\"img/RS20210830G01268010100.jpg\"/> </a>…]\n```", "```py\nlen(links)\n…\n277\n```", "```py\nurls = [link.get('href') for link in links]\nurls\n…\n['http://english.ryukyushimpo.jp',\n 'http://english.ryukyushimpo.jp',\n 'http://english.ryukyushimpo.jp/special-feature-okinawa-holds-mass-protest-rally-against-us-base/',\n 'http://english.ryukyushimpo.jp/2021/09/03/34020/',\n 'http://english.ryukyushimpo.jp/2021/09/03/34020/',\n 'http://english.ryukyushimpo.jp/2021/09/03/34020/',\n 'http://english.ryukyushimpo.jp/2021/09/03/34020/'…]\n```", "```py\nlen(urls)\n…\n277\n```", "```py\nurl = 'http://english.ryukyushimpo.jp/2021/09/03/34020/'\ndata = requests.get(url).text\nsoup = BeautifulSoup(data, 'html.parser')\nsoup.get_text()\n…\n\"\\n\\n\\n\\n\\nRyukyu Shimpo – Okinawa, Japanese newspaper, local news  » Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTuesdaySeptember 07,2021Ryukyu Shimpo – Okinawa, Japanese newspaper, local news\\n\\n\\n\\n\\n\\n\\r\\nTOPICS:Special Feature: Okinawa holds mass protest rally against US base\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"…\"\n```", "```py\ntext = soup.get_text()\ntext[0:500]\n…\n'\\n\\n\\n\\n\\nRyukyu Shimpo – Okinawa, Japanese newspaper, local news  » Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTuesdaySeptember 07,2021Ryukyu Shimpo – Okinawa, Japanese newspaper, local news\\n\\n\\n\\n\\n\\n\\r\\nTOPICS:Special Feature: Okinawa holds mass protest rally against US base\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHirokazu Ueyonabaru returns home t'\n```", "```py\ntext = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').replace('\\xa0', ' ')\ntext\n…\n\"     Ryukyu Shimpo – Okinawa, Japanese newspaper, local news  » Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"                                                            Home    Search           TuesdaySeptember 07,2021Ryukyu Shimpo – Okinawa, Japanese newspaper, local news        TOPICS:Special Feature: Okinawa holds mass protest rally against US base             Hirokazu Ueyonabaru returns home to Okinawa from the Tokyo Paralympics with two bronze medals in wheelchair T52 races, \"the cheers gave me power\"   Tokyo Paralympic bronze medalist Hirokazu Ueyonabaru receiving congratulations from some young supporters at Naha Airport on August 30…\"\n```", "```py\ncutoff = text.index('Go to Japanese')\ncutoff\n…\n1984\n```", "```py\ntext = text[0:cutoff]\n```", "```py\ncutoff = text.rindex('Hirokazu Ueyonabaru')\ntext = text[cutoff:]\n```", "```py\ntext\n…\n'Hirokazu Ueyonabaru, 50, – SMBC Nikko Securities Inc. – who won the bronze medal in both the 400-meter and 1,500-meter men's T52 wheelchair race, returned to Okinawa the evening of August 30, landing at Naha airport. Seeing the people who came out to meet him, he said \"It was a sigh of relief (to win a medal)\" beaming a big smile. That morning he contended with press conferences in Tokyo before heading home. He showed off his two bronze medals, holding them up from his neck in the airport lobby, saying \"I could not have done it without all the cheers from everyone…'\n```", "```py\n    url = 'https://www.gutenberg.org/files/5200/5200-0.txt'\n    ```", "```py\n    text = requests.get(url).text\n    ```", "```py\n    cutoff = text.index('One morning')\n    ```", "```py\n    text = text[cutoff:]\n    ```", "```py\n    cutoff = text.rindex('*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***')\n    ```", "```py\n    text = text[:cutoff]\n    ```", "```py\ntext[-500:]\n…\n'talking, Mr. and Mrs.\\r\\nSamsa were struck, almost simultaneously, with the thought of how their\\r\\ndaughter was blossoming into a well built and beautiful young lady.\\r\\nThey became quieter. Just from each otherâ\\x80\\x99s glance and almost without\\r\\nknowing it they agreed that it would soon be time to find a good man\\r\\nfor her. And, as if in confirmation of their new dreams and good\\r\\nintentions, as soon as they reached their destination Grete was the\\r\\nfirst to get up and stretch out her young body.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n'\nWe have successfully removed both header and footer junk. I can see that there are a lot of line breaks, so let's remove all of those as well.\ntext = text.replace('\\r', ' ').replace('\\n', ' ')\ntext\n…\n'One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections…'\n```", "```py\ntext = text.replace('â\\x80\\x99', '\\'').replace('â\\x80\\x9c', '\"').replace('â\\x80\\x9d', '\"\"')\\\n.replace('â\\x80\\x94', ' ')\nprint(text)\n…\nOne morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin. He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off  any moment. His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.    \"What's happened to me?\"\" he thought…\n```", "```py\n    def get_data():\n    ```", "```py\n        url = 'https://www.gutenberg.org/files/5200/5200-0.txt'\n    ```", "```py\n        text = requests.get(url).text\n    ```", "```py\n        # strip header junk\n    ```", "```py\n        cutoff = text.index('One morning')\n    ```", "```py\n        text = text[cutoff:]\n    ```", "```py\n        # strip footer junk\n    ```", "```py\n        cutoff = text.rindex('*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***')\n    ```", "```py\n        text = text[:cutoff]\n    ```", "```py\n        # pre-processing to clean the text\n    ```", "```py\n        text = text.replace('\\r', ' ').replace('\\n', ' ')\n    ```", "```py\n        text = text.replace('â\\x80\\x99', '\\'').replace('â\\x80\\x9c', '\"')\\\n    ```", "```py\n         .replace('â\\x80\\x9d', '\"\"').replace('â\\x80\\x94', ' ')\n    ```", "```py\n        return text\n    ```", "```py\n    text = get_data()\n    ```", "```py\n    text\n    ```", "```py\n    …\n    ```", "```py\n    'One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin. He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off  any moment. His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.    \"What\\'s happened to me?\"\" he thought'\n    ```", "```py\nfrom nltk.tokenize import sent_tokenize\nsentences = sent_tokenize(text)\nsentences[0:5]\n…\n['One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.',\n 'He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.',\n 'The bedding was hardly able to cover it and seemed ready to slide off  any moment.',\n 'His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.',\n '\"What\\'s happened to me?\"\"']\n```", "```py\nimport nltk\nnltk.pos_tag(sentences)\n[('One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.',\n  'NNP'),\n ('He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.',\n  'NNP'),\n ('The bedding was hardly able to cover it and seemed ready to slide off  any moment.',\n  'NNP'),\n ('His many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.',\n  'NNP'),\n ('\"What\\'s happened to me?\"\"', 'NNP'),\n ('he thought.', 'NN')…]\n```", "```py\nsentence = sentences[0]\nsentence\n…\n'One morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.'\n```", "```py\nfrom nltk.tokenize import casual_tokenize\ntokens = casual_tokenize(sentence)\ntokens\n…\n['One',\n 'morning',\n ',',\n 'when',\n 'Gregor',\n 'Samsa',\n 'woke',\n 'from',\n 'troubled',\n 'dreams',\n ',',\n 'he',\n 'found',\n 'himself',\n 'transformed',\n 'in',\n 'his',\n 'bed',\n 'into',\n 'a',\n 'horrible',\n 'vermin',\n '.']\n```", "```py\nnltk.pos_tag(tokens)\n…\n[('One', 'CD'),\n ('morning', 'NN'),\n (',', ','),\n ('when', 'WRB'),\n ('Gregor', 'NNP'),\n ('Samsa', 'NNP'),\n ('woke', 'VBD'),\n ('from', 'IN'),\n ('troubled', 'JJ'),\n ('dreams', 'NNS'),\n (',', ','),\n ('he', 'PRP'),\n ('found', 'VBD'),\n ('himself', 'PRP'),\n ('transformed', 'VBN'),\n ('in', 'IN'),\n ('his', 'PRP$'),\n ('bed', 'NN'),\n ('into', 'IN'),\n ('a', 'DT'),\n ('horrible', 'JJ'),\n ('vermin', 'NN'),\n ('.', '.')]\n```", "```py\nentities = []\nfor row in nltk.pos_tag(tokens):\n    token = row[0]\n    tag = row[1]\n    if tag == 'NNP':\n        entities.append(token)\nentities\n…\n['Gregor', 'Samsa']\n```", "```py\ndef extract_entities(sentence):\n    entities = []\n    tokens = casual_tokenize(sentence)\n    for row in nltk.pos_tag(tokens):\n        token = row[0]\n        tag = row[1]\n        if tag == 'NNP':\n            entities.append(token)\n    return entities\n```", "```py\nextract_entities(sentence)\n…\n['Gregor', 'Samsa']\n```", "```py\nentities = [extract_entities(sentence) for sentence in sentences]\nentities\n[['Gregor', 'Samsa'],\n [],\n [],\n [],\n [\"What's\"],\n [],\n [],\n [],\n ['Samsa'],\n [],\n ['Gregor'],\n [],\n [],\n [],\n [],\n ['Oh', 'God', '\"', '\"', \"I've\"],\n [],\n [],\n ['Hell']]\n```", "```py\ndef extract_entities(sentence):\n    entities = []\n    tokens = casual_tokenize(sentence)\n    for row in nltk.pos_tag(tokens):\n        token = row[0]\n        tag = row[1]\n        if tag == 'NNP':\n            entities.append(token)\n    if len(entities) > 0:\n        return entities\n    else:\n        return None\nentities = [extract_entities(sentence) for sentence in sentences]\nentities\n[['Gregor', 'Samsa'],\n None,\n None,\n None,\n [\"What's\"],\n None,\n None,\n None,\n ['Samsa'],\n None,\n ['Gregor'],\n None,\n None,\n None,\n None,\n ['Oh', 'God', '\"', '\"', \"I've\"],\n None,\n None,\n ['Hell']]\nimport pandas as pd\ndf = pd.DataFrame({'sentence':sentences, 'entities':entities})\ndf.head(10)\n```", "```py\ndf = df.dropna()\ndf = df[df['entities'].apply(len) > 1]\nentities = df['entities'].to_list()\nentities\n[['Gregor', 'Samsa'],\n ['Oh', 'God', '\"', '\"', \"I've\"],\n ['\"', '\"'],\n [\"I'd\", \"I'd\"],\n [\"I've\", \"I'll\"],\n ['First', \"I've\"],\n ['God', 'Heaven'],\n ['Gregor', '\"', '\"', '\"'],\n ['Gregor', \"I'm\"],\n ['Gregor', 'Gregor', '\"', '\"', '\"'],\n ['Gregor', \"I'm\"],\n ['\"', '\"', 'Gregor', '\"'],\n ['Seven', '\"'],\n [\"That'll\", '\"', '\"'],\n [\"They're\", '\"', '\"', 'Gregor'],\n [\"Gregor's\", 'Gregor'],\n ['Yes', '\"', 'Gregor'],\n ['Gregor', '\"', '\"'],\n ['Mr', 'Samsa', '\"', '\"']]\n```", "```py\ndef extract_entities(sentence):\n    entities = []\n    tokens = casual_tokenize(sentence)\n    for row in nltk.pos_tag(tokens):\n        token = row[0]\n        tag = row[1]\n        if tag == 'NNP':\n            if \"'\" in token:\n                cutoff = token.index('\\'')\n                token = token[:cutoff]\n            entities.append(token)\n    if len(entities) > 0:\n        return entities\n    else:\n        return None\nentities = [extract_entities(sentence) for sentence in sentences]\nentities\n[['Gregor', 'Samsa'],\n None,\n None,\n None,\n ['What'],\n None,\n None,\n None,\n ['Samsa']…]\n```", "```py\ndf = pd.DataFrame({'sentence':sentences, 'entities':entities})\ndf = df.dropna()\ndf = df[df['entities'].apply(len) > 1]\nentities = df['entities'].to_list()\nentities\n[['Gregor', 'Samsa'],\n ['Oh', 'God', '\"', '\"', 'I'],\n ['\"', '\"'],\n ['I', 'I'],\n ['I', 'I'],\n ['First', 'I'],\n ['God', 'Heaven'],\n ['Gregor', '\"', '\"', '\"'],\n ['Gregor', 'I'],\n ['Gregor', 'Gregor', '\"', '\"', '\"'],\n ['Gregor', 'I'],\n ['\"', '\"', 'Gregor', '\"'],\n ['Seven', '\"'],\n ['That', '\"', '\"'],\n ['They', '\"', '\"', 'Gregor'],\n ['Gregor', 'Gregor'],\n ['Yes', '\"', 'Gregor'],\n ['Gregor', '\"', '\"'],\n ['Mr', 'Samsa', '\"', '\"']]\n```", "```py\nfrom string import punctuation\ndef extract_entities(sentence):\n    entities = []\n    tokens = casual_tokenize(sentence)\nfor row in nltk.pos_tag(tokens):\n        token = row[0]\n        tag = row[1]\n        if tag == 'NNP':\n            for p in punctuation:\n                if p in token:\n                    cutoff = token.index(p)\n                    token = token[:cutoff]\n            if len(token) > 1:\n                entities.append(token)\nif len(entities) > 0:\n        return entities\n    else:\n        return None\nentities = [extract_entities(sentence) for sentence in sentences]\ndf = pd.DataFrame({'sentence':sentences, 'entities':entities})\ndf = df.dropna()\ndf = df[df['entities'].apply(len) > 1]\nentities = df['entities'].to_list()\nentities\n…\n[['Gregor', 'Samsa'],\n ['Oh', 'God'],\n ['God', 'Heaven'],\n ['Gregor', 'Gregor'],\n ['They', 'Gregor'],\n ['Gregor', 'Gregor'],\n ['Yes', 'Gregor'],\n ['Mr', 'Samsa'],\n ['He', 'Gregor'],\n ['Well', 'Mrs', 'Samsa'],\n ['No', 'Gregor'],\n ['Mr', 'Samsa'],\n ['Mr', 'Samsa'],\n ['Sir', 'Gregor'],\n ['Oh', 'God']]\n```", "```py\ndef get_book_entities():\n    text = get_data()\n    sentences = sent_tokenize(text)\n    entities = [extract_entities(sentence) for sentence in sentences]\n    df = pd.DataFrame({'sentence':sentences, 'entities':entities})\n    df = df.dropna()\n    df = df[df['entities'].apply(len) > 1]\n    entities = df['entities'].to_list()\n    return entities\nentities = get_book_entities()\nentities[0:5]\n…\n[['Gregor', 'Samsa'],\n ['Oh', 'God'],\n ['God', 'Heaven'],\n ['Gregor', 'Gregor'],\n ['They', 'Gregor']]\n```", "```py\npython -m spacy download en_core_web_md\n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_md\")\n```", "```py\ndef get_data():\n    url = 'https://www.gutenberg.org/files/5200/5200-0.txt'\ntext = requests.get(url).text\n    # strip header junk\n    cutoff = text.index('One morning')\n    text = text[cutoff:]\n    # strip footer junk\n    cutoff = text.rindex('*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***')\n    text = text[:cutoff]\n    # pre-processing to clean the text\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    text = text.replace('â\\x80\\x99', '\\'').replace('â\\x80\\x9c', '\"').replace('â\\x80\\x9d', '\"\"').replace('â\\x80\\x94', ' ')\nreturn text\nThat looks good. We are loading The Metamorphosis, cleaning out header and footer junk, and then returning text that is clean enough for our purposes. Just to lay eyes on the text, let's call our function and inspect the returned text.\ntext = get_data()\ntext[0:279]\n…\n'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.'\n```", "```py\ndoc = nlp(text)\nsentences = list(doc.sents)\n```", "```py\nfor s in sentences[0:6]:\n    print(s)\n    print()\n…\nOne morning, when Gregor Samsa woke from troubled dreams, he found  himself transformed in his bed into a horrible vermin.\nHe lay on his  armour-like back, and if he lifted his head a little he could see his  brown belly, slightly domed and divided by arches into stiff sections.\nThe bedding was hardly able to cover it and seemed ready to slide off  any moment.\nHis many legs, pitifully thin compared with the size of the  rest of him, waved about helplessly as he looked.\n\"What's happened to me?\"\n\" he thought.\n```", "```py\nfor token in sentences[0]:\n    print('{}: {}'.format(token.text, token.tag_))\n…\nOne: CD\nmorning: NN\n,: ,\nwhen: WRB\nGregor: NNP\nSamsa: NNP\nwoke: VBD\nfrom: IN\ntroubled: JJ\ndreams: NNS\n,: ,\nhe: PRP\nfound: VBD\n :\nhimself: PRP\ntransformed: VBD\nin: IN\nhis: PRP$\nbed: NN\ninto: IN\na: DT\nhorrible: JJ\nvermin: NN\n.: .\n```", "```py\nfor token in sentences[0]:\n    print('{}: {}'.format(token.text, token.pos_))\n…\nOne: NUM\nmorning: NOUN\n,: PUNCT\nwhen: ADV\nGregor: PROPN\nSamsa: PROPN\nwoke: VERB\nfrom: ADP\ntroubled: ADJ\ndreams: NOUN\n,: PUNCT\nhe: PRON\nfound: VERB\n : SPACE\nhimself: PRON\ntransformed: VERB\nin: ADP\nhis: ADJ\nbed: NOUN\ninto: ADP\na: DET\nhorrible: ADJ\nvermin: NOUN\n.: PUNCT\n```", "```py\nentities = []\nfor token in sentences[0]:\n    if token.tag_ == 'NNP':\n        entities.append(token.text)\nentities\n…\n['Gregor', 'Samsa']\nPerfect! But this is only working on a single sentence. Let's do the same for all sentences.\nentities = []\nfor sentence in sentences:\n    sentence_entities = []\n    for token in sentence:\n        if token.tag_ == 'NNP':\n            sentence_entities.append(token.text)\n    entities.append(sentence_entities)\nentities[0:10]\n…\n[['Gregor', 'Samsa'], [], [], [], [], [], [], [], [], ['Samsa']]\n```", "```py\ndef extract_entities(text):\n    doc = nlp(text)\n    sentences = list(doc.sents)\n    entities = []\n    for sentence in sentences:\n        sentence_entities = []\n        for token in sentence:\n            if token.tag_ == 'NNP':\n                sentence_entities.append(token.text)\n        if len(sentence_entities) > 0:\n            entities.append(sentence_entities)\n    return entities\n```", "```py\nextract_entities(text)\n…\n[['Gregor', 'Samsa'],\n ['Samsa'],\n ['Gregor'],\n ['God'],\n ['Travelling'],\n ['God', 'Heaven'],\n ['Gregor'],\n ['Gregor'],\n ['Gregor'],\n ['Gregor'],\n ['Gregor']…]\n```", "```py\nfor token in sentences[0]:\n    print('{}: {}'.format(token.text, token.ent_type_))\n…\nOne: TIME\nmorning: TIME\n,:\nwhen:\nGregor: PERSON\nSamsa: PERSON\nwoke:\nfrom:\ntroubled:\ndreams:\n,:\nhe:\nfound:\n :\nhimself:\ntransformed:\nin:\nhis:\nbed:\ninto:\na:\nhorrible:\nvermin:\n.:\n```", "```py\ndoc = nlp(sentences[0].text)\nfor ent in doc.ents:\n    print('{}: {}'.format(ent, ent.label_))\n…\nOne morning: TIME\nGregor Samsa: PERSON\n```", "```py\ndef extract_entities(text):\n    doc = nlp(text)\n    sentences = list(doc.sents)\n    entities = []\n    for sentence in sentences:\n        sentence_entities = []\n        sent_doc = nlp(sentence.text)\n        for ent in sent_doc.ents:\n            if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n                entity = ent.text.strip()\n                if \"'s\" in entity:\n                    cutoff = entity.index(\"'s\")\n                    entity = entity[:cutoff]\n                if entity != '':\n                    sentence_entities.append(entity)\n        sentence_entities = list(set(sentence_entities))\n        if len(sentence_entities) > 1:\n            entities.append(sentence_entities)\n    return entities\n```", "```py\nmorph_entities = extract_entities(text)\nmorph_entities\n…\n[['Gregor', 'Grete'],\n ['Gregor', 'Grete'],\n ['Grete', 'Gregor'],\n ['Gregor', 'Grete'],\n ['Grete', 'Gregor'],\n ['Grete', 'Gregor'],\n ['Grete', 'Gregor'],\n ['Samsa', 'Gregor'],\n ['Samsa', 'Gregor'],\n ['Samsa', 'Grete'],\n ['Samsa', 'Grete'],\n ['Samsa', 'Grete']]\n```", "```py\ndef get_data():\n    url = 'https://www.gutenberg.org/files/11/11-0.txt'\n    text = requests.get(url).text\n    # strip header junk\n    cutoff = text.index('Alice was beginning')\n    text = text[cutoff:]\n    # strip footer junk\n    cutoff = text.rindex('THE END')\n    text = text[:cutoff]\n    # pre-processing to clean the text\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    text = text.replace('â\\x80\\x99', '\\'').replace('â\\x80\\x9c', '\"').replace('â\\x80\\x9d', '\"\"').replace('â\\x80\\x94', ' ')\n    return text\ntext = get_data()\ntext[0:310]\n…\n'Alice was beginning to get very tired of sitting by her sister on the  bank, and of having nothing to do: once or twice she had peeped into  the book her sister was reading, but it had no pictures or  conversations in it, \"and what is the use of a book,\"\" thought Alice  \"without pictures or conversations?\"\"  '\n```", "```py\nalice_entities = extract_entities(text)\nalice_entities[0:10]\n…\n[['Alice', 'Rabbit'],\n ['Alice', 'Longitude'],\n ['New Zealand', \"Ma'am\", 'Australia'],\n ['Fender', 'Alice'],\n ['Alice', 'Rabbit'],\n ['Mabel', 'Ada'],\n ['Rome', 'Paris', 'London'],\n ['Improve', 'Nile'],\n ['Alice', 'Mabel'],\n ['Alice', 'William the Conqueror']]\n```", "```py\nalice_entities[0:10]\n…\n[['Alice', 'Rabbit'],\n ['Alice', 'Longitude'],\n ['New Zealand', \"Ma'am\", 'Australia'],\n ['Fender', 'Alice'],\n ['Alice', 'Rabbit'],\n ['Mabel', 'Ada'],\n ['Rome', 'Paris', 'London'],\n ['Improve', 'Nile'],\n ['Alice', 'Mabel'],\n ['Alice', 'William the Conqueror']]\n```", "```py\n['Jack', 'Jill', 'Mark']\n```", "```py\nfinal_sources = []\nfinal_targets = []\nfor row in alice_entities:\n    source = row[0]\n    targets = row[1:]\n    for target in targets:\n        final_sources.append(source)\n        final_targets.append(target)\n```", "```py\nfinal_sources[0:5]\n…\n['Alice', 'Alice', 'New Zealand', 'New Zealand', 'Fender']\nNice. How about our final targets?\nfinal_targets[0:5]\n…\n['Rabbit', 'Longitude', \"Ma'am\", 'Australia', 'Alice']\n```", "```py\n['John', 'Aaron', 'Jake']\n```", "```py\ndef get_network_data(entities):\n    final_sources = []\n    final_targets = []\n    for row in entities:\n        source = row[0]\n        targets = row[1:]\n        for target in targets:\n            final_sources.append(source)\n            final_targets.append(target)\n    df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n    return df\n```", "```py\nalice_network_df = get_network_data(alice_entities)\nalice_network_df.head()\n```", "```py\nmorph_network_df = get_network_data(morph_entities)\nmorph_network_df.head()\n```", "```py\npip install networkx\n```", "```py\nimport networkx as nx\nG_alice = nx.from_pandas_edgelist(alice_network_df)\nG_morph = nx.from_pandas_edgelist(morph_network_df)\n```", "```py\nnx.info(G_alice)\n…\n'Graph with 68 nodes and 71 edges'\n…\nnx.info(G_morph)\n…\n'Graph with 3 nodes and 3 edges'\n```", "```py\ndef draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5):\n    import numpy as np\n    from IPython.display import SVG\n    from sknetwork.visualization import svg_graph\n    from sknetwork.data import Bunch\n    from sknetwork.ranking import PageRank\n    adjacency = nx.to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight='weight', format='csr')\n    names = np.array(list(G.nodes()))\n    graph = Bunch()\n    graph.adjacency = adjacency\n    graph.names = np.array(names)\n    pagerank = PageRank()\n    scores = pagerank.fit_transform(adjacency)\n    if show_names:\n        image = svg_graph(graph.adjacency, font_size=font_size, node_size=node_size, names=graph.names, width=700, height=500, scores=scores, edge_width=edge_width)\n    else:\n        image = svg_graph(graph.adjacency, node_size=node_size, width=700, height=500, scores = scores, edge_width=edge_width)\n    return SVG(image)\n```", "```py\ndef draw_ego_graph(G, ego, center=True, k=0, show_names=True, edge_width=0.1, node_size=3, font_size=12):\n    ego = nx.ego_graph(G, ego, center=center)\n    ego = nx.k_core(ego, k)\n    return draw_graph(ego, node_size=node_size, font_size=font_size, show_names=show_names, edge_width=edge_width)\n```", "```py\ndraw_graph(G_alice)\n```", "```py\ndraw_graph(G_alice, edge_width=0.2, node_size=3, show_names=True)\n```", "```py\ndraw_graph(G_alice, edge_width=0.2, node_size=2, show_names=True, font_size=12)\n```", "```py\ndraw_ego_graph(G_alice, 'Alice')\n```", "```py\ndraw_ego_graph(G_alice, 'Alice', center=False)\n```", "```py\ndraw_ego_graph(G_alice, 'Alice', center=False, k=1)\n```", "```py\ndraw_graph(G_morph, show_names=True, node_size=3, font_size=12)\n```", "```py\nmorph_network_df\n```", "```py\nmorph_network_df.drop_duplicates()\n```"]