- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP and Network Synergy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed **natural language processing** (**NLP**),
    network analysis, and the tools used in the Python programming language for both.
    We also discussed non-programmatic tools for doing network analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to put all of that knowledge to work. I hope to
    explain the power and insights that can be unveiled by combining NLP and network
    analysis, which is the theme of this book. In later chapters, we will continue
    with this theme, but we’ll also discuss other tools for working with NLP and networks,
    such as unsupervised and supervised machine learning. This chapter will demonstrate
    techniques for determining who or what a piece of text is talking about.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Why are we learning about NLP in a network book?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking questions to tell a story
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing between libraries, APIs, and source data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Natural Language Toolkit (NLTK) library for part-of-speech (PoS) tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using spaCy for PoS tagging and named-entity recognition (NER)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting entity lists into network data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting network data into networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing a network visualization spot check
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using several different Python libraries. The `pip
    install` command is listed in each section for installing each library, so just
    follow along and do the installations as needed. If you run into installation
    problems, there is usually an answer on Stack Overflow. Google the error!
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, I would like to explain one thing so that the number of libraries
    we are using doesn’t seem so overwhelming. It is the reason why we use each library
    that matters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most of this book, we will be doing one of three things: network analysis,
    network visualization, or using network data for machine learning (also known
    as GraphML).'
  prefs: []
  type: TYPE_NORMAL
- en: Anytime we have network data, we will be using `NetworkX` to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Anytime we are doing analysis, we will probably be using `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NetworkX`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-network`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn` and `Karate Club`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at the first words. If I want to do network analysis, then I’ll be using
    `NetworkX` and `pandas`. If I want to do network visualization, I will be using
    `NetworkX` and `scikit-network`. If I want to do machine learning using network
    data, I will be using `NetworkX`, `scikit-learn`, and possibly `Karate Club`.
  prefs: []
  type: TYPE_NORMAL
- en: These are the core libraries that will be used in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Also, *you must keep the code for the* `draw_graph()` *function handy*, as you
    will be using it throughout this book. That code is a bit gnarly because, at the
    time of writing, it needs to be. Unfortunately, `NetworkX` is not great for network
    visualization, and `scikit-network` is not great for network construction or analysis,
    so I use both libraries together for visualization, and that works well. I am
    hoping this will not be the case for later editions of this book and that visualization
    will be improved and simplified over time.
  prefs: []
  type: TYPE_NORMAL
- en: All of the necessary code can be found at [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Why are we learning about NLP in a network book?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I briefly answered this question in the introduction of the first chapter,
    but it is worth repeating in more detail. Many people who work on text analysis
    are aware of sentiment analysis and text classification. **Text classification**
    is the ability to predict whether a piece of text can be classified as something.
    For example, let’s take this string:'
  prefs: []
  type: TYPE_NORMAL
- en: “*How are you* *doing today?*”
  prefs: []
  type: TYPE_NORMAL
- en: What can we tell about this string? Is it a question or a statement? It’s a
    question. Who is being asked this question? You are. Is there a positive, negative,
    or neutral emotion tied to the question? It looks neutral to me. Let’s try another
    string.
  prefs: []
  type: TYPE_NORMAL
- en: “*Jack and Jill went up the hill, but Jack fell down because he is* *an idiot.*”
  prefs: []
  type: TYPE_NORMAL
- en: This is a statement about Jack and Jill, and Jack is being called an idiot,
    which is not a very nice thing to say. However, it seems like the insult was written
    jokingly, so it is unclear whether the author was angry or laughing when it was
    written. I can confirm that I was laughing when I wrote it, so there is a positive
    emotion behind it, but text classification would struggle to pick up on that.
    Let’s try another.
  prefs: []
  type: TYPE_NORMAL
- en: “*I have never been so angry in my entire life as I am* *right now!*”
  prefs: []
  type: TYPE_NORMAL
- en: The author is expressing a very strong negative sentiment. Sentiment analysis
    and text classification would easily pick up on that.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis** is a set of techniques that use algorithms to automatically
    detect emotions that are embedded into a piece of text or transcribed recording.
    Text classification uses the same algorithms, but the goal is not to identify
    an emotion but rather a theme. For instance, text classification can detect whether
    there is abusive language in text.'
  prefs: []
  type: TYPE_NORMAL
- en: This is not a chapter about sentiment analysis or text classification. The purpose
    of this chapter is to explain how to automatically extract the entities (people,
    places, things, and more) that exist in text so that you can identify and study
    the social network that is being described in the text. However, text classification
    and sentiment analysis can be coupled with these techniques to provide additional
    context about social networks, or for building specialized social networks, such
    as friendship networks.
  prefs: []
  type: TYPE_NORMAL
- en: Asking questions to tell a story
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I approach my work from a storytelling perspective; I let my story dictate
    the work, not the other way around. For instance, if I am starting a project,
    I’ll ponder or even write down a series of who, what, where, when, why, and how
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What data do we have? Is it enough?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where do we get more?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we get more?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What blockers prevent us from getting more?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often will we need more?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But this is a different kind of project. We want to gain deeper insights into
    a piece of text than we might gather through reading. Even after reading a whole
    book, most people can’t memorize the relationships that are described in the text,
    and it would likely be a faulty recollection anyway. But we should have questions
    such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: Who is mentioned in the text?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who do they know?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are their adversaries?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the themes of this text?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What emotions are present?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What places are mentioned?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When did this take place?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to come up with a solid set of questions before starting any
    kind of analysis. More questions will come to you the deeper you go.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will give you the tools to automatically investigate all of these
    questions other than the ones about themes and adversaries. The knowledge from
    this chapter, coupled with an understanding of text classification and sentiment
    analysis, will give you the ability to answer *all* these questions. That is the
    “why” of this chapter. We want to automatically extract people, places, and maybe
    even some things that are mentioned. Most of the time, I only want people and
    places, as I want to study the social network.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to explain that this is useful for accompanying text
    classification and sentiment analysis. For example, a positive Amazon or Yelp
    review is of little use if you do not know what is being reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can do any work to uncover the relationships that exist in text,
    we need to get some text. For practice, we have a few options. We could load it
    using a Python library such as NLTK, we could harvest it using the Twitter (or
    another social network) library, or we could scrape it ourselves. Even scraping
    has options, but I am only going to explain one way: using Python’s `BeautifulSoup`
    library. Just know that there are options, but I love the flexibility of `BeautifulSoup`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the demos will use text scraped off of the internet, and you
    can make slight changes to the code for your own web scraping needs.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing web scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, what even is **web scraping**, and who can do it? Anyone with any programming
    skill can do scraping using several different programming languages, but we will
    do this with Python. Web scraping is the action of harvesting content from web
    resources so that you may use the data in your products and software. You can
    use scraping to pull information that a website hasn’t exposed as a data feed
    or through an API. But one warning: do not scrape too aggressively; otherwise,
    you could knock down a web server through an accidental **denial-of-service**
    (**DoS**) attack. Just get what you need as often as you need it. Go slow. Don’t
    be greedy or selfish.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing BeautifulSoup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**BeautifulSoup** is a powerful Python library for scraping anything that you
    have access to online. I frequently use this to harvest story URLs from news websites,
    and then I scrape each of these URLs for their text content. I typically do not
    want the actual HTML, CSS, or JavaScript, so I render the web page and then scrape
    the content.'
  prefs: []
  type: TYPE_NORMAL
- en: '`BeautifulSoup` is an important Python library to know about if you are going
    to be doing web scraping. There are other options for web scraping with Python,
    but `BeautifulSoup` is commonly used.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no better way to explain `BeautifulSoup` than to see it in action,
    so let’s get to work!
  prefs: []
  type: TYPE_NORMAL
- en: Loading and scraping data with BeautifulSoup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this hands-on demonstration, we will see three different ways of loading
    and scraping data. They are all useful independently, but they can also be used
    together in ways.
  prefs: []
  type: TYPE_NORMAL
- en: First, the easiest approach is to use a library that can get exactly what you
    want in one shot, with minimal cleanup. Several libraries such as `pandas`, `Wikipedia`,
    and `NLTK` have ways to load data, so let’s start with them. As this book is primarily
    about extracting relationships from text and then analyzing them, we want text
    data. I will demonstrate a few approaches, and then describe the pros and cons
    of each.
  prefs: []
  type: TYPE_NORMAL
- en: Python library – Wikipedia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is a powerful library for pulling data from Wikipedia, also called `Wikipedia`.
    It can be installed by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it has been installed, you may import it into your code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it has been imported, you have access to Wikipedia programmatically, allowing
    you to search for and use any content you are curious about. As we will be doing
    a lot of social network analysis while working through this book, let’s see what
    `Wikipedia` has on the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line, `content[0:680]`, just shows what is inside the `content` string,
    up to the 680th character, which is the end of the sentence shown in the following
    code. There is a lot more past 680\. I chose to cut it off for this demonstration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With a few lines of code, we were able to pull text data directly from Wikipedia.
    Can we see what links exist on that Wikipedia page? Yes, we can!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'I’m using a square bracket, `[`, to select only the first 10 entries from `links`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are only in the A-links after choosing only the first 10 links, I think
    it’s safe to say that there are probably many more. There’s quite a lot to learn
    about social network analysis on Wikipedia! That was so simple! Let’s move on
    to another option for easy scraping: the `pandas` library.'
  prefs: []
  type: TYPE_NORMAL
- en: Python library – pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are interested in using Python for data science, you will inevitably
    learn about and use `pandas`. This library is powerful and versatile for working
    with data. For this demonstration, I will use it for pulling tabular data from
    web pages, but it can do much more. If you are interested in data science, learn
    as much as you can about `pandas`, and get comfortable using it.
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` can be useful for pulling tabular data from web pages, but it is much
    less useful for raw text. If you want to load text from Wikipedia, you should
    use the `Wikipedia` library shown previously. If you want to pull text off other
    web pages, you should use the `Requests` and `BeautifulSoup` libraries together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use `pandas` to pull some tabular data from Wikipedia. First, let’s try
    to scrape the same Wikipedia page using `pandas` and see what we get. If you have
    installed Jupyter on your computer, you likely already have `pandas` installed,
    so, let’s jump straight to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we imported the `pandas` library and gave it a shorter name, and then
    used `Pandas` to read the same Wikipedia page on social network analysis. What
    did we get back? What does `type(data)` show? I would expect a `pandas` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following code. Just type `data` and run the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see that we get a list back. In `pandas`, if you do a `read` operation,
    you will usually get a DataFrame back, so why did we get a list? This happened
    because there are multiple data tables on this page, so `pandas` has returned
    a Python list of all of the tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check out the elements of the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should give us a `pandas` DataFrame of the first table from a Wikipedia
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – pandas DataFrame of the first element of Wikipedia data](img/B17105_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – pandas DataFrame of the first element of Wikipedia data
  prefs: []
  type: TYPE_NORMAL
- en: This data is already looking a bit problematic. Why are we getting a bunch of
    text inside of a table? Why does the last row look like a bunch of code? What
    does the next table look like?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us a `pandas` DataFrame of the second table from the same Wikipedia
    page. Please be aware that Wikipedia pages are occasionally edited, so your results
    might be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – pandas DataFrame of the second element of Wikipedia data](img/B17105_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – pandas DataFrame of the second element of Wikipedia data
  prefs: []
  type: TYPE_NORMAL
- en: Gross. This looks even worse. I say that because it appears that we have some
    very short strings that look like web page sections. This doesn’t look very useful.
    Remember, `pandas` is great for loading tabular data, but not great on tables
    of text data, which Wikipedia uses. We can already see that this is less useful
    than the results we very easily captured using the Wikipedia library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try a page that has useful data in a table. This page contains tabular
    data about crime in Oregon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show us the last five rows of a `Pandas` DataFrame containing Oregon
    crime data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Pandas DataFrame of tabular numeric Wikipedia data](img/B17105_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Pandas DataFrame of tabular numeric Wikipedia data
  prefs: []
  type: TYPE_NORMAL
- en: Wow, this looks like useful data. However, the data does not show anything more
    recent than 2009, which is quite a while ago. Maybe there is a better dataset
    out there that we should use instead. Regardless, this shows that `Pandas` can
    easily pull tabular data off the web. However, there are a few things to keep
    in mind.
  prefs: []
  type: TYPE_NORMAL
- en: First, if you use scraped data in your projects, know that you are putting yourself
    at the mercy of the website administrators. If they decided to throw away the
    data table or rename or reorder the columns, that may break your application if
    it reads directly from Wikipedia. You can protect yourself against this by keeping
    a local copy of the data when you do a scrape, as well as including error handling
    in your code to watch for exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Be prepared for the worst. When you are scraping, build in any error-checking
    that you need as well as ways of knowing when your scrapers are no longer able
    to pull data. Let’s move on to the next approach – using `NLTK`.
  prefs: []
  type: TYPE_NORMAL
- en: Python library – NLTK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s get straight to it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, NLTK does not come installed with Jupyter, so you will need to install
    it. You can do so with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Python’s **NLTK** library can easily pull data from Project Gutenberg, which
    is a library of over 60,000 freely available books. Let’s see what we can get:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can easily see, this is far fewer than 60,000 results, so we are limited
    in what we can get using this approach, but it is still useful data for practicing
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what is inside the `blake-poems.txt` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can load the entire file. It is messy in that it contains line breaks and
    other formatting, but we will clean that out. What if we want one of the other
    books from the full library of 60,000 books that are not on this list? Are we
    out of luck? Browsing the website, I can see that I can read the plain text of
    one of my favorite books, Franz Kafka’s *The Metamorphosis*, at [https://www.gutenberg.org/files/5200/5200-0.txt](https://www.gutenberg.org/files/5200/5200-0.txt).
    Let’s try to get that data, but this time, we will use Python’s Requests library.
  prefs: []
  type: TYPE_NORMAL
- en: Python library – Requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Requests** library comes pre-installed with Python, so there should be
    nothing to do but import it. Requests is used to scrape raw text off the web,
    but it can do more. Please research the library to learn about its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you use this approach, please note that you should not do this aggressively.
    It is okay to load one book at a time like this, but if you attempt to download
    too many books at once, or aggressively crawl all books available on Project Gutenberg,
    you will very likely end up getting your IP address temporarily blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our demonstration, let’s import the library and then scrape the raw text
    from Gutenberg’s offering of *The Metamorphosis*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: I chopped off all text after “`vermin`” just to briefly show what is in the
    data. Just like that, we’ve got the full text of the book. As was the case with
    NLTK, the data is full of formatting and other characters, so this data needs
    to be cleaned before it will be useful. Cleaning is a very important part of everything
    that I will explain in this book. Let’s keep moving; I will show some ways to
    clean text during these demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve shown how easy it is to pull text from Gutenberg or Wikipedia. But these
    two do not even begin to scratch the surface of what is available for scraping
    on the internet. `pandas` can read tabular data from any web page, but that is
    limited. Most content on the web is not perfectly formatted or clean. What if
    we want to set up scrapers to harvest text and content from various news websites
    that we are interested in? NLTK won’t be able to help us get that data, and Pandas
    will be limited in what it can return. What are our options? We saw that the Requests
    library was able to pull another Gutenberg book that wasn’t available through
    NLTK. Can we similarly use requests to pull HTML from websites? Let’s try getting
    some news from Okinawa, Japan!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: There we go! We’ve just loaded the raw HTML from the given URL, and we could
    do that for any publicly accessible web page. However, if you thought the Gutenberg
    data was messy, look at this! Do we have any hope in the world of using this,
    let alone building automation to parse HTML and mine useful data? Amazingly, the
    answer is yes, and we can thank the `BeautifulSoup` Python library and other scraping
    libraries for that. They have truly opened up a world of data for us. Let’s see
    what we can get out of this using `BeautifulSoup`.
  prefs: []
  type: TYPE_NORMAL
- en: Python library – BeautifulSoup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, `BeautifulSoup` is used along with the `Requests` library. Requests
    comes pre-installed with Python, but `BeautifulSoup` does not, so you will need
    to install it. You can do so with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s a lot you can do with `BeautifulSoup`, so please go explore the library.
    But what would it take to extract all of the links from the Okinawa News URL?
    This is how you can do exactly that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: That was easy! I am only showing the first several extracted links. The first
    line imported the library, the second line set `BeautifulSoup` up for parsing
    HTML, the third line looked for all links containing an `href` attribute, and
    then finally the last line displayed the links. How many links did we harvest?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Your result might be different, as the page may have been edited after this
    book was written.
  prefs: []
  type: TYPE_NORMAL
- en: '277 links were harvested in less than a second! Let’s see whether we can clean
    these up and just extract the URLs. Let’s not worry about the link text. We should
    also convert this into a list of URLs rather than a list of `<a>` HTML tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using list comprehension and `BeautifulSoup` to extract the value
    stored in `href` for each of our harvested links. I can see that there is some
    duplication, so we should remove that before eventually storing the results. Let’s
    see whether we lost any of the former 277 links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect! Let’s take one of them and see whether we can extract the raw text
    from the page, with all HTML removed. Let’s try this URL that I have hand-selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Done! We have captured pretty clean and usable text from a web page! This can
    be automated to constantly harvest links and text from any website of interest.
    Now, we have what we need to get to the fun stuff in this chapter: extracting
    entities from text, and then using entities to build social networks. It should
    be obvious by now that some cleanup will be needed for any of the options that
    we have explored, so let’s just make short work of that now. To get this perfect,
    you need to do more than I am about to do, but let’s at least make this somewhat
    usable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing that stands out to me is the amount of text formatting and
    special characters that exist in this text. We have a few options. First, we could
    convert all of the line spaces into empty spaces. Let’s see what that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'If you scroll further down the text, you may see that the story ends at “`Go
    to Japanese`,” so let’s remove that as well as everything after:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that the cutoff string starts at the 1,984th character. Let’s keep
    everything up to the cutoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This has successfully removed the footer junk, but there is still some header
    junk to deal with, so let’s see whether we can remove that. This part is always
    tricky, and every website is unique in some way, but let’s remove everything before
    the story as an exercise. Looking closer, I can see that the story starts at the
    second occurrence of “`Hirokazu Uevonabaru`.” Let’s capture everything from that
    point and beyond. We will be using `.rindex()` instead of `.index()`to capture
    the last occurrence. This code is too specific for real-world use, but hopefully,
    you can see that you have options for cleaning dirty data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: If you are not familiar with Python, that might look a bit strange to you. We
    are keeping everything after the beginning of the last occurrence of “`Hirokazu
    Ueyonabaru`,” which is where the story starts. How does it look now?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: That looks just about perfect! There is always more cleaning that can be done,
    but this is good enough! When you first get started with any new scraping, you
    will need to clean, inspect, clean, inspect, clean, and inspect – gradually, you
    will stop finding obvious things to remove. You don’t want to cut too much. Just
    get the text to be usable – we will do additional cleaning at later steps.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between libraries, APIs, and source data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As part of this demonstration, I showed several ways to pull useful data off
    of the internet. I showed that several libraries have ways to load data directly
    but that there are limitations to what they have available. NLTK only offered
    a small portion of the complete Gutenberg book archive, so we had to use the `Requests`
    library to load *The Metamorphosis*. I also demonstrated that `Requests` accompanied
    by `BeautifulSoup` can easily harvest links and raw text.
  prefs: []
  type: TYPE_NORMAL
- en: Python libraries can also make loading data very easy when those libraries have
    data loading functionality as part of their library, but you are limited by what
    those libraries make available. If you just want some data to play with, with
    minimal cleanup, this may be ideal, but there will still be cleanup. You will
    not get away from that when working with text.
  prefs: []
  type: TYPE_NORMAL
- en: Other web resources expose their own APIs, which makes it pretty simple to load
    data after sending a request to them. Twitter does this. You authenticate using
    your API key, and then you can pull whatever data you want. This is a happy middle-ground
    between Python libraries and web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, web scraping opens up the entire web to you. If you can access a web
    page, you can scrape it and use any text and data that it has made available.
    You have flexibility with web scraping, but it is more difficult, and the results
    require more cleanup.
  prefs: []
  type: TYPE_NORMAL
- en: 'I tend to approach my own scraping and data enrichment projects by making considerations
    in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Is there a Python library that will make it easy for me to load the data I want?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No? OK, is there an API that I can use to pull the data that I want?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No? OK, can I just scrape it using `BeautifuSoup`? Yes? Game on. Let’s dance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start simple and scale out in terms of complexity only as needed. Starting simple
    means starting with the simplest approach – in this case, Python libraries. If
    libraries won’t help, add a little complexity by seeing whether an API is available
    to help and whether it is affordable. If one is not available, then web scraping
    is the solution that you need, and there is no way around it, but you will get
    the data that you need.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have text, we are going to move into NLP. Specifically, we will
    be using **PoS tagging and NER** as two distinct ways to extract entities (people
    and things) from raw text.
  prefs: []
  type: TYPE_NORMAL
- en: Using NLTK for PoS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I will explain how to do what is called PoS tagging using the
    NLTK Python library. NLTK is an older Python NLP library, but it is still very
    useful. There are also pros and cons when comparing NLTK with other Python NLP
    libraries, such as `spaCy`, so it doesn’t hurt to understand the pros and cons
    of each. However, during my coding for this demonstration, I realized just how
    much easier `spaCy` has made both PoS tagging as well as NER, so, if you want
    the easiest approach, feel free to just skip ahead to `spaCy`. I am still fond
    of `NLTK`, and in some ways, the library still feels more natural to me than `spaCy`,
    but that may simply be due to years of use. Anyway, I’d like to demonstrate `PoS
    tagging` with `NLTK`, and then I will demonstrate both `PoS tagging` and NER with
    `spaCy` in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: PoS tagging is a process that takes text tokens and identifies the PoS that
    the token belongs to. Just as a review, a token is a single word. A token might
    be *apples*.
  prefs: []
  type: TYPE_NORMAL
- en: With NLP, tokens are useful, but bigrams are often even more useful, and they
    can improve the results for text classification, sentiment analysis, and even
    unsupervised learning. A bigram is essentially two tokens – for example, t*wo
    tokens*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s not overthink this. It’s just two tokens. What do you think a trigram
    is? That’s right – three tokens. For instance, if filler words were removed from
    some text before the trigrams were captured, you could have one for *green* *eggs
    ham*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different `pos_tags`, and you can see the list here: [https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).'
  prefs: []
  type: TYPE_NORMAL
- en: For the work that we are doing, we will only use the NLP features that we need,
    and `PoS tagging` and NER are two different approaches that are useful for identifying
    entities (people and things) that are being described in text. In the mentioned
    list, the ones that we want are NNP and NNPS, and in most cases, we’ll find NNP,
    not NNPS.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain what we are trying to do, we are going to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get some text to work with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the text into sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split each sentence into tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the PoS tag for each token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract each token that is a proper noun.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A proper noun is the name of a person, place, or thing. I have been saying
    that we want to extract entities and define entities as people, places, or things,
    so the NNP tag will identify exactly what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get to work and get some text data!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used this code previously to load the entire text from Kafka’s book *The
    Metamorphosis*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot of junk in the header of this file, but the story starts at
    “`One morning`,” so let’s remove everything from before that. Feel free to explore
    the `text` variable as we go. I am leaving out repeatedly showing the data to
    save space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have identified the starting point of the phrase, `One morning`, and
    then removed everything up to that point. It’s all just header junk that we don’t
    need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, if you look at the bottom of the text, you can see that the story ends
    at, `*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS`, so let’s cut from
    that point onward as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Look closely at the cutoff and you will see that the cutoff is in a different
    position from that used for removing the header. I am essentially saying, “*Give
    me everything up to the cutoff*.” How does the ending text look now?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Not bad. That’s a noticeable improvement, and we are getting closer to clean
    text. Apostrophes are also being mangled, being shown as `â\x80\x99`, so let’s
    replace those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'That is about perfect, so let’s convert these steps into a reusable function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should have fairly clean text after running this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Outstanding! We are now ready for the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, I want to explain one thing: if you do `PoS tagging` on
    the complete text of any book or article, then the text will be treated as one
    massive piece of text, and you lose your opportunity to understand how entities
    relate and interact. All you will end up with is a giant entity list, which isn’t
    very helpful for our needs, but it can be useful if you just want to extract entities
    from a given piece of text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, the first thing you need to do is split the text into sentences,
    chapters, or some other desirable bucket. For simplicity, let’s do this by sentences.
    This can easily be done using NLTK’s sentence tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '*Beautiful*! We have a list of sentences to work with. Next, we want to take
    each of these sentences and extract any mentioned entities. We want the NNP-tagged
    tokens. This part takes a little work, so I will walk you through it. If we just
    feed the sentences to NLTK’s `pos_tag tool`, it will misclassify everything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Good effort, but that is not what we need. What we need to do is go through
    each sentence and identify the PoS tags, so let’s do this manually for a single
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to tokenize the sentence. There are many different tokenizers
    in NLTK, with strengths and weaknesses. I have gotten comfortable with the casual
    tokenizer, so I’ll just use that. The casual tokenizer does well with casual text,
    but there are several other tokenizers available to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Great. Now, for each token, we can find its corresponding `pos_tag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s also perfect! We are interested in extracting the NNPs. Can you see
    the two tokens that we want to extract? That’s right, it’s Gregor Samsa. Let’s
    loop through these PoS tags and extract the NNP tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we need. NER would hopefully identify these two results as one
    person, but once this is thrown into a graph, it’s very easy to correct. Let’s
    convert this into a function that will take a sentence and return the NNP tokens
    – the entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: That looks good. Let’s give it a shot!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s be bold and try this out against every sentence in the entire book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to make analysis a bit easier, let’s do two things: first, let’s replace
    those empty lists with `None`. Second, let’s throw all of this into a `Pandas`
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us a DataFrame of sentences and entities extracted from sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Pandas DataFrame of sentence entities](img/B17105_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Pandas DataFrame of sentence entities
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s a good start. We can see that “**What’s**” has been incorrectly flagged
    by NLTK, but it’s normal for junk to get through when dealing with text. That’ll
    get cleaned up soon. For now, we want to be able to build a social network using
    this book, so let’s grab every entity list that contains two or more entities.
    We need at least two to identify a relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks pretty good other than some punctuation sneaking in. Let’s revisit
    the previous code and disregard any non-alphabetical characters. That way, `Gregor''s`
    will become `Gregor`, `I''d` will become `I`, and so forth. That’ll make cleanup
    a lot easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s put this back into a DataFrame and repeat our steps to see whether the
    entity list is looking better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'This is getting better, but some double quotes are still in the data. Let’s
    just remove any punctuation and anything after:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'This is good enough! We could add a little more logic to prevent the same token
    from appearing twice in a row, but we can very easily remove those from a network,
    so let’s refactor our code and move on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: This is excellent. At this point, no matter whether we do PoS tagging or NER,
    we want an entity list, and this is close enough. Next, we will do the same using
    spaCy, and you should be able to see how spaCy is much easier in some regards.
    However, there is more setup involved, as you need to install a language model
    to work with spaCy. There are pros and cons to everything.
  prefs: []
  type: TYPE_NORMAL
- en: Using spaCy for PoS tagging and NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I am going to explain how to do what we have just done with
    NLTK, but this time using spaCy. I will also show you how to use NER as an often-superior
    alternative to PoS tagging for identifying and extracting entities. Before I started
    working on this chapter, I primarily used NLTK’s `PoS tagging` as the heart of
    my entity extraction, but since writing the code for this section and exploring
    a bit more, I have come to realize that spaCy has improved quite a bit, so I do
    think that what I am about to show you in this section is superior to what I previously
    did with NLTK. I do think that it is helpful to explain the usefulness of NLTK.
    Learn both and use what works best for you. But for entity extraction, I believe
    spaCy is superior in terms of ease of use and speed of delivery.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, I wrote a function to load Franz Kafka’s book *The Metamorphosis*,
    so we will use that loader here as well, as it has no dependence on either NLTK
    or spaCy, and it can be easily modified to load any book from the Project Gutenberg
    archive.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do anything with spaCy, the first thing you need to do is load the spaCy
    language model of choice. Before we can load it, we must first install it. You
    can do so by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several different models available, but the three that I use for
    English text are small, medium, and large. `md` stands for medium. You could swap
    that out for `sm` or `lg` to get the small or large models, respectively. You
    can find out more about spaCy’s models here: [https://spacy.io/usage/models](https://spacy.io/usage/models).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been installed, we can load it into our Python scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned previously, you could swap that `md` for `sm` or `lg`, depending
    on what model you want to use. The larger model requires more storage and memory
    for use. The smaller model requires less. Pick the one that works well enough
    for what you are doing. You may not need the large model. The medium and small
    ones work well, and the difference between models is often unnoticeable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need some text, so let’s reuse the function that we previously wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few extra spaces in the middle of sentences, but that won’t present
    any problem for us at all. Tokenization will clean that up without any additional
    effort on our part. We will get to that shortly. First, as done with NLTK, we
    need to split the text into sentences so that we can build a network based on
    the entities that are uncovered in each sentence. That is much easier to do using
    spaCy rather than NLTK, and here is how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line feeds the full text of *The Metamorphosis* into spaCy and uses
    our language model of choice, while the second line extracts the sentences that
    are in the text. Now, we should have a Python list of sentences. Let’s inspect
    the first six sentences in our list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Six probably seems like a strange number of sentences to inspect, but I wanted
    to show you something. Take a look at the last two sentences. SpaCy has successfully
    extracted the main character’s inner dialog as a standalone sentence, as well
    as created a separate sentence to complete the surrounding sentence. For our entity
    extraction, it wouldn’t present any problem at all if those sentences were combined,
    but I like this. It’s not a bug, it’s a feature, as we software engineers like
    to say.
  prefs: []
  type: TYPE_NORMAL
- en: SpaCy PoS tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our sentences, let’s use spaCy’s PoS tagging as pre-processing
    for entity extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice. What we want are the NNPs as these are proper nouns. You can see this
    if you use `pos_` instead of `tag_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s add a little logic for extracting them. We need to do two things – we
    need a list to store the results, and we need some logic to extract the NNPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'For NLTK, we created a function that would extract entities for a given sentence,
    but this way, I just did everything in one go, and it was quite simple. Let’s
    convert this into a function so that we can easily use this for other future work.
    Let’s also prevent empty lists from being returned inside the entity list, as
    we have no use for those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now have a clean entity list, with no empty inner lists included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: This looks a lot better than NLTK’s results, and with fewer steps. This is simple
    and elegant. I didn’t need to use `Pandas` for anything, drop empty rows, or clean
    out any punctuation that somehow slipped in. We can use this function on any text
    we have, after cleaning. You can use it before cleaning, but you’ll end up getting
    a bunch of junk entities, especially if you use it against scraped web data.
  prefs: []
  type: TYPE_NORMAL
- en: SpaCy NER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SpaCy’s NER is equally easy and simple. The difference between `PoS tagging`
    and NER is that NER goes a step further and identifies people, places, things,
    and more. For a detailed description of spaCy’s linguistic features, I heartily
    recommend Duygu Altinok’s book *Mastering spaCy*. To be concise, spaCy labels
    tokens as one of *18* different kinds of entities. In my opinion, that is a bit
    excessive, as `MONEY` is not an entity, but I just take what I want. Please check
    spaCy for the full list of entity types. What we want are entities that are labeled
    as `PERSON`, `ORG`, or `GPE`. `ORG` stands for organization, and `GPE` contains
    countries, cities, and states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s loop through all the tokens in the first sentence and see how this looks
    in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'This works, but there is a slight problem: we want Gregor Samsa to appear as
    one entity, not as two. What we need to do is create a new spaCy doc and then
    loop through the doc’s `ents` rather than the individual tokens. In that regard,
    the NER approach is slightly different from `PoS tagging`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect! Let’s do a few things: we will redo our previous entity extraction
    function, but this time using NER rather than `PoS tagging`, and then limit our
    entities to `PERSON`, `ORG`, and `GPE`. Please note that I am only adding entities
    if there is more than one in a sentence. We are looking to identify relationships
    between people, and you need at least two people to have a relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'I added a bit of code to remove any whitespace that has snuck in and also to
    remove duplicates from each `sentence_entity` list. I also removed any `''s` characters
    that appeared after a name – for example, `Gregor''s` – so that it’d show up as
    `Gregor`. I could have cleaned this up in network cleanup, but this is a nice
    optimization. Let’s see how our results look. I named the entity list `morph_entities`
    for `metaMORPHosis` entities. I wanted a descriptive name, and this is the best
    I could come up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: That looks great! Wow, I haven’t read *The Metamorphosis* in many years and
    forgot how few characters were in the story!
  prefs: []
  type: TYPE_NORMAL
- en: SpaCy’s NER is done using a pre-trained deep learning model, and machine learning
    is never perfect. Please keep that in mind. There is always some cleanup. SpaCy
    allows you to customize their language models for your documents, which is useful
    if you work within a specialized domain, but I prefer to use spaCy’s models as
    general-purpose tools, as I deal with a huge variety of different text. I don’t
    want to customize spaCy for tweets, literature, disinformation, and news. I would
    prefer to just use it as is and clean up as needed. That has worked very well
    for me.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to see that there is a lot of duplication. Apparently, in
    *The Metamorphosis*, a lot of time is spent talking about Gregor. We will be able
    to remove those duplicates with a single line of `NetworkX` code later, so I’ll
    just leave them in rather than tweaking the function. Good enough is good enough.
    If you are working with massive amounts of data and paying for cloud storage,
    you should probably fix inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this chapter, I’m going to use the NER results as our network
    data. We could just as easily use the `pos_tag` entities, but this is better,
    as NER can combine first name and last name. In our current entities, none of
    those came through, but they will with other text. This is just how *The Metamorphosis*
    was written. We’ll just clean that up as part of the network creation.
  prefs: []
  type: TYPE_NORMAL
- en: Just for a sanity check, let’s check the entities from *Alice’s Adventures*
    *in Wonderland*!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: I agree with you, Alice.
  prefs: []
  type: TYPE_NORMAL
- en: I have tweaked the loading function to load the book *Alice’s Adventures in
    Wonderland* and chop off any header or footer text. That’s actually kind of funny.
    OFF WITH THEIR HEAD(er)S! Let’s try extracting entities. I expect this to be a
    bit messy, as we are working with fantasy characters, but let’s see what happens.
    Something by Jane Austen might give better results. We’ll see!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: That is much better than I expected, but some junk has snuck in. We’ll use both
    of these entity lists for network creation and visualization. This is a good foundation
    for our next steps. Now that we have some pretty useful-looking entities, let’s
    work toward creating a Pandas DataFrame that we can load into a NetworkX graph!
    That’s what’s needed to convert an entity list into an actual social network.
  prefs: []
  type: TYPE_NORMAL
- en: 'That concludes our demonstration on using spaCy for both PoS tagging as well
    as NER. I hope you can see that although there was one additional dependency (the
    language model), the process of entity extraction was much simpler. Now, it is
    time to move on to what I think is the most exciting part: converting entity lists
    into network data, which is then used to create a social network, which we can
    visualize and investigate.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting entity lists into network data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have pretty clean entity data, it is time to convert it into a
    Pandas DataFrame that we can easily load into NetworkX for creating an actual
    social network graph. There’s a bit to unpack in that sentence, but this is our
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Load text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract entities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create network data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a graph using network data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, I use the terms graph and network interchangeably. That does cause confusion,
    but I did not come up with the names. I prefer to say “network,” but then people
    think I am talking about computer networks, so then I have to remind them that
    I am talking about graphs, and they then think I am talking about bar charts.
    You just can’t win when it comes to explaining graphs and networks to those who
    are not familiar, and even I get confused when people start talking about networks
    and graphs. Do you mean nodes and edges, or do you mean TCP/IP and bar charts?
    Oh well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this next part, we do have choices in how we implement this, but I will
    explain my typical method. Look at the entities from *Alice’s Adventures* *in
    Wonderland*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'In most of these, there are only two entities in each inner list, but sometimes,
    there are three or more. What I usually do is consider the first entity as the
    source and any additional entities as targets. What does that look like in a sentence?
    Let’s take this sentence: “*Jack and Jill went up the hill to say hi to their
    friend Mark.*” If we converted that into entities, we would have this list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'To implement my approach, I will take the first element of my list and add
    it to my sources list, and then take everything after the first element and add
    that to my target list. Here is what that looks like in code, but using entities
    from *Alice*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Take a close look at the two lines that capture both `source` and `targets`.
    `source` is the first element of each entity list, and `targets` is everything
    after the first element of each entity list. Then, for each target, I add the
    source and target to `final_sources` and `final_targets`. I loop through `targets`
    because there can be one or more of them. There will never be more than one `source`,
    as it is the first element. This is important to understand because this procedure
    is crucial for how relationships are shown in the resulting social network. We
    could have used an alternative approach of linking each entity to the other, but
    I prefer my shown approach. Later lists may bridge any gaps if there is evidence
    of those relationships. How do our final sources look?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Both look great. Remember, we used NER to capture people, places, and things,
    so this looks fine. Later, I will very easily drop a few of these sources and
    targets directly from the social network. This is good enough for now.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of taking the first element and linking it to targets is something
    that I still regularly consider. Another approach would be to take every entity
    that appears in the same sentence and link them together. I prefer my approach,
    but you should consider both options.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first entity interacts with other entities from the same sentence, but
    it is not always the case that all entities interact with each other. For instance,
    look at this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: “*John went to see his good friend Aaron, and then he went to the park* *with
    Jake.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the entity list would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, Aaron may know Jake, but we can’t tell for certain based on
    this sentence. The hope is that if there is a relationship, that will be picked
    up eventually. Maybe in another sentence, such as this one:'
  prefs: []
  type: TYPE_NORMAL
- en: “*Aaron and Jake went ice skating and then ate* *some pizza.*”
  prefs: []
  type: TYPE_NORMAL
- en: After that sentence, there will be a definite connection. My preferred approach
    requires further evidence before connecting entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have code to take an entity list and create two lists: `final_sources`
    and `final_targets`, but this isn’t practical for feeding to NetworkX to create
    a graph. Let’s do two more things: use these two lists to create a Pandas DataFrame,
    and then create a reusable function that takes any entity list and returns this
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: That looks great. Let’s see it in action!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display a DataFrame of network data consisting of source and target
    nodes. This is called an edge list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Pandas DataFrame of Alice in Wonderland entity relationships](img/B17105_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Pandas DataFrame of Alice in Wonderland entity relationships
  prefs: []
  type: TYPE_NORMAL
- en: Great. How does it do with our entities from *The Metamorphosis*?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the network edge list for *The Metamorphosis*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Pandas DataFrame of The Metamorphosis entity relationships](img/B17105_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Pandas DataFrame of The Metamorphosis entity relationships
  prefs: []
  type: TYPE_NORMAL
- en: Perfect, and the function is reusable!
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to convert both of these into actual NetworkX graphs. This
    is where things get interesting, in my opinion. Everything we did previously was
    just pre-processing. Now, we get to play with networks and specifically social
    network analysis! After this chapter, we will primarily be learning about social
    network analysis and network science. There are some areas of NLP that I blew
    past, such as lemmatization and stemming, but I purposefully did so because they
    are less relevant to extracting entities than PoS tagging and NER. I recommend
    that you check out Duygu Altinok’s book *Mastering spaCy* if you want to go deeper
    into NLP. This is as far as we will go with NLP in this book because it is all
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: Converting network data into networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is time to take our created network data and create two graphs, one for *Alice’s
    Adventures in Wonderland*, and another for *The Metamorphosis*. We aren’t going
    to dive deep into network analysis yet, as that is for later chapters. But let’s
    see how they look and see what insights emerge.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to import the NetworkX library, and then we need to create our
    graphs. This is extremely easy to do because we have created Pandas DataFrames,
    which NetworkX will use. This is the easiest way I have found of creating graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, if you haven’t done so yet, you need to install NetworkX. You can do
    so with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have installed NetworkX, let’s create our two networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s that easy. We have already done the difficult work in our text pre-processing.
    Did it work? Let’s peek into each graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: Nice! Already, we are gaining insights into the differences between both social
    networks. In a graph, a node is just a thing that has relationships with other
    nodes, typically. Nodes without any relationships are called isolates, but due
    to the way our graphs have been constructed, there will be no isolates. It’s not
    possible, as we looked for sentences with two or more entities. Try to take a
    mental picture of those two entities as dots with a line between them. That’s
    literally what a graph/network visualization looks like, except that there are
    typically many dots and many lines. The relationship that exists between two nodes
    is called an edge. You will need to understand the difference between nodes and
    edges to work on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the summary information about the *Alice* graph, we can see that
    there are 68 nodes (characters) and 71 edges (relationships between those characters).
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the summary information about the network from *The Metamorphosis*,
    we can see that there are only three nodes (characters) and three edges (relationships
    between those characters. When visualized, this is going to be a really basic
    network to look at, so I am glad that we did *Alice* as well.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other useful metrics and summaries tucked away inside NetworkX,
    and we will discuss those when we go over centralities, shortest paths, and other
    social network analysis and network science topics.
  prefs: []
  type: TYPE_NORMAL
- en: Doing a network visualization spot check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s visualize these networks, take a brief look, and then complete this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Here are two visualization functions that I frequently use. In my opinion, `sknetwork`,
    I have not looked back to NetworkX for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: The first function converts a NetworkX graph into an adjacency matrix, which
    sknetwork uses to calculate `PageRank` (an importance score) and then to render
    the network as an SVG image. The second function uses the first function, but
    the goal is to visualize an `ego_graph`, which will be described later. In an
    ego graph, you explore the relationships that exist around a single node. The
    first function is more general-purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough talk. This will be more understandable when you see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s create a function for displaying ego graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be clear, having `import` statements inside a function is not ideal. It
    is best to keep import statements external to functions. However, in this case,
    it makes it easier to copy and paste into your various Jupyter or Colab notebooks,
    so I am making an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'Look closely at these two functions. Pick them apart and try to figure out
    what they are doing. To quickly complete this chapter, I’m going to show the results
    of using these functions. I have abstracted away the difficulty of visualizing
    these networks so that you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'As we are not passing any parameters to the function, this should display a
    very simple network visualization. It will look like a bunch of dots (nodes),
    with some dots connected to other dots by a line (edge):'
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs: []
  type: TYPE_NORMAL
- en: Please keep the `draw_graph` function handy. We will use it throughout this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Rough social network of Alice’s Adventures in Wonderland](img/B17105_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Rough social network of Alice’s Adventures in Wonderland
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, that’s a bit unhelpful to look at, but this is intentional. I typically
    work with large networks, so I prefer to keep node names left out at first so
    that I can visually inspect the network. However, you can override the default
    values I am using. Let’s do that as well as decrease the line width a bit, increase
    the node size, and add node names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: This will draw our social network, with labels!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Labeled social network of Alice’s Adventures in Wonderland](img/B17105_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Labeled social network of Alice’s Adventures in Wonderland
  prefs: []
  type: TYPE_NORMAL
- en: 'The font size is a bit small and difficult to read, so let’s increase that
    and reduce `node_size` by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the following network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Finalized social network of Alice in Wonderland](img/B17105_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Finalized social network of Alice in Wonderland
  prefs: []
  type: TYPE_NORMAL
- en: That is excellent. Consider what we have done. We have taken raw text from *Alice*,
    extracted all entities, and built the social network that is described in this
    book. This is so powerful, and it also opens the door for you to learn more about
    social network analysis and network science. For instance, would you rather analyze
    somebody else’s toy dataset, or would you rather investigate something you are
    interested in, such as your favorite book? I prefer to chase my own curiosity.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what the ego graph looks like around Alice!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Alice ego graph](img/B17105_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Alice ego graph
  prefs: []
  type: TYPE_NORMAL
- en: What?! That’s incredible. We can see that some trash came through in the entity
    list, but we’ll learn how to clean that up in the next chapter. We can take this
    one step further. What if we want to take Alice out of her ego graph and just
    explore the relationships that exist around her? Is that possible?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Alice ego graph with dropped center](img/B17105_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Alice ego graph with dropped center
  prefs: []
  type: TYPE_NORMAL
- en: Too easy. But it’s difficult to analyze the clusters of groups that exist. After
    dropping the center, many nodes became isolates. If only there were a way to remove
    the isolates so that we could more easily see the groups. OH, WAIT!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Alice ego graph with dropped center and dropped isolates](img/B17105_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Alice ego graph with dropped center and dropped isolates
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the *Alice* social network looks pretty good. There’s some cleanup
    to do, but we can investigate relationships. What does the social network of *The
    Metamorphosis* look like? Remember, there are only three nodes and three edges.
    Even Alice’s ego graph is more complicated than the social network from *The Metamorphosis*.
    Let’s visualize it!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the following network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Labeled social network of The Metamorphosis](img/B17105_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Labeled social network of The Metamorphosis
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait, but why are there six edges? I only see three. The reason is that `sknetwork`
    will draw multiple edges as a single edge. We do have options, such as increasing
    the line width according to the number of edges but let’s just look at the Pandas
    DataFrame to make sure my thinking is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'This gets us the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Pandas DataFrame of network data for The Metamorphosis](img/B17105_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Pandas DataFrame of network data for The Metamorphosis
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we drop duplicates?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Pandas DataFrame of network data for The Metamorphosis (dropped
    duplicates)](img/B17105_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Pandas DataFrame of network data for The Metamorphosis (dropped
    duplicates)
  prefs: []
  type: TYPE_NORMAL
- en: Aha! There is a relationship between Gregor and Grete, but the reverse is also
    true. One thing that I can see is that Samsa links to Gregor and Grete, but Grete
    does not link back to Samsa. Another way of saying this, which we will discuss
    in this book, is that directionality also matters. You can have a directed graph.
    In this case, I am just using an undirected graph, because relationships are often
    (but not always) reciprocal.
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of this demonstration. We originally set out to take raw
    text and use it to create a social network, and we easily accomplished our goals.
    Now, we have network data to play with. Now, this book is going to get more interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Additional NLP and network considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been a marathon of a chapter. Please bear with me a little longer.
    I have a few final thoughts that I’d like to express, and then we can conclude
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleanup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, if you work with language data, there will always be cleanup. Language
    is messy and difficult. If you are only comfortable working with pre-cleaned tabular
    data, this is going to feel very messy. I love that, as every project allows me
    to improve my techniques and tactics.
  prefs: []
  type: TYPE_NORMAL
- en: 'I showed two different approaches for extracting entities: PoS tagging and
    NER. Both approaches work very well, but consider which approach gets us closer
    to a clean and useful entity list the quickest and easiest. With `PoS tagging`,
    we get one token at a time. With NER, we very quickly get to entities, but the
    models occasionally misbehave or don’t catch everything, so there is always cleanup
    with this as well.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no silver bullet. I want to use whatever approach gets me as close
    to the goal as quickly as possible because cleanup is inevitable. The less correction
    I have to do, the quicker I am playing with and pulling insights out of networks.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing PoS tagging and NER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PoS tagging can involve extra steps, but cleanup is often easier. On the other
    hand, NER can involve fewer steps, but you can get mangled results if you use
    it against scraped web text. There may be fewer steps for some things, but the
    cleanup may be daunting. I have seen spaCy’s NER false a lot on scraped web content.
    If you are dealing with web text, spend extra time on cleanup before feeding the
    data to NER.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, slightly messy results are infinitely better than no results. This
    stuff is so useful for enriching datasets and extracting the “who, what, and where”
    parts of any piece of text.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are also a few things to keep in mind when planning any scraping project.
    First, privacy. If you are scraping social media text, and you are extracting
    entities from someone else’s text, you are running surveillance on them. Ponder
    how you would feel if someone did the same to you. Further, if you store this
    data, you are storing personal data, and there may be legal considerations as
    well. To save yourself headaches, unless you work in government or law enforcement,
    it might be wise to just use these techniques against literature and news, until
    you have a plan for other types of content.
  prefs: []
  type: TYPE_NORMAL
- en: There are also ethical considerations. If you decide to use these techniques
    to build a surveillance engine, you should consider whether building this is an
    ethical thing to do. Consider whether it is ethical to run surveillance on random
    strangers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, scraping is like browsing a website, automatically, but scrapers can
    do damage. If you hit a website with a scraper a thousand times in a second, you
    could accidentally hit it with a **DoS** attack. Get what you need at the pace
    that you need it. If you are looping through all of the links on a website and
    then scraping them, add a 1-second delay before each scrape rather than hitting
    it a thousand times every second. You will be liable if you take down a web server,
    even by accident.
  prefs: []
  type: TYPE_NORMAL
- en: That was a lot of words just to say that unless you are using this for news
    or literature, be mindful of what you are doing. For news and literature, this
    can be revealing and may allow new technologies to be created. For other types
    of content, think about what you are doing before you jump into the work.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to find and scrape raw text, convert it into
    an entity list, and then convert that entity list into an actual social network
    so that we can investigate revealed entities and relationships. Did we capture
    the *who*, *what*, and *where* of a piece of text? Absolutely. I hope you can
    now understand the usefulness of NLP and social network analysis when used together.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I showed several ways to get data. If you are not familiar
    with web scraping, then this might seem a bit overwhelming, but it’s not so bad
    once you get started. However, in the next chapter, I will show several easier
    ways to get data.
  prefs: []
  type: TYPE_NORMAL
