["```py\npip install <library name>\n```", "```py\npip install networkx\n```", "```py\npip install beautifulsoup4\n```", "```py\npip install newspaper3k\n```", "```py\npip install newspaper3k\n```", "```py\nimport newspaper\ndomain = 'https://www.goodnewsnetwork.org'\npaper = newspaper.build(domain, memoize_articles=False)\nurls = paper.article_urls()\n```", "```py\nurls\n['https://www.goodnewsnetwork.org/2nd-annual-night-of-a-million-lights/',\n'https://www.goodnewsnetwork.org/cardboard-pods-for-animals-displaced-by-wildfires/',\n'https://www.goodnewsnetwork.org/category/news/',\n'https://www.goodnewsnetwork.org/category/news/animals/',\n'https://www.goodnewsnetwork.org/category/news/arts-leisure/',\n'https://www.goodnewsnetwork.org/category/news/at-home/',\n'https://www.goodnewsnetwork.org/category/news/business/',\n'https://www.goodnewsnetwork.org/category/news/celebrities/',\n'https://www.goodnewsnetwork.org/category/news/earth/',\n'https://www.goodnewsnetwork.org/category/news/founders-blog/']\n```", "```py\nurls = sorted([u for u in urls if 'category' not in u and len(u)>60])\n```", "```py\nurls[0:10]\n…\n['https://www.goodnewsnetwork.org/2nd-annual-night-of-a-million-lights/',\n 'https://www.goodnewsnetwork.org/cardboard-pods-for-animals-displaced-by-wildfires/',\n 'https://www.goodnewsnetwork.org/couple-living-in-darkest-village-lights-sky-with-huge-christmas-tree/',\n 'https://www.goodnewsnetwork.org/coya-therapies-develop-breakthrough-treatment-for-als-by-regulating-t-cells/',\n 'https://www.goodnewsnetwork.org/enorme-en-anidacion-de-tortugasen-tailandia-y-florida/',\n 'https://www.goodnewsnetwork.org/good-talks-sustainable-dish-podcast-with-shannon-hayes/',\n 'https://www.goodnewsnetwork.org/gopatch-drug-free-patches-good-gifts/',\n 'https://www.goodnewsnetwork.org/horoscope-from-rob-brezsnys-free-will-astrology-12-10-21/',\n 'https://www.goodnewsnetwork.org/how-to-recognize-the-eight-forms-of-capital-in-our-lives/',\n 'https://www.goodnewsnetwork.org/mapa-antiguo-de-la-tierra-te-deja-ver-su-evolucion/']\n```", "```py\nfrom newspaper import Article\nurl = urls[0]\narticle = Article(url)\narticle.download()\narticle.parse()\narticle.nlp()\n```", "```py\n    title = article.title\n    ```", "```py\n    title\n    ```", "```py\n    …\n    ```", "```py\n    'After Raising $2.8M to Make Wishes Come True for Sick Kids, The 'Night of a Million Lights' Holiday Tour is Back'\n    ```", "```py\n    text = article.text\n    ```", "```py\n    text[0:500]\n    ```", "```py\n    …\n    ```", "```py\n    'The Night of A Million Lights is back—the holiday spectacular that delights thousands of visitors and raises millions to give sick children and their weary families a vacation.\\n\\n'Give Kids The World Village' has launched their second annual holiday lights extravaganza, running until Jan. 2\\n\\nIlluminating the Central Florida skyline, the 52-night open house will once again provide the public with a rare glimpse inside Give Kids The World Village, an 89-acre, whimsical nonprofit resort that provide'\n    ```", "```py\n    summary = article.summary\n    ```", "```py\n    summary\n    ```", "```py\n    …\n    ```", "```py\n    'The Night of A Million Lights is back—the holiday spectacular that delights thousands of visitors and raises millions to give sick children and their weary families a vacation.\\nWhat began as an inventive pandemic pivot for Give Kids The World has evolved into Central Florida's most beloved new holiday tradition.\\n\"Last year's event grossed $2.8 million to make wishes come true for children struggling with illness and their families,\" spokesperson Cindy Elliott told GNN.\\nThe \n    ```", "```py\n    display features 1.25M linear feet of lights, including 3.2 million lights that were donated by Walt Disney World.\\nAll proceeds from Night of a Million Lights will support Give Kids The World, rated Four Stars by Charity Navigator 15 years in a row.'\n    ```", "```py\n    language = article.meta_lang\n    ```", "```py\n    language\n    ```", "```py\n    …\n    ```", "```py\n    'en'\n    ```", "```py\n    keywords = article.keywords\n    ```", "```py\n    keywords\n    ```", "```py\n    …\n    ```", "```py\n    ['million',\n    ```", "```py\n     'kids',\n    ```", "```py\n     'children',\n    ```", "```py\n     'lights',\n    ```", "```py\n     'world',\n    ```", "```py\n     'true',\n    ```", "```py\n     'tour',\n    ```", "```py\n     'wishes',\n    ```", "```py\n     'sick',\n    ```", "```py\n     'raising',\n    ```", "```py\n     'night',\n    ```", "```py\n     'village',\n    ```", "```py\n     'guests',\n    ```", "```py\n     'holiday',\n    ```", "```py\n     'wish']\n    ```", "```py\n    image = article.meta_img\n    ```", "```py\n    image\n    ```", "```py\n    …\n    ```", "```py\n    'https://www.goodnewsnetwork.org/wp-content/uploads/2021/12/Christmas-disply-Night-of-a-Million-Lights-released.jpg'\n    ```", "```py\n    from newspaper import Config\n    ```", "```py\n    config = Config()\n    ```", "```py\n    config.browser_user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 12.0; rv:95.0) Gecko/20100101 Firefox/95.0'\n    ```", "```py\n    config.request_timeout = 3\n    ```", "```py\n    import time\n    ```", "```py\n    time.sleep(1)\n    ```", "```py\n    import spacy\n    ```", "```py\n    nlp = spacy.load(\"en_core_web_md\")\n    ```", "```py\n    def extract_entities(text):\n    ```", "```py\n        doc = nlp(text)\n    ```", "```py\n        sentences = list(doc.sents)\n    ```", "```py\n        entities = []\n    ```", "```py\n        for sentence in sentences:\n    ```", "```py\n            sentence_entities = []\n    ```", "```py\n            sent_doc = nlp(sentence.text)\n    ```", "```py\n            for ent in sent_doc.ents:\n    ```", "```py\n                if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n    ```", "```py\n                    entity = ent.text.strip()\n    ```", "```py\n                    if \"'s\" in entity:\n    ```", "```py\n                        cutoff = entity.index(\"'s\")\n    ```", "```py\n                        entity = entity[:cutoff]\n    ```", "```py\n                    if entity != '':\n    ```", "```py\n                        sentence_entities.append(entity)\n    ```", "```py\n            sentence_entities = list(set(sentence_entities))\n    ```", "```py\n            if len(sentence_entities) > 1:\n    ```", "```py\n                entities.append(sentence_entities)\n    ```", "```py\n        return entities\n    ```", "```py\n    entities = extract_entities(text)\n    ```", "```py\n    entities\n    ```", "```py\n    …\n    ```", "```py\n    [['Night', 'USA'],  ['Florida', 'Kissimmee'],  ['GNN', 'Cindy Elliott'],  ['the Centers for Disease Control and Prevention', 'CDC'],  ['Florida', 'Santa'],  ['Disney World', 'Central Florida']]\n    ```", "```py\n    import pandas as pd\n    ```", "```py\n    def get_network_data(entities):\n    ```", "```py\n        final_sources = []\n    ```", "```py\n        final_targets = []\n    ```", "```py\n        for row in entities:\n    ```", "```py\n            source = row[0]\n    ```", "```py\n            targets = row[1:]\n    ```", "```py\n            for target in targets:\n    ```", "```py\n                final_sources.append(source)\n    ```", "```py\n                final_targets.append(target)\n    ```", "```py\n        df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n    ```", "```py\n        return df\n    ```", "```py\n    network_df = get_network_data(entities)\n    ```", "```py\n    network_df.head()\n    ```", "```py\nimport newspaper\nfrom newspaper import Article\nfrom tqdm import tqdm\ndef get_story_df(domain):\n    paper = newspaper.build(domain, memoize_articles=False)\n    urls = paper.article_urls()\n    urls = sorted([u for u in urls if 'category' not in u and len(u)>60])\n    titles = []\n    texts = []\n    languages = []\n    keywords = []\n    for url in tqdm(urls):\n        article = Article(url)\n        article.download()\n        article.parse()\n        article.nlp()\n        titles.append(article.title)\n        texts.append(article.text)\n        languages.append(article.meta_lang)\n        keywords.append(article.keywords)\n    df = pd.DataFrame({'urls':urls, 'title':titles, 'text':texts, 'lang':languages, 'keywords':keywords})\n    return df\n```", "```py\ndomain = 'https://www.goodnewsnetwork.org'\ndf = get_story_df(domain)\ndf.head()\n```", "```py\n    text = df['text'][1]\n    ```", "```py\n    entities = extract_entities(text)\n    ```", "```py\n    network_df = get_network_data(entities)\n    ```", "```py\n    G = nx.from_pandas_edgelist(network_df)\n    ```", "```py\n    draw_graph(G, show_names=True, node_size=4, edge_width=1, font_size=12)\n    ```", "```py\n    text = df['text'][3]\n    ```", "```py\n    entities = extract_entities(text)\n    ```", "```py\n    network_df = get_network_data(entities)\n    ```", "```py\n    G = nx.from_pandas_edgelist(network_df)\n    ```", "```py\n    draw_graph(G, show_names=True, node_size=4, edge_width=1, font_size=12)\n    ```", "```py\n    pip install python-twitter-v2\n    ```", "```py\n    from pytwitter import Api\n    ```", "```py\n    bearer_token = 'your_bearer_token'\n    ```", "```py\n    twitter_api = Api(bearer_token=bearer_token)\n    ```", "```py\n    def get_user_id(twitter_api, username):\n    ```", "```py\n        user_data = twitter_api.get_users(usernames=username)\n    ```", "```py\n        return user_data.data[0].id\n    ```", "```py\n    def get_timeline(twitter_api, username):\n    ```", "```py\n        tweet_fields = ['created_at', 'text', 'lang']\n    ```", "```py\n        user_id = get_user_id(twitter_api, username)\n    ```", "```py\n        timeline_data = twitter_api.get_timelines(user_id, return_json=True, max_results=100, tweet_fields=tweet_fields)\n    ```", "```py\n        df = pd.DataFrame(timeline_data['data'])\n    ```", "```py\n        df.drop('id', axis=1, inplace=True)\n    ```", "```py\n        return df\n    ```", "```py\n    df = get_timeline(twitter_api, 'officialsanta')\n    ```", "```py\n    df.head()\n    ```", "```py\n    def wrangle_and_enrich(df):\n    ```", "```py\n        # give some space for splitting, sometimes things get smashed together\n    ```", "```py\n        df['text'] = df['text'].str.replace('http', ' http')\n    ```", "```py\n        df['text'] = df['text'].str.replace('@', ' @')\n    ```", "```py\n        df['text'] = df['text'].str.replace('#', ' #')\n    ```", "```py\n        # enrich dataframe with user mentions and hashtags\n    ```", "```py\n        df['users'] = df['text'].apply(lambda tweet: [clean_user(token) for token in tweet.split() if token.startswith('@')])\n    ```", "```py\n        df['tags'] = df['text'].apply(lambda tweet: [clean_hashtag(token) for token in tweet.split() if token.startswith('#')])\n    ```", "```py\n        return df\n    ```", "```py\n    df = get_timeline(twitter_api, 'officialsanta')\n    ```", "```py\n    df = wrangle_and_enrich(df)\n    ```", "```py\n    df.head()\n    ```", "```py\ndef get_following(twitter_api, username):\n    user_fields = ['username', 'description']\n    user_id = get_user_id(twitter_api, username)\n    following = twitter_api.get_following(user_id=user_id, return_json=True, max_results=1000, user_fields=user_fields)\n    df = pd.DataFrame(following['data'])\n    return df[['name', 'username', 'description']]\n```", "```py\ndf = get_following(twitter_api, 'officialsanta')\ndf.head()\n```", "```py\ndef get_followers(twitter_api, username):\n    user_fields = ['username', 'description']\n    user_id = get_user_id(twitter_api, username)\n    followers = twitter_api.get_followers(user_id=user_id, return_json=True, max_results=1000, user_fields=user_fields)\n    df = pd.DataFrame(followers['data'])\n    return df[['name', 'username', 'description']]\n```", "```py\ndf = get_followers(twitter_api, 'officialsanta')\ndf.head()\n```", "```py\ndef search_tweets(twitter_api, search_string):\n    tweet_fields = ['created_at', 'text', 'lang']\n    user_fields = ['username']\n    expansions = ['author_id']\n    search_data = twitter_api.search_tweets(search_string, return_json=True, expansions=expansions, tweet_fields=tweet_fields, user_fields=user_fields, max_results=100)\n    df = pd.DataFrame(search_data['data'])\n    user_df = pd.DataFrame(search_data['includes']['users'])\n    df = df.merge(user_df, left_on='author_id', right_on='id')\n    df['username'] = df['username'].str.lower()\n    return df[['username', 'text', 'created_at', 'lang']]\n```", "```py\ndf = search_tweets(twitter_api, 'natural language processing')\ndf = wrangle_and_enrich(df)\ndf.head()\n```", "```py\ndef extract_user_network_data(df):\n    user_network_df = df[['username', 'users', 'text']].copy()\n    user_network_df = user_network_df.explode('users').dropna()\n    user_network_df['users'] = user_network_df['users'].str.replace('\\@', '', regex=True)\n    user_network_df.columns = ['source', 'target', 'count'] # text data will be counted\n    user_network_df = user_network_df.groupby(['source', 'target']).count()\n    user_network_df.reset_index(inplace=True)\n    user_network_df.sort_values(['source', 'target'], ascending=[True, True])\n    return user_network_df\n```", "```py\nuser_network_df = extract_user_network_data(df)\nuser_network_df.head()\n```", "```py\ndef extract_hashtag_network_data(df):\n    hashtag_network_df = df[['users', 'tags', 'text']].copy()\n    hashtag_network_df = hashtag_network_df.explode('users')\n    hashtag_network_df = hashtag_network_df.explode('tags')\n    hashtag_network_df.dropna(inplace=True)\n    hashtag_network_df['users'] = hashtag_network_df['users'].str.replace('\\@', '', regex=True)\n    hashtag_network_df.columns = ['source', 'target', 'count'] # text data will be counted\n    hashtag_network_df = hashtag_network_df.groupby(['source', 'target']).count()\n    hashtag_network_df.reset_index(inplace=True)\n    hashtag_network_df.sort_values(['source', 'target'], ascending=[True, True])\n    # remove some junk that snuck in\n    hashtag_network_df = hashtag_network_df[hashtag_network_df['target'].apply(len)>2]\n    return hashtag_network_df\nYou can pass the search_tweets DataFrame to this function.\nhashtag_network_df = extract_hashtag_network_data(df)\nhashtag_network_df.head()\n```", "```py\ndf = search_tweets(twitter_api, 'network science')\ndf = wrangle_and_enrich(df)\nuser_network_df = extract_user_network_data(df)\nhashtag_network_df = extract_hashtag_network_data(df)\nG_user = nx.from_pandas_edgelist(user_network_df )\nG_hash = nx.from_pandas_edgelist(hashtag_network_df)\n```", "```py\n    draw_graph(G_user, show_names=True, node_size=3, edge_width=0.5, font_size=12)\n    ```", "```py\n    draw_graph(G_user, show_names=False, node_size=3, edge_width=0.5, font_size=12)\n    ```", "```py\n    draw_graph(G_hash, show_names=True, node_size=3, edge_width=0.5, font_size=12)\n    ```"]