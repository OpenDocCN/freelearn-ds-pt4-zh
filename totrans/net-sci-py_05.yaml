- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even Easier Scraping!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the basics of web scraping, which is the
    act of harvesting data from the web for your uses and projects. In this chapter,
    we will explore even easier approaches to web scraping and will also introduce
    you to social media scraping. The previous chapter was very long, as we had a
    lot to cover, from defining scraping to explaining how the `Requests` library,
    and `BeautifulSoup` can be used to collect web data. I will show simpler approaches
    to getting useful text data with less cleaning involved. Keep in mind that these
    easier ways do not necessarily replace what was explained in the previous chapter.
    When working with data or in software projects and things do not immediately work
    out, it is useful to have options. But for now, we’re going to push forward with
    a simpler approach to scraping web content, as well as giving an introduction
    to scraping social media text.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will cover the `Newspaper3k` Python library, as well as the `Twitter
    V2` Python Library.
  prefs: []
  type: TYPE_NORMAL
- en: When I say that `Newspaper3k` is an easier approach to collecting web text data,
    that is because the authors of `Newspaper3k` did an amazing job of simplifying
    the process of collecting and enriching web data. They have done much of the work
    that you would normally need to do for yourself. For instance, if you wanted to
    gather metadata about a website, such as what language it uses, what keywords
    are in a story, or even the summary of a news story, `Newspaper3k` gives that
    to you. This would otherwise be a lot of work. It’s easier because, this way,
    you don’t have to do it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Second, you will learn how to use the `Twitter V2` Python Library, because this
    is a very easy way to harvest tweets from Twitter, and this will be useful for
    NLP as well as network analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Why cover Requests and BeautifulSoup?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Newspaper3k
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the Twitter Python Library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the NetworkX and pandas Python libraries.
    Both of these libraries should be installed by now, so they should be ready for
    your use. If they are not installed, you can install Python libraries with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, to install NetworkX, you would do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also be discussing a few other libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Requests`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BeautifulSoup`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Newspaper3k`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests should already be included with Python and should not need to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: '`BeautifulSoup` can be installed with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Newspaper3k` can be installed with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In [*Chapter 4*](B17105_04.xhtml#_idTextAnchor158), we also introduced a `draw_graph()`
    function that uses both NetworkX and scikit-network. You will need that code whenever
    we do network visualization. Keep it handy!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the code for this chapter in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).'
  prefs: []
  type: TYPE_NORMAL
- en: Why cover Requests and BeautifulSoup?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We all like it when things are easy, but life is challenging, and things don’t
    always work out the way we want. In scraping, that’s laughably common. Initially,
    you can count on more things going wrong than going right, but if you are persistent
    and know your options, you will eventually get the data that you want.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the `Requests` Python library, because this
    gives you the ability to access and use any publicly available web data. You get
    a lot of freedom when working with `Requests`. This gives you the data, but making
    the data useful is difficult and time-consuming. We then used `BeautifulSoup`,
    because it is a rich library for dealing with HTML. With `BeautifulSoup`, you
    can be more specific about the kinds of data you extract and use from a web resource.
    For instance, we can easily harvest all of the external links from a website,
    or even the full text of a website, excluding all HTML.
  prefs: []
  type: TYPE_NORMAL
- en: However, `BeautifulSoup` doesn’t give perfectly clean data by default, especially
    if you are scraping news stories from hundreds of websites, all of which have
    different headers and footers. The methods we explored in the previous chapter
    for using offsets to chop off headers and footers are useful when you are collecting
    text from only a few websites, and you can easily set up a few rules for dealing
    with each unique website, but then you have a scalability problem. The more websites
    you decide to scrape, the more randomness you need to be able to deal with. The
    headers are different, navigation is likely different, and the languages may be
    different. The difficulties faced during scraping are part of what makes it so
    interesting to me.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Newspaper3k
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to do NLP or transform text data into network data for use in Social
    Network Analysis and Network Science, you need clean data. `Newspaper3k` gives
    you the next step after `BeautifulSoup`, abstracting away even more of the cleanup,
    giving you clean text and useful metadata with less work. Much of the performance
    and data cleanup have been abstracted away. Also, since you now understand some
    approaches to cleaning data from the previous chapter, when you see the cleanliness
    of Newspaper3k’s data, you will probably have a better understanding of what is
    happening under the hood, and hopefully, you will also be quite impressed and
    thankful for the work they have done and the time they are saving for you.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we can consider `Newspaper3k` the “easy way” of collecting text off
    of the web, but it’s easy because it builds off of previous foundations.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will still likely need to use `Requests` and `BeautifulSoup` in your web
    scraping and content analysis projects. We needed to cover them first. When pursuing
    a web scraping project, rather than start with the most basic and reinventing
    every tool we need along the way, perhaps we should figure out where to start
    by asking ourselves a few questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Can I do this with `Newspaper3k`?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No? OK, can I do this with `Requests` and `BeautifulSoup`?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No? OK, can I at least get to some data using `Requests`?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wherever you get useful results is where you should start. If you can get everything
    you need from `Newpaper3k`, start there. If you can’t get what you need with `Newspaper3k`,
    but you can with `BeautifulSoup`, start there. If neither of these approaches
    works, then you’re going to need to use `Requests` to pull data, and then you’ll
    need to write text cleanup code.
  prefs: []
  type: TYPE_NORMAL
- en: I usually recommend that people start at the basics and only add complexity
    as needed, but I do not recommend this approach for data collection or when it
    comes to text scraping. There is little use in reinventing HTML parsers. There
    is no glory in unnecessary headaches. Use whatever gets you useful data the quickest,
    so long as it meets your project needs.
  prefs: []
  type: TYPE_NORMAL
- en: What is Newspaper3k?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Newspaper3k` is a Python library that is useful for loading web text from
    news websites. However, unlike with `BeautifulSoup`, the objective is less about
    flexibility as it is about getting useful data *quickly*. I am most impressed
    with Newspaper3k’s ability to clean data, as the results are quite pure. I have
    compared my work using `BeautifulSoup` and `Newspaper3k`, and I am very impressed
    with the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Newspaper3k` is not a replacement for `BeautifulSoup`. It can do some things
    that `BeautifulSoup` can do, but not everything, and it wasn’t written with flexibility
    for dealing with HTML in mind, which is where `BeautifulSoup` is strong. You give
    it a website, and it gives you the text on that website. With `BeautifulSoup`,
    you have more flexibility, in that you can choose to look for only links, paragraphs,
    or headers. `Newspaper3k` gives you text, summaries, and keywords. `BeautifulSoup`
    is a step below that in abstraction. It is important to understand where different
    libraries sit in terms of tech stacks. `BeautifulSoup` is a high-level abstraction
    library, but it is lower level than `Newspaper3k`. Similarly, `BeautifulSoup`
    is a higher-level library than `Requests`. It’s like the movie *Inception* – there
    are layers and layers and layers.'
  prefs: []
  type: TYPE_NORMAL
- en: What are Newspaper3k’s uses?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Newspaper3k` is useful for getting to the clean text that exists in a web
    news story. That means parsing the HTML, chopping out non-useful text, and returning
    the news story, headline, keywords, and even text summary. The fact that it can
    return keywords and text summaries means that it’s got some pretty interesting
    NLP capabilities in the background. You don’t need to create a machine learning
    model for text summarization. `Newspaper3k` will do the work for you transparently,
    and surprisingly fast.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Newspaper3k` seems to have been inspired by the idea of parsing online news,
    but it is not limited to that. I have also used it for scraping blogs. If you
    have a website that you’d like to try scraping, give `Newspaper3k` a chance and
    see how it does. If that doesn’t work, use `BeautifulSoup`.'
  prefs: []
  type: TYPE_NORMAL
- en: One weakness of `Newspaper3k` is that it is unable to parse websites that use
    JavaScript obfuscation to hide their content inside JavaScript rather than HTML.
    Web developers occasionally do this to discourage scraping, for various reasons.
    If you point `Newspaper3k` or `BeautifulSoup` at a website that is using JavaScript
    obfuscation, both of them will return very little or no useful results, as the
    data is hidden in JavaScript, which neither of these libraries is built to handle.
    The workaround is to use a library such as `Selenium` along with `Requests`, and
    that will often be enough to get to the data that you want. `Selenium` is outside
    the scope of this book, and often feels like more trouble than it is worth, so
    please explore the documentation if you get stuck behind JavaScript obfuscation,
    or just move on and scrape easier websites. Most websites are scrapable, and the
    ones that aren’t can often just be ignored as they may not be worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Newspaper3k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before you can use `Newspaper3k`, you must install it. This is as simple as
    running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: At one point in a previous installation, I received an error stating that an
    NLTK component was not downloaded. Keep an eye out for weird errors. The fix was
    as simple as running a command for an NLTK download. Other than that, the library
    has worked very well for me. Once the installation is complete, you will be able
    to import it into your Python code and make use of it immediately.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, I showed flexible but more manual approaches to scraping
    websites. A lot of junk text snuck through, and cleaning the data was quite involved
    and difficult to standardize. `Newspaper3k` takes scraping to another level, making
    it easier than I have ever seen anywhere else. I recommend that you use `Newspaper3k`
    for your news scraping whenever you can.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping all news URLs from a website
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Harvesting URLs from a domain using `Newspaper3k` is simple. This is all of
    the code required to load all the hyperlinks from a web domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there is one thing I want to point out: when you scrape all URLs this
    way, you will also find what I consider “junk URLs” that point to other areas
    of the website, not to articles. These can be useful, but in most cases, I just
    want the article URLs. This is what the URLs will look like if I don’t do anything
    to remove the junk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Please take note that if you crawl a website at a different time, you will likely
    get different results. New content may have been added, and old content may have
    been removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything under those first two URLs is what I consider junk, in most of my
    scraping. I want the article URLs, and those are URLs to specific category pages.
    There are several ways that this problem can be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: You could drop URLs that include the word “category.” In this case, that looks
    perfect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could drop URLs where the length of the URL is greater than a certain threshold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could combine the two options into a single approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this example, I have decided to go with the third option. I will drop all
    URLs that include the word “category,” as well as any URLs that are less than
    60 characters in length. You may want to experiment with various cutoff thresholds
    to see what works for you. The simple cleanup code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Our URL list now looks much cleaner, containing only article URLs. This is
    what we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We now have a clean URL list that we can iterate through, scrape each story,
    and load the text for our use.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, one thing that you should notice is that on this single web
    domain, the stories that they publish are multilingual. Most of the stories that
    they publish are in English, but some of them are not. If you were to point `Newspaper3k`
    at the domain (rather than at individual story URLs), it would likely be unable
    to correctly classify the language of the domain. It is best to do language lookups
    at the story level, not the domain level. I will show how to do this at the story
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping a news story from a website
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have a list of story URLs that we want to scrape article text and metadata
    from. The next step is to use a chosen URL and harvest any data that we want.
    For this example, I will download and use the first story URL in our URL list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few confusing lines in this code snippet, so I will explain it
    line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: First, I load the `Article` function from the newspaper library, as that is
    used for downloading article data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, I point `Article` at the first URL from our URL list, which is `urls[0]`.
    It has not done anything at this point; it has just been pointed at the source
    URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, I download and parse the text from the given URL. This is useful for grabbing
    full text and headlines, but it will not capture article keywords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, I run the `nlp` component of `Article` to extract keywords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these four steps, I should now have all the data that I want for this article.
    Let’s dive in and see what we have!
  prefs: []
  type: TYPE_NORMAL
- en: What is the article title?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Nice and clean. What about the text?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What is the article summary?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What language was the article written in?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What keywords were found in the article?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What image accompanies this story?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And there is even more that you can do with `Newspaper3k`. I encourage you to
    read the library’s documentation and see what else can be useful to your work.
    You can read more at [https://newspaper.readthedocs.io/en/latest/](https://newspaper.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: Scraping nicely and blending in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two things that I try to do when building any scraper:'
  prefs: []
  type: TYPE_NORMAL
- en: Blend in with the crowd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t scrape too aggressively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is some overlap between both of these. If I blend in with actual website
    visitors, my scrapers will be less noticeable, and less likely to get blocked.
    Second, if I don’t scrape too aggressively, my scrapers are not likely to be noticed,
    and thus also less likely to be blocked. However, the second one is important,
    as it is not friendly to hit web servers too aggressively. It is better to throw
    in a 0.5 or 1-second wait between URL scrapes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first idea, blending in with the crowd, you can spoof a browser user-agent.
    For instance, if you want your scraper to pretend to be the latest Mozilla browser
    running on macOS, this is how to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, to add a 1-second sleep between each URL scrape, you could use a `sleep`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `user_agent` configuration is often enough to get past simple bot detection,
    and the 1-second sleep is a friendly thing to do that also helps with blending
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text into network data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To convert our freshly scraped text into network data, we can reuse the function
    that was created in the previous chapter. As a reminder, this function was created
    to use an NLP technique called **Named-Entity Recognition** (**NER**) to extract
    the people, places, and organizations mentioned in a document:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function that we will use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can simply throw our scraped text into this function and it should return
    an entity list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Perfect! Now, we can pass these entities to one additional function to get a
    `pandas` DataFrame of edge list data that we can use to construct a network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will use the `get_network_data` function, which is coded as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use it by passing in an entity list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon inspection, this looks great. A network edge list must contain a source
    node and a target node, and we’ve now got both in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – pandas DataFrame edge list of entities](img/B17105_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – pandas DataFrame edge list of entities
  prefs: []
  type: TYPE_NORMAL
- en: That’s great. The fourth row is interesting, as NER successfully caught two
    different ways of representing the CDC, both spelled out and as an acronym. There
    seems to be a false positive in the first row, but I will explain how to clean
    network data in the next chapter. This is perfect for now.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end Network3k scraping and network visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have everything we need to demonstrate two things. I want to show how
    to scrape several URLs and combine the data into a single DataFrame for use and
    storage, and you will learn how to convert raw text into network data and visualize
    it. We did the latter in the previous chapter, but it will be useful to do it
    one more time so that the learning sticks.
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple URL scrapes into a DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Between these two demonstrations, this is the most foundational and important
    part. We will use the results of this process in our next demonstration. In most
    real-world scraping projects, it is not useful to scrape a single URL repeatedly.
    Typically, you want to repeat these steps for any given domain that you scrape:'
  prefs: []
  type: TYPE_NORMAL
- en: Scrape all URLs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the ones that you have already scraped text for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scrape the text of the URLs that remain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this demonstration, we will only be doing *step 1* and *step 3*. For your
    projects, you will usually need to come up with a process to drop the URLs you
    have already scraped, and this is dependent on where you are writing the post-scraped
    data. Essentially, you need to take a look at what you have and disregard any
    URLs that you have already used. This prevents repeated work, unnecessary scraping
    noise, unnecessary scraping burden on web servers, and duplicated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code scrapes all URLs for a given domain, scrapes text for each
    URL discovered, and creates a `pandas` DataFrame for use or writing output to
    a file or a database. I am throwing one additional Python library at this: `tqdm`.
    The `tqdm` library is useful when you want to understand how long a process will
    take. If you are using this in backend automation, you will likely not want the
    `tqdm` functionality, but it is useful now, as you are learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install `tqdm` by running `pip` `install tqdm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – TQDM progress bar in action](img/B17105_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – TQDM progress bar in action
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the end-to-end Python code that takes a domain name and returns a `pandas`
    DataFrame of scraped stories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'To use this function, you can run the following code, and point it at any news
    domain of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a clean DataFrame of news stories to work with. If you are
    running into 404 (Page Not Found) errors, you may need to place some try/except
    exception handling code into the function. I leave this and other edge cases in
    your hands. However, the closer the time is between URL scraping and article text
    scraping, the less likely you will run into 404 errors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s inspect the results!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – pandas DataFrame of scraped URL data](img/B17105_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – pandas DataFrame of scraped URL data
  prefs: []
  type: TYPE_NORMAL
- en: Cool! The `tqdm` progress bar worked until completion, and we can also see that
    the final story’s language was set to Spanish. This is exactly what we want. If
    we had tried to detect the language of the overall domain, by scraping the landing
    page (home page), the language detection component might have given a false reading
    or even returned nothing. This website has both English and Spanish language stories,
    and we can see this at the story level.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing the language of a piece of text is very useful for NLP work. Often,
    machine learning classifiers that are trained in one language will have difficulty
    when used against another, and when using unsupervised machine learning (clustering)
    against text data, data written in various languages will clump together. My advice
    is to use the captured language data to split your data by language for any downstream
    NLP enrichment work. You will have better results this way, and your results will
    be much simpler to analyze as well.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s use these stories to create network data and visualizations!
  prefs: []
  type: TYPE_NORMAL
- en: Converting text data into a network for visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several times in this book, we have taken text, extracted entities, created
    network data, created a network, and then visualized the network. We will be doing
    the same thing here. The difference is that we now have a `pandas` DataFrame that
    consists of several news articles, and each of them can be converted into a network.
    For this example, I’ll only do it twice. From this point on, you should have no
    trouble converting text into networks, and you can reuse the code that has already
    been written.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our entity extraction is built upon an English language NLP model, so let’s
    only use English language stories. To keep things simple, we will do this demonstration
    with the second and fourth articles in the DataFrame, as these gave interesting
    and clean results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will use the second article. You should see that I am loading `df[''text''][1]`,
    where `[1]` is the second row as indexing starts at `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the network visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Network visualization of article entity relationships (second
    article)](img/B17105_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Network visualization of article entity relationships (second article)
  prefs: []
  type: TYPE_NORMAL
- en: This looks good but is very simple. A news article is typically about a few
    individuals and organizations, so this is not surprising. We can still see a relationship
    between a couple of wildlife groups, a relationship between a person and a university,
    and a relationship between **Australia** and **Koalas**. All of this seems realistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s try the fourth article:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the network visualization. This one is much more interesting and involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Network visualization of article entity relationships (fourth
    article)](img/B17105_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Network visualization of article entity relationships (fourth article)
  prefs: []
  type: TYPE_NORMAL
- en: This is a much richer set of entities than is commonly found in news stories,
    and in fact, this story has more entities and relationships than the book that
    we investigated in the previous chapter, *The Metamorphosis*. This looks great,
    and we can investigate the relationships that have been uncovered.
  prefs: []
  type: TYPE_NORMAL
- en: From this point on in this book, I will primarily be using Twitter data to create
    networks. I wanted to explain how to do this to any text, as this gives freedom
    to uncover relationships in any text, not just social media text. However, you
    should understand this by now. The remainder of this book will focus more on analyzing
    networks than on creating network data. Once text data has been converted into
    a network, the rest of the network analysis information is equally relevant and
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Twitter Python Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter is a goldmine for NLP projects. It is a very active social network with
    not too strict moderation, which means that users are pretty comfortable posting
    about a wide variety of topics. This means that Twitter can be useful for studying
    lighthearted topics, but it can also be used to study more serious topics. You
    have a lot of flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter also has a simple API to work with, compared to other social networks.
    It is relatively simple to get started with, and it can be used to capture data
    that can be used for a lifetime of NLP research. In my personal NLP research,
    learning to scrape Twitter supercharged and accelerated my NLP learning. Learning
    NLP is much more enjoyable when you have data that is interesting to you. I have
    used Twitter data to understand various networks, create original NLP techniques,
    and create machine learning training data. I don’t use Twitter much, but I have
    found it to be a goldmine for all things NLP.
  prefs: []
  type: TYPE_NORMAL
- en: What is the Twitter Python Library?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For several years, Twitter has been exposing its API, to allow software developers
    and researchers to make use of their data. The API is a bit of a challenge to
    use as the documentation is a bit scattered and confusing, so a Python library
    was created to make working with the API much easier. I will explain how to use
    the Python library, but you will need to explore the Twitter API itself to learn
    more about the various limits that Twitter has set in place to prevent overuse
    of the API.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the Twitter API at [https://developer.twitter.com/en/docs](https://developer.twitter.com/en/docs).
  prefs: []
  type: TYPE_NORMAL
- en: What are the Twitter Library’s uses?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you can use the Twitter library and API, you have total flexibility in
    what you use it for to research. You could use it to learn about the K-Pop music
    scene, or you could use it to keep an eye on the latest happenings in machine
    learning or data science.
  prefs: []
  type: TYPE_NORMAL
- en: Another use of this is analyzing entire audiences. For instance, if an account
    has 50,000 followers, you can use the Twitter Library to load data about all 50,000
    of the followers, including their usernames and descriptions. With these descriptions,
    you could use clustering techniques to identify the various subgroups that exist
    in a larger group. You could also use this kind of data to potentially identify
    bots and other forms of artificial amplification.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend that you find something that you are curious about, and then chase
    it and see where it leads. This curiosity is an excellent driver for building
    skills in NLP and Social Network Analysis.
  prefs: []
  type: TYPE_NORMAL
- en: What data can be harvested from Twitter?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even compared to 2 years ago, it seems that Twitter has expanded its API offerings
    to allow for many different kinds of data science and NLP projects. However, in
    their **version one** (**V1**) API, Twitter would return a dictionary containing
    a great deal of the data that they have available. This has changed a bit in their
    V2 API, as they now require developers to specify which data they are requesting.
    This has made it more difficult to know all of the data that Twitter has made
    available. Anyone who will be working with the Twitter API is going to need to
    spend time reading through the documentation to see what is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'For my research, I am typically only interested in a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Who is posting something?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What have they posted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When was it posted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are they mentioning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What hashtags are they using?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of this is easy to pull from the Twitter API, and I will show you how. But
    this is not the limit of Twitter’s offerings. I have recently been impressed by
    some of the more recently discovered data that the V2 API exposes, but I do not
    understand it well enough to write about it, yet. When you run into something
    that feels like it should be exposed by the API, check the documentation. In my
    experience working with the Twitter API, some things that should be exposed by
    default now take a bit of extra work than in V1\. Try to figure out how to get
    what you need.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Twitter API access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you can do anything with the Twitter API, the first thing you need to
    do is get access:'
  prefs: []
  type: TYPE_NORMAL
- en: First, create a Twitter account. You don’t need to use it to post anything,
    but you do need to have an account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, go to the following URL to request API access: [https://developer.twitter.com/en/apply-for-access](https://developer.twitter.com/en/apply-for-access).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying for access can vary in time from a few minutes to a few days. You will
    need to fill out a few forms specifying how you will be using the data and agreeing
    to abide by Twitter’s Terms of Service. In describing your use of Twitter data,
    you can specify that you are using this for learning NLP and Social Network Analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have been granted access, you will have your own Developer Portal.
    Search around in the authentication section until you see something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Twitter Authentication Bearer Token](img/B17105_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Twitter Authentication Bearer Token
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, you are looking for a **Bearer Token**. Generate one and keep
    it somewhere safe. You will use this to authenticate with the Twitter API.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have generated a Bearer Token, you should be all set to work with the
    Twitter API through the Twitter Python library.
  prefs: []
  type: TYPE_NORMAL
- en: Authenticating with Twitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you can authenticate, you need to install the Twitter Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do so by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, in your notebook of choice, try importing the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you will need to authenticate with Twitter using your Bearer Token. Replace
    the `bearer_token` text in the following code with your own Bearer Token and try
    authenticating:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If this doesn’t fail, then you should be authenticated and ready to start scraping
    tweets, connections, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping user tweets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I’ve created two helper functions for loading user tweets into a `pandas` DataFrame.
    If you want more data than this function returns, you will need to extend `tweet_fields`,
    and possibly add `user_fields`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please see the `search_tweets()` function in the following code block to see
    how `user_fields` can be added to a Twitter call. This function does not use `user_fields`,
    as we are already passing in a username:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This first function takes a Twitter username and returns its `user_id`. This
    is important because some Twitter calls require a `user_id`, not a `username`.
    The following function uses `user_id` to look up a user’s tweets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this function, the `twitter_api.get_timelines()` function is doing most of
    the work. I have specified `tweet_fields` that I want, I’ve passed in a user’s
    `user_id`, I’ve specified that I want the latest `100` tweets by that person,
    and I’ve specified that I want the data returned in JSON format, which is easy
    to convert into a `pandas` DataFrame. If I call this function, I should get immediate
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what Santa Claus talks about:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should now see a preview of five of Santa’s tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – pandas DataFrame of Santa Claus tweets](img/B17105_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – pandas DataFrame of Santa Claus tweets
  prefs: []
  type: TYPE_NORMAL
- en: Perfect. We now have the most recent `100` tweets made by Santa Claus.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have one additional helper function that I would like to give you. This one
    takes the `text` field and extracts entities and hashtags; we’ll be using these
    to draw social networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can add this function as an enrichment step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us additional useful data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – pandas DataFrame of enriched Santa Claus tweets](img/B17105_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – pandas DataFrame of enriched Santa Claus tweets
  prefs: []
  type: TYPE_NORMAL
- en: This is perfect for the rest of the work we will be doing in this chapter, but
    before we move on, we will also see how to scrape connections.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping user following
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can easily scrape all accounts that an account follows. This can be done
    with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Here, I have specified `max_results=1000`. That is the maximum that Twitter
    will return at a time, but you can load much more than 1,000\. You will need to
    pass in a `'next_token'` key to continue harvesting sets of `1000` followers.
    You can do something similar to load more than 100 tweets by a person. Ideally,
    you should use recursion in programming to do this, if you have the need. You
    can use the preceding function to load the first batch, and you should be able
    to extend it if you need to build in recursion.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can call the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you results in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – pandas DataFrame of accounts Santa Claus follows on Twitter](img/B17105_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – pandas DataFrame of accounts Santa Claus follows on Twitter
  prefs: []
  type: TYPE_NORMAL
- en: For investigating subgroups that exist inside a group, it is useful to include
    the account description, as people are often descriptive about their interests
    and political affiliations. To capture the description, you need to include the
    description in the `user_fields` list.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping user followers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scraping followers is nearly identical. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'You can call the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: This will give you results in the same format as was shown previously. Be sure
    to include the account description.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping using search terms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collecting tweets about a search term is also useful. You can use this to explore
    *who* participates in discussions about a search term, but also to collect the
    tweets themselves, for reading and processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code that I have written for scraping by search term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: This function is a bit more involved than the previous functions, as I have
    specified `tweet_fields` as well as `user_fields` that I am interested in. To
    capture the username, I needed to specify an expansion on `author_id`, and finally,
    I want 100 of the latest tweets. If you want to include additional data, you will
    need to explore the Twitter API to find out how to add the data field of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can call the function like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'I am also enriching the `pandas` DataFrame so that it includes user mentions
    and hashtags via the `wrangle_and_enrich()` function call. This results in the
    following `pandas` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – pandas DataFrame of Twitter search tweets](img/B17105_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – pandas DataFrame of Twitter search tweets
  prefs: []
  type: TYPE_NORMAL
- en: These search tweets will be perfect for creating social network visualizations
    as the tweets come from multiple accounts. In the top two rows of data, you may
    visually notice that there is a relationship between **intempestades**, **pascal_bornet**,
    and **cogautocom**. This would show as connected nodes if we were to visualize
    this network.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Twitter tweets into network data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Converting social media data into network data is much easier than raw text.
    With Twitter, this is fortunate, as tweets tend to be quite short. This is because
    users frequently tag each other in their tweets for visibility and interaction,
    and they often associate their tweets with hashtags as well, and these associations
    can be used to build networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using user mentions and hashtags, there are several different kinds of networks
    that we can create:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Account to Mention Networks (@ -> @)*: Useful for analyzing social networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Account to Hashtag Networks (@ -> #)*: Useful for finding communities that
    exist around a theme (hashtag).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mention to Hashtag Network (@ -> #)*: Similar to the previous one, but linking
    to mentioned accounts, not the tweet account. This is also useful for finding
    communities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hashtag to Hashtag Networks (# -> #)*: Useful for finding related themes (hashtags)
    and emerging trending topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, you could use NER to extract additional entities from the text,
    but tweets are pretty short, so this may not give much useful data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, you will learn how to do the first and third types
    of networks.
  prefs: []
  type: TYPE_NORMAL
- en: Account to Mention Network (@ -> @)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I have created a useful helper function to convert a `pandas` DataFrame into
    this Account to Mention network data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s quite a lot going on in this function:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we take a copy of the username, users, and text fields from the `df`
    DataFrame and use them in the `user_network_df` DataFrame. Each row of the `users`
    field contains a list of users, so we then “explode” the `users` field, creating
    a separate row for each user in the DataFrame. We also drop rows that do not contain
    any users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we remove all `@` characters so that the data and visualization will be
    more readable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we rename all the columns in the DataFrame, in preparation for creating
    our graph. NetworkX’s graphs expect a source and target field, and the count field
    can also be passed in as additional data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we do aggregation and count each source-target relationship in the DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we sort and return the DataFrame. We did not need to sort the DataFrame,
    but I tend to do this, as it can help with looking through the DataFrame or troubleshooting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can pass the `search_tweets` DataFrame to this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get an edge list DataFrame back of user relationships. We will use
    this to construct and visualize our network. Look closely and you should see that
    there is an additional **count** field. We will use this in later chapters as
    a threshold for choosing which edges and nodes to show in a visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Account to Mention pandas DataFrame edge list](img/B17105_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Account to Mention pandas DataFrame edge list
  prefs: []
  type: TYPE_NORMAL
- en: Each row in this DataFrame shows a relationship between one user (**source**)
    and another (**target**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mention to Hashtag Network (@ -> #)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I have created a useful helper function to convert a `pandas` DataFrame into
    **Mention** to Hashtag network data. This function is similar to the previous
    one, but we load users and hashtags and do not use the original account’s username
    at all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get an edge list DataFrame back of user relationships. As shown previously,
    a **count** field is also returned, and we will use it in a later chapter as a
    threshold for choosing which nodes and edges to show:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Mention to Hashtag pandas DataFrame edge list](img/B17105_05_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Mention to Hashtag pandas DataFrame edge list
  prefs: []
  type: TYPE_NORMAL
- en: Each row in this DataFrame shows a relationship between one user (source) and
    a hashtag (target).
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end Twitter scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I hope that the preceding code and examples have shown how easy it is to use
    the Twitter API to scrape tweets, and I hope you can also see how easy it is to
    transform tweets into networks. For this chapter’s final demonstration, I want
    you to follow a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load a `pandas` DataFrame containing tweets related to Network Science.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enrich the DataFrame so that it includes user mentions and hashtags as separate
    fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create Account to Mention network data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create Mention to Hashtag network data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an Account to Mention network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Mention to Hashtag network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the Account to Mention network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the Mention to Hashtag network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s do this sequentially in code, reusing the Python functions we have been
    using throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for the first six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: It is really that simple. There are a lot of moving pieces under the hood, but
    the more that you practice with network data, the simpler it becomes to write
    this kind of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these networks are now ready for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll start with the Account to Mention network visualization. This is a social
    network. We can draw it like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should render a network visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Account to Mention social network visualization](img/B17105_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Account to Mention social network visualization
  prefs: []
  type: TYPE_NORMAL
- en: This is a bit difficult to read since the account names overlap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the network looks without labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give us a network visualization without node labels. This will allow
    us to see what the whole network looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Account to Mention social network visualization (no labels)](img/B17105_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Account to Mention social network visualization (no labels)
  prefs: []
  type: TYPE_NORMAL
- en: Wow! To me, that’s beautiful and useful. I can see that there are several islands
    or clusters of users. If we look closer, we will be able to identify communities
    that exist in the data. We will do this in [*Chapter 9*](B17105_09.xhtml#_idTextAnchor364),
    which is all about Community Detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the Mention to Hashtag network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should render a network visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Mention to Hashtag network visualization](img/B17105_05_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Mention to Hashtag network visualization
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the Account to Mention network, this one is easily readable. We can see
    users that are associated with various hashtags. There’s no value in showing this
    without labels, as it is unreadable and unusable without them. This marks the
    end of the demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered two easier ways to scrape text data from the internet.
    `Newspaper3k` made short work of scraping news websites, returning clean text,
    headlines, keywords, and more. It allowed us to skip steps we’d done using `BeautifulSoup`
    and get to clean data much quicker. We used this clean text and NER to create
    and visualize networks. Finally, we used the Twitter Python library and V2 API
    to scrape tweets and connections, and we also used tweets to create and visualize
    networks. Between what you learned in this chapter and the previous one, you now
    have a lot of flexibility in scraping the web and converting text into networks
    so that you can explore embedded and hidden relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is some good news: collecting and cleaning data is the most difficult
    part of what we are going to do, and this marks the end of data collection and
    most of the cleanup. After this chapter, we will mostly be having fun with networks!'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at graph construction. We will make use of
    the techniques we used in this chapter to create networks for analysis and visualization.
  prefs: []
  type: TYPE_NORMAL
