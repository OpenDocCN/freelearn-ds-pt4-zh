- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised Machine Learning on Network Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we spent a lot of time exploring how to collect text data
    from the internet, transform it into network data, visualize networks, and analyze
    networks. We were able to use centralities and various network metrics for additional
    contextual awareness about individual nodes’ placement and influence in networks,
    and we used community detection algorithms to identify the various communities
    that exist in a network.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to begin an exploration of how network data can
    be useful in **machine learning** (**ML**). As this is a data science and network
    science book, I expect that many readers will be familiar with ML, but I’ll give
    a very quick explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is composed of the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beginning with ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the Python libraries NetworkX, pandas, and
    scikit-learn. These libraries should be installed by now, so they should be ready
    for your use. If they are not installed, you can install Python libraries with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, to install NetworkX, you would do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In [*Chapter 4*](B17105_04.xhtml#_idTextAnchor158), we also introduced a `draw_graph()`
    function that uses both NetworkX and `scikit-network`. You will need that code
    any time that we do network visualization. Keep it handy!
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is on GitHub: [https://github.com/PacktPublishing/Network-Science-with-Python](https://github.com/PacktPublishing/Network-Science-with-Python).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML is a set of techniques that enable computers to learn from patterns and
    behavior in data. It is often said that there are three different kinds of ML:
    **Supervised**, **Unsupervised**, and **Reinforcement** learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In supervised ML, an answer – called a **label** – is provided with the data
    to allow for an ML model to learn the patterns that will allow it to predict the
    correct answer. To put it simply, you give the model data *and* an answer, and
    it figures out how to predict correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised ML, no answer is provided to the model. The goal is usually
    to find clusters of similar pieces of data. For instance, you could use clustering
    to identify the different types of news articles present in a dataset of news
    articles, or to find topics that exist in a corpus of text. This is similar to
    what we have done with community detection.
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning, a model is given a goal and it gradually learns how
    to get to this goal. In many reinforcement learning demos, you’ll see a model
    play pong or another video game, or learn to walk.
  prefs: []
  type: TYPE_NORMAL
- en: These are ultra-simplistic descriptions of the types of ML, and there are more
    variations (**semi-supervised** and so on). ML is a rich topic, so I encourage
    you to find books if this chapter interests you. For me, it pushed NLP into an
    obsession.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning with ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many guides and books on how to do sentiment analysis using NLP. There
    are very few guides and books that give a step-by-step demonstration of how to
    convert graph data into a format that can be used for classification with ML.
    In this chapter, you will see how to use graph data for ML classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, I created a little game I’m calling “Spot the Revolutionary.”
    As with the last two chapters, we will be using the networkx *Les Miserables*
    network as it contains enough nodes and communities to be interesting. In previous
    chapters, I pointed out that the revolutionary community was densely connected.
    As a reminder, this is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Les Miserables Les Amis de l’ABC network](img/B17105_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Les Miserables Les Amis de l’ABC network
  prefs: []
  type: TYPE_NORMAL
- en: Each member of the community is connected with each other member, for the most
    part. There are no connections with outside people.
  prefs: []
  type: TYPE_NORMAL
- en: Other parts of the network look different.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Les Miserables whole network](img/B17105_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Les Miserables whole network
  prefs: []
  type: TYPE_NORMAL
- en: Even a visual inspection shows that in different parts of the network, nodes
    are are connecteded differently. The structure is different. In some places, connectivity
    resembles a star. In others, it resembles a mesh. Network metrics will give us
    these values, and ML models will be able to use them for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use these metrics to play a game of “Spot the Revolutionary.”
    This will be fun.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: I will not be explaining ML in much depth, only giving a preview of its capabilities.
    If you are interested in data science or software engineering, I strongly recommend
    that you spend some time learning about ML. It is not only for academics, mathematicians,
    and scientists. It gets complicated, so a foundation in mathematics and statistics
    is strongly recommended, but you should feel free to explore the topic.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will not be a mathematics lesson. All work will be done via code.
    I’m going to show one example of using network data for classification. This is
    not the only use case. This is not at all the only use case for using network
    data with ML. I will also be showing only one model (Random Forest), not all available
    models.
  prefs: []
  type: TYPE_NORMAL
- en: I’m also going to show that sometimes incorrect predictions can be as insightful
    as correct ones and how there are sometimes useful insights in model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: I’m going to show the workflow to go from graph data to prediction and insights
    so that you can use this in your own experiments. You don’t need a graph **neural
    network** (**NN**) for everything. This is totally possible with simpler models,
    and they can give good insights, too.
  prefs: []
  type: TYPE_NORMAL
- en: Enough disclaimers. Let’s go.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can use ML, we first need to collect our data and convert it into
    a format that the model can use. We can’t just feed the graph G to Random Forest
    and call it a day. We could feed a graph’s adjacency matrix and a set of labels
    to Random Forest and it’d work, but I want to showcase some feature engineering
    that we can do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature engineering is using domain knowledge to create additional features
    (most call them columns) that will be useful for our models. For instance, looking
    at the networks from the previous section, if we want to be able to spot the revolutionaries,
    then we may want to give our model additional data such as each node’s number
    of degrees (connections), betweenness centrality, closeness centrality, page rank,
    clustering, and triangles:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by first building our network. This should be easy by now, as we
    have done this several times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We took these steps in previous chapters, but as a reminder, the *Les Miserables*
    graph comes with edge weights, and I don’t want those. The first line loads the
    graph. The second line creates an edge list from the graph, dropping the edge
    weights. And the third line rebuilds the graph from the edge list, without weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should now have a useful graph. Let’s take a look:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Les Miserables graph without node names](img/B17105_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Les Miserables graph without node names
  prefs: []
  type: TYPE_NORMAL
- en: Looks good! I can clearly see that there are several different clusters of nodes
    in this network and that other parts of the network are more sparse.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we convert this crazy tangled knot into something that an ML model
    can use? Well, we looked at centralities and other measures in previous chapters.
    So, we already have the foundation that we will use here. I’m going to create
    several DataFrames with the data that I want and then merge them together for
    use as training data.
  prefs: []
  type: TYPE_NORMAL
- en: Degrees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **degrees** are simply the number of connections that a node has with other
    nodes. We’ll grab that first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Les Miserables feature engineering: degrees](img/B17105_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4 – Les Miserables feature engineering: degrees'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we’ll grab the clustering coefficient, which tells us how densely connected
    the nodes are around a given node. A value of 1.0 means that every node is connected
    to every other node. 0.0 means that no neighbor nodes are connected to other neighbor
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s capture clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the clustering output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Les Miserables feature engineering: clustering](img/B17105_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5 – Les Miserables feature engineering: clustering'
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that **MlleBaptistine** and **MmeMagloire** both are part of densely
    connected communities, meaning that these two also know the same people. **Napoleon**
    doesn’t have any overlap with other people, and neither does **CountessDeLo**.
  prefs: []
  type: TYPE_NORMAL
- en: Triangles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`triangle_df` is a count of how many triangles a given node is a part of. If
    a node is part of many different triangles, it is connected with many nodes in
    a network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Les Miserables feature engineering: triangles](img/B17105_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6 – Les Miserables feature engineering: triangles'
  prefs: []
  type: TYPE_NORMAL
- en: This is another way of understanding the connectedness of the nodes around a
    node. These nodes are people, so it is a way of understanding the connectedness
    of peopleto other people. Notice that the results are similar but not identical
    to clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Betweenness centrality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Betweenness centrality** has to do with a node’s placement between other
    nodes. As a reminder, in a hypothetical situation where there are three people
    (*A*, *B*, and *C*), and if *B* sits between *A* and *C*, then all information
    passing from *A* to *C* flows through person *B*, putting them in an important
    and influential position. That’s just one example of the usefulness of betweenness
    centrality. We can get this information by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Les Miserables feature engineering: betweenness centrality](img/B17105_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7 – Les Miserables feature engineering: betweenness centrality'
  prefs: []
  type: TYPE_NORMAL
- en: Closeness centrality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Closeness centrality** has to do with how close a given node is to all other
    nodes in a network. It has to do with the shortest path. As such, closeness centrality
    is computationally very slow for large networks. However, it will work just fine
    for the *Les Miserables* network, as this is a very small network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 10.8 – Les Miserables feature engineering: closeness centrality](img/B17105_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8 – Les Miserables feature engineering: closeness centrality'
  prefs: []
  type: TYPE_NORMAL
- en: PageRank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, the `pagerank` remains useful even on large networks. As such, it
    is very commonly used to gauge importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This gives us *Figure 10**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Les Miserables feature engineering: pagerank](img/B17105_10_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9 – Les Miserables feature engineering: pagerank'
  prefs: []
  type: TYPE_NORMAL
- en: Adjacency matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can include the **adjacency matrix** in our training data so that
    our models can use neighbor nodes as features for making predictions. For instance,
    let’s say that you have 10 friends but one of them is a criminal, and every person
    that friend introduces you to is also a criminal. You will probably learn over
    time that you shouldn’t associate with that friend or their friends. Your other
    friends do not have that problem. In your head, you’ve already begun to make judgments
    about that person and who they associate with.
  prefs: []
  type: TYPE_NORMAL
- en: If we left the adjacency matrix out, the model would attempt to learn from the
    other features, but it’d have no contextual awareness of neighboring nodes. In
    the game of “Spot the Revolutionary,” it would use the centralities, clustering,
    degrees, and other features only, as it’d have no way of learning from anything
    else.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to use the adjacency matrix. It feels almost like leakage (where
    the answer is hidden in another feature) because like often attracts like in a
    social network, but that also shows the usefulness of using network data with
    ML. You can drop the adjacency matrix if you feel that it is cheating. I do not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Les Miserables feature engineering: adjacency matrix](img/B17105_10_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10 – Les Miserables feature engineering: adjacency matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Merging DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have all of these useful features, it’s time to merge the DataFrames
    together. This is simple, but requires a few steps, as demonstrated in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the first step, I created an empty DataFrame just so that I could rerun the
    Jupyter cell over and over without creating duplicate columns with weird names.
    It just saves work and aggravation. Then, I sequentially merge the DataFrames
    into `clf_df`, based on the DataFrame indexes. The DataFrame indexes are character
    names from Les Miserables. This just makes sure that each row from each DataFrame
    is joined together correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Les Miserables feature engineering: combined training data](img/B17105_10_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11 – Les Miserables feature engineering: combined training data'
  prefs: []
  type: TYPE_NORMAL
- en: Adding labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we need to add labels for the revolutionaries. I have done the work
    of quickly looking up the names of the members of Les Amis de l’ABC (Friends of
    the ABC), which is the name of the group of revolutionaries. First, I will add
    the members to a Python list, and then I’ll do a spot check to make sure that
    I’ve spelled their names correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Les Misérables feature engineering: Friends of the ABC members](img/B17105_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12 – Les Misérables feature engineering: Friends of the ABC members'
  prefs: []
  type: TYPE_NORMAL
- en: 'This looks perfect. The list had 11 names, and the DataFrame has 11 rows. To
    create training data for supervised ML, we need to add a `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s that easy. Let’s take a quick look at the DataFrame just to make sure
    we have labels. I’ll sort on the index so that we can see a few `1` labels in
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Les Miserables feature engineering: label spot check](img/B17105_10_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13 – Les Miserables feature engineering: label spot check'
  prefs: []
  type: TYPE_NORMAL
- en: Perfect. We have nodes, and they each have a label. A label of **1** means that
    they are a member of Friends of the ABC, and a label of **0** means that they
    are not. With that, our training data is ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this exercise, my goal is to simply show you how network data may be useful
    in ML, not to go into great detail about ML. There are many, many, many thick
    books on the subject. This is a book about how NLP and networks can be used together
    to understand the hidden strings that exist around us and the influence that they
    have on us. So, I am going to speed past the discussion on how different models
    work. For this exercise, we are going to use one very useful and powerful model
    that often works well enough. This model is called **Random Forest**.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest can take both numeric and categorical data as input. Our chosen
    features should work very well for this exercise. Random Forest is also easy to
    set up and experiment with, and it’s also very easy to learn what the model found
    most useful for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Other models would work. I attempted to use **k-nearest neighbors** and had
    nearly the same level of success, and I’m sure that **Logistic regression** would
    have also worked well after some additional preprocessing. **XGBoost** and **SVM**s
    would have also worked. Some of you might also be tempted to use an NN. Please
    feel free. I chose to not use an NN, as the setup is more difficult and the training
    time is typically longer, for an occasional tiny boost to accuracy that may also
    just be a fluke. Experiment with models! It’s a good way to learn, even when you
    are learning what *not* to do.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We should do a few more data checks. Most importantly, let’s check the balance
    between classes in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The data is imbalanced, but not too badly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get this in percentage form, just to make this a little easier to understand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It looks like we have about an 86/14 balance between the classes. Not awful.
    Keep this in mind, because the model should be able to predict with about 86%
    accuracy just based on the imbalance alone. It won’t be an impressive model at
    all if it only hits 86%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to cut up our data for our model. We will use the features as
    our `X` data, and the answers as our `y` data. As the label was added last, this
    will be simple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_cols` is every column except for the last one, which is the label. `X` is
    a DataFrame containing only `X_cols` fields, and `y` is an array of our answers.
    Don’t take my word for it. Let’s do a spot check.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will show a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll all the way to the right on the DataFrame. If you don’t see the **label**
    column, we are good to go:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will show the first five labels in `y`. This is an array. We are all set.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to split the data into training data and test data. The training
    data will be used to train the model, and the test data is completely unknown
    to the model. We do not care about the model accuracy on the training data. Remember
    that. We do not care about the model accuracy or any performance metrics about
    the training data. We only care about how the model does on unseen data. That
    will tell us how well it generalizes and how well it will work in the wild. Yes,
    I understand that this model will not be useful in the wild, but that is the idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will split the data using the `scikit-learn` `train_test_split` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we have so little training data, and since there are so few members of
    Friends of the ABC, I set `test_size` to `0.4`, which is twice as high as the
    default. If there were less imbalance, I would have reduced this to `0.3` or `0.2`.
    If I really wanted the model to have as much training data as possible and I was
    comfortable that it would do well enough, I might even experiment with `0.1`.
    But for this exercise, I went with `0.4`. That’s my reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: This function does a 60/40 split of the data, putting 60% of the data into `X_train`
    and `y_train`, and the other 40% into `X_test` and `y_test`. This sets aside 40%
    of the data as unseen data that the model will not be aware of. If the model does
    well against this 40% unseen data, then it’s a decent model.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to train our model and see how well it does!
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model training gets the most attention when people talk about ML but it is usually
    the easiest step, once the data has been collected and prepared. A lot of time
    and energy can and should be spent on optimizing your models, via **hyperparameter
    tuning**. Whichever model you are interested in learning about and using, do some
    research on how to tune the model, and any additional steps required for data
    preparation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this simple network, the default Random Forest model was already optimal.
    I ran through several checks, and the default model did well enough. Here’s the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We are using a Random Forest classifier, so we first need to import the model
    from the `sklearn.ensemble` module. Random Forest uses an ensemble of decision
    trees to make its predictions. Each ensemble is trained on different features
    from the training data, and then a final prediction is made.
  prefs: []
  type: TYPE_NORMAL
- en: Set `random_state` to whatever number you like. I like `1337`, as an old hacker
    joke. It’s *1337*, *leet*, *elite*. Setting `n_jobs` to `-1` ensures that all
    CPUs will be used in training the model. Setting `n_estimators` to `100` will
    allow for 100 ensembles of decision trees. The number of estimators can be experimented
    with. Increasing it can be helpful, but in this case, it was not.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I collect and print the training accuracy and test accuracy. How are
    our scores?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Not bad results on the test set. This set is unseen data, so we want it to be
    high. As mentioned before, due to class imbalance, the model should at least get
    86% accuracy simply due to 86% of the labels being in the majority class. 93.5%
    is not bad. However, you should be aware of **underfitting** and **overfitting**.
    If both your train and test accuracy are very low, the model is likely underfitting
    the data, and you likely need more data. However, if the training accuracy is
    much higher than the test set accuracy, this can be a sign of overfitting, and
    this model appears to be overfitting. However, with as little data as we have,
    and for the sake of this experiment, good enough is good enough today.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important that you know that model accuracy is never enough. It doesn’t
    tell you nearly enough about how the model is doing, especially how it is doing
    with the minority class. We should take a look at the confusion matrix and classification
    report to learn more. To use both of these, we should first place the predictions
    for `X_test` into a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Great. We have an array of predictions. Next, let’s import the `confusion_matrix`
    and `classification_report` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use both by feeding both of them the `X_test` data as well as the predictions
    made against `X_test`. First, let’s look at the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If this isn’t clear enough, we can also visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This produces the following matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Model confusion matrix](img/B17105_10_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Model confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix shows how well a model predicts by classes. The figure
    depicts this well. The *y-a*xis shows the true label of either **0** or **1**,
    and the *x-a*xis shows the predicted label of either **0** or **1**. I can see
    that 26 of the characters were correctly predicted to not be members of Friends
    of the ABC (revolutionaries). Our model correctly predicted three of the members
    of Friends of the ABC, but it also predicted that two of the non-members were
    members. We should look into that! Sometimes the misses can help us find problems
    in the data, or they’ll shine a light on an interesting insight.
  prefs: []
  type: TYPE_NORMAL
- en: 'I also find it extremely helpful to take a look at the classification report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Model classification report](img/B17105_10_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Model classification report
  prefs: []
  type: TYPE_NORMAL
- en: 'This report clearly shows us that the model does very well at predicting non-members
    of Friends of the ABC, but it does less well at predicting the revolutionaries.
    Why is that? What is it getting tripped up by? Looking at the networks, the model
    should have been able to learn that there is a clear difference in the structure
    of different groups of people, especially comparing the community of Friends of
    the ABC against everyone else. What gives?! Let’s build a simple DataFrame to
    check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – DataFrame for prediction checks](img/B17105_10_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – DataFrame for prediction checks
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create a mask to look up all rows where the label does not match
    the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: That gives us *Figure 10**.17*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – DataFrame of missed predictions](img/B17105_10_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – DataFrame of missed predictions
  prefs: []
  type: TYPE_NORMAL
- en: OK, now we can see what the model got wrong, but why? To save you some time,
    I looked into both of these characters. Joly actually *is* a member of Friends
    of the ABC, and Madame Hucheloup runs a cafe where members of Friends of the ABC
    regularly meet. She was the proprietress of the Corinthe Tavern, the meeting place
    and last defense of the members! Because she was connected with members of the
    group, the model predicted that she also was one of them.
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, I bet a human might have made the same judgment about Madame Hucheloup!
    To me, this is a beautiful misclassification!
  prefs: []
  type: TYPE_NORMAL
- en: The next steps would be to definitely give Joly a correct label and retrain
    the model. I would leave Madame Hucheloup as is, as she is not a member, but if
    I were counter-insurgency, I would keep an eye on her.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the model did very well, in my opinion, and it did so entirely using
    graph data.
  prefs: []
  type: TYPE_NORMAL
- en: Model insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To me, the model insights are more exciting than building and using the models
    for prediction. I enjoy learning about the world around me, and ML models (and
    networks) allow me to understand the world in a way that my eyes do not. We cannot
    see all of the lines that connect us as people, and we cannot easily understand
    influence based on how the people around us are strategically placed in the social
    networks that exist in real life. These models can help with that! Networks can
    provide the structure to extract contextual awareness of information flow and
    influence. ML can tell us which of these pieces of information is most useful
    in understanding something. Sometimes, ML can cut right through the noise and
    get right to the signals that affect our lives.
  prefs: []
  type: TYPE_NORMAL
- en: With the model that we just built, one insight is that the book *Les Miserables*
    has different characters by type in different network structures. Revolutionaries
    are close to each other and tightly connected. Students are also densely connected,
    and I’m surprised and pleased that the model did not false out on a lot of students.
    Other characters in the book have very few connections, and their neighbors are
    sparsely connected. I think it’s beautiful that the author put so much work into
    defining the social network that exists in this story. It can give a new appreciation
    for the creation of the story.
  prefs: []
  type: TYPE_NORMAL
- en: But what features were most important to the model that helped it make its predictions
    so well? Let’s take a look. Random Forest makes this very easy for us!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get to the **feature importances** very easily by doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'But this data is not very useful in this format. It is much more useful if
    you put the feature importances into a DataFrame, as they can then be sorted and
    visualized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18 – DataFrame for feature importances](img/B17105_10_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – DataFrame for feature importances
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the importances in numerical format. This shows the 10 features that
    the model found most useful in making predictions. Notably, a character’s connection
    to Bossuet and Enjolras was a good indicator of whether a character was a revolutionary
    or not. Out of the network features, triangles was the only one to make the top
    10\. The rest of the important features came from the adjacency matrix. Let’s
    visualize this as a bar chart so that we can see more, as well as the level of
    importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19 – Horizontal bar chart of feature importances](img/B17105_10_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – Horizontal bar chart of feature importances
  prefs: []
  type: TYPE_NORMAL
- en: Much better. This is much easier to look at, and it shows exactly how useful
    the model found each individual feature.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, you can use feature importances to aggressively identify features
    that you can cut out of your training data, making for leaner models. I often
    create a baseline Random Forest model to assist in aggressive feature selection.
    Aggressive feature selection is what I call it when I ruthlessly cut data that
    I do not need before training models. For this model, I did not do aggressive
    feature selection. I used all of the data that I pulled together.
  prefs: []
  type: TYPE_NORMAL
- en: Other use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this may be interesting, most of us don’t hunt revolutionaries as part
    of our day jobs. So, what good is this? Well, there are lots of uses for doing
    predictions against networks. Lately, graph ML has gotten a lot of interest, but
    most articles and books tend to showcase models built by other people (not how
    to build them from scratch), or use NNs. This is fine, but it’s complicated, and
    not always practical.
  prefs: []
  type: TYPE_NORMAL
- en: This approach that I showed is lightweight and practical. If you have network
    data, you could do something similar.
  prefs: []
  type: TYPE_NORMAL
- en: But what other use cases are there? For me, the ones I’m most interested in
    are bot detection, and the detection of artificial amplification. How would we
    do that? For bot detection, you may want to look at features such as the age of
    the account in days, the number of posts made over time (real people tend to slowly
    learn how to use a social network before becoming active), and so on. For artificial
    amplification, you might look at how many followers an account picks up for each
    tweet that they make. For instance, if an account came online a week ago, made
    2 posts, and picked up 20 million followers, what caused that kind of growth?
    Organic growth is much slower. Perhaps they brought followers from another social
    network. Perhaps their account was pushed by hundreds of blogs.
  prefs: []
  type: TYPE_NORMAL
- en: What other use cases can you think of? Be creative. You know what networks are
    now, and you know how to build them and work with them. What would you like to
    predict or better understand?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We did it! We made it to the end of another chapter. I truly hope you found
    this chapter especially interesting because there are so few sources that explain
    how to do this from scratch. One of the reasons I decided to write this book is
    because I was hoping that ideas like this would take off. So, I hope this chapter
    grabbed your attention and sparked your creativity.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we transformed an actual network into training data that we
    could use for machine learning. This was a simplified example, but the steps will
    work for any network. In the end, we created a model that was able to identify
    members of Friends of the ABC revolutionary group, though it was a very simple
    model and not suitable for anything real-world.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is going to be very similar to this one, but we will be using
    unsupervised ML to identify nodes that are similar to other nodes. Very likely,
    unsupervised ML will also identify members of Friends of the ABC, but it will
    likely also bring out other interesting insights.
  prefs: []
  type: TYPE_NORMAL
