- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Five Steps of Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will dive into the five core steps involved in the data science
    process, with examples every step of the way. These five steps include defining
    a real problem, collecting and preprocessing the data, exploring and analyzing
    the data, drawing conclusions, and communicating results effectively.
  prefs: []
  type: TYPE_NORMAL
- en: We will also delve into the important topics of data exploration and data visualization.
    Data exploration involves examining the characteristics and patterns in your data
    to better understand it, while data visualization involves using graphs, charts,
    and other visual aids to represent and communicate your data and findings.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid understanding of the data
    science process and how to apply it to solve real-world problems. So, let’s get
    started!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to what data science really is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data effectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploration tips for all levels of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **pandas** to manipulate and optimize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A question I’ve gotten at least once a month for the past decade is *What’s
    the difference between data science and data analytics?* One could argue that
    there is no difference between the two; others will argue that there are hundreds
    of differences! I believe that, regardless of how many differences there are between
    the two terms, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data science follows a structured, step-by-step process that, when followed,
    preserves the integrity of the results and leads to a deeper understanding of
    the data and the environment the data* *comes from.*'
  prefs: []
  type: TYPE_NORMAL
- en: As with any other scientific endeavor, this process must be adhered to, or else
    the analysis and the results are in danger of scrutiny. On a simpler level, following
    a strict process can make it much easier for any data scientist, hobbyist, or
    professional to obtain results faster than if they were exploring data with no
    clear vision.
  prefs: []
  type: TYPE_NORMAL
- en: While these steps are a guiding lesson for amateur analysts, they also provide
    the foundation for all data scientists, even those in the highest levels of business
    and academia. Every data scientist recognizes the value of these steps and follows
    them in some way or another.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the five steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of data science involves a series of steps that are essential for
    effectively extracting insights and knowledge from data. These steps are presented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Asking an interesting question**: The first step in any data science project
    is to identify a question or challenge that you want to address with your analysis.
    This involves finding a topic that is relevant, important, and that can be addressed
    with data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Obtaining the data**: Once you have identified your question, the next step
    is to collect the data that you will need to answer it. This can involve sourcing
    data from a variety of sources, such as databases, online platforms, or through
    data scraping or data collection methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Exploring the data**: After you have collected your data, the next step is
    to explore it and get a better understanding of its characteristics and patterns.
    This might involve examining summary statistics, visualizing the data, or applying
    statistical or **machine learning** (**ML**) techniques to identify trends or
    relationships.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Modeling the data**: Once you have explored your data, the next step is to
    build models that can be used to make predictions or inform decision-making. This
    might involve applying ML algorithms, building statistical models, or using other
    techniques to find patterns in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Communicating and visualizing the results**: Finally, it’s important to communicate
    your findings to others in a clear and effective way. This might involve creating
    reports, presentations, or visualizations that help to explain your results and
    their implications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these five essential steps, you can effectively use data science
    to solve real-world problems and extract valuable insights from data.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that different data scientists may have different approaches
    to the data science process, and the steps outlined previously are just one way
    of organizing the process. Some data scientists might group the steps differently
    or include additional steps such as feature engineering or model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these differences, most data scientists agree that the steps listed
    previously are essential to the data science process. Whether they are organized
    in this specific way or not, these steps are all crucial for effectively using
    data to solve problems and extract valuable insights. Let’s dive into these steps
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Asking an interesting question
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is probably my favorite step. Asking an interesting and relevant question
    is the first and perhaps most important step in the data science process. It sets
    the direction and focus of your analysis and determines the data and resources
    that you will need to collect and analyze.
  prefs: []
  type: TYPE_NORMAL
- en: As an entrepreneur, you are likely accustomed to constantly asking questions
    and seeking answers. This step can be approached like a brainstorming session,
    where you write down questions and ideas regardless of whether or not you think
    data exists to answer them. This helps to avoid bias and allows you to consider
    a wide range of possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to be specific and narrow in your focus when asking your question.
    This will help you to effectively address the problem and extract valuable insights
    from your data. It’s also important to consider the scope and feasibility of your
    question, as well as the resources and data that you will need to answer it.
  prefs: []
  type: TYPE_NORMAL
- en: By asking an interesting and relevant question, you can set the foundation for
    a successful data science project and begin the journey of extracting valuable
    insights from data.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Obtaining the data is a crucial step in the data science process. It involves
    sourcing and collecting the data that you will need to answer the question or
    solve the problem you have identified. The data can come from a variety of sources,
    including databases, online platforms, research studies, or data scraping or data
    collection methods.
  prefs: []
  type: TYPE_NORMAL
- en: This step can be very creative as you will need to think creatively about where
    to find the data that is most relevant to your question. You may need to explore
    different sources and platforms, and you may need to use a variety of data collection
    methods to gather the data you need.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to be mindful of the quality of the data you are collecting,
    as well as any potential biases or limitations that may be present in the data.
    It’s also important to consider ethical and legal considerations, such as obtaining
    proper consent and protecting sensitive or confidential data.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have collected your data, it’s essential to clean and preprocess it
    so that it is in a usable format. This can involve removing missing or inaccurate
    data, formatting the data in a way that makes it easier to work with, and ensuring
    that the data is consistent and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: By effectively collecting and preprocessing your data, you can set the stage
    for a successful data science project and be well prepared to move on to the next
    steps of exploring and analyzing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploring the data is an essential step in the data science process as it involves
    examining the characteristics and patterns in your data to gain a better understanding
    of it. This step is crucial for identifying trends, relationships, and insights
    that can inform your analysis and answer your research question.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to explore data, including visualizing it using
    graphs, charts, and plots, as well as applying statistical and ML techniques to
    identify patterns and relationships. It’s important to be mindful of the types
    of data you are working with, as different types of data may require different
    approaches to exploration.
  prefs: []
  type: TYPE_NORMAL
- en: This step can be time-consuming as it may involve spending several hours learning
    about the domain and using code or other tools to manipulate and explore the data.
    By the time this step is completed, the analyst should have a good understanding
    of the potential insights that the data might contain and be able to form hypotheses
    about what the data might be trying to tell them.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data is a crucial step in the data science process as it helps
    to inform the direction of your analysis and guide your choice of modeling and
    analysis techniques. It’s important to be thorough and meticulous in this step
    as the insights you gain can have significant implications for your results and
    conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modeling the data is an important step in the data science process as it involves
    using statistical and ML techniques to build models that can be used to make predictions
    or inform decision-making. This step can be complex as it involves fitting and
    choosing the appropriate models for your data and objectives, as well as implementing
    mathematical validation metrics to quantify the effectiveness of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Many different types of models can be used in data science, including linear
    regression models, logistic regression models, decision tree models, and **neural
    network** (**NN**) models, to name a few. It’s important to choose the model that
    is most appropriate for your data and the question you are trying to answer.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have chosen your model, you will need to fit it to your data. This
    involves using statistical or ML algorithms to find the parameters that best fit
    the patterns in your data. You will also need to evaluate the performance of your
    model using validation metrics such as accuracy, precision, and recall. These
    metrics can help you to understand the effectiveness of your model and identify
    any areas for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: By effectively modeling your data, you can gain insights and make informed decisions
    based on your analysis. It’s important to carefully consider the choices you make
    at this step, as the quality of your model can greatly affect the accuracy and
    usefulness of your results.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating and visualizing the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is arguably the most important step. Communicating and visualizing results
    involves effectively sharing your findings and insights with others. This step
    can be challenging as it requires you to clearly and concisely convey your results
    in a way that is understandable and digestible to your audience.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to communicate and visualize your results, including
    creating reports, presentations, and visualizations such as graphs, charts, and
    plots. It’s important to choose the method that is most appropriate for your audience
    and the message you are trying to convey.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to consider the impact of your results and the implications
    of your findings. This can involve discussing the limitations of your analysis,
    the implications of your results, and any recommendations or next steps that may
    be warranted.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will focus mainly on *steps 3*, *4*, and *5* of the data science
    process, which involve exploring and analyzing the data, modeling the data, and
    communicating and visualizing the results. While the first two steps of asking
    an interesting question and obtaining the data are also essential to the process,
    we will only touch upon these steps briefly and focus mainly on the more scientific
    aspects of the process. By focusing on these steps, we aim to provide interesting
    questions and datasets that can be used to explore and analyze data.
  prefs: []
  type: TYPE_NORMAL
- en: Why are we skipping steps 1 and 2 in this book?
  prefs: []
  type: TYPE_NORMAL
- en: While the first two steps are undoubtedly imperative to the process, they generally
    precede statistical and programmatic systems. Later in this book, we will touch
    upon the different ways to obtain data; however, to focus on the more scientific
    aspects of the process, we will begin with exploration right away.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on the third step a bit more – exploring our data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of exploring data is not always straightforward and can involve
    a variety of approaches and techniques. Some common tasks that are involved in
    data exploration include recognizing different types of data, transforming data
    types, and using code to systematically improve the quality of the entire dataset.
    These tasks can be accomplished using tools such as the `pandas` Python package,
    which is commonly used for data manipulation and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few basic questions that you should consider when exploring a new
    dataset. These questions can help you to get a sense of the data and guide your
    analysis. The three basic questions are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the types of data that are present in the dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the characteristics and patterns of the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the data organized, and what transformations might be necessary to make
    it more usable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By answering these questions and exploring your data thoroughly, you can gain
    a deeper understanding of the data and be well prepared to move on to the next
    steps of modeling and analyzing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Guiding questions for data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When given a new dataset, whether it is familiar to you or not, it is important
    to use the following questions as guidelines for your preliminary analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Is the data structured or not?**: We are checking whether or not the data
    is presented in a row/column structure. For the most part, data will be presented
    in a structured fashion. In this book, over 90% of our examples will begin with
    structured data. Nevertheless, this is the most basic question that we can answer
    before diving any deeper into our analysis. A general rule of thumb is that if
    we have unstructured data, we want to transform it into a row/column structure.
    For example, earlier in this book, we looked at ways to transform text into a
    row/column structure by counting the number of words/phrases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What does each row represent?**: Once we have an answer to how the data is
    organized and are looking at a nice row/column-based dataset, we should identify
    what each row actually represents. This step is usually very quick and can help
    put things into perspective much more quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What does each column represent?**: We should identify each column by the
    level of data, whether or not it is quantitative/qualitative, and so on. This
    categorization might change as our analysis progresses, but it is important to
    begin this step as early as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Are there any missing data points?**: Data isn’t perfect. Sometimes, we might
    be missing data because of human or mechanical error. When this happens, we, as
    data scientists, must make decisions about how to deal with these discrepancies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do we need to perform any transformations on the columns?**: Depending on
    the level/type of data in each column, we might need to perform certain types
    of transformation. For example, generally speaking, for the sake of statistical
    modeling and ML, we would like each column to be numerical, so would use Python
    to make any transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the while, we are asking ourselves the overall question *What can we infer
    from the preliminary inferential statistics?* We want to be able to understand
    our data better than when we first found it.
  prefs: []
  type: TYPE_NORMAL
- en: Enough talk; let’s see a hands-on example.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 1 – Yelp
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first dataset we will look at is a public dataset made available by the
    restaurant review site *Yelp*. All **personally identifiable information** (**PII**)
    has been removed. I am purposefully not giving much information because part of
    our goal is to ascertain elements about this data for ourselves. If I told you
    more, where would be the fun in that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we were just given this data. The first thing we have to do is read
    it in, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a quick recap of what the preceding code does:'
  prefs: []
  type: TYPE_NORMAL
- en: It imports the **pandas** package and nicknames it **pd**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It reads in the **.csv** file from the web and calls it **yelp_raw_data/**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It looks at the head of the data (just the first few rows).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The first five rows (the head) of the pandas DataFrame of our
    Yelp dataset reveals a mix of data levels, including nominal (for example, text,
    business_id) and ordinal (cool, useful, and funny)](img/B19488_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The first five rows (the head) of the pandas DataFrame of our Yelp
    dataset reveals a mix of data levels, including nominal (for example, text, business_id)
    and ordinal (cool, useful, and funny)
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we can begin to ask some key questions. Let’s start with this one:
    *Is the data structured or not?* Because we have a nice row/column structure,
    we can conclude that this data seems to be structured.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, then, the next logical question would be *What does each row represent?*
    It seems pretty obvious that each row represents a user giving a review of a business.
    The next thing we should do is examine each row and label it by the type of data
    it contains. At this point, we can also use Python to figure out just how big
    our dataset is. We can use the shape quality of a DataFrame to find this out,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It tells us that this dataset has `10000` rows and `10` columns. Another way
    to say this is that this dataset has 10,000 observations and 10 characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK—so, then, the next question: *What does* each column represent? (Note that
    we have 10 columns.) Let’s have a look at the answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**business_id**: This is likely to be a unique identifier for the business
    the review is for. This would be at the **nominal level** because there is no
    natural order to this identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**date**: This is probably the date on which the review was posted. Note that
    it seems to be only specific to the day, month, and year. Even though time is
    usually considered continuous, this column would likely be considered discrete
    and at the **ordinal level** because of the natural order that dates have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**review_id**: This is likely to be a unique identifier for the review that
    each post represents. This would be at the nominal level because, again, there
    is no natural order to this identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stars**: From a quick look (don’t worry; we will perform some further analysis
    soon), we can see that this is an ordered column that represents what the reviewer
    gave the restaurant as a final score. This is ordered and qualitative, so is at
    the ordinal level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text**: This is probably the raw text that each reviewer wrote. As with most
    text, we place this at the nominal level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**type**: In the first five rows, all we see is the word **review**. This might
    be a column that identifies that each row is a review, implying that there might
    be another type of row other than a review. We will take a look at this later.
    We place this at the nominal level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**user_id**: This is likely to be a unique identifier for the user who is writing
    the review. Just as with the other unique identifiers, we place this data at the
    nominal level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our next question is *Are there any missing data points?* Perform an `isnull`
    operation. For example, if your DataFrame is called `awesome_dataframe`, then
    try the `awesome_dataframe.isnull().sum()` Python command, which will show the
    number of missing values in each column.
  prefs: []
  type: TYPE_NORMAL
- en: Our question after that would be *Do we need to perform any transformations
    on the columns?* At this point, we are looking for a few things. For example,
    will we need to change the scale of some of the quantitative data, or do we need
    to create dummy variables for the qualitative variables? As this dataset only
    has qualitative columns, we can only focus on transformations at the ordinal and
    nominal scales.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting, let’s go over some quick terminology for `pandas`, the Python
    data exploration module.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we read in a dataset, `pandas` creates a custom object called `DataFrame`.
    Think of this as the Python version of a spreadsheet (but way better). In this
    case, the `yelp_raw_data` variable is a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether this is true in Python, type in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: DataFrames are two-dimensional in nature, meaning that they are organized in
    a row/column structure just as spreadsheets are. The main benefit of using DataFrames
    over, say, spreadsheet software is that a DataFrame can handle much larger data
    than most common spreadsheet software. If you are familiar with the R language,
    you might recognize the word DataFrame. This is because the name was actually
    borrowed from the language!
  prefs: []
  type: TYPE_NORMAL
- en: As most of the data that we will deal with is structured, DataFrames are likely
    to be the most used object in `pandas`, second only to the `Series` object.
  prefs: []
  type: TYPE_NORMAL
- en: Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Series` object is simply a DataFrame, but only with one dimension. Essentially,
    it is a list of data points. Each column of a DataFrame is considered to be a
    `Series` object. Let’s check this—the first thing we need to do is grab a single
    column from our DataFrame; we generally use what is known as **bracket notation**.
    The following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will list the first and last few rows, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use the `type` function to check that this column is a `Series` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `pandas` `Series` object will come up time and time again as it is a core
    component of `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration tips for qualitative data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using these two `pandas` objects, let’s start performing some preliminary data
    exploration. For qualitative data, we will specifically look at the nominal and
    ordinal levels.
  prefs: []
  type: TYPE_NORMAL
- en: Nominal-level columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we are at the nominal level, let’s recall that at this level, data is qualitative
    and is described purely by name. In this dataset, this refers to `business_id`,
    `review_id`, `text`, `type`, and `user_id`. Let’s use `pandas` in order to dive
    a bit deeper, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `describe` function will give us some quick stats about the column whose
    name we enter into the quotation marks. Note how `pandas` automatically recognized
    that `business_id` was a qualitative column and gave us stats that make sense.
    When `describe` is called on a qualitative column, we will always get the following
    four items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**count**: How many values are filled in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unique**: How many unique values are filled in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top**: The name of the most common item in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**freq**: How often the most common item appears in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the nominal level, we are usually looking for a few things that would signal
    a transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: Do we have a reasonable number (usually under 20) of unique items?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this column text?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this column unique across all rows?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, for the `business_id` column, we have a count of 10,000\. Don’t be fooled,
    though! This does not mean that we have 10,000 businesses being reviewed here.
    It just means that of the 10,000 rows of reviews, the `business_id` column is
    filled in all 10,000 times. The next qualifier, `unique`, tells us that we have
    4,174 unique businesses being reviewed in this dataset. The most reviewed business
    is the `JokKtdXU7zXHcr20Lrk29A` business, which was reviewed 37 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We have a `count` of `10000` and `unique` of `10000`. Think for a second—does
    this make sense? Think about what each row represents and what this column represents.
  prefs: []
  type: TYPE_NORMAL
- en: '*(Insert Jeopardy theme* *song here.)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course it does! Each row of this dataset is supposed to represent a single,
    unique review of a business, and this column is meant to serve as a unique identifier
    for a review. So, it makes sense that the `review_id` column has `10000` unique
    items in it. So, why is `eTa5KD-LTgQv6UT1Zmijmw` the *most common* review? This
    is just a random choice from `10000` and means nothing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This column, which represents the actual text people wrote, is interesting.
    We would imagine that this should also be similar to `review_id` in that each
    one should contain unique text because it would be weird if two people wrote exactly
    the same thing, but we have two reviews with the exact same text! Let’s take a
    second to learn about DataFrame filtering to examine this further.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering in pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s talk a bit about how filtering works. Filtering rows based on certain
    criteria is quite easy in `pandas`. In a DataFrame, if we wish to filter out rows
    based on some search criteria, we will need to go row by row and check whether
    or not a row satisfies that particular condition; `pandas` handles this by passing
    in a `Series` object of `True` and `False` (Booleans).
  prefs: []
  type: TYPE_NORMAL
- en: 'We literally pass into the DataFrame a list of `True` and `False` data that
    mean the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True**: This row satisfies the condition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False**: This row does not satisfy the condition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, first, let’s make the conditions. In the following lines of code, I will
    grab the text that occurs twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a snippet of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Right off the bat, we can guess that this might actually be one person who went
    to review two businesses that belong to the same chain and wrote the exact same
    review. However, this is just a guess right now.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The **duplicate_text** variable is of the **string** type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have this text, let’s use some magic to create that `Series` object
    of `True` and `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Right away, you might be confused. What we have done here is take the `text`
    column of the DataFrame and compare it to the `duplicate_text string`. This is
    strange because we seem to be comparing a list of `10,000` elements to a single
    string. Of course, the answer should be a straight false, right?
  prefs: []
  type: TYPE_NORMAL
- en: '`Series` has a very interesting feature in that if you compare the series to
    an object, it will return another series of Booleans of the same length where
    each `True` and `False` instance is the answer to the question *Is this element
    the same as the element you are comparing it to?* Very handy!'
  prefs: []
  type: TYPE_NORMAL
- en: This next code block shows us a preview of the contents of this new Series.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python, we can add and subtract `True` and `False` as if they were `1` and
    `0`, respectively—for example, `True + False – True + False + True == 1`. So,
    we can verify that this `Series` object is correct by adding up all of the values.
    As only two of these rows should contain the duplicate text, the sum of the `Series`
    object should only be `2`, which it is! This is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our series of Booleans, we can pass them directly into our
    DataFrame, using bracket notation, and get our filtered rows, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.2 – Representing two rows with duplicate text, stars, and survey
    scores](img/B19488_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Representing two rows with duplicate text, stars, and survey scores
  prefs: []
  type: TYPE_NORMAL
- en: Data is hardly ever perfect, and a common thing to check for when dealing with
    raw text is the amount of duplicate fields. In this figure, we can see that there
    are two rows with duplicate text, stars, and survey scores. This is likely written
    by the same person given to two locations of a chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems that our suspicions were correct, and one person, on the same day,
    gave the exact same review to two different `business IDs` column, presumably
    a part of the same chain. Let’s keep moving along to the rest of our columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember this column? Turns out they are all the exact same type, namely `review`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `business_id` column, all 10,000 values are filled in with 6,403
    unique users and one user reviewing 38 times!
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we won’t have to perform any transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal-level columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As far as ordinal columns go, we are looking at `date` and `stars`. For each
    of these columns, let’s look at what the `describe` method brings back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Woah! Even though this column is ordinal, the `describe` method returns stats
    that we might expect for a quantitative column. This is because the software saw
    a bunch of numbers and just assumed that we wanted stats such as the mean or the
    min and max. This is not a problem. Let’s use a method called `value_counts` to
    see the count distribution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `value_counts` method will return the distribution of values for any column.
    In this case, we see that the star rating `4` is the most common, with `3526`
    values, followed closely by rating `5`. We can also plot this data to get a nice
    visual. First, let’s sort by star rating, and then use the prebuilt `plot` method
    to make a bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.3 – A distribution of the star ratings given reveals that most  people
    are giving 4 or 5 stars, with the most common star rating being 4](img/B19488_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – A distribution of the star ratings given reveals that most people
    are giving 4 or 5 stars, with the most common star rating being 4
  prefs: []
  type: TYPE_NORMAL
- en: From this graph, we can conclude that people are definitely more likely to give
    good star ratings over bad ones! We can follow this procedure for the `date` column.
    I will leave you to try it on your own. For now, let’s look at a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 2 – Titanic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Titanic` dataset is one of those “rites of passage” datasets in data science.
    Everyone, at some point, has seen this dataset in some tutorial or book. It contains
    a sample of people who were on the Titanic when it struck an iceberg in 1912\.
    I give you my word that, going forward, we will be using much more interesting
    datasets, but trust me, it’s easier to start off with a dataset such as this one
    and Yelp to warm up the data science muscles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go ahead and import our new dataset and output the first five rows using
    the `head` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This table represents the DataFrame for the `short_titanic.csv` dataset. This
    data is definitely organized in a row/column structure, as is most spreadsheet
    data. Let’s take a quick peek at its size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We have `891` rows and `5` columns. Each row seems to represent a single passenger
    on the ship, and as far as columns are concerned, the following list tells us
    what they indicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Survived**: This is a binary variable that indicates whether or not the passenger
    survived the accident (**1** if they survived, **0** if they died). This is likely
    to be at the nominal level because there are only two options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pclass**: This is the class that the passenger was traveling in (**3** for
    third class, and so on). This is at the ordinal level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Name**: This is the name of the passenger, and it is definitely at the nominal
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sex**: This indicates the gender of the passenger. It is at the nominal level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Age**: This one is a bit tricky. Arguably, you may place age at either a
    qualitative or quantitative level; however, I think that age belongs to a quantitative
    state, and thus, to the ratio level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As far as transformations are concerned, usually, we want all columns to be
    numerical, regardless of their qualitative state. This means that `Name` and `Sex`
    will have to be converted into numerical columns somehow. For `Sex`, we can change
    the column to hold 1 if the passenger was female and 0 if they were male. Let’s
    use `pandas` to make the change. We will have to import another Python module,
    called `numpy` or numerical Python, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `np.where` method takes in three things, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of Booleans (**True** or **False**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A backup value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The method will replace all `True` instances with the first value (in this
    case, 1) and the `False` instances with the second value (in this case, 0), leaving
    us with a new numerical column that represents the same thing as the original
    `Sex` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use a shortcut and describe all the columns at once so that we can get
    a bird’s-eye view of what our data looks like and how it is roughly shaped. The
    code to do this is shown in this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.4 – Descriptive statistics of our Titanic dataset’s numerically
    formatted columns ](img/B19488_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Descriptive statistics of our Titanic dataset’s numerically formatted
    columns
  prefs: []
  type: TYPE_NORMAL
- en: Reveal that these features have pretty different ranges and means. Note that
    just because the column is “numerical” doesn’t mean it is quantitative. Pclass
    would generally be recommended as qualitative even though it is shown here as
    integers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table lists descriptive statistics of the Titanic dataset. Note how our
    qualitative columns are being treated as quantitative; however, I’m looking for
    something irrelevant to the data type. Note the `count` row: `Survived`, `Pclass`,
    and `Sex` all have `891` values (the number of rows), but `Age` only has `714`
    values. Some are missing! To double-verify, let’s use the `pandas` `isnull` and
    `sum functions`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This will show us the number of missing values in each column. So, `Age` is
    the only column with missing values to deal with.
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with missing values, you usually have the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Drop the row with the missing value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to fill it in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropping the row is the easy choice; however, you run the risk of losing valuable
    data! For example, in this case, we have 177 missing age values (891-714), which
    is nearly 20% of the data. To fill in the data, we could either go back to the
    history books, find each person one by one, and fill in their age, or we could
    fill in the age with a placeholder value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fill in each missing value of the `Age` column with the overall average
    age of the people in the dataset. For this, we will use two new methods, called
    `mean` and `fillna`. We use `isnull` to tell us which values are `null` and the
    `mean` function to give us the average value of the `Age` column. The `fillna`
    method is a `pandas` method that replaces null values with a given value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re done! We have replaced each value with `26.69`, the average age in the
    dataset. The following code now confirms that no null values exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! Nothing is missing, and we did not have to remove any rows. Let’s check
    back in with our data by looking at `head` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.5 – The first five rows of our Titanic dataset](img/B19488_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The first five rows of our Titanic dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we could start getting a bit more complicated with our questions—for
    example, *What is the average age for a female or a male?* To answer this, we
    can filter by each gender and take the mean age; `pandas` has a built-in function
    for this, called `groupby`, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the following: group the data by the `Sex` column, and then give
    me the mean age for each group. This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We will ask more of these difficult and complex questions and will be able to
    answer them with Python and statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring the data is only one of the essential steps in the data science process,
    and it is something that we will continue to do throughout this book as we work
    with different datasets. By following the steps of data exploration, we can transform,
    break down, and standardize our data to prepare it for modeling and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Our five steps serve as a standard practice for data scientists and can be applied
    to any dataset that requires analysis. While they are only guidelines, they provide
    a framework for exploring and understanding new data, and they can help us to
    identify trends, relationships, and insights that can inform our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress in this book, we will delve into the use of statistical, probabilistic,
    and ML models to analyze and make predictions from data. Before we can fully delve
    into these more complex models, however, it is important to review some of the
    basic mathematics that underlie these techniques. In the next chapter, we will
    cover some of the math that is necessary to perform some of the more complicated
    operations in modeling. Don’t worry—the math required for this process is minimal,
    and we will go through it step by step to ensure that you have a solid foundation.
    By understanding the underlying math, we can better understand the models and
    techniques that we will be using, and we can more effectively apply them to our
    data analysis
  prefs: []
  type: TYPE_NORMAL
