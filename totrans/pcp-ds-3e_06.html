<html><head></head><body>
<div id="_idContainer177" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-83"><a id="_idTextAnchor170" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-84" class="calibre6"><a id="_idTextAnchor171" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">Advanced Probability</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">In the previous chapter, we went over the basics of probability and how we can apply simple theorems to complex tasks. </span><span class="kobospan" id="kobo.3.2">To briefly summarize, probability is the mathematics of modeling events that may or may not occur. </span><span class="kobospan" id="kobo.3.3">We use formulas in order to describe these events and even look at how multiple events can </span><span><span class="kobospan" id="kobo.4.1">behave together.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.5.1">In this chapter, we will explore more complicated theorems of probability and how we can use them in a predictive capacity. </span><span class="kobospan" id="kobo.5.2">Advanced topics, such </span><a id="_idIndexMarker260" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.6.1">as </span><strong class="bold"><span class="kobospan" id="kobo.7.1">Bayes’ theorem</span></strong><span class="kobospan" id="kobo.8.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.9.1">random variables</span></strong><span class="kobospan" id="kobo.10.1">, give</span><a id="_idIndexMarker261" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.11.1"> rise to common machine learning algorithms, such</span><a id="_idIndexMarker262" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.12.1"> as the </span><strong class="bold"><span class="kobospan" id="kobo.13.1">Naïve Bayes algorithm</span></strong><span class="kobospan" id="kobo.14.1"> (also covered in </span><span><span class="kobospan" id="kobo.15.1">this book).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.16.1">This chapter will focus on some of the more advanced topics in probability theory, including the </span><span><span class="kobospan" id="kobo.17.1">following topics:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span><span class="kobospan" id="kobo.18.1">Exhaustive events</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.19.1">Bayes’ theorem</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.20.1">Basic </span><span><span class="kobospan" id="kobo.21.1">prediction rules</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.22.1">Random variables</span></span></li>
</ul>
<p class="calibre3"><a id="_idTextAnchor172" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.23.1">In later chapters, we will revisit Bayes’ theorem and use it to create a very powerful and fast machine learning algorithm, called the Naïve Bayes algorithm. </span><span class="kobospan" id="kobo.23.2">This algorithm captures the power of Bayesian thinking and applies it directly to the problem of predictive learning. </span><span class="kobospan" id="kobo.23.3">For now, let’s get started with </span><span><span class="kobospan" id="kobo.24.1">Bayesian thinking!</span></span></p>
<h1 id="_idParaDest-85" class="calibre6"><a id="_idTextAnchor173" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.25.1">Bayesian ideas revisited</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.26.1">In the last chapter, we talked very briefly about Bayesian ways of thinking. </span><span class="kobospan" id="kobo.26.2">Recall that the Bayesian way of thinking is to let our data shape and update our beliefs. </span><span class="kobospan" id="kobo.26.3">We start with a prior probability, or what we naïvely think about a hypothesis, and then we have a posterior probability, which is what we think about a hypothesis, given </span><span><span class="kobospan" id="kobo.27.1">some data.</span></span></p>
<h2 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor174" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.28.1">Bayes’ theorem</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.29.1">Bayes’ theorem is arguably</span><a id="_idIndexMarker263" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.30.1"> the most well-known part of Bayesian inference. </span><span class="kobospan" id="kobo.30.2">Recall that we previously defined </span><span><span class="kobospan" id="kobo.31.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.32.1">P(A) = the probability that event </span><span><span class="kobospan" id="kobo.33.1">A occurs</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.34.1">P(A|B) = the probability that A occurs, given that </span><span><span class="kobospan" id="kobo.35.1">B occurred</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.36.1">P(A, B) = the probability that A and </span><span><span class="kobospan" id="kobo.37.1">B occur</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.38.1">P(A, B) = P(A) * </span><span><span class="kobospan" id="kobo.39.1">P(B|A)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.40.1">That last bullet can be read as “the probability that both A and B occur is equal to the probability that A occurs x times the probability that B occurred, given that A has </span><span><span class="kobospan" id="kobo.41.1">already occurred.”</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.42.1">Starting from the last bullet points, we know </span><span><span class="kobospan" id="kobo.43.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.44.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/70.png" class="calibre83"/></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.45.1">and</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.46.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/71.png" class="calibre84"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">And we also </span><span><span class="kobospan" id="kobo.48.1">know that:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.49.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/72.png" class="calibre85"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.50.1">So, we can combine these together and </span><span><span class="kobospan" id="kobo.51.1">see that:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.52.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/73.png" class="calibre86"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.53.1">Dividing both sides by P(B) gives us Bayes’ theorem, as </span><span><span class="kobospan" id="kobo.54.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.55.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/74.png" class="calibre87"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.56.1">So, our end result is Bayes’ theorem! </span><span class="kobospan" id="kobo.56.2">This is a way to get from P(A|B) to P(B|A) (if you only have one of them) and a way to get P(A|B) if you already know P(A) (without </span><span><span class="kobospan" id="kobo.57.1">knowing B).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.58.1">Let’s try thinking about Bayes using the terms </span><em class="italic"><span class="kobospan" id="kobo.59.1">hypothesis</span></em> <span><span class="kobospan" id="kobo.60.1">and </span></span><span><em class="italic"><span class="kobospan" id="kobo.61.1">data</span></em></span><span><span class="kobospan" id="kobo.62.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.63.1">Suppose H = your hypothesis about the given data and D = the data that you </span><span><span class="kobospan" id="kobo.64.1">are given.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.65.1">Bayes can be interpreted as trying to figure out P(H|D) (the probability that our hypothesis is correct, given the data </span><span><span class="kobospan" id="kobo.66.1">at hand).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.67.1">Let’s use our terminology </span><span><span class="kobospan" id="kobo.68.1">from before:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.69.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;H&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/75.png" class="calibre88"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.70.1">Let’s take a look at </span><span><span class="kobospan" id="kobo.71.1">that formula:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.72.1">P(H)</span></em><span class="kobospan" id="kobo.73.1"> is the probability of the hypothesis before we observe the</span><a id="_idIndexMarker264" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.74.1"> data, called the </span><strong class="bold"><span class="kobospan" id="kobo.75.1">prior probability</span></strong><span class="kobospan" id="kobo.76.1">, or</span><a id="_idIndexMarker265" class="pcalibre calibre4 pcalibre1"/> <span><span class="kobospan" id="kobo.77.1">just </span></span><span><strong class="bold"><span class="kobospan" id="kobo.78.1">prior</span></strong></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.79.1">P(H|D)</span></em><span class="kobospan" id="kobo.80.1"> is what we want to</span><a id="_idIndexMarker266" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.81.1"> compute, the probability of the hypothesis after we observe the data, called </span><span><span class="kobospan" id="kobo.82.1">the </span></span><span><strong class="bold"><span class="kobospan" id="kobo.83.1">posterior</span></strong></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.84.1">P(D|H)</span></em><span class="kobospan" id="kobo.85.1"> is the probability of the data under the given hypothesis, called </span><a id="_idIndexMarker267" class="pcalibre calibre4 pcalibre1"/><span><span class="kobospan" id="kobo.86.1">the </span></span><span><strong class="bold"><span class="kobospan" id="kobo.87.1">likelihood</span></strong></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.88.1">P(D)</span></em><span class="kobospan" id="kobo.89.1"> is the probability of the data under any hypothesis, called</span><a id="_idIndexMarker268" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.90.1"> the </span><span><strong class="bold"><span class="kobospan" id="kobo.91.1">normalizing constant</span></strong></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.92.1">This concept is not far off from the idea of machine learning and predictive analytics. </span><span class="kobospan" id="kobo.92.2">In many cases, when considering predictive analytics, we use the given data to predict an outcome. </span><span class="kobospan" id="kobo.92.3">Using the current terminology, H (our hypothesis) can be considered our outcome, and P(H|D) (the probability that our hypothesis is true, given our data) is another way of saying, what is the chance that my hypothesis is correct, given the data in front </span><span><span class="kobospan" id="kobo.93.1">of me?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">Let’s take a look at an example of how we can use Bayes’ formula in </span><span><span class="kobospan" id="kobo.95.1">the workplace.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.96.1">Consider that you have </span><a id="_idIndexMarker269" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.97.1">two people in charge of writing blog posts for your company, Lucy and Avinash. </span><span class="kobospan" id="kobo.97.2">From past performance, you like 80% of Lucy’s work and only 50% of Avinash’s work. </span><span class="kobospan" id="kobo.97.3">A new blog post comes to your desk in the morning, but the author isn’t mentioned. </span><span class="kobospan" id="kobo.97.4">You love the article. </span><span class="kobospan" id="kobo.97.5">A+. </span><span class="kobospan" id="kobo.97.6">What is the probability that it came from Avinash? </span><span class="kobospan" id="kobo.97.7">Each blogger blogs at a very </span><span><span class="kobospan" id="kobo.98.1">similar rate.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.99.1">Before we freak out, let’s do what any experienced mathematician (and now you) would do. </span><span class="kobospan" id="kobo.99.2">Let’s write out all of our information, as </span><span><span class="kobospan" id="kobo.100.1">shown here:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.101.1">H</span></em><span class="kobospan" id="kobo.102.1"> (hypothesis) = the blog came </span><span><span class="kobospan" id="kobo.103.1">from Avinash</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.104.1">D</span></em><span class="kobospan" id="kobo.105.1"> (data) = you loved the </span><span><span class="kobospan" id="kobo.106.1">blog post</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.107.1">P(H|D)</span></em><span class="kobospan" id="kobo.108.1"> = the chance that it came from Avinash, given that you </span><span><span class="kobospan" id="kobo.109.1">loved it</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.110.1">P(D|H)</span></em><span class="kobospan" id="kobo.111.1"> = the chance that you loved it, given that it came </span><span><span class="kobospan" id="kobo.112.1">from Avinash</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.113.1">P(H)</span></em><span class="kobospan" id="kobo.114.1"> = the chance that an article came </span><span><span class="kobospan" id="kobo.115.1">from Avinash</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.116.1">P(D)</span></em><span class="kobospan" id="kobo.117.1"> = the chance that you love </span><span><span class="kobospan" id="kobo.118.1">an article</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.119.1">Note that some of these variables make almost no sense without context. </span><span class="kobospan" id="kobo.119.2">P(D), the probability that you would love any given article put on your desk, is a weird concept, but trust me – in the context of Bayes’ formula, it will be relevant </span><span><span class="kobospan" id="kobo.120.1">very soon.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.121.1">Also, note that in the last two items, they assume nothing else. </span><em class="italic"><span class="kobospan" id="kobo.122.1">P(D) does not assume the origin of the blog post; think of P(D) as asking, if an article was plopped on your desk from some unknown source, what is the chance that you’d like it?</span></em><span class="kobospan" id="kobo.123.1"> (again, I know it sounds weird out </span><span><span class="kobospan" id="kobo.124.1">of context).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.125.1">So, we want to know </span><em class="italic"><span class="kobospan" id="kobo.126.1">P(H|D)</span></em><span class="kobospan" id="kobo.127.1">. </span><span class="kobospan" id="kobo.127.2">Let’s try to use Bayes’ theorem, as </span><span><span class="kobospan" id="kobo.128.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.129.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;H&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/75.png" class="calibre88"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.130.1">But do we know the numbers on the right-hand side of this equation? </span><span class="kobospan" id="kobo.130.2">I say that we do! </span><span class="kobospan" id="kobo.130.3">Let’s </span><span><span class="kobospan" id="kobo.131.1">see here:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.132.1">P(H)</span></em><span class="kobospan" id="kobo.133.1"> is the probability that any given blog post comes from Avinash. </span><span class="kobospan" id="kobo.133.2">As bloggers write at a very similar rate, we can assume this is 0.5 because we have a 50/50 chance that it came from either blogger (note how I did not assume </span><em class="italic"><span class="kobospan" id="kobo.134.1">D</span></em><span class="kobospan" id="kobo.135.1">, the data, </span><span><span class="kobospan" id="kobo.136.1">for this).</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.137.1">P(D|H)</span></em><span class="kobospan" id="kobo.138.1"> is the probability that you love a post from Avinash, which we previously said was 50% – </span><span><span class="kobospan" id="kobo.139.1">so, 0.5.</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.140.1">P(D)</span></em><span class="kobospan" id="kobo.141.1"> is interesting. </span><span class="kobospan" id="kobo.141.2">This is the chance that you love an article </span><em class="italic"><span class="kobospan" id="kobo.142.1">in general</span></em><span class="kobospan" id="kobo.143.1">. </span><span class="kobospan" id="kobo.143.2">It means that we must take into account the scenario where the post came from either Lucy or Avinash. </span><span class="kobospan" id="kobo.143.3">Now, if the hypothesis forms a suite, we can use our laws of probability, as mentioned in the previous chapter. </span><span class="kobospan" id="kobo.143.4">A suite is formed when a set of hypotheses is both collectively exhaustive – meaning at least one must occur – and mutually exclusive. </span><span class="kobospan" id="kobo.143.5">In layman’s terms, in a suite of events, exactly one and only one hypothesis can occur. </span><span class="kobospan" id="kobo.143.6">In our case, the two hypotheses are that the article came from Lucy, or that the article came from Avinash. </span><span class="kobospan" id="kobo.143.7">This is definitely a suite for the </span><span><span class="kobospan" id="kobo.144.1">following reasons:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.145.1">At least one of them </span><span><span class="kobospan" id="kobo.146.1">wrote it</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.147.1">At most one of them </span><span><span class="kobospan" id="kobo.148.1">wrote it</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.149.1">Therefore, </span><em class="italic"><span class="kobospan" id="kobo.150.1">exactly one</span></em><span class="kobospan" id="kobo.151.1"> of them </span><span><span class="kobospan" id="kobo.152.1">wrote it</span></span></li></ul></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.153.1">When we have a suite, we </span><a id="_idIndexMarker270" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.154.1">can use our multiplication and addition rules, </span><span><span class="kobospan" id="kobo.155.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer112">
<span class="kobospan" id="kobo.156.1"><img alt="Figure 6.1 – An example of the multiplication and addition rules in action" src="image/B19488_06_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.157.1">Figure 6.1 – An example of the multiplication and addition rules in action</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.158.1">Whew! </span><span class="kobospan" id="kobo.158.2">Way to go! </span><span class="kobospan" id="kobo.158.3">Now, we can finish our equation, as </span><span><span class="kobospan" id="kobo.159.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.160.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;65&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;38&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/77.png" class="calibre89"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.161.1">This means that there is a 38% chance that this article comes from Avinash. </span><span class="kobospan" id="kobo.161.2">What is interesting is that P(H) = 0.5 and P(H|D) = 0.38. </span><span class="kobospan" id="kobo.161.3">This means that without any data, the chance that a blog post came from Avinash was a coin flip, or 50/50. </span><span class="kobospan" id="kobo.161.4">Given some data (your thoughts on the article), we updated our beliefs about the </span><a id="_idIndexMarker271" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.162.1">hypothesis, and it actually lowered the chance. </span><span class="kobospan" id="kobo.162.2">This is what Bayesian thinking is all about – updating our posterior beliefs about something from a prior assumption, given some new data about </span><span><span class="kobospan" id="kobo.163.1">the subject.</span></span></p>
<h2 id="_idParaDest-87" class="calibre7"><span class="kobospan" id="kobo.164.1">M</span><a id="_idTextAnchor175" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.165.1">ore applications of Bayes’ theorem</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.166.1">Bayes’ theorem shows up in a lot </span><a id="_idIndexMarker272" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.167.1">of applications, usually when we need to make fast decisions based on data and probability. </span><span class="kobospan" id="kobo.167.2">Most recommendation engines, such as Netflix’s, use some elements of Bayesian updating. </span><span class="kobospan" id="kobo.167.3">And if you consider why that might be, it </span><span><span class="kobospan" id="kobo.168.1">makes sense.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.169.1">Let’s suppose that, in our simplistic world, Netflix only has 10 categories to choose from. </span><span class="kobospan" id="kobo.169.2">Now, suppose that, given no data, a user’s chance of liking a comedy movie out of 10 categories is 10% (</span><span><span class="kobospan" id="kobo.170.1">just 1/10).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.171.1">Okay, now suppose that the user has given a few comedy movies 5/5 stars. </span><span class="kobospan" id="kobo.171.2">Now, when Netflix wonders what the chance is that the user would like another comedy, the probability that they might like a comedy, P(H|D), is going to be larger than a random guess </span><span><span class="kobospan" id="kobo.172.1">of 10%!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.173.1">Let’s try some more examples of applying Bayes’ theorem using more data. </span><span class="kobospan" id="kobo.173.2">This time, let’s get a </span><span><span class="kobospan" id="kobo.174.1">bit grittier.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.175.1">E</span><a id="_idTextAnchor176" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.176.1">xample – Titanic</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.177.1">A very famous dataset </span><a id="_idIndexMarker273" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.178.1">involves looking at the survivors of the sinking of the Titanic in 1912. </span><span class="kobospan" id="kobo.178.2">We will use an application of probability in order to figure out whether there were any demographic features that showed a relationship to passenger survival. </span><span class="kobospan" id="kobo.178.3">Mainly, we are curious to see whether we can isolate any features of our dataset that can tell us more about the types of people who were likely to survive </span><span><span class="kobospan" id="kobo.179.1">this disaster.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.180.1">First, let’s read in the data, as </span><span><span class="kobospan" id="kobo.181.1">shown here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.182.1">
titanic =
pd.read_csv(https://raw.githubusercontent.com/sinanuozdemir/SF_DAT_15/maste r/data/titanic.csv')#read in a csv
titanic = titanic[['Sex', 'Survived']] #the Sex and Survived column titanic.head()</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.183.1">We get the </span><span><span class="kobospan" id="kobo.184.1">following table:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer114">
<span class="kobospan" id="kobo.185.1"><img alt="Table 6.1 – Representing the Titanic dataset options Sex and Survived" src="image/B19488_06_02.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.186.1">Table 6.1 – Representing the Titanic dataset options Sex and Survived</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.187.1">The Titanic dataset only has two options for </span><strong class="bold"><span class="kobospan" id="kobo.188.1">Sex</span></strong><span class="kobospan" id="kobo.189.1"> and two options for </span><strong class="bold"><span class="kobospan" id="kobo.190.1">Survived</span></strong><span class="kobospan" id="kobo.191.1">. </span><span class="kobospan" id="kobo.191.2">While Survived is a relatively straightforward feature, this is one of our first examples of using a dataset in which data interpretations have “drifted” over time. </span><span class="kobospan" id="kobo.191.3">We will revisit the concept of drift in a later chapter, and how datasets such as the Titanic dataset can be easy to work with but are outdated in </span><span><span class="kobospan" id="kobo.192.1">their utility.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.193.1">In the preceding table, each r</span><a id="_idIndexMarker274" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.194.1">ow represents a single passenger on the ship, and, for now, we will look at two specific features – the sex of the individual and whether or not they survived the sinking. </span><span class="kobospan" id="kobo.194.2">For example, the first row represents a man who did not survive, while the fourth row (with index 3 – remember how Python indexes lists) represents a female who </span><span><span class="kobospan" id="kobo.195.1">did survive.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.196.1">Let’s start with some basics. </span><span class="kobospan" id="kobo.196.2">Let’s start by calculating the probability that any given person on the ship survived, regardless of their gender. </span><span class="kobospan" id="kobo.196.3">To do this, let’s count the number of yeses in the </span><strong class="bold"><span class="kobospan" id="kobo.197.1">Survived</span></strong><span class="kobospan" id="kobo.198.1"> column and divide this figure by the total number of rows, as </span><span><span class="kobospan" id="kobo.199.1">shown here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.200.1">
  num_rows = float(titanic.shape[0]) # == 891 rows
  p_survived = (titanic.Survived=="yes").sum() / num_rows #
==
.38
  p_notsurvived = 1 - p_survived
#
==
.61</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.201.1">Note that I only had to calculate </span><strong class="source-inline"><span class="kobospan" id="kobo.202.1">P(Survived)</span></strong><span class="kobospan" id="kobo.203.1">, and I used the law of conjugate probabilities to calculate </span><strong class="source-inline"><span class="kobospan" id="kobo.204.1">P(Died)</span></strong><span class="kobospan" id="kobo.205.1"> because those two events are complementary. </span><span class="kobospan" id="kobo.205.2">Now, let’s calculate the probability that any single passenger is male </span><span><span class="kobospan" id="kobo.206.1">or female:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.207.1">
p_male = (titanic.Sex=="male").sum() / num_rows # == .65
p_female = 1 - p_male # == .35</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.208.1">Now, let’s ask ourselves a question – did having a certain gender affect the survival rate? </span><span class="kobospan" id="kobo.208.2">For this, we can estimate </span><em class="italic"><span class="kobospan" id="kobo.209.1">P(Survived|Female)</span></em><span class="kobospan" id="kobo.210.1"> or the chance that someone survived given that they were female. </span><span class="kobospan" id="kobo.210.2">For this, we need to divide the number of women who survived by the total number of women, as </span><span><span class="kobospan" id="kobo.211.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.212.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/78.png" class="calibre90"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.213.1">Here’s the code for </span><span><span class="kobospan" id="kobo.214.1">that calculation:</span></span></p>
<table class="no-table-style" id="table001-3">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2" colspan="2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.215.1">number_of_women = </span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.216.1">titanic[titanic.Sex=='female'].shape[0] #==</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.217.1">314</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.218.1">women_who_lived = </span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.219.1">titanic[(titanic.Sex=='female') &amp;</span></strong></span></p>
</td>
<td class="no-table-style2"/>
<td class="no-table-style2"/>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.220.1">(</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.221.1">titanic.Survived=='yes')].shape[0]</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.222.1">#==</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"> </strong><span><strong class="source-inline"><span class="kobospan" id="kobo.223.1">233</span></strong></span></p>
</td>
</tr>
</tbody>
</table>
<pre class="source-code"><span class="kobospan1" id="kobo.224.1">
p_survived_given_woman = women_who_lived / float(number_of_women)
p_survived_given_woman # == .74</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.225.1">That’s a pretty big difference. </span><span class="kobospan" id="kobo.225.2">It seems that gender plays a big part in </span><span><span class="kobospan" id="kobo.226.1">this dataset.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor177" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.227.1">Example – medical studies</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.228.1">A classic use of Bayes’ theorem </span><a id="_idIndexMarker275" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.229.1">is the interpretation of medical trials. </span><span class="kobospan" id="kobo.229.2">Routine testing for illegal drug use is increasingly common in workplaces and schools. </span><span class="kobospan" id="kobo.229.3">The companies that perform these tests maintain that the tests have a high sensitivity, which means that they are likely to produce a positive result if there are drugs in their system. </span><span class="kobospan" id="kobo.229.4">They claim that these tests are also highly specific, which means that they are likely to yield a negative result if there are </span><span><span class="kobospan" id="kobo.230.1">no drugs.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.231.1">On average, let’s assume that the sensitivity of common drug tests is about 60% and the specificity is about 99%. </span><span class="kobospan" id="kobo.231.2">It means that if an employee is using drugs, the test has a 60% chance of being positive, while if an employee is not on drugs, the test has a 99% chance of being negative. </span><span class="kobospan" id="kobo.231.3">Now, suppose these tests are applied to a workforce where the actual rate of drug use </span><span><span class="kobospan" id="kobo.232.1">is 5%.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.233.1">The real question here is, of the people who test positive, how many actually </span><span><span class="kobospan" id="kobo.234.1">use drugs?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.235.1">In Bayesian terms, we want to compute the probability of drug use, given a </span><span><span class="kobospan" id="kobo.236.1">positive test:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.237.1">Let </span><em class="italic"><span class="kobospan" id="kobo.238.1">D</span></em><span class="kobospan" id="kobo.239.1"> = the event that drugs are </span><span><span class="kobospan" id="kobo.240.1">in use</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.241.1">Let </span><em class="italic"><span class="kobospan" id="kobo.242.1">E</span></em><span class="kobospan" id="kobo.243.1"> = the event that the test </span><span><span class="kobospan" id="kobo.244.1">is positive</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.245.1">Let </span><em class="italic"><span class="kobospan" id="kobo.246.1">N</span></em><span class="kobospan" id="kobo.247.1"> = the event that drugs are </span><em class="italic"><span class="kobospan" id="kobo.248.1">NOT</span></em> <span><span class="kobospan" id="kobo.249.1">in use</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.250.1">We are looking </span><span><span class="kobospan" id="kobo.251.1">for P(D|E).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.252.1">By using </span><em class="italic"><span class="kobospan" id="kobo.253.1">Bayes’ theorem</span></em><span class="kobospan" id="kobo.254.1">, we can extrapolate </span><span><span class="kobospan" id="kobo.255.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.256.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/79.png" class="calibre91"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.257.1">The prior, P(D), is the probability of drug use before we see the outcome of the test, which is 5%. </span><span class="kobospan" id="kobo.257.2">The likelihood, P(E|D), is the probability of a positive test assuming drug use, which is the same thing as the sensitivity of the test. </span><span class="kobospan" id="kobo.257.3">The normalizing constant, P(E), is a little </span><span><span class="kobospan" id="kobo.258.1">bit trickier.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.259.1">We have to consider two things – P(E and D) as well as P(E and N). </span><span class="kobospan" id="kobo.259.2">Basically, we must assume that the test is capable of being incorrect when the user is not using drugs. </span><span class="kobospan" id="kobo.259.3">Check out the </span><span><span class="kobospan" id="kobo.260.1">following equations:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.261.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/80.png" class="calibre92"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.262.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/81.png" class="calibre93"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.263.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;05&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;95&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;01&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/82.png" class="calibre94"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.264.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.0395&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/83.png" class="calibre95"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.265.1">So, our original equation becomes </span><span><span class="kobospan" id="kobo.266.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.267.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;05&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;0.0395&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/84.png" class="calibre96"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.268.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;76&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/85.png" class="calibre97"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.269.1">This means that of</span><a id="_idIndexMarker276" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.270.1"> the people who test positive for drug use, about a quarter </span><span><span class="kobospan" id="kobo.271.1">are innocent!</span></span></p>
<h1 id="_idParaDest-88" class="calibre6"><a id="_idTextAnchor178" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.272.1">Random variables</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.273.1">A </span><strong class="bold"><span class="kobospan" id="kobo.274.1">random variable</span></strong><span class="kobospan" id="kobo.275.1"> uses real</span><a id="_idIndexMarker277" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.276.1"> numerical values to describe a probabilistic event. </span><span class="kobospan" id="kobo.276.2">In our previous work with variables (both in math and programming), we were used to the fact that a variable takes on a certain value. </span><span class="kobospan" id="kobo.276.3">For example, we might have a right-angled triangle in which we are given the variable </span><strong class="source-inline"><span class="kobospan" id="kobo.277.1">h</span></strong><span class="kobospan" id="kobo.278.1"> for the hypotenuse, and we must figure out the length of the hypotenuse. </span><span class="kobospan" id="kobo.278.2">We also might have the following, </span><span><span class="kobospan" id="kobo.279.1">in Python:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.280.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/86.png" class="calibre98"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.281.1">Both of these variables are equal to one value at a time. </span><span class="kobospan" id="kobo.281.2">In a random variable, we are subject to randomness, which means that our variables’ values are, well, just that – variable! </span><span class="kobospan" id="kobo.281.3">They might take on multiple values depending on </span><span><span class="kobospan" id="kobo.282.1">the environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.283.1">A random variable still, as shown previously, holds a value. </span><span class="kobospan" id="kobo.283.2">The main distinction between variables as we have seen them and a random variable is the fact that a random variable’s value may change, depending on </span><span><span class="kobospan" id="kobo.284.1">the situation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.285.1">However, if a random variable can have many values, how do we keep track of them all? </span><span class="kobospan" id="kobo.285.2">Each value that a random variable might take on is associated with a percentage, and for each value, there is a single probability that the variable will be </span><span><span class="kobospan" id="kobo.286.1">that value.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.287.1">With a random variable, we can also obtain the probability distribution of a random variable, which gives the variable’s possible values and </span><span><span class="kobospan" id="kobo.288.1">their probabilities.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.289.1">Written out, we generally use single capital letters (mostly the specific letter </span><em class="italic"><span class="kobospan" id="kobo.290.1">X</span></em><span class="kobospan" id="kobo.291.1">) to denote random variables. </span><span class="kobospan" id="kobo.291.2">For example, we might have </span><span><span class="kobospan" id="kobo.292.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.293.1">X</span></em><span class="kobospan" id="kobo.294.1">: The outcome of a </span><span><span class="kobospan" id="kobo.295.1">dice roll</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.296.1">Y</span></em><span class="kobospan" id="kobo.297.1">: The revenue earned by a company </span><span><span class="kobospan" id="kobo.298.1">this year</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.299.1">Z</span></em><span class="kobospan" id="kobo.300.1">: The score of an applicant on an interview coding </span><span><span class="kobospan" id="kobo.301.1">quiz (0–100%)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.302.1">Effectively, a random variable is a function that maps values from the sample space of an event (the set of all possible outcomes) to a probability value (between 0 and 1). </span><span class="kobospan" id="kobo.302.2">Think about the event as being expressed </span><span><span class="kobospan" id="kobo.303.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.304.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/87.png" class="calibre99"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.305.1">The function assigns a</span><a id="_idIndexMarker278" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.306.1"> probability to each individual option. </span><span class="kobospan" id="kobo.306.2">There are two main types of random variables – </span><strong class="bold"><span class="kobospan" id="kobo.307.1">discrete</span></strong> <span><span class="kobospan" id="kobo.308.1">and </span></span><span><strong class="bold"><span class="kobospan" id="kobo.309.1">continuous</span></strong></span><span><span class="kobospan" id="kobo.310.1">.</span></span></p>
<h2 id="_idParaDest-89" class="calibre7"><a id="_idTextAnchor179" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.311.1">Discrete random variables</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.312.1">A discrete random</span><a id="_idIndexMarker279" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.313.1"> variable only takes on</span><a id="_idIndexMarker280" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.314.1"> a countable number of possible values, such as the outcome of a dice roll, as </span><span><span class="kobospan" id="kobo.315.1">shown here:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer125">
<span class="kobospan" id="kobo.316.1"><img alt="Figure 6.2 – Representing a discrete random variable" src="image/B19488_06_03.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.317.1">Figure 6.2 – Representing a discrete random variable</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.318.1">Note how I use a capital X to define the random variable. </span><span class="kobospan" id="kobo.318.2">This is a common practice. </span><span class="kobospan" id="kobo.318.3">Also, note how the random variable maps a probability to each individual outcome. </span><span class="kobospan" id="kobo.318.4">Random variables have many properties, two of which are their </span><em class="italic"><span class="kobospan" id="kobo.319.1">expected value</span></em><span class="kobospan" id="kobo.320.1"> and the </span><em class="italic"><span class="kobospan" id="kobo.321.1">variance</span></em><span class="kobospan" id="kobo.322.1">. </span><span class="kobospan" id="kobo.322.2">We will use a </span><strong class="bold"><span class="kobospan" id="kobo.323.1">probability mass function</span></strong><span class="kobospan" id="kobo.324.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.325.1">PMF</span></strong><span class="kobospan" id="kobo.326.1">) to describe </span><a id="_idIndexMarker281" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.327.1">a discrete </span><span><span class="kobospan" id="kobo.328.1">random variable.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.329.1">They take on the </span><span><span class="kobospan" id="kobo.330.1">following appearance:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.331.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/88.png" class="calibre100"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.332.1">So, for a </span><span><span class="kobospan" id="kobo.333.1">dice roll,</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.334.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/89.png" class="calibre101"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.335.1">Consider the following examples of </span><span><span class="kobospan" id="kobo.336.1">discrete variables:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.337.1">The likely result of a survey question (for example, on a scale </span><span><span class="kobospan" id="kobo.338.1">of 1–10)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.339.1">Whether the CEO will resign within the year (either true </span><span><span class="kobospan" id="kobo.340.1">or false)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.341.1">The expected value of a random variable defines the mean value of a long run of repeated samples of the random variable. </span><span class="kobospan" id="kobo.341.2">This is sometimes called </span><a id="_idIndexMarker282" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.342.1">the </span><em class="italic"><span class="kobospan" id="kobo.343.1">mean</span></em><span class="kobospan" id="kobo.344.1"> of </span><span><span class="kobospan" id="kobo.345.1">the variable.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.346.1">For example, refer to the following Python code, which defines the random variable of a </span><span><span class="kobospan" id="kobo.347.1">dice roll:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.348.1">
import random
def random_variable_of_dice_roll():
return random.randint(1, 7) # a range of (1,7) # includes 1, 2, 3, 4, 5, 6, but NOT 7</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.349.1">This function will invoke a random variable and come out with a response. </span><span class="kobospan" id="kobo.349.2">Let’s roll </span><strong class="source-inline"><span class="kobospan" id="kobo.350.1">100</span></strong><span class="kobospan" id="kobo.351.1"> dice and average the result, </span><span><span class="kobospan" id="kobo.352.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.353.1">
trials = []
num_trials = 100
for trial in range(num_trials):
trials.append( random_variable_of_dice_roll() ) print(sum(trials)/float(num_trials)) # == 3.77</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.354.1">So, taking </span><strong class="source-inline"><span class="kobospan" id="kobo.355.1">100</span></strong><span class="kobospan" id="kobo.356.1"> dice </span><a id="_idIndexMarker283" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.357.1">rolls and </span><a id="_idIndexMarker284" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.358.1">averaging them gives us a value of </span><strong class="source-inline"><span class="kobospan" id="kobo.359.1">3.77</span></strong><span class="kobospan" id="kobo.360.1">! </span><span class="kobospan" id="kobo.360.2">Let’s try this with a wide variety of trial numbers, as </span><span><span class="kobospan" id="kobo.361.1">illustrated here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.362.1">
num_trials = range(100,10000, 10)
avgs = []
for num_trial in num_trials:
trials = []
for trial in range(1,num_trial):
trials.append( random_variable_of_dice_roll() )
avgs.append(sum(trials)/float(num_trial))
plt.plot(num_trials, avgs)
plt.xlabel('Number of Trials')
plt.ylabel("Average")</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.363.1">We get the </span><span><span class="kobospan" id="kobo.364.1">following graph:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer128">
<span class="kobospan" id="kobo.365.1"><img alt="Figure 6.3 – Representing the average of 100 dice rolls" src="image/B19488_06_04.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.366.1">Figure 6.3 – Representing the average of 100 dice rolls</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.367.1">The preceding graph </span><a id="_idIndexMarker285" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.368.1">represents the average dice roll as we look at more and more dice rolls. </span><span class="kobospan" id="kobo.368.2">We can see that the average dice </span><a id="_idIndexMarker286" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.369.1">roll rapidly approaches </span><strong class="source-inline"><span class="kobospan" id="kobo.370.1">3.5</span></strong><span class="kobospan" id="kobo.371.1">. </span><span class="kobospan" id="kobo.371.2">If we look at the left of the graph, we see that if we only roll a die about 100 times, then we are not guaranteed to get an average dice roll of </span><strong class="source-inline"><span class="kobospan" id="kobo.372.1">3.5</span></strong><span class="kobospan" id="kobo.373.1">. </span><span class="kobospan" id="kobo.373.2">However, if we roll 10,000 dice one after another, we see that we would very likely expect the average dice roll to be </span><span><span class="kobospan" id="kobo.374.1">about </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.375.1">3.5</span></strong></span><span><span class="kobospan" id="kobo.376.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.377.1">For a discrete random variable, we can also use a simple formula, shown as follows, to calculate the </span><span><span class="kobospan" id="kobo.378.1">expected value:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.379.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/90.png" class="calibre102"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.380.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.381.1">xi</span></em><span class="kobospan" id="kobo.382.1"> is the </span><em class="italic"><span class="kobospan" id="kobo.383.1">ith</span></em><span class="kobospan" id="kobo.384.1"> outcome and </span><em class="italic"><span class="kobospan" id="kobo.385.1">pi</span></em><span class="kobospan" id="kobo.386.1"> is the </span><span><em class="italic"><span class="kobospan" id="kobo.387.1">ith</span></em></span><span><span class="kobospan" id="kobo.388.1"> probability.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.389.1">So, for our dice roll, we can find the exact expected value </span><span><span class="kobospan" id="kobo.390.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.391.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3.5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/91.png" class="calibre103"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.392.1">The preceding result shows us that for any given dice roll, we can “expect” a dice roll of 3.5. </span><span class="kobospan" id="kobo.392.2">Now, obviously, that doesn’t make sense because we can’t get a 3.5 on a dice roll, but it does make sense when put in the context of many dice rolls. </span><span class="kobospan" id="kobo.392.3">If you roll 10,000 dice, your average dice roll should approach 3.5, as shown in the graph and </span><span><span class="kobospan" id="kobo.393.1">code previously.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.394.1">The average of the expected value of a random variable is generally not enough to grasp the full idea behind the variable. </span><span class="kobospan" id="kobo.394.2">For this reason, we will introduce a new concept, </span><span><span class="kobospan" id="kobo.395.1">called variance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.396.1">The variance of a random variable represents the spread of the variable. </span><span class="kobospan" id="kobo.396.2">It quantifies the variability of the </span><span><span class="kobospan" id="kobo.397.1">expected value.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.398.1">The formula for the variance of a discrete random variable is expressed </span><span><span class="kobospan" id="kobo.399.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.400.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;script&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;mi mathvariant=&quot;script&quot;&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/92.png" class="calibre104"/></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.401.1">xi</span></em><span class="kobospan" id="kobo.402.1"> and </span><em class="italic"><span class="kobospan" id="kobo.403.1">pi</span></em><span class="kobospan" id="kobo.404.1"> represent the same values as before, and </span><span class="kobospan" id="kobo.405.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/93.png" class="calibre105"/></span><span> </span><span class="kobospan" id="kobo.406.1">represents the expected value of the variable. </span><span class="kobospan" id="kobo.406.2">In this formula, I also mentioned the sigma of </span><em class="italic"><span class="kobospan" id="kobo.407.1">X</span></em><span class="kobospan" id="kobo.408.1">. </span><span class="kobospan" id="kobo.408.2">Sigma, in this case, is the standard deviation, which is defined simply as the square root of the variance. </span><span class="kobospan" id="kobo.408.3">Let’s look at a more complicated example of a discrete </span><span><span class="kobospan" id="kobo.409.1">random variable.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.410.1">Variance can be thought of as a </span><em class="italic"><span class="kobospan" id="kobo.411.1">give or take</span></em><span class="kobospan" id="kobo.412.1"> metric. </span><span class="kobospan" id="kobo.412.2">If I say you can expect to win $100 from a poker hand, you might be very happy. </span><span class="kobospan" id="kobo.412.3">If I append that statement with the additional detail that you might win $100, give $80, or take $80, you now have a wide range of expectations to deal with, which can be frustrating and might make a risk-averse player more wary of joining the game. </span><span class="kobospan" id="kobo.412.4">We can usually say that we have an expected value, give or take the </span><span><span class="kobospan" id="kobo.413.1">standard deviation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.414.1">Consider that your team measures the success of a new product on a </span><strong class="bold"><span class="kobospan" id="kobo.415.1">Likert scale</span></strong><span class="kobospan" id="kobo.416.1"> – that is, as being in one of five categories, where a value of 0 represents a complete </span><a id="_idIndexMarker287" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.417.1">failure and 4 represents a great success. </span><span class="kobospan" id="kobo.417.2">They estimate that a new project has the following chances of success (shown in </span><span><em class="italic"><span class="kobospan" id="kobo.418.1">Figure 6</span></em></span><em class="italic"><span class="kobospan" id="kobo.419.1">.4</span></em><span class="kobospan" id="kobo.420.1">), based on user testing and the preliminary results of the performance of </span><span><span class="kobospan" id="kobo.421.1">the product.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.422.1">We first have to define our random variable. </span><span class="kobospan" id="kobo.422.2">Let the </span><em class="italic"><span class="kobospan" id="kobo.423.1">X</span></em><span class="kobospan" id="kobo.424.1"> random variable represent the success of our product. </span><em class="italic"><span class="kobospan" id="kobo.425.1">X</span></em><span class="kobospan" id="kobo.426.1"> is indeed a discrete random variable because the </span><em class="italic"><span class="kobospan" id="kobo.427.1">X</span></em><span class="kobospan" id="kobo.428.1"> variable can only take on one of five options – 0, 1, 2, 3, </span><span><span class="kobospan" id="kobo.429.1">or 4.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.430.1">The following is the probability</span><a id="_idIndexMarker288" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.431.1"> distribution </span><a id="_idIndexMarker289" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.432.1">of our random variable, </span><em class="italic"><span class="kobospan" id="kobo.433.1">X</span></em><span class="kobospan" id="kobo.434.1">. </span><span class="kobospan" id="kobo.434.2">Note how we have a column for each potential outcome of </span><em class="italic"><span class="kobospan" id="kobo.435.1">X</span></em><span class="kobospan" id="kobo.436.1">, and following each outcome, we have the probability that that particular outcome will </span><span><span class="kobospan" id="kobo.437.1">be achieved:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer133">
<span class="kobospan" id="kobo.438.1"><img alt="Figure 6.4 – The probability distribution of our random variable" src="image/B19488_06_05.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.439.1">Figure 6.4 – The probability distribution of our random variable</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.440.1">For example, the project has a 2% chance of failing completely and a 26% chance of being a great success! </span><span class="kobospan" id="kobo.440.2">We can calculate our expected value </span><span><span class="kobospan" id="kobo.441.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.442.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.02&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.07&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.26&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2.81&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/94.png" class="calibre106"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.443.1">This number means that the manager can expect a success of about </span><strong class="source-inline"><span class="kobospan" id="kobo.444.1">2.81</span></strong><span class="kobospan" id="kobo.445.1"> with this project. </span><span class="kobospan" id="kobo.445.2">Now, by itself, that number is not very useful. </span><span class="kobospan" id="kobo.445.3">Perhaps, if given several products to choose from, an expected value might be a way to compare the potential successes of several products. </span><span class="kobospan" id="kobo.445.4">However, in this case, when we have just one product to evaluate, we will need more. </span><span class="kobospan" id="kobo.445.5">Now, let’s check the variance, as </span><span><span class="kobospan" id="kobo.446.1">shown here:</span></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.447.1">V</span></span><span><span class="kobospan" id="kobo.448.1">a</span></span><span><span class="kobospan" id="kobo.449.1">r</span></span><span><span class="kobospan" id="kobo.450.1">i</span></span><span><span class="kobospan" id="kobo.451.1">a</span></span><span><span class="kobospan" id="kobo.452.1">n</span></span><span><span class="kobospan" id="kobo.453.1">c</span></span><span><span class="kobospan" id="kobo.454.1">e</span></span><span> </span><span><span class="kobospan" id="kobo.455.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.456.1">V</span></span><span><span class="kobospan" id="kobo.457.1">[</span></span><span><span class="kobospan" id="kobo.458.1">X</span></span><span><span class="kobospan" id="kobo.459.1">]</span></span><span> </span><span><span class="kobospan" id="kobo.460.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.461.1">X</span></span><span><span class="kobospan" id="kobo.462.1">2</span></span><span> </span><span><span class="kobospan" id="kobo.463.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.464.1">(</span></span><span><span class="kobospan" id="kobo.465.1">x</span></span><span><span class="kobospan" id="kobo.466.1">i</span></span><span> </span><span><span class="kobospan" id="kobo.467.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.468.1">μ</span></span><span><span class="kobospan" id="kobo.469.1">X</span></span><span><span class="kobospan" id="kobo.470.1">)</span></span><span><span class="kobospan" id="kobo.471.1">2</span></span><span><span class="kobospan" id="kobo.472.1">p</span></span><span><span class="kobospan" id="kobo.473.1">i</span></span><span> </span><span><span class="kobospan" id="kobo.474.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.475.1">(</span></span><span><span class="kobospan" id="kobo.476.1">0</span></span><span> </span><span><span class="kobospan" id="kobo.477.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.478.1">2.81</span></span><span><span class="kobospan" id="kobo.479.1">)</span></span><span><span class="kobospan" id="kobo.480.1">2</span></span><span><span class="kobospan" id="kobo.481.1">(</span></span><span><span class="kobospan" id="kobo.482.1">0.02</span></span><span><span class="kobospan" id="kobo.483.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.484.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.485.1">(</span></span><span><span class="kobospan" id="kobo.486.1">1</span></span><span> </span><span><span class="kobospan" id="kobo.487.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.488.1">2.81</span></span><span><span class="kobospan" id="kobo.489.1">)</span></span><span><span class="kobospan" id="kobo.490.1">2</span></span><span><span class="kobospan" id="kobo.491.1">(</span></span><span><span class="kobospan" id="kobo.492.1">0.07</span></span><span><span class="kobospan" id="kobo.493.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.494.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.495.1">(</span></span><span><span class="kobospan" id="kobo.496.1">2</span></span><span>
</span><span> </span><span><span class="kobospan" id="kobo.497.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.498.1">2.81</span></span><span><span class="kobospan" id="kobo.499.1">)</span></span><span><span class="kobospan" id="kobo.500.1">2</span></span><span><span class="kobospan" id="kobo.501.1">(</span></span><span><span class="kobospan" id="kobo.502.1">0.25</span></span><span><span class="kobospan" id="kobo.503.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.504.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.505.1">(</span></span><span><span class="kobospan" id="kobo.506.1">3</span></span><span> </span><span><span class="kobospan" id="kobo.507.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.508.1">2.81</span></span><span><span class="kobospan" id="kobo.509.1">)</span></span><span><span class="kobospan" id="kobo.510.1">2</span></span><span><span class="kobospan" id="kobo.511.1">(</span></span><span><span class="kobospan" id="kobo.512.1">0.4</span></span><span><span class="kobospan" id="kobo.513.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.514.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.515.1">(</span></span><span><span class="kobospan" id="kobo.516.1">4</span></span><span> </span><span><span class="kobospan" id="kobo.517.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.518.1">2.81</span></span><span><span class="kobospan" id="kobo.519.1">)</span></span><span><span class="kobospan" id="kobo.520.1">2</span></span><span><span class="kobospan" id="kobo.521.1">(</span></span><span><span class="kobospan" id="kobo.522.1">0.26</span></span><span><span class="kobospan" id="kobo.523.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.524.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.525.1">.</span></span><span><span><span class="kobospan" id="kobo.526.1">93</span></span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.527.1">Now that we have both the standard deviation and the expected value of the score of the project, let’s try to summarize our results. </span><span class="kobospan" id="kobo.527.2">We could say that our project will have an expected score of 2.81, plus or minus 0.96, meaning that we can expect something between 1.85 </span><span><span class="kobospan" id="kobo.528.1">and 3.77.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.529.1">So, one way we can address this project is that it is probably going to have a success rating of 2.81, give or take about </span><span><span class="kobospan" id="kobo.530.1">a point.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.531.1">You might be thinking, wow, Sinan – so at best, the project will be a 3.8, and at worst it will be a 1.8? </span><span><span class="kobospan" id="kobo.532.1">Not quite.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.533.1">It might be better than a 4, and it might also be worse than a 1.8. </span><span class="kobospan" id="kobo.533.2">To take this one step further, let’s calculate </span><span><span class="kobospan" id="kobo.534.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.535.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;&gt;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/95.png" class="calibre107"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.536.1">First, take a minute and convince yourself that you can read that formula to yourself. </span><span class="kobospan" id="kobo.536.2">What am I asking when I ask for P(X &gt;= 3)? </span><span class="kobospan" id="kobo.536.3">Honestly, take a minute and figure </span><span><span class="kobospan" id="kobo.537.1">it out.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.538.1">P(X &gt;= 3) is the probability that our random variable will take on a value at least as big as 3. </span><span class="kobospan" id="kobo.538.2">In other words, what is the chance that</span><a id="_idIndexMarker290" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.539.1"> our</span><a id="_idIndexMarker291" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.540.1"> product will have a success rating of 3 or higher? </span><span class="kobospan" id="kobo.540.2">To calculate this, we can calculate </span><span><span class="kobospan" id="kobo.541.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.542.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;66&lt;/mml:mn&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;66&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;%&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/96.png" class="calibre108"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.543.1">This means that we have a 66% chance that our product will rate as either a 3 or </span><span><span class="kobospan" id="kobo.544.1">a 4.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.545.1">Another way to calculate this would be the conjugate way, as </span><span><span class="kobospan" id="kobo.546.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.547.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/97.png" class="calibre109"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.548.1">Again, take a moment to convince yourself that this formula holds up. </span><span class="kobospan" id="kobo.548.2">I am claiming that to find the probability that the product will be rated at least a 3 is the same as 1, minus the probability that the product will receive a rating below 3. </span><span class="kobospan" id="kobo.548.3">If this is true, then the two events (X &gt;=3 and X &lt; 3) must complement </span><span><span class="kobospan" id="kobo.549.1">one another.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.550.1">This is obviously true! </span><span class="kobospan" id="kobo.550.2">The product can be either of the following </span><span><span class="kobospan" id="kobo.551.1">two options:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.552.1">Be rated 3 </span><span><span class="kobospan" id="kobo.553.1">or above</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.554.1">Be rated below </span><span><span class="kobospan" id="kobo.555.1">a 3</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.556.1">Let’s check </span><span><span class="kobospan" id="kobo.557.1">our math:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.558.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/98.png" class="calibre110"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.559.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.02&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;0.07&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/99.png" class="calibre111"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.560.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;0341&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/100.png" class="calibre112"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.561.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;34&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/101.png" class="calibre113"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.562.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;66&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/102.png" class="calibre114"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.563.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&gt;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/103.png" class="calibre115"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.564.1">It </span><span><span class="kobospan" id="kobo.565.1">c</span></span><span><a id="_idIndexMarker292" class="pcalibre calibre4 pcalibre1"/></span><span><span class="kobospan" id="kobo.566.1">hecks </span></span><span><a id="_idIndexMarker293" class="pcalibre calibre4 pcalibre1"/></span><span><span class="kobospan" id="kobo.567.1">out!</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.568.1">Type</span><a id="_idTextAnchor180" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.569.1">s of discrete random variables</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.570.1">We can get a better idea of how </span><a id="_idIndexMarker294" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.571.1">random variables work in practice by looking at specific types of random variables. </span><span class="kobospan" id="kobo.571.2">These specific types of random variables model different types of situations and end up revealing much simpler calculations for very complex </span><span><span class="kobospan" id="kobo.572.1">event modeling.</span></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.573.1">Bino</span><a id="_idTextAnchor181" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.574.1">mial random variables</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.575.1">The first type of discrete random</span><a id="_idIndexMarker295" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.576.1"> variable we will look at is called </span><a id="_idIndexMarker296" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.577.1">a </span><strong class="bold"><span class="kobospan" id="kobo.578.1">binomial random variable</span></strong><span class="kobospan" id="kobo.579.1">. </span><span class="kobospan" id="kobo.579.2">With a binomial random variable, we look at a setting in which a single event happens over and over, and we try to count the number of times the result </span><span><span class="kobospan" id="kobo.580.1">is positive.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.581.1">Before we can understand the random variable itself, we must look at the conditions in which it is </span><span><span class="kobospan" id="kobo.582.1">even appropriate.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.583.1">A binomial setting has the following </span><span><span class="kobospan" id="kobo.584.1">four conditions:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.585.1">The possible outcomes are either success </span><span><span class="kobospan" id="kobo.586.1">or failure</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.587.1">The outcomes of trials cannot affect the outcome of </span><span><span class="kobospan" id="kobo.588.1">another trial</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.589.1">The number of trials was set (a fixed </span><span><span class="kobospan" id="kobo.590.1">sample size)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.591.1">The chance of success of each trial must always </span><span><span class="kobospan" id="kobo.592.1">be </span></span><span><em class="italic"><span class="kobospan" id="kobo.593.1">p</span></em></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.594.1">A binomial random variable is a discrete random variable, </span><em class="italic"><span class="kobospan" id="kobo.595.1">X</span></em><span class="kobospan" id="kobo.596.1">, that counts the number of successes in a binomial setting. </span><span class="kobospan" id="kobo.596.2">The parameters are </span><em class="italic"><span class="kobospan" id="kobo.597.1">n = the number of trials</span></em><span class="kobospan" id="kobo.598.1"> and </span><em class="italic"><span class="kobospan" id="kobo.599.1">p = the chance of success of </span></em><span><em class="italic"><span class="kobospan" id="kobo.600.1">each trial</span></em></span><span><span class="kobospan" id="kobo.601.1">.</span></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.602.1">Exam</span><a id="_idTextAnchor182" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.603.1">ple – fundraising meetings</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.604.1">In this example, a start-up is taking 20 </span><strong class="bold"><span class="kobospan" id="kobo.605.1">Venture Capital</span></strong><span class="kobospan" id="kobo.606.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.607.1">VC</span></strong><span class="kobospan" id="kobo.608.1">) meetings to fund and count the number of offers </span><span><span class="kobospan" id="kobo.609.1">they receive.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.610.1">The </span><strong class="bold"><span class="kobospan" id="kobo.611.1">probability mass function</span></strong><span class="kobospan" id="kobo.612.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.613.1">PMF</span></strong><span class="kobospan" id="kobo.614.1">) for </span><a id="_idIndexMarker297" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.615.1">a binomial</span><a id="_idIndexMarker298" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.616.1"> random variable is </span><span><span class="kobospan" id="kobo.617.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.618.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/104.png" class="calibre117"/></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.619.1">Here,</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.620.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;h&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;b&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;c&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;c&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;k&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;k&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/105.png" class="calibre118"/></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.621.1">Exam</span><a id="_idTextAnchor183" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.622.1">ple – restaurant openings</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.623.1">In this example, a new restaurant </span><a id="_idIndexMarker299" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.624.1">in a town has a 20% chance of surviving its first year. </span><span class="kobospan" id="kobo.624.2">If 14 restaurants open this year, find the probability that exactly four restaurants survive their first year of being open to </span><span><span class="kobospan" id="kobo.625.1">the public.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.626.1">First, we should prove that this is a </span><span><span class="kobospan" id="kobo.627.1">binomial setting:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.628.1">The possible outcomes are either success or failure (the restaurants either survive </span><span><span class="kobospan" id="kobo.629.1">or not)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.630.1">The outcomes of trials cannot affect the outcome of another trial (assume that the opening of one restaurant doesn’t affect another restaurant’s opening </span><span><span class="kobospan" id="kobo.631.1">and survival)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.632.1">The number of trials was set (14 </span><span><span class="kobospan" id="kobo.633.1">restaurants opened)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.634.1">The chance of success of each trial must always be </span><em class="italic"><span class="kobospan" id="kobo.635.1">p</span></em><span class="kobospan" id="kobo.636.1"> (we assume that it is </span><span><span class="kobospan" id="kobo.637.1">always 20%)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.638.1">Here, we have our two parameters of n = 14 and p = 0.2. </span><span class="kobospan" id="kobo.638.2">So, we can now plug these numbers into our binomial formula, as </span><span><span class="kobospan" id="kobo.639.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.640.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac linethickness=&quot;0pt&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;14&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mn&gt;17&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/106.png" class="calibre119"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.641.1">So, we have a 17% chance that exactly four of these restaurants will be open after </span><span><span class="kobospan" id="kobo.642.1">a year.</span></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.643.1">Exam</span><a id="_idTextAnchor184" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.644.1">ple – blood types</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.645.1">In this example, a couple </span><a id="_idIndexMarker300" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.646.1">has a 25% chance of having a child with type O blood. </span><span class="kobospan" id="kobo.646.2">What is the chance that three of their five kids have type </span><span><span class="kobospan" id="kobo.647.1">O blood?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.648.1">Let X = the number of children with type O blood with n = 5 and p = 0.25, as </span><span><span class="kobospan" id="kobo.649.1">shown here:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.650.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.75&lt;/mn&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;25&lt;/mn&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.75&lt;/mn&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.087&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/107.png" class="calibre120"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.651.1">We can calculate this probability for the values of 0, 1, 2, 3, 4, and 5 to get a sense of the </span><span><span class="kobospan" id="kobo.652.1">probability distribution:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer148">
<span class="kobospan" id="kobo.653.1"><img alt="Figure 6.5 – The probability for the values of 0, 1, 2, 3, 4, and 5" src="image/B19488_06_06.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.654.1">Figure 6.5 – The probability for the values of 0, 1, 2, 3, 4, and 5</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.655.1">From here, we can calculate an expected value and the variance of </span><span><span class="kobospan" id="kobo.656.1">this variable:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.657.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;mi mathvariant=&quot;script&quot;&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;script&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1.25&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/108.png" class="calibre121"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.658.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;script&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;mi mathvariant=&quot;script&quot;&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.9375&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/109.png" class="calibre122"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.659.1">So, this family can expect to have probably one or two kids with type </span><span><span class="kobospan" id="kobo.660.1">O blood!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.661.1">What if we want to know the probability that at  least three of their kids have type O blood? </span><span class="kobospan" id="kobo.661.2">To know the probability that at least three of their kids have type O blood, we can use the following formula for discrete </span><span><span class="kobospan" id="kobo.662.1">random variables:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.663.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&gt;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;00098&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;01465&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;08789&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.103&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/110.png" class="calibre123"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.664.1">So, there is about a 10% chance that three of their kids have type </span><span><span class="kobospan" id="kobo.665.1">O blood.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.666.1">Shortcuts to binomial expected values and variance</span></p>
<p class="callout"><span class="kobospan" id="kobo.667.1">Binomial random variables have special calculations for the exact values of the expected values and variance. </span><span class="kobospan" id="kobo.667.2">If </span><em class="italic"><span class="kobospan" id="kobo.668.1">X</span></em><span class="kobospan" id="kobo.669.1"> is a binomial random variable, then we get </span><span><span class="kobospan" id="kobo.670.1">the following:</span></span></p>
<p class="callout"><span class="kobospan" id="kobo.671.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/111.png" class="calibre124"/></span></p>
<p class="callout"><span class="kobospan" id="kobo.672.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/112.png" class="calibre125"/></span></p>
<p class="callout"><span class="kobospan" id="kobo.673.1">For our preceding example, we can use the following formulas to calculate an exact expected value </span><span><span class="kobospan" id="kobo.674.1">and variance:</span></span></p>
<p class="callout"><span class="kobospan" id="kobo.675.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;25&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1.25&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/113.png" class="calibre126"/></span></p>
<p class="callout"><span class="kobospan" id="kobo.676.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1.25&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;75&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.9375&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/114.png" class="calibre127"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.677.1">A binomial random variable is a discrete random variable that counts the number of successes in a binomial setting. </span><span class="kobospan" id="kobo.677.2">It is used in a wide variety of data-driven experiments, such as counting the number of people who will </span><a id="_idIndexMarker301" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.678.1">sign up for a website given a chance of conversion, or even, at a simple level, predicting stock price movements given a chance of decline (don’t worry – we will apply much more sophisticated models to predict the stock </span><span><span class="kobospan" id="kobo.679.1">market later).</span></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.680.1">Geome</span><a id="_idTextAnchor185" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.681.1">tric random variables</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.682.1">The second discrete </span><a id="_idIndexMarker302" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.683.1">random variable</span><a id="_idIndexMarker303" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.684.1"> we will take a look at is called a </span><strong class="bold"><span class="kobospan" id="kobo.685.1">geometric random variable</span></strong><span class="kobospan" id="kobo.686.1">. </span><span class="kobospan" id="kobo.686.2">It is actually quite similar to the binomial random variable in the way that we are concerned with a setting, in which a single event occurs over and over. </span><span class="kobospan" id="kobo.686.3">However, in the case of a geometric setting, the major difference is that we are not fixing the </span><span><span class="kobospan" id="kobo.687.1">sample size.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.688.1">We are not going into exactly 20 VC meetings as a start-up, nor are we having exactly five kids. </span><span class="kobospan" id="kobo.688.2">Instead, in a geometric setting, we are modeling the number of trials we will need to see before we obtain even a single success. </span><span class="kobospan" id="kobo.688.3">Specifically, a geometric setting has the following </span><span><span class="kobospan" id="kobo.689.1">four conditions:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.690.1">The possible outcomes are either a success </span><span><span class="kobospan" id="kobo.691.1">or failure</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.692.1">The outcomes of trials cannot affect the outcome of </span><span><span class="kobospan" id="kobo.693.1">another trial</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.694.1">The number of trials was </span><span><span class="kobospan" id="kobo.695.1">not set</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.696.1">The chance of success of each trial must always </span><span><span class="kobospan" id="kobo.697.1">be </span></span><span><em class="italic"><span class="kobospan" id="kobo.698.1">p</span></em></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.699.1">Note that these are the exact same conditions as a binomial variable, except for the </span><span><span class="kobospan" id="kobo.700.1">third condition.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.701.1">A </span><strong class="bold"><span class="kobospan" id="kobo.702.1">geometric random variable</span></strong><span class="kobospan" id="kobo.703.1"> is</span><a id="_idIndexMarker304" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.704.1"> a discrete random variable, X, that counts the number of trials needed to obtain one success. </span><span class="kobospan" id="kobo.704.2">The parameters are </span><em class="italic"><span class="kobospan" id="kobo.705.1">p = the chance of success of each trial and (1 − p) = the chance of failure of </span></em><span><em class="italic"><span class="kobospan" id="kobo.706.1">each trial</span></em></span><span><span class="kobospan" id="kobo.707.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.708.1">To transform the previous binomial examples into geometric examples, we might do </span><span><span class="kobospan" id="kobo.709.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.710.1">Count the number of VC meetings that a start-up must take in order to get their </span><span><span class="kobospan" id="kobo.711.1">first </span></span><span><em class="italic"><span class="kobospan" id="kobo.712.1">yes</span></em></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.713.1">Count the number of coin flips needed in order to get a head (yes, I know it’s boring, but it’s a </span><span><span class="kobospan" id="kobo.714.1">solid example!)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.715.1">The formula for the PMF is </span><span><span class="kobospan" id="kobo.716.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.717.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/115.png" class="calibre128"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.718.1">Both the binomial and geometric settings involve outcomes that are either successes or failures. </span><span class="kobospan" id="kobo.718.2">The big difference is that binomial random variables have a fixed number of trials, denoted as </span><em class="italic"><span class="kobospan" id="kobo.719.1">n</span></em><span class="kobospan" id="kobo.720.1">. </span><span class="kobospan" id="kobo.720.2">Geometric random variables do </span><a id="_idIndexMarker305" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.721.1">not have </span><a id="_idIndexMarker306" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.722.1">a fixed number of trials. </span><span class="kobospan" id="kobo.722.2">Instead, geometric random variables model the number of samples needed in order to obtain the first successful trial, whatever success might mean in those </span><span><span class="kobospan" id="kobo.723.1">experimental conditions.</span></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.724.1">Examp</span><a id="_idTextAnchor186" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.725.1">le – weather</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.726.1">In this example, there</span><a id="_idIndexMarker307" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.727.1"> is a 34% chance that it will rain on any day in April. </span><span class="kobospan" id="kobo.727.2">Find the probability that the first day of rain in April will occur on </span><span><span class="kobospan" id="kobo.728.1">April 4.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.729.1">Let X = the number of days until it rains (success) with p = 0.34 and (1 − p) = 0.66. </span><span class="kobospan" id="kobo.729.2">So then, the probability that it will rain by April 4 is </span><span><span class="kobospan" id="kobo.730.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.731.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;34&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;14&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;&gt;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;8&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/116.png" class="calibre129"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.732.1">So, there is an 80% chance that the first rain of the month will happen within the first </span><span><span class="kobospan" id="kobo.733.1">four days.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.734.1">Shortcuts to geometric expected values and variance</span></p>
<p class="callout"><span class="kobospan" id="kobo.735.1">Geometric random variables also have special calculations for the exact values of the expected values and variance. </span><span class="kobospan" id="kobo.735.2">If </span><em class="italic"><span class="kobospan" id="kobo.736.1">X</span></em><span class="kobospan" id="kobo.737.1"> is a </span><a id="_idIndexMarker308" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.738.1">geometric random variable, then we get </span><span><span class="kobospan" id="kobo.739.1">the following:</span></span></p>
<p class="callout"><span class="kobospan" id="kobo.740.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/117.png" class="calibre130"/></span></p>
<p class="callout"><span class="kobospan" id="kobo.741.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;V&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/118.png" class="calibre131"/></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.742.1">Poiss</span><a id="_idTextAnchor187" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.743.1">on random variable</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.744.1">The third and last </span><a id="_idIndexMarker309" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.745.1">specific example of a </span><a id="_idIndexMarker310" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.746.1">discrete random variable is a Poisson </span><span><span class="kobospan" id="kobo.747.1">random variable.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.748.1">To understand why we would need this random variable, imagine that an event that we wish to model has a small probability of happening and that we wish to count the number of times that the event occurs in a certain time frame. </span><span class="kobospan" id="kobo.748.2">If we have an idea of the average number of occurrences, µ, over a specific period of time, given from past instances, then the Poisson random variable, denoted by </span><em class="italic"><span class="kobospan" id="kobo.749.1">X = Poi(µ)</span></em><span class="kobospan" id="kobo.750.1">, counts the total number of occurrences of the event during that given </span><span><span class="kobospan" id="kobo.751.1">time period.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.752.1">In other words, the Poisson distribution is a discrete probability distribution that counts the number of events that occur in a given interval </span><span><span class="kobospan" id="kobo.753.1">of time.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.754.1">Consider the following examples of Poisson </span><span><span class="kobospan" id="kobo.755.1">random variables:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.756.1">Finding the probability of having a certain number of visitors on your site within an hour, knowing the past performance of </span><span><span class="kobospan" id="kobo.757.1">the site</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.758.1">Estimating the number of car crashes at an intersection, based on past </span><span><span class="kobospan" id="kobo.759.1">police reports</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.760.1">If we let </span><em class="italic"><span class="kobospan" id="kobo.761.1">X = the number of events in a given interval</span></em><span class="kobospan" id="kobo.762.1">, and the average number of events per interval is the </span><em class="italic"><span class="kobospan" id="kobo.763.1">λ</span></em><span class="kobospan" id="kobo.764.1"> number, then the probability of observing </span><em class="italic"><span class="kobospan" id="kobo.765.1">X</span></em><span class="kobospan" id="kobo.766.1"> events in a given interval is given by the </span><span><span class="kobospan" id="kobo.767.1">following formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.768.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/119.png" class="calibre132"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.769.1">Here, e = Euler’s </span><span><span class="kobospan" id="kobo.770.1">constant (2.718....).</span></span></p>
<h4 class="calibre116"><span class="kobospan" id="kobo.771.1">Examp</span><a id="_idTextAnchor188" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.772.1">le – a call center</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.773.1">The number of calls arriving at </span><a id="_idIndexMarker311" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.774.1">your call center follows a Poisson distribution at the rate of five calls per hour. </span><span class="kobospan" id="kobo.774.2">What is the probability that exactly six calls will come in between 10 and </span><span><span class="kobospan" id="kobo.775.1">11 p.m.?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.776.1">To set up this example, let’s write out our given information. </span><span class="kobospan" id="kobo.776.2">Let </span><em class="italic"><span class="kobospan" id="kobo.777.1">X</span></em><span class="kobospan" id="kobo.778.1"> be the number of calls that arrive between 10 and 11 p.m. </span><span class="kobospan" id="kobo.778.2">This is our Poisson random variable, with the mean </span><em class="italic"><span class="kobospan" id="kobo.779.1">λ = 5</span></em><span class="kobospan" id="kobo.780.1">. </span><span class="kobospan" id="kobo.780.2">The mean is </span><em class="italic"><span class="kobospan" id="kobo.781.1">5</span></em><span class="kobospan" id="kobo.782.1"> because we are using </span><em class="italic"><span class="kobospan" id="kobo.783.1">5</span></em><span class="kobospan" id="kobo.784.1"> as the expected value of the number of calls to come in at this time. </span><span class="kobospan" id="kobo.784.2">This number could have come from the previous work on estimating the number of calls that come in every hour, or that specifically come in after 10 p.m. </span><span class="kobospan" id="kobo.784.3">The main point is that we do have some idea of how many calls should be coming in, and then we use that information to create our </span><em class="italic"><span class="kobospan" id="kobo.785.1">Poisson random variable</span></em><span class="kobospan" id="kobo.786.1">, using it to </span><span><span class="kobospan" id="kobo.787.1">make predictions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.788.1">Continuing with our example, we have </span><span><span class="kobospan" id="kobo.789.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.790.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.146&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/120.png" class="calibre133"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.791.1">This means that there is about a 14.6% chance that exactly six calls will come in between 10 and </span><span><span class="kobospan" id="kobo.792.1">11 p.m.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.793.1">Shortcuts to Poisson expected values and variance</span></p>
<p class="callout"><span class="kobospan" id="kobo.794.1">Poisson random variables also have special calculations for the exact values of the expected values and variance. </span><span class="kobospan" id="kobo.794.2">If </span><em class="italic"><span class="kobospan" id="kobo.795.1">X</span></em><span class="kobospan" id="kobo.796.1"> is a Poisson random variable with the mean, then we get </span><span><span class="kobospan" id="kobo.797.1">the following:</span></span></p>
<p class="callout"><span class="kobospan" id="kobo.798.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/121.png" class="calibre134"/></span></p>
<p class="callout"><span class="kobospan" id="kobo.799.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/122.png" class="calibre135"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.800.1">This is actually interesting because both the expected value and the variance are the same number, and that number is simply the</span><a id="_idIndexMarker312" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.801.1"> given parameter! </span><span class="kobospan" id="kobo.801.2">Now that we’ve seen three examples of discrete random variables, we must take a look at the other type of random variable, called the continuous </span><span><span class="kobospan" id="kobo.802.1">random variable.</span></span></p>
<h2 id="_idParaDest-90" class="calibre7"><span class="kobospan" id="kobo.803.1">Conti</span><a id="_idTextAnchor189" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.804.1">nuous random variables</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.805.1">Switching gears </span><a id="_idIndexMarker313" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.806.1">entirely, unlike a</span><a id="_idIndexMarker314" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.807.1"> discrete random variable, a continuous random variable can take on an </span><em class="italic"><span class="kobospan" id="kobo.808.1">infinite</span></em><span class="kobospan" id="kobo.809.1"> number of possible values, not just a few countable ones. </span><span class="kobospan" id="kobo.809.2">We call the functions that describe the distribution density curves instead of probability </span><span><span class="kobospan" id="kobo.810.1">mass functions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.811.1">Consider the following examples of </span><span><span class="kobospan" id="kobo.812.1">continuous variables:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.813.1">The length of a sales representative’s phone call (not the number </span><span><span class="kobospan" id="kobo.814.1">of calls)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.815.1">The actual amount of oil in a drum marked 20 gallons (not the number of </span><span><span class="kobospan" id="kobo.816.1">oil drums)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.817.1">If </span><em class="italic"><span class="kobospan" id="kobo.818.1">X</span></em><span class="kobospan" id="kobo.819.1"> is a continuous random variable, then there is a function, </span><em class="italic"><span class="kobospan" id="kobo.820.1">f(x)</span></em><span class="kobospan" id="kobo.821.1">, for any constants, </span><em class="italic"><span class="kobospan" id="kobo.822.1">a</span></em> <span><span class="kobospan" id="kobo.823.1">and </span></span><span><em class="italic"><span class="kobospan" id="kobo.824.1">b</span></em></span><span><span class="kobospan" id="kobo.825.1">:</span></span>
</p>
<p class="calibre3" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.826.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/123.png" class="calibre136"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.827.1">The preceding f(x) function is known as</span><a id="_idIndexMarker315" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.828.1"> the </span><strong class="bold"><span class="kobospan" id="kobo.829.1">probability density function</span></strong><span class="kobospan" id="kobo.830.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.831.1">PDF</span></strong><span class="kobospan" id="kobo.832.1">). </span><span class="kobospan" id="kobo.832.2">The PDF is the continuous random variable version of the PMF for discrete </span><span><span class="kobospan" id="kobo.833.1">random variables.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.834.1">The most important continuous distribution </span><a id="_idIndexMarker316" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.835.1">is the </span><strong class="bold"><span class="kobospan" id="kobo.836.1">standard normal distribution</span></strong><span class="kobospan" id="kobo.837.1">. </span><span class="kobospan" id="kobo.837.2">You have, no doubt, either heard of the normal distribution or dealt with it. </span><span class="kobospan" id="kobo.837.3">The idea behind it is quite simple. </span><span class="kobospan" id="kobo.837.4">The PDF of this distribution is </span><span><span class="kobospan" id="kobo.838.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.839.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/124.png" class="calibre137"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.840.1">Here, μ is the mean of the variable, and σ is the standard deviation. </span><span class="kobospan" id="kobo.840.2">This might look confusing, but let’s graph it in Python, with a mean of 0 and a standard deviation of 1, as </span><span><span class="kobospan" id="kobo.841.1">shown here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.842.1">
import numpy as np
import matplotlib.pyplot as plt
def normal_pdf(x, mu = 0, sigma = 1):
return (1./np.sqrt(2*3.14 * sigma**2)) * np.exp((-(x-mu)**2 / (2.* sigma**2)))
x_values = np.linspace(-5,5,100)
y_values = [normal_pdf(x) for x in x_values] plt.plot(x_values, y_values)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.843.1">We get </span><span><span class="kobospan" id="kobo.844.1">this graph:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer166">
<span class="kobospan" id="kobo.845.1"><img alt="Figure 6.6 – Representing a mean of 0 and a standard deviation of 1" src="image/B19488_06_07.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.846.1">Figure 6.6 – Representing a mean of 0 and a standard deviation of 1</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.847.1">This reveals the all-too-familiar</span><a id="_idIndexMarker317" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.848.1"> bell curve. </span><span class="kobospan" id="kobo.848.2">Note</span><a id="_idIndexMarker318" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.849.1"> that the graph is symmetrical around the x = 0 line. </span><span class="kobospan" id="kobo.849.2">Let’s try changing some of the parameters. </span><span class="kobospan" id="kobo.849.3">First, let’s try with </span><span class="kobospan" id="kobo.850.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/93.png" class="calibre105"/></span><span class="kobospan" id="kobo.851.1"> = </span><span><span class="kobospan" id="kobo.852.1">5:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer168">
<span class="kobospan" id="kobo.853.1"><img alt="Figure 6.7 – Representing the all-too-familiar bell curve" src="image/B19488_06_08.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.854.1">Figure 6.7 – Representing the all-too-familiar bell curve</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.855.1">Next, let’s try</span><a id="_idIndexMarker319" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.856.1"> with the</span><a id="_idIndexMarker320" class="pcalibre calibre4 pcalibre1"/> <span><span class="kobospan" id="kobo.857.1">value </span></span><span><span class="kobospan" id="kobo.858.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/126.png" class="calibre138"/></span></span><span><span class="kobospan" id="kobo.859.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer170">
<span class="kobospan" id="kobo.860.1"><img alt="Figure 6.8 – Representing the value ​&lt;?AID d835?&gt;&lt;?AID df48?&gt; = 5​" src="image/B19488_06_09.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.861.1">Figure 6.8 – Representing the value </span><span class="kobospan" id="kobo.862.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;σ&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/127.png" class="calibre139"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.863.1">Lastly, we will try with the values </span><span class="kobospan" id="kobo.864.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/93.png" class="calibre105"/></span><span class="kobospan" id="kobo.865.1"> = </span><span><span class="kobospan" id="kobo.866.1">5 </span></span><span><span class="kobospan" id="kobo.867.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/129.png" class="calibre140"/></span></span><span><span class="kobospan" id="kobo.868.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer174">
<span class="kobospan" id="kobo.869.1"><img alt="Figure 6.9 – A graph representing the values ​&lt;?AID d835?&gt;&lt;?AID df41?&gt;​ = 5 ​, ﻿&lt;?AID d835?&gt;&lt;?AID df48?&gt; = 5​" src="image/B19488_06_10.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.870.1">Figure 6.9 – A graph representing the values </span><span class="kobospan" id="kobo.871.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;μ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/130.png" class="calibre141"/></span><span class="kobospan" id="kobo.872.1"> = 5 </span><span class="kobospan" id="kobo.873.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;σ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/131.png" class="calibre142"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.874.1">In all the graphs, we have the standard bell shape that we are all familiar with, but as we change our parameters, we can see that the bell might get skinnier, thicker, or move from left </span><span><span class="kobospan" id="kobo.875.1">to right.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.876.1">In the following </span><a id="_idIndexMarker321" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.877.1">chapters, which</span><a id="_idIndexMarker322" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.878.1"> focus on statistics, we will make much more use of the normal distribution as it applies to </span><span><span class="kobospan" id="kobo.879.1">statistical thinking.</span></span></p>
<h1 id="_idParaDest-91" class="calibre6"><a id="_idTextAnchor190" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.880.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.881.1">Pr</span><a id="_idTextAnchor191" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.882.1">obability as a field works to explain our random and chaotic world. </span><span class="kobospan" id="kobo.882.2">Using the basic laws of probability, we can model real-life events that involve randomness. </span><span class="kobospan" id="kobo.882.3">We can use random variables to represent values that may take on several values, and we can use the probability mass or density functions to compare product lines or look at the </span><span><span class="kobospan" id="kobo.883.1">test results.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.884.1">We have seen some of the more complicated uses of probability in prediction. </span><span class="kobospan" id="kobo.884.2">Using random variables and Bayes’ theorem are excellent ways to assign probabilities to </span><span><span class="kobospan" id="kobo.885.1">real-life situations.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.886.1">The next two chapters focus on statistical thinking. </span><span class="kobospan" id="kobo.886.2">Like probability, these chapters will use mathematical formulas to model real-world events. </span><span class="kobospan" id="kobo.886.3">The main difference, however, will be the terminology we use to describe the world and the way we model different types of events. </span><span class="kobospan" id="kobo.886.4">In these upcoming chapters, we will attempt to model entire populations of data points based solely on </span><span><span class="kobospan" id="kobo.887.1">a sample.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.888.1">We will revisit many concepts in probability to make sense of statistical theorems, as they are closely linked, and both are important mathematical concepts in the realm of </span><span><span class="kobospan" id="kobo.889.1">data science.</span></span></p>
</div>
</body></html>