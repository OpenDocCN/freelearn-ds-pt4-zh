<html><head></head><body>
<div id="_idContainer197" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor192" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-93" class="calibre6"><a id="_idTextAnchor193" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">What Are the Chances? </span><span class="kobospan" id="kobo.2.2">An Introduction to Statistics</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">This chapter will focus on the statistical knowledge required by any aspiring </span><span><span class="kobospan" id="kobo.4.1">data scientist.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.5.1">We will explore ways of sampling and obtaining data without being affected by bias and then use measures of statistics to quantify and visualize our data. </span><span class="kobospan" id="kobo.5.2">Using the z-score and the empirical rule, we will see how we can standardize data for the purposes of both graphing </span><span><span class="kobospan" id="kobo.6.1">and interpretability.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.7.1">In this chapter, we will look at the </span><span><span class="kobospan" id="kobo.8.1">following topics:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.9.1">How to obtain and </span><span><span class="kobospan" id="kobo.10.1">sample data</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.11.1">The measures of center, variance, and </span><span><span class="kobospan" id="kobo.12.1">relative standing</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.13.1">Normalization of data using </span><span><span class="kobospan" id="kobo.14.1">the z-score</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.15.1">The </span><span><span class="kobospan" id="kobo.16.1">empirical rule</span></span></li>
</ul>
<h1 id="_idParaDest-94" class="calibre6"><a id="_idTextAnchor194" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.17.1">What are statistics?</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.18.1">This might seem like an odd question</span><a id="_idIndexMarker323" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker324" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.19.1"> to ask, but I am frequently surprised by the number of people who cannot answer this simple and yet powerful question: what are statistics? </span><span class="kobospan" id="kobo.19.2">Statistics are the numbers you always see on the news and in the paper. </span><span class="kobospan" id="kobo.19.3">Statistics are useful when trying to prove a point or trying to scare someone, but what </span><span><span class="kobospan" id="kobo.20.1">are they?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.21.1">To answer this question, we need to back up for a minute and talk about why we even measure them in the first place. </span><span class="kobospan" id="kobo.21.2">The goal of this field is to try to explain and model the world around us. </span><span class="kobospan" id="kobo.21.3">To do that, we have to take a look at </span><span><span class="kobospan" id="kobo.22.1">the population.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.23.1">We can define a </span><strong class="bold"><span class="kobospan" id="kobo.24.1">population</span></strong><span class="kobospan" id="kobo.25.1"> as the entire pool of</span><a id="_idIndexMarker325" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker326" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.26.1"> subjects of an experiment or </span><span><span class="kobospan" id="kobo.27.1">a model.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.28.1">Essentially, your population is who you care about. </span><span class="kobospan" id="kobo.28.2">Who are you trying to talk about? </span><span class="kobospan" id="kobo.28.3">If you are trying to test whether smoking leads to heart disease, your population would be the smokers of the world. </span><span class="kobospan" id="kobo.28.4">If you are trying to study teenage drinking problems, your population would be </span><span><span class="kobospan" id="kobo.29.1">all teenagers.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.30.1">Now, imagine that you want to ask a question about your population. </span><span class="kobospan" id="kobo.30.2">For example, if your population is all of your employees (assume that you have over 1,000 employees), perhaps you want to know what percentage of them enjoy traveling. </span><span class="kobospan" id="kobo.30.3">The</span><a id="_idIndexMarker327" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker328" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.31.1"> question is called a </span><strong class="bold"><span class="kobospan" id="kobo.32.1">parameter</span></strong><span class="kobospan" id="kobo.33.1"> – a numerical measurement describing a characteristic of a population. </span><span class="kobospan" id="kobo.33.2">For example, if you ask all 1,000 employees and 100 of them enjoy traveling, the rate of travel enjoyment is 10%. </span><span class="kobospan" id="kobo.33.3">The parameter here </span><span><span class="kobospan" id="kobo.34.1">is 10%.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.35.1">You probably can’t ask every single employee whether they enjoy traveling. </span><span class="kobospan" id="kobo.35.2">What if you have over 10,000 employees? </span><span class="kobospan" id="kobo.35.3">It would be very difficult to track everyone down in order to get your answer. </span><span class="kobospan" id="kobo.35.4">When this happens, it’s impossible to figure out this parameter. </span><span class="kobospan" id="kobo.35.5">In this case, we can </span><em class="italic"><span class="kobospan" id="kobo.36.1">estimate</span></em> <span><span class="kobospan" id="kobo.37.1">the parameter.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.38.1">First, we will take a </span><em class="italic"><span class="kobospan" id="kobo.39.1">sample</span></em><span class="kobospan" id="kobo.40.1"> of the population. </span><span class="kobospan" id="kobo.40.2">We can define a sample of a population as a subset (not necessarily random) of the population. </span><span class="kobospan" id="kobo.40.3">Perhaps ask 200 of the 1,000 employees you have. </span><span class="kobospan" id="kobo.40.4">Of these 200, suppose 26 enjoy traveling, making the rate 13%. </span><span class="kobospan" id="kobo.40.5">Here, 13% is not a parameter because we didn’t get a chance to ask everyone. </span><span class="kobospan" id="kobo.40.6">This 13% is </span><a id="_idIndexMarker329" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker330" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.41.1">an estimate of a parameter. </span><span class="kobospan" id="kobo.41.2">Do you know what that’s called? </span><span class="kobospan" id="kobo.41.3">That’s right, </span><span><span class="kobospan" id="kobo.42.1">a </span></span><span><strong class="bold"><span class="kobospan" id="kobo.43.1">statistic</span></strong></span><span><span class="kobospan" id="kobo.44.1">!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.45.1">We can define a statistic as a numerical measurement describing a characteristic of a sample of a population. </span><span class="kobospan" id="kobo.45.2">A statistic is just an estimation of a parameter. </span><span class="kobospan" id="kobo.45.3">It is a number that attempts to describe an entire population by describing a subset of that population. </span><span class="kobospan" id="kobo.45.4">This is necessary because you can never hope to give a survey to every single teenager or to every single smoker in the world. </span><span class="kobospan" id="kobo.45.5">That’s what the field of statistics is all about: taking samples of populations and running tests on </span><span><span class="kobospan" id="kobo.46.1">these samples.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">So, the next time you are given a statistic, just remember that number only represents a sample of that population, not the entire pool </span><span><span class="kobospan" id="kobo.48.1">of subjects.</span></span></p>
<h1 id="_idParaDest-95" class="calibre6"><a id="_idTextAnchor195" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.49.1">How do we obtain and sample data?</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.50.1">If statistics is about taking </span><a id="_idIndexMarker331" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker332" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.51.1">samples of populations, it must be very important to know how we obtain these samples, and you’d be correct. </span><span class="kobospan" id="kobo.51.2">Let’s focus on just a few of the many ways of obtaining and </span><span><span class="kobospan" id="kobo.52.1">sampling data.</span></span></p>
<h2 id="_idParaDest-96" class="calibre7"><a id="_idTextAnchor196" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.53.1">Obtaining data</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.54.1">There are two main ways </span><a id="_idIndexMarker333" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker334" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.55.1">of collecting data for</span><a id="_idIndexMarker335" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker336" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.56.1"> our analysis: </span><strong class="bold"><span class="kobospan" id="kobo.57.1">observational</span></strong><span class="kobospan" id="kobo.58.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.59.1">experimentation</span></strong><span class="kobospan" id="kobo.60.1">. </span><span class="kobospan" id="kobo.60.2">Both these ways have their pros and cons, of course. </span><span class="kobospan" id="kobo.60.3">They each produce different types of behavior and, therefore, warrant different types </span><span><span class="kobospan" id="kobo.61.1">of analysis.</span></span></p>
<h2 id="_idParaDest-97" class="calibre7"><a id="_idTextAnchor197" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.62.1">Observational</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.63.1">We might obtain data through observational means, which consists of measuring specific characteristics but not attempting to modify the subjects being </span><a id="_idIndexMarker337" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker338" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.64.1">studied. </span><span class="kobospan" id="kobo.64.2">For example, if you had tracking software on your website that observes users’ behavior on the website, such as length of time spent on certain pages and the rate of clicking on ads, all the while not affecting</span><a id="_idIndexMarker339" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker340" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.65.1"> the user’s experience, then that would be an </span><span><span class="kobospan" id="kobo.66.1">observational study.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.67.1">This is one of the most common ways to get data because it’s just plain easy. </span><span class="kobospan" id="kobo.67.2">All you have to do is observe and collect data. </span><span class="kobospan" id="kobo.67.3">Observational studies are also limited in the types of data that can be collected. </span><span class="kobospan" id="kobo.67.4">This is because the observer (you) is not in control of the environment. </span><span class="kobospan" id="kobo.67.5">You may only watch and collect natural behavior. </span><span class="kobospan" id="kobo.67.6">If you are looking to induce a certain type of behavior, an observational study would not </span><span><span class="kobospan" id="kobo.68.1">be useful.</span></span></p>
<h2 id="_idParaDest-98" class="calibre7"><a id="_idTextAnchor198" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.69.1">Experimental</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.70.1">An </span><strong class="bold"><span class="kobospan" id="kobo.71.1">experiment</span></strong><span class="kobospan" id="kobo.72.1"> consists </span><a id="_idIndexMarker341" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker342" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.73.1">of a treatment</span><a id="_idIndexMarker343" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker344" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.74.1"> and the observation of its effect on the subjects. </span><span class="kobospan" id="kobo.74.2">Subjects in an experiment are called </span><strong class="bold"><span class="kobospan" id="kobo.75.1">experimental units</span></strong><span class="kobospan" id="kobo.76.1">. </span><span class="kobospan" id="kobo.76.2">This is usually how </span><a id="_idIndexMarker345" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker346" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.77.1">most scientific labs collect data. </span><span class="kobospan" id="kobo.77.2">They will put people into two or more groups (usually just two) and call them the control and the </span><span><span class="kobospan" id="kobo.78.1">experimental groups.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.79.1">The control group is exposed to a certain </span><a id="_idIndexMarker347" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker348" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.80.1">environment and then observed. </span><span class="kobospan" id="kobo.80.2">The experimental group is exposed to a different environment and then observed. </span><span class="kobospan" id="kobo.80.3">The experimenter then aggregates data from both groups and makes a decision about which environment was more favorable (favorable is a quality that the experimenter gets </span><span><span class="kobospan" id="kobo.81.1">to decide).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.82.1">In a marketing example, imagine that we expose half of our users to a certain landing page with certain images and a certain style (</span><em class="italic"><span class="kobospan" id="kobo.83.1">website A</span></em><span class="kobospan" id="kobo.84.1">), and we measure whether or not they sign up for the service. </span><span class="kobospan" id="kobo.84.2">Then, we expose the other half to a different landing page, different images, and different styles (</span><em class="italic"><span class="kobospan" id="kobo.85.1">website B</span></em><span class="kobospan" id="kobo.86.1">) and again measure whether or not they sign up. </span><span class="kobospan" id="kobo.86.2">We can then decide which of the two sites performed better and should be used going further. </span><span class="kobospan" id="kobo.86.3">This, specifically, is </span><a id="_idIndexMarker349" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker350" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.87.1">called an </span><em class="italic"><span class="kobospan" id="kobo.88.1">A/B test</span></em><span class="kobospan" id="kobo.89.1">. </span><span class="kobospan" id="kobo.89.2">Let’s see an example </span><span><span class="kobospan" id="kobo.90.1">in Python!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.91.1">Let’s suppose we run the preceding test and obtain the following results as a list </span><span><span class="kobospan" id="kobo.92.1">of lists:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.93.1">
results = [ ['A', 1], ['B', 1], ['A', 0], ['A', 0] ... </span><span class="kobospan1" id="kobo.93.2">]</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">Here, each object in the list result represents a subject (person). </span><span class="kobospan" id="kobo.94.2">Each person then has the following </span><span><span class="kobospan" id="kobo.95.1">two attributes:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.96.1">Which website they were exposed to, represented by a </span><span><span class="kobospan" id="kobo.97.1">single character</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.98.1">Whether or not they converted (</span><strong class="source-inline1"><span class="kobospan" id="kobo.99.1">0</span></strong><span class="kobospan" id="kobo.100.1"> for no and </span><strong class="source-inline1"><span class="kobospan" id="kobo.101.1">1</span></strong> <span><span class="kobospan" id="kobo.102.1">for yes)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.103.1">We can then aggregate and come up with the following </span><span><span class="kobospan" id="kobo.104.1">results table:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.105.1">
users_exposed_to_A = []
users_exposed_to_B = []
# create two lists to hold the results of each individual website</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.106.1">Once we create these two lists that will eventually hold each individual conversion value as Booleans (</span><strong class="source-inline"><span class="kobospan" id="kobo.107.1">0</span></strong><span class="kobospan" id="kobo.108.1"> or </span><strong class="source-inline"><span class="kobospan" id="kobo.109.1">1</span></strong><span class="kobospan" id="kobo.110.1">), we will iterate all of our results of the test and add them to the appropriate list, </span><span><span class="kobospan" id="kobo.111.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.112.1">
for website, converted in results: # iterate through the results
will look something like website == 'A' and converted == 0 if website == 'A':
users_exposed_to_A.append(converted) elif website == 'B':
users_exposed_to_B.append(converted)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.113.1">Now, each list </span><a id="_idIndexMarker351" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker352" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.114.1">contains a series of </span><strong class="source-inline"><span class="kobospan" id="kobo.115.1">1</span></strong><span class="kobospan" id="kobo.116.1"> and </span><span><strong class="source-inline"><span class="kobospan" id="kobo.117.1">0</span></strong></span><span><span class="kobospan" id="kobo.118.1"> values.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.119.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.120.1">Remember that </span><strong class="source-inline1"><span class="kobospan" id="kobo.121.1">1</span></strong><span class="kobospan" id="kobo.122.1"> represents a user actually converting to the site after seeing that web page, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.123.1">0</span></strong><span class="kobospan" id="kobo.124.1"> represents a user seeing the page and leaving before </span><span><span class="kobospan" id="kobo.125.1">signing up/converting.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.126.1">To get the total number of people exposed to website </span><strong class="source-inline"><span class="kobospan" id="kobo.127.1">A</span></strong><span class="kobospan" id="kobo.128.1">, we can use the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.129.1">len()</span></strong></span><span><span class="kobospan" id="kobo.130.1"> feature.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.131.1">Let’s use Python to illustrate the elements of our </span><span><span class="kobospan" id="kobo.132.1">two lists:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.133.1">
len(users_exposed_to_A) == 188 #number of people exposed to website A
len(users_exposed_to_B) == 158 #number of people exposed to website B</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.134.1">To count the number of people who converted, we can use the sum of the list, </span><span><span class="kobospan" id="kobo.135.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.136.1">
sum(users_exposed_to_A) == 54 # people converted from website A
sum(users_exposed_to_B) == 48 # people converted from website B</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.137.1">If we subtract the length of the lists and the sum of the list, we are left with the number of people who did </span><em class="italic"><span class="kobospan" id="kobo.138.1">not</span></em><span class="kobospan" id="kobo.139.1"> convert for each site, </span><span><span class="kobospan" id="kobo.140.1">as illustrated:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.141.1">
len(users_exposed_to_A) - sum(users_exposed_to_A) == 134 # did not convert from website A
len(users_exposed_to_B) - sum(users_exposed_to_B) == 110 # did not convert from website B</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.142.1">We can aggregate and summarize our results in the following table, which represents our experiment on website </span><span><span class="kobospan" id="kobo.143.1">conversion testing:</span></span></p>
<table class="no-table-style" id="table001-4">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.144.1">Did not </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.145.1">sign up</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.146.1">Signed up</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.147.1">Website A</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.148.1">134</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.149.1">54</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.150.1">Website B</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.151.1">110</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.152.1">48</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.153.1">Table 7.1 – The results of our A/B test</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.154.1">We can quickly drum up some descriptive statistics. </span><span class="kobospan" id="kobo.154.2">We can say that the website conversion rates for the two websites are </span><span><span class="kobospan" id="kobo.155.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.156.1">Conversion for website A</span></em><span class="kobospan" id="kobo.157.1">: </span><em class="italic"><span class="kobospan" id="kobo.158.1">154 /(154+34) = .</span></em><span><em class="italic"><span class="kobospan" id="kobo.159.1">288</span></em></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.160.1">Conversion for website B</span></em><span class="kobospan" id="kobo.161.1">: </span><span><em class="italic"><span class="kobospan" id="kobo.162.1">48/(110+48)= .3</span></em></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.163.1">Not much difference, but different nonetheless. </span><span class="kobospan" id="kobo.163.2">Even </span><a id="_idIndexMarker353" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker354" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.164.1">though B has a higher conversion rate, can we really say that version B significantly converts better? </span><span class="kobospan" id="kobo.164.2">Not yet. </span><span class="kobospan" id="kobo.164.3">To test the </span><em class="italic"><span class="kobospan" id="kobo.165.1">statistical significance</span></em><span class="kobospan" id="kobo.166.1"> of such a result, a hypothesis test should be used. </span><span class="kobospan" id="kobo.166.2">These tests will be covered in depth in the next chapter, where we will revisit this exact same example and finish it using a proper </span><span><span class="kobospan" id="kobo.167.1">statistical test.</span></span></p>
<h2 id="_idParaDest-99" class="calibre7"><a id="_idTextAnchor199" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.168.1">Sampling data</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.169.1">Remember how statistics are the result of</span><a id="_idIndexMarker355" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker356" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.170.1"> measuring a sample of a population. </span><span class="kobospan" id="kobo.170.2">Well, we should talk about two very common ways to decide who gets the honor of being in the sample that we measure. </span><span class="kobospan" id="kobo.170.3">We will discuss the main type of sampling, called random sampling, which is the most common way to decide our sample sizes and our </span><span><span class="kobospan" id="kobo.171.1">sample members.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor200" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.172.1">Probability sampling</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.173.1">Probability sampling is a way of sampling </span><a id="_idIndexMarker357" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker358" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.174.1">from a population, in which every person has a known probability of being chosen, but that probability </span><em class="italic"><span class="kobospan" id="kobo.175.1">might</span></em><span class="kobospan" id="kobo.176.1"> be a different value than another user has. </span><span class="kobospan" id="kobo.176.2">The simplest (and probably the most common) probability </span><a id="_idIndexMarker359" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker360" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.177.1">sampling method is </span><span><strong class="bold"><span class="kobospan" id="kobo.178.1">random sampling</span></strong></span><span><span class="kobospan" id="kobo.179.1">.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor201" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.180.1">Random sampling</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.181.1">Suppose that we are</span><a id="_idIndexMarker361" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker362" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.182.1"> running an A/B test and we need to</span><a id="_idIndexMarker363" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker364" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.183.1"> figure out who will be in Group A and who will be in Group B. </span><span class="kobospan" id="kobo.183.2">The following are three suggestions from our </span><span><span class="kobospan" id="kobo.184.1">data team:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.185.1">Separate users based on location</span></em><span class="kobospan" id="kobo.186.1">: Users on the West Coast are placed in Group A, while users on the East Coast are placed in </span><span><span class="kobospan" id="kobo.187.1">Group B</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.188.1">Separate users based on the time of day they visit the site</span></em><span class="kobospan" id="kobo.189.1">: Users who visit between 7 p.m. </span><span class="kobospan" id="kobo.189.2">and 4 a.m. </span><span class="kobospan" id="kobo.189.3">are group A, while the rest are placed in </span><span><span class="kobospan" id="kobo.190.1">group B</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.191.1">Make it completely random</span></em><span class="kobospan" id="kobo.192.1">: Every new user has a 50/50 chance of being placed in </span><span><span class="kobospan" id="kobo.193.1">either group</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.194.1">The first two are valid options for choosing samples and are fairly simple to implement, but they both have one fundamental flaw: they are both at risk of introducing a </span><span><span class="kobospan" id="kobo.195.1">sampling bias.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.196.1">A sampling bias occurs when the way the sample is obtained systematically favors some other outcome over the target outcome. </span><span class="kobospan" id="kobo.196.2">This can occur in various ways, such as using an unrepresentative sample, selecting participants based on certain criteria, or using biased sampling methods. </span><span class="kobospan" id="kobo.196.3">When a sample is biased, it can lead to incorrect or misleading conclusions about the population being studied, and therefore it is important to ensure that sampling methods are appropriate and unbiased in order to obtain accurate and </span><span><span class="kobospan" id="kobo.197.1">reliable </span></span><span><a id="_idIndexMarker365" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker366" class="pcalibre calibre4 pcalibre1"/></span><span><span class="kobospan" id="kobo.198.1">results.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.199.1">It is not difficult to see why choosing the first or second option from the preceding list might introduce bias. </span><span class="kobospan" id="kobo.199.2">If we choose our groups based on where they live or what time they log in, we are priming our experiment incorrectly and, now, we have much less control over </span><span><span class="kobospan" id="kobo.200.1">the results.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.201.1">Specifically, we are at risk </span><a id="_idIndexMarker367" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker368" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.202.1">of introducing a </span><strong class="bold"><span class="kobospan" id="kobo.203.1">confounding factor</span></strong><span class="kobospan" id="kobo.204.1"> into our analysis, which is </span><span><span class="kobospan" id="kobo.205.1">bad news.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.206.1">A </span><strong class="bold"><span class="kobospan" id="kobo.207.1">confounding factor</span></strong><span class="kobospan" id="kobo.208.1"> is a variable that we are not directly measuring but that connects the variables that are being measured. </span><span class="kobospan" id="kobo.208.2">Basically, a confounding factor is like the missing element in our analysis that is invisible but affects </span><span><span class="kobospan" id="kobo.209.1">our results.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.210.1">In this case, the first option is not taking into account the potential confounding factor of </span><em class="italic"><span class="kobospan" id="kobo.211.1">geographical taste</span></em><span class="kobospan" id="kobo.212.1">. </span><span class="kobospan" id="kobo.212.2">For example, if website A is unappealing, in general, to West Coast users, it will affect your </span><span><span class="kobospan" id="kobo.213.1">results drastically.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.214.1">Similarly, the second option might introduce a temporal (time-based) confounding factor. </span><span class="kobospan" id="kobo.214.2">What if website B is better viewed in a night-time environment (which was reserved for A), and users react negatively to the style purely because the time of day when they view it? </span><span class="kobospan" id="kobo.214.3">These are both factors that we want to avoid, so we should go with the third option, which is a </span><span><span class="kobospan" id="kobo.215.1">random sample.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.216.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.217.1">While sampling bias can cause confounding, it is a different concept than confounding. </span><span class="kobospan" id="kobo.217.2">The first and second options were both sampling biases because we chose the samples incorrectly, and were also examples of confounding factors because there was a third variable in each case that affected </span><span><span class="kobospan" id="kobo.218.1">our decision.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.219.1">A random sample is chosen such that every single member of a population has an equal chance of being chosen as any </span><span><span class="kobospan" id="kobo.220.1">other member.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.221.1">This is probably one of the easiest and most convenient ways to decide who will be a part of your sample. </span><span class="kobospan" id="kobo.221.2">Everyone has the exact same chance of being in any particular group. </span><span class="kobospan" id="kobo.221.3">Random sampling is an effective way of reducing the impact of </span><span><span class="kobospan" id="kobo.222.1">confounding factors.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor202" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.223.1">Unequal probability sampling</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.224.1">Recall that I previously said that a probability sampling might have different probabilities for different potential sample members. </span><span class="kobospan" id="kobo.224.2">But what if this actually introduced problems? </span><span class="kobospan" id="kobo.224.3">Suppose we are interested in measuring the happiness level of our employees. </span><span class="kobospan" id="kobo.224.4">We already know that we can’t ask every single member of staff because that would be silly</span><a id="_idIndexMarker369" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker370" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.225.1"> and exhausting. </span><span class="kobospan" id="kobo.225.2">So, we need to take a sample. </span><span class="kobospan" id="kobo.225.3">Our data team suggests random sampling, and at first, everyone high-fives because they feel very smart and statistical. </span><span class="kobospan" id="kobo.225.4">But then someone asks a seemingly harmless </span><a id="_idIndexMarker371" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker372" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker373" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.226.1">question: does anyone know the percentage of men/women who </span><span><span class="kobospan" id="kobo.227.1">work here?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.228.1">The high-fives stop and the room </span><span><span class="kobospan" id="kobo.229.1">goes silent.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.230.1">This question is extremely important because sex is likely to be a confounding factor. </span><span class="kobospan" id="kobo.230.2">The team looks into it and discovers a split of 75% men and 25% women in </span><span><span class="kobospan" id="kobo.231.1">the company.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.232.1">This means that if we introduce a random sample, our sample will likely have a similar split and thus favor the results for men and not women. </span><span class="kobospan" id="kobo.232.2">To combat this, we can opt to include more women than men in our survey in order to make the split of our sample less biased </span><span><span class="kobospan" id="kobo.233.1">toward men.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.234.1">At first glance, introducing a favoring system in our random sampling seems like a bad idea; however, alleviating unequal sampling and, therefore, working to remove systematic bias among gender, race, disability, and so on is much more pertinent. </span><span class="kobospan" id="kobo.234.2">A simple random sample, where everyone has the same chance as everyone else, is very likely to drown out the voices and opinions of minority population members. </span><span class="kobospan" id="kobo.234.3">Therefore, it can be okay to introduce such a favoring system in your </span><span><span class="kobospan" id="kobo.235.1">sampling techniques.</span></span></p>
<h1 id="_idParaDest-100" class="calibre6"><a id="_idTextAnchor203" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.236.1">How do we measure statistics?</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.237.1">Once we have our sample, it’s time to </span><a id="_idIndexMarker374" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker375" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.238.1">quantify our results. </span><span class="kobospan" id="kobo.238.2">Suppose we wish to generalize the happiness of our employees or we want to figure out whether salaries in the company are very different from person </span><span><span class="kobospan" id="kobo.239.1">to person.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.240.1">These are some common ways of measuring </span><span><span class="kobospan" id="kobo.241.1">our results.</span></span></p>
<h2 id="_idParaDest-101" class="calibre7"><a id="_idTextAnchor204" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.242.1">Measures of center</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.243.1">Measures of the center are how we define the </span><a id="_idIndexMarker376" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker377" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.244.1">middle, or center, of a dataset. </span><span class="kobospan" id="kobo.244.2">We do this because sometimes we wish to make generalizations about data values. </span><span class="kobospan" id="kobo.244.3">For example, perhaps we’re curious about what the average rainfall in Seattle is or what the median height of European males is. </span><span class="kobospan" id="kobo.244.4">It’s a way to generalize a large set of data so that it’s easier to convey </span><span><span class="kobospan" id="kobo.245.1">to someone.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.246.1">A measure of center is a value in the </span><em class="italic"><span class="kobospan" id="kobo.247.1">middle</span></em><span class="kobospan" id="kobo.248.1"> of a dataset. </span><span class="kobospan" id="kobo.248.2">This can mean different things to different people. </span><span class="kobospan" id="kobo.248.3">Who’s to say where the </span><a id="_idIndexMarker378" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker379" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.249.1">middle of a dataset is? </span><span class="kobospan" id="kobo.249.2">There are so many different ways of defining the center of data. </span><span class="kobospan" id="kobo.249.3">Let’s take a look at </span><span><span class="kobospan" id="kobo.250.1">a few.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.251.1">The </span><strong class="bold"><span class="kobospan" id="kobo.252.1">arithmetic mean</span></strong><span class="kobospan" id="kobo.253.1"> of a dataset is found by adding up all of the values and then dividing it by the number of data values. </span><span class="kobospan" id="kobo.253.2">This</span><a id="_idIndexMarker380" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker381" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.254.1"> is likely the most common way</span><a id="_idIndexMarker382" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker383" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.255.1"> to define the center of data, but can be flawed! </span><span class="kobospan" id="kobo.255.2">Suppose we wish to find the mean </span><a id="_idIndexMarker384" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker385" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.256.1">of the </span><span><span class="kobospan" id="kobo.257.1">following numbers:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.258.1">
import numpy as np
np.mean([11, 15, 17, 14]) == 14.25</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.259.1">Simple enough; our average is </span><strong class="source-inline"><span class="kobospan" id="kobo.260.1">14.25</span></strong><span class="kobospan" id="kobo.261.1"> and all of our values are fairly close to it. </span><span class="kobospan" id="kobo.261.2">But what if we introduce a new </span><span><span class="kobospan" id="kobo.262.1">value: </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.263.1">31</span></strong></span><span><span class="kobospan" id="kobo.264.1">?</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.265.1">
np.mean([11, 15, 17, 14, 31]) == 17.6</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.266.1">This greatly affects the mean because the arithmetic mean is sensitive to outliers. </span><span class="kobospan" id="kobo.266.2">The new value, </span><strong class="source-inline"><span class="kobospan" id="kobo.267.1">31</span></strong><span class="kobospan" id="kobo.268.1">, is almost twice as large as the rest of the numbers, and therefore skews </span><span><span class="kobospan" id="kobo.269.1">the mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.270.1">Another, and sometimes better, measure of the center is the median. </span><span class="kobospan" id="kobo.270.2">The </span><strong class="bold"><span class="kobospan" id="kobo.271.1">median</span></strong><span class="kobospan" id="kobo.272.1"> is the</span><a id="_idIndexMarker386" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker387" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.273.1"> number found in the middle of the dataset when it is sorted in order, </span><span><span class="kobospan" id="kobo.274.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.275.1">
np.median([11, 15, 17, 14]) == 14.5
np.median([11, 15, 17, 14, 31]) == 15</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.276.1">Note how the introduction of </span><strong class="source-inline"><span class="kobospan" id="kobo.277.1">31</span></strong><span class="kobospan" id="kobo.278.1"> using the median did not affect the median of the dataset greatly. </span><span class="kobospan" id="kobo.278.2">This is because the median is less sensitive </span><span><span class="kobospan" id="kobo.279.1">to outliers.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.280.1">When working with datasets with many outliers, it is sometimes more useful to use the median of the dataset, while if your data does not have many outliers and the datapoints are mostly close to one another, then the mean is likely a </span><span><span class="kobospan" id="kobo.281.1">better option.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.282.1">But how can we tell if the data is spread out? </span><span class="kobospan" id="kobo.282.2">Well, we will have to introduce a new type </span><span><span class="kobospan" id="kobo.283.1">of statistic.</span></span></p>
<h2 id="_idParaDest-102" class="calibre7"><a id="_idTextAnchor205" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.284.1">Measures of variation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.285.1">Measures of the center are used to quantify </span><a id="_idIndexMarker388" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker389" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.286.1">the middle of the data, but now we will explore ways of measuring how to </span><em class="italic"><span class="kobospan" id="kobo.287.1">spread out</span></em><span class="kobospan" id="kobo.288.1"> the</span><a id="_idIndexMarker390" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker391" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.289.1"> data we collect is. </span><span class="kobospan" id="kobo.289.2">This is a useful way to identify whether our data has many outliers lurking inside. </span><span class="kobospan" id="kobo.289.3">Let’s start with </span><span><span class="kobospan" id="kobo.290.1">an example.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.291.1">Imagine that we take a random sample of 24 of our friends on Facebook and record how many friends each of them has on Facebook. </span><span class="kobospan" id="kobo.291.2">Here’s </span><span><span class="kobospan" id="kobo.292.1">the list:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.293.1">
friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125, 455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]
np.mean(friends) == 789.1</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.294.1">The average of the values in this list is just over </span><strong class="source-inline"><span class="kobospan" id="kobo.295.1">789</span></strong><span class="kobospan" id="kobo.296.1">. </span><span class="kobospan" id="kobo.296.2">So, we could say that according to this sample, the average Facebook friend has 789 friends. </span><span class="kobospan" id="kobo.296.3">But what about the person who only has 89 friends or the person who has over 1,600 </span><a id="_idIndexMarker392" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker393" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.297.1">friends? </span><span class="kobospan" id="kobo.297.2">In fact, not many of these numbers are really that close </span><span><span class="kobospan" id="kobo.298.1">to 789.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.299.1">Well, how about we use the median? </span><span class="kobospan" id="kobo.299.2">The median generally is not as affected </span><span><span class="kobospan" id="kobo.300.1">by outliers:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.301.1">
np.median(friends) == 769.5</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.302.1">The median is </span><strong class="source-inline"><span class="kobospan" id="kobo.303.1">769.5</span></strong><span class="kobospan" id="kobo.304.1">, which is fairly close to the mean. </span><span class="kobospan" id="kobo.304.2">Hmm, good try, but still, it doesn’t really account for how drastically different a lot of these datapoints are. </span><span class="kobospan" id="kobo.304.3">This is what statisticians call measuring the variation of data. </span><span class="kobospan" id="kobo.304.4">Let’s start by introducing the most basic measure of variation: the range. </span><span class="kobospan" id="kobo.304.5">The range is simply the maximum value minus the minimum value, </span><span><span class="kobospan" id="kobo.305.1">as illustrated:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.306.1">
np.max(friends) - np.min(friends) == 1684</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.307.1">The range tells us how far away the two most extreme values are. </span><span class="kobospan" id="kobo.307.2">Now, the range isn’t typically widely used but it does have some pertinent applications. </span><span class="kobospan" id="kobo.307.3">Sometimes, we wish to know just how spread apart the outliers are. </span><span class="kobospan" id="kobo.307.4">This is most useful in scientific and </span><span><span class="kobospan" id="kobo.308.1">safety measurements.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.309.1">Suppose a car company wants to measure</span><a id="_idIndexMarker394" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker395" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.310.1"> how long it takes for an airbag to deploy. </span><span class="kobospan" id="kobo.310.2">Knowing the average of that time is nice, but they also really want to know how spread apart the slowest time is versus the fastest time. </span><span class="kobospan" id="kobo.310.3">This literally could be the difference between life </span><span><span class="kobospan" id="kobo.311.1">and death.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.312.1">Shifting back to the Facebook example, </span><strong class="source-inline"><span class="kobospan" id="kobo.313.1">1684</span></strong><span class="kobospan" id="kobo.314.1"> is our range, but I’m not quite sure it’s saying very much about our data. </span><span class="kobospan" id="kobo.314.2">Now, let’s </span><a id="_idIndexMarker396" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker397" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.315.1">take a look at the most commonly used measure of variation, the </span><span><strong class="bold"><span class="kobospan" id="kobo.316.1">standard deviation</span></strong></span><span><span class="kobospan" id="kobo.317.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.318.1">I’m sure many of you have heard this term thrown around a lot and it might even incite a degree of fear, but what does it really mean? </span><span class="kobospan" id="kobo.318.2">In essence, standard deviation, denoted by </span><em class="italic"><span class="kobospan" id="kobo.319.1">s</span></em><span class="kobospan" id="kobo.320.1"> when we are working with a sample of a population, measures by how much data values deviate from the </span><span><span class="kobospan" id="kobo.321.1">arithmetic mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.322.1">It’s basically a way to see how spread out the data is. </span><span class="kobospan" id="kobo.322.2">There is a general formula to calculate the standard deviation, which is </span><span><span class="kobospan" id="kobo.323.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer178">
<span class="kobospan" id="kobo.324.1"><img alt="Figure 7.1 – Formula for standard deviation" src="image/B19488_07_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.325.1">Figure 7.1 – Formula for standard deviation</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.326.1">The formula for standard deviation might seem scary at first, but with time and practice, it will become a familiar friend, helping illustrate </span><a id="_idIndexMarker398" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker399" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.327.1">how spread out data </span><span><span class="kobospan" id="kobo.328.1">really is.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.329.1">Let’s look at each of the elements in this formula </span><span><span class="kobospan" id="kobo.330.1">in turn:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.331.1">s</span></em><span class="kobospan" id="kobo.332.1"> is our sample’s </span><span><span class="kobospan" id="kobo.333.1">standard deviation</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.334.1">x</span></em><span class="kobospan" id="kobo.335.1"> is each individual </span><span><span class="kobospan" id="kobo.336.1">data point</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.337.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/132.png" class="calibre143"/></span><span> </span><span class="kobospan" id="kobo.338.1">is the mean of </span><span><span class="kobospan" id="kobo.339.1">the data</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.340.1">n</span></em><span class="kobospan" id="kobo.341.1"> is the number </span><span><span class="kobospan" id="kobo.342.1">of datapoints</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.343.1">Before you freak out and close this book at the seemingly complicated equation, let’s break it down. </span><span class="kobospan" id="kobo.343.2">For each value in the sample, we will take that value, subtract the arithmetic mean from it, square the difference, and, once we’ve added up every </span><a id="_idIndexMarker400" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker401" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.344.1">single point this way, we will divide the entire thing by </span><em class="italic"><span class="kobospan" id="kobo.345.1">n</span></em><span class="kobospan" id="kobo.346.1">, the number of points in the sample. </span><span class="kobospan" id="kobo.346.2">Finally, we take a square root </span><span><span class="kobospan" id="kobo.347.1">of everything.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.348.1">Without going into an in-depth analysis of the formula, think about it this way: it’s basically derived from the distance formula. </span><span class="kobospan" id="kobo.348.2">Essentially, what the standard deviation is calculating is a sort of average distance of the data values from the </span><span><span class="kobospan" id="kobo.349.1">arithmetic mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.350.1">If you take a closer look at the formula, you will see that it actually </span><span><span class="kobospan" id="kobo.351.1">makes sense:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.352.1">By taking </span><em class="italic"><span class="kobospan" id="kobo.353.1">x</span></em><span class="kobospan" id="kobo.354.1">-</span><span class="kobospan" id="kobo.355.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/133.png" class="calibre144"/></span><span class="kobospan" id="kobo.356.1">, you are finding the literal difference between the value and the mean of </span><span><span class="kobospan" id="kobo.357.1">the sample.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.358.1">By squaring the result, (</span><em class="italic"><span class="kobospan" id="kobo.359.1">x</span></em><span class="kobospan" id="kobo.360.1">-</span><span class="kobospan" id="kobo.361.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/132.png" class="calibre145"/></span><span><span class="kobospan" id="kobo.362.1">)</span></span><span class="superscript"><span class="kobospan1" id="kobo.363.1">2</span></span><span class="kobospan" id="kobo.364.1">, we are putting a greater penalty on outliers because squaring a large error only makes it </span><span><span class="kobospan" id="kobo.365.1">much larger.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.366.1">By dividing by the number of items in the sample, we are taking (literally) the average squared distance between each point and </span><span><span class="kobospan" id="kobo.367.1">the mean.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.368.1">By taking the square root of the answer, we are putting the number in terms that we can understand. </span><span class="kobospan" id="kobo.368.2">For example, by squaring the number of friends minus the mean, we changed our units to friends squared, which makes no sense. </span><span class="kobospan" id="kobo.368.3">Taking the square root puts our units back to </span><span><span class="kobospan" id="kobo.369.1">just “friends.”</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.370.1">Let’s go back to our Facebook example for a visualization and further explanation </span><span><span class="kobospan" id="kobo.371.1">of this.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.372.1">Let’s begin by calculating the standard deviation – a few of them, in </span><span><span class="kobospan" id="kobo.373.1">this case.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.374.1">Recall that the arithmetic mean of the data was around </span><strong class="source-inline"><span class="kobospan" id="kobo.375.1">789</span></strong><span class="kobospan" id="kobo.376.1">, so we’ll use </span><strong class="source-inline"><span class="kobospan" id="kobo.377.1">789</span></strong><span class="kobospan" id="kobo.378.1"> as </span><span><span class="kobospan" id="kobo.379.1">the mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.380.1">We start by taking the difference between each data value and the mean, squaring it, adding them all up, dividing it by one less than the number of values, and then taking its square root. </span><span class="kobospan" id="kobo.380.2">This would look </span><span><span class="kobospan" id="kobo.381.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer182">
<span class="kobospan" id="kobo.382.1"><img alt="Figure 7.2 – Representation of an example of calculating the standard deviation of a series of data" src="image/B19488_07_02.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.383.1">Figure 7.2 – Representation of an example of calculating the standard deviation of a series of data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.384.1">An example of calculating the standard deviation of a series of data has us subtracting each item in the list by the average of the list, squaring that </span><a id="_idIndexMarker402" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker403" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.385.1">sum, adding it all up, dividing by the number of items, and finally, taking the square root of that </span><span><span class="kobospan" id="kobo.386.1">whole thing.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.387.1">On the other hand, we can take the</span><a id="_idIndexMarker404" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker405" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.388.1"> Python approach and do all this programmatically (which is </span><span><span class="kobospan" id="kobo.389.1">usually preferred):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.390.1">
np.std(friends) # == 425.2</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.391.1">What the number </span><strong class="source-inline"><span class="kobospan" id="kobo.392.1">425</span></strong><span class="kobospan" id="kobo.393.1"> represents is the spread of data. </span><span class="kobospan" id="kobo.393.2">You could say that 425 is a kind of average distance the data values are from the mean. </span><span class="kobospan" id="kobo.393.3">What this means, in simple words, is that this data is pretty </span><span><span class="kobospan" id="kobo.394.1">spread out.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.395.1">So, our standard deviation is about </span><strong class="source-inline"><span class="kobospan" id="kobo.396.1">425</span></strong><span class="kobospan" id="kobo.397.1">. </span><span class="kobospan" id="kobo.397.2">This means that the number of friends that these people have on Facebook doesn’t seem to be close to a single number and that’s quite evident when we plot the data in a bar graph, and also plot the mean as well as the visualizations of the standard deviation. </span><span class="kobospan" id="kobo.397.3">In the following plot, every person will be represented by a single bar in the bar chart, and the height of the bars represent the number of friends that the </span><span><span class="kobospan" id="kobo.398.1">individuals have:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.399.1">
import matplotlib.pyplot as plt
friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125, 455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]
y_pos = range(len(friends))
plt.bar(y_pos, friends)
plt.plot((0, 25), (789, 789), 'b-')
plt.plot((0, 25), (789+425, 789+425), 'g-')
plt.plot((0, 25), (789-425, 789-425), 'r-')</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.400.1">Here’s the chart that </span><span><span class="kobospan" id="kobo.401.1">we get:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer183">
<span class="kobospan" id="kobo.402.1"><img alt="Figure 7.3 – Plotting each of our datapoints as a bar in a bar graph with a line showing the average value (blue, in the middle)﻿, the average minus 1 standard deviation (red, below)﻿, and the average plus 1 standard deviation (green, the line above)" src="image/B19488_07_03.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.403.1">Figure 7.3 – Plotting each of our datapoints as a bar in a bar graph with a line showing the average value (blue, in the middle), the average minus 1 standard deviation (red, below), and the average plus 1 standard deviation (green, the line above)</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.404.1">The blue line in the center is drawn at the mean (789), the red line near the bottom is drawn at the mean minus the standard deviation (789 - 425 = 364), and finally, the green line toward the top is drawn at the mean plus the standard deviation (789 + 425 = </span><span><span class="kobospan" id="kobo.405.1">1,214).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.406.1">Note how most of the data lives between the green and the red lines while the outliers live outside the lines. </span><span class="kobospan" id="kobo.406.2">There are three people who have </span><a id="_idIndexMarker406" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker407" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.407.1">friend counts below the red line and three people who have a friend count above </span><a id="_idIndexMarker408" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker409" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.408.1">the green line. </span><span class="kobospan" id="kobo.408.2">This is a common way to discuss outliers in data – through standard deviations. </span><span class="kobospan" id="kobo.408.3">It’s often used as a unit distance. </span><span class="kobospan" id="kobo.408.4">You can say, for example, that in the realm of this “friends” data, the value of 1,214 is “one standard deviation above the mean” or that 1,639 is “two standard deviations above </span><span><span class="kobospan" id="kobo.409.1">the mean."</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.410.1">It’s important to mention that the units for standard deviation are, in fact, the same units as the data’s units. </span><span class="kobospan" id="kobo.410.2">So, in this example, we would say that the standard deviation is 425 friends </span><span><span class="kobospan" id="kobo.411.1">on Facebook.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.412.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.413.1">Another measure of variation is the variance, as described in the previous chapter. </span><span class="kobospan" id="kobo.413.2">The variance is simply the standard </span><span><span class="kobospan" id="kobo.414.1">deviation squared.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.415.1">So, now we know that the standard deviation and variance are good for checking how spread out our data is, and that we can use it along with the mean to create a kind of range that a lot of our data lies in. </span><span class="kobospan" id="kobo.415.2">But what if we want to compare</span><a id="_idIndexMarker410" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker411" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.416.1"> the spread of two different datasets, maybe even with completely different units? </span><span class="kobospan" id="kobo.416.2">That’s where the coefficient of variation comes </span><span><span class="kobospan" id="kobo.417.1">into play.</span></span></p>
<h2 id="_idParaDest-103" class="calibre7"><span class="kobospan" id="kobo.418.1">The co</span><a id="_idTextAnchor206" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.419.1">efficient of variation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.420.1">The </span><strong class="bold"><span class="kobospan" id="kobo.421.1">coefficient of variation</span></strong><span class="kobospan" id="kobo.422.1"> is defined as</span><a id="_idIndexMarker412" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker413" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.423.1"> the ratio of the data’s standard deviation to </span><span><span class="kobospan" id="kobo.424.1">its mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.425.1">This ratio (which, by the way, is only helpful if we’re working in the ratio level of measurement, where the division is allowed and is meaningful) is a way to standardize the standard deviation, which makes it easier to compare across datasets. </span><span class="kobospan" id="kobo.425.2">We use this measure frequently when attempting to compare means, and it spreads across populations that exist at </span><span><span class="kobospan" id="kobo.426.1">different scales.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.427.1">Exampl</span><a id="_idTextAnchor207" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.428.1">e – employee salaries</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.429.1">If we look at the mean and standard deviation of </span><a id="_idIndexMarker414" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker415" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.430.1">employees’ salaries in the same company but among different departments, we see that, at first glance, it may be tough to compare the standard deviations because they are on such </span><span><span class="kobospan" id="kobo.431.1">different scales:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer184">
<span class="kobospan" id="kobo.432.1"><img alt="Figure 7.4 – Glancing at the means and standard deviations of salaries across departments" src="image/B19488_07_04.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.433.1">Figure 7.4 – Glancing at the means and standard deviations of salaries across departments</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.434.1">Glancing at the means and standard deviations of salaries across departments can be challenging without scaling them to the same scale as one another using the coefficient of variation (the final column). </span><span class="kobospan" id="kobo.434.2">Only then can we see that the spread of salaries at the executive level is somewhat larger than the spread in the </span><span><span class="kobospan" id="kobo.435.1">other departments</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.436.1">This is especially true when the mean salary of one department is $25,000, while another department has a mean salary in the </span><span><span class="kobospan" id="kobo.437.1">six-figure area.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.438.1">However, if we look at the last column, which is our coefficient of variation, it becomes clearer that the people in the executive department may be getting paid more but they are also getting wildly different salaries. </span><span class="kobospan" id="kobo.438.2">This is probably because the CEO is earning way more than an office manager, who is still in the executive department, which makes the data very </span><span><span class="kobospan" id="kobo.439.1">spread out.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.440.1">On the other hand, everyone in the mailroom, while not making as much money, is making just about the same as everyone else in the mailroom, which is why their coefficient of variation is </span><span><span class="kobospan" id="kobo.441.1">only 8%.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.442.1">With measures of variation, we can begin to answer big questions, such as how to spread out this data or how we can come up with a good range that most of the data </span><span><span class="kobospan" id="kobo.443.1">falls into.</span></span></p>
<h2 id="_idParaDest-104" class="calibre7"><span class="kobospan" id="kobo.444.1">Measure</span><a id="_idTextAnchor208" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.445.1">s of relative standing</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.446.1">We can combine both the measures of centers and</span><a id="_idIndexMarker416" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker417" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.447.1"> variations to create measures of relative standing. </span><strong class="bold"><span class="kobospan" id="kobo.448.1">Measures of variation</span></strong><span class="kobospan" id="kobo.449.1"> measure where </span><a id="_idIndexMarker418" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker419" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.450.1">particular data values are positioned, relative to the </span><span><span class="kobospan" id="kobo.451.1">entire dataset.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.452.1">Let’s begin by learning a very important value in statistics, the z-score. </span><span class="kobospan" id="kobo.452.2">The z-score is a way of telling us how far away a single data value is from the mean. </span><span class="kobospan" id="kobo.452.3">Think back to the previous section where I was referring to datapoints being a certain number of standard deviations away from the mean. </span><span class="kobospan" id="kobo.452.4">The z-score of an </span><em class="italic"><span class="kobospan" id="kobo.453.1">x</span></em><span class="kobospan" id="kobo.454.1"> data value exactly the calculation of this, the formula for which is </span><span><span class="kobospan" id="kobo.455.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.456.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/135.png" class="calibre146"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.457.1">Let’s break down </span><span><span class="kobospan" id="kobo.458.1">this formula:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.459.1">X</span></em><span class="kobospan" id="kobo.460.1"> is the </span><span><span class="kobospan" id="kobo.461.1">data point</span></span></li>
<li class="calibre14"> <span class="kobospan" id="kobo.462.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/136.png" class="calibre147"/></span><span class="kobospan" id="kobo.463.1">is </span><span><span class="kobospan" id="kobo.464.1">the mean</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.465.1">s</span></em><span class="kobospan" id="kobo.466.1"> is the </span><span><span class="kobospan" id="kobo.467.1">standard deviation</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.468.1">Remember that the standard deviation was (sort of) an average distance of the data from the mean, and the z-score is an individualized value for each particular data point. </span><span class="kobospan" id="kobo.468.2">We can find the z-score of a data value by subtracting it from the mean and dividing it by the standard deviation. </span><span class="kobospan" id="kobo.468.3">The output is the standardized distance a value is from a mean. </span><span class="kobospan" id="kobo.468.4">We use the z-score all over statistics. </span><span class="kobospan" id="kobo.468.5">It is a very effective way of normalizing data that exists on very different scales, and also to put data in the context of </span><span><span class="kobospan" id="kobo.469.1">its mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.470.1">Let’s take our previous data on the numbers of friends on Facebook and standardize the data to the z-score. </span><span class="kobospan" id="kobo.470.2">For each data point, we will find its z-score by applying the preceding formula. </span><span class="kobospan" id="kobo.470.3">We will take each individual, subtract the average-friends number from the value, and divide that by the standard deviation, </span><span><span class="kobospan" id="kobo.471.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.472.1">
z_scores = []
m = np.mean(friends) # average friends on Facebook
s = np.std(friends) # standard deviation friends on Facebook
for friend in friends:
z = (friend - m)/s# z-score
z_scores.append(z) # make a list of the scores for plotting</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.473.1">Now, let’s plot these z-scores on a bar chart. </span><span class="kobospan" id="kobo.473.2">The following chart shows the same individuals from our previous example using friends on Facebook, but instead of the bar height revealing the raw number of friends, now each bar is the z-score of the </span><a id="_idIndexMarker420" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker421" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.474.1">number of friends they have on Facebook. </span><span class="kobospan" id="kobo.474.2">If we plot the z-scores, we’ll notice a </span><span><span class="kobospan" id="kobo.475.1">few things:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.476.1">
plt.bar(y_pos, z_scores)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.477.1">We get </span><span><span class="kobospan" id="kobo.478.1">this chart:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer187">
<span class="kobospan" id="kobo.479.1"><img alt="Figure 7.5 – The z scores of our data" src="image/B19488_07_05.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.480.1">Figure 7.5 – The z scores of our data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.481.1">The z scores of our data quickly show us which datapoints are below and above the average and by how many </span><span><span class="kobospan" id="kobo.482.1">standard deviations</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.483.1">We can see that we have negative values (meaning that the data point is below the mean). </span><span class="kobospan" id="kobo.483.2">The bars’ lengths no longer represent the raw number of friends, but the degree to which that friend count differs from </span><span><span class="kobospan" id="kobo.484.1">the mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.485.1">This chart makes it very easy to pick out the </span><a id="_idIndexMarker422" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker423" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.486.1">individuals with much lower and higher friends on average. </span><span class="kobospan" id="kobo.486.2">For example, the individual at index 0 has fewer friends on average (they had 109 friends where the average </span><span><span class="kobospan" id="kobo.487.1">was 789).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.488.1">What if we want to graph the standard deviations? </span><span class="kobospan" id="kobo.488.2">Recall that we earlier plotted three horizontal lines: one at the mean, one at the mean plus the standard deviation (</span><em class="italic"><span class="kobospan" id="kobo.489.1">x+s</span></em><span class="kobospan" id="kobo.490.1">), and one at the mean minus the standard </span><span><span class="kobospan" id="kobo.491.1">deviation (</span></span><span><em class="italic"><span class="kobospan" id="kobo.492.1">x-s</span></em></span><span><span class="kobospan" id="kobo.493.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.494.1">If we plug these values into the formula for the z-score, we get </span><span><span class="kobospan" id="kobo.495.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.496.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;Z&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/137.png" class="calibre148"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.497.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;Z&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/138.png" class="calibre149"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.498.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;Z&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/139.png" class="calibre150"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.499.1">This is no coincidence! </span><span class="kobospan" id="kobo.499.2">When we </span><a id="_idIndexMarker424" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker425" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.500.1">standardize the data using the z-score, our standard deviations become the metric of choice. </span><span class="kobospan" id="kobo.500.2">Let’s plot a new</span><a id="_idIndexMarker426" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker427" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.501.1"> graph with the standard </span><span><span class="kobospan" id="kobo.502.1">deviations added:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.503.1">
plt.bar(y_pos, z_scores)
plt.plot((0, 25), (1, 1), 'g-')
plt.plot((0, 25), (0, 0), 'b-')
plt.plot((0, 25), (-1, -1), 'r-')</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.504.1">The preceding code is adding the following </span><span><span class="kobospan" id="kobo.505.1">three lines:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.506.1">A blue line at </span><em class="italic"><span class="kobospan" id="kobo.507.1">y = 0</span></em><span class="kobospan" id="kobo.508.1"> that represents zero standard deviations away from the mean (which is on the </span><span><em class="italic"><span class="kobospan" id="kobo.509.1">x</span></em></span><span><span class="kobospan" id="kobo.510.1"> axis)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.511.1">A green line that represents one standard deviation above </span><span><span class="kobospan" id="kobo.512.1">the mean</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.513.1">A red line that represents one standard deviation below </span><span><span class="kobospan" id="kobo.514.1">the mean</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.515.1">Let’s look at the graph we get as </span><span><span class="kobospan" id="kobo.516.1">a result:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer191">
<span class="kobospan" id="kobo.517.1"><img alt="Figure 7.6 – Our z-score graph with lines at 1 and -1" src="image/B19488_07_06.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.518.1">Figure 7.6 – Our z-score graph with lines at 1 and -1</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.519.1">Our z-score graph with lines at 1 and -1 parallels </span><span><em class="italic"><span class="kobospan" id="kobo.520.1">Figure 7</span></em></span><em class="italic"><span class="kobospan" id="kobo.521.1">.3</span></em><span class="kobospan" id="kobo.522.1">, where we had values within 1 standard deviation of the mean shown between the green and red (top and </span><span><span class="kobospan" id="kobo.523.1">bottom) lines</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.524.1">The colors of the lines match up with the lines drawn in the earlier graph of the raw friend count. </span><span class="kobospan" id="kobo.524.2">If you look carefully, you’ll see the same people still fall outside of the green and the red lines. </span><span class="kobospan" id="kobo.524.3">Namely, the same three people still fall below the red (lower) line, and the same three people fall above the green (</span><span><span class="kobospan" id="kobo.525.1">upper) line.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.526.1">Z-scores are an effective way to </span><em class="italic"><span class="kobospan" id="kobo.527.1">standardize</span></em><span class="kobospan" id="kobo.528.1"> data. </span><span class="kobospan" id="kobo.528.2">This means that we can put the entire set on the same scale. </span><span class="kobospan" id="kobo.528.3">For example, if we also measure </span><a id="_idIndexMarker428" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker429" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.529.1">each person’s general happiness scale (which is between 0 and 1), we might have a dataset similar to the </span><span><span class="kobospan" id="kobo.530.1">following dataset:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.531.1">
friends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125, 455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]
happiness = [.8, .6, .3, .6, .6, .4, .8, .5, .4, .3, .3, .6, .2, .8, 1, .6, .2, .7, .5, .3, .1, 0, .3, 1]
import pandas as pd
df = pd.DataFrame({'friends':friends, 'happiness':happiness})
df.head()</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.532.1">We get </span><span><span class="kobospan" id="kobo.533.1">this table:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer192">
<span class="kobospan" id="kobo.534.1"><img alt="Figure 7.7 – Representing data with two columns, one for the number of friends, the other for happiness measures between 0 and 1" src="image/B19488_07_07.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.535.1">Figure 7.7 – Representing data with two columns, one for the number of friends, the other for happiness measures between 0 and 1</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.536.1">These datapoints are on two different </span><a id="_idIndexMarker430" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker431" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.537.1">dimensions, each with a very different scale. </span><span class="kobospan" id="kobo.537.2">The friend count can be in the thousands while our happiness score is stuck between 0 </span><span><span class="kobospan" id="kobo.538.1">and 1.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.539.1">To remedy this (and for some statistical/machine learning modeling, this practice will become essential), we can simply standardize the dataset using a prebuilt standardization package in scikit-learn, </span><span><span class="kobospan" id="kobo.540.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.541.1">
from sklearn import preprocessing
df_scaled = pd.DataFrame(preprocessing.scale(df), columns = ['friends_scaled', 'happiness_scaled'])
df_scaled.head()</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.542.1">This code will scale both the friends and happiness columns simultaneously, thus revealing the z-score for each column. </span><span class="kobospan" id="kobo.542.2">It is important to note that when running the preceding code, the preprocessing module in </span><strong class="source-inline"><span class="kobospan" id="kobo.543.1">sklearn</span></strong><span class="kobospan" id="kobo.544.1"> does the </span><a id="_idIndexMarker432" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker433" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.545.1">following things </span><a id="_idIndexMarker434" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.546.1">separately for </span><span><span class="kobospan" id="kobo.547.1">each column:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.548.1">Finding the mean of </span><span><span class="kobospan" id="kobo.549.1">the column</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.550.1">Finding the standard deviation of </span><span><span class="kobospan" id="kobo.551.1">the column</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.552.1">Applying the </span><em class="italic"><span class="kobospan" id="kobo.553.1">z</span></em><span class="kobospan" id="kobo.554.1">-score function to each element in </span><span><span class="kobospan" id="kobo.555.1">the column</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.556.1">The result is two columns, as shown, that exist on the same scale as each other even if they were </span><span><span class="kobospan" id="kobo.557.1">not previously:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer193">
<span class="kobospan" id="kobo.558.1"><img alt="Figure 7.8 – Using the z-score, standardizing each column to be on the same scale" src="image/B19488_07_08.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.559.1">Figure 7.8 – Using the z-score, standardizing each column to be on the same scale</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.560.1">Each of these numbers now has the same unit – </span><span><em class="italic"><span class="kobospan" id="kobo.561.1">standard deviations</span></em></span><span><span class="kobospan" id="kobo.562.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.563.1">Now, we can plot friends and happiness on the same scale and the graph will at least </span><span><span class="kobospan" id="kobo.564.1">be readable:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.565.1">
df_scaled.plot(kind='scatter', x = 'friends_scaled', y = 'happiness_scaled')</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.566.1">The preceding code gives us </span><span><span class="kobospan" id="kobo.567.1">this graph:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer194">
<span class="kobospan" id="kobo.568.1"><img alt="Figure 7.9 – Plotting scaled data" src="image/B19488_07_09.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.569.1">Figure 7.9 – Plotting scaled data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.570.1">Plotting scaled data is often much easier to read than unscaled data because it’s much easier to compare data when it is all in the </span><span><span class="kobospan" id="kobo.571.1">same unit</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.572.1">Now, our data is standardized to the z-score </span><a id="_idIndexMarker435" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker436" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.573.1">and this scatter plot is fairly easily interpretable! </span><span class="kobospan" id="kobo.573.2">In later chapters, this practice of standardization will not </span><a id="_idIndexMarker437" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker438" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.574.1">only make our data more interpretable but will also be an essential part of our model optimization. </span><span class="kobospan" id="kobo.574.2">Many machine learning algorithms require us to have standardized columns as they are reliant on the notion </span><span><span class="kobospan" id="kobo.575.1">of scale.</span></span></p>
<div class="calibre9"/><h2 id="_idParaDest-105" class="calibre7"><span class="kobospan" id="kobo.576.1">The insightfu</span><a id="_idTextAnchor209" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.577.1">l part – correlations in data</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.578.1">Throughout this book, we discuss the difference between having data and having actionable insights about your data. </span><span class="kobospan" id="kobo.578.2">Having data is only one step in achieving a successful data science operation. </span><span class="kobospan" id="kobo.578.3">Being able to obtain, clean, and plot data helps to tell the story that the data has to offer but cannot reveal the moral of the story. </span><span class="kobospan" id="kobo.578.4">In order to take this entire example one step further, we will look at the relationship between having friends on Facebook </span><span><span class="kobospan" id="kobo.579.1">and happiness.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.580.1">In subsequent chapters, we will look at a specific machine learning algorithm that attempts to find relationships between quantitative features, called </span><strong class="bold"><span class="kobospan" id="kobo.581.1">linear regression</span></strong><span class="kobospan" id="kobo.582.1">, but we do</span><a id="_idIndexMarker439" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker440" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.583.1"> not have to wait until then to begin to form hypotheses. </span><span class="kobospan" id="kobo.583.2">We have a sample of people, a measure of their online social presence, and their reported happiness. </span><span class="kobospan" id="kobo.583.3">The question of the day here is this: can we find a relationship between the number of friends on Facebook and </span><span><span class="kobospan" id="kobo.584.1">overall happiness?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.585.1">Now, obviously, this is a big question and should be treated respectfully. </span><span class="kobospan" id="kobo.585.2">Experiments to answer this question should be conducted in a laboratory setting, but we can begin to form a hypothesis about this question. </span><span class="kobospan" id="kobo.585.3">Given the nature of our data, we really only have the following three options for </span><span><span class="kobospan" id="kobo.586.1">a hypothesis:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.587.1">There is a positive association between the number of online friends and happiness (as one goes up, so does </span><span><span class="kobospan" id="kobo.588.1">the other)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.589.1">There is a negative association between them (as the number of friends goes up, your happiness </span><span><span class="kobospan" id="kobo.590.1">goes down)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.591.1">There is no association between the variables (as one changes, the other doesn’t really change </span><span><span class="kobospan" id="kobo.592.1">that much)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.593.1">Can we use basic statistics to form a hypothesis about this </span><a id="_idIndexMarker441" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker442" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.594.1">question? </span><span class="kobospan" id="kobo.594.2">I say we can! </span><span class="kobospan" id="kobo.594.3">But first, we must introduce a concept </span><span><span class="kobospan" id="kobo.595.1">called </span></span><span><strong class="bold"><span class="kobospan" id="kobo.596.1">correlation</span></strong></span><span><span class="kobospan" id="kobo.597.1">.</span></span></p>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.598.1">Correlation coefficients</span></strong><span class="kobospan" id="kobo.599.1"> are a quantitative measure</span><a id="_idIndexMarker443" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker444" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.600.1"> that describes the strength of association/relationship between </span><span><span class="kobospan" id="kobo.601.1">two variables.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.602.1">The correlation between the two sets of data tells us about how they move together. </span><span class="kobospan" id="kobo.602.2">The hope is to understand whether changing one value might help us predict the other. </span><span class="kobospan" id="kobo.602.3">This concept is not only interesting but also one of the core assumptions that many machine learning models make on data. </span><span class="kobospan" id="kobo.602.4">For many prediction algorithms to work, they rely on the fact that there is some sort of relationship between the variables being looked at. </span><span class="kobospan" id="kobo.602.5">The learning algorithms then exploit this relationship in order to make </span><span><span class="kobospan" id="kobo.603.1">accurate predictions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.604.1">A few things to note about the standard </span><span><span class="kobospan" id="kobo.605.1">correlation coefficient:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.606.1">It will lie between -1 </span><span><span class="kobospan" id="kobo.607.1">and 1</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.608.1">The greater the absolute value (closer to -1 or 1), the stronger the relationship between </span><span><span class="kobospan" id="kobo.609.1">the variables:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.610.1">The strongest correlation is -1 </span><span><span class="kobospan" id="kobo.611.1">or 1</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.612.1">The weakest correlation </span><span><span class="kobospan" id="kobo.613.1">is 0</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.614.1">A positive correlation means that as one variable increases, the other one tends to increase </span><span><span class="kobospan" id="kobo.615.1">as well</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.616.1">A negative correlation means that as one variable increases, the other one tends </span><span><span class="kobospan" id="kobo.617.1">to decrease</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.618.1">We can use pandas to quickly show us correlation coefficients between every feature and every other feature in the DataFrame, as </span><span><span class="kobospan" id="kobo.619.1">illustrated here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.620.1">
df.corr(). </span><span class="kobospan1" id="kobo.620.2"># correlation between variables</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.621.1">We get </span><span><span class="kobospan" id="kobo.622.1">this table:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer195">
<span class="kobospan" id="kobo.623.1"><img alt="Figure 7.10 – Correlation between friends and happiness" src="image/B19488_07_10.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.624.1">Figure 7.10 – Correlation between friends and happiness</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.625.1">The correlation between friends and happiness is about -0.2, which says that, according to this data, an increase in 1 friend tends to lead to a reduction of happiness by </span><span><span class="kobospan" id="kobo.626.1">0.2 units</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.627.1">The preceding table shows the correlation between </span><strong class="bold"><span class="kobospan" id="kobo.628.1">friends</span></strong><span class="kobospan" id="kobo.629.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.630.1">happiness</span></strong><span class="kobospan" id="kobo.631.1">. </span><span class="kobospan" id="kobo.631.2">Note the first </span><span><span class="kobospan" id="kobo.632.1">two things:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.633.1">The diagonal of the matrix is filled with positive 1s. </span><span class="kobospan" id="kobo.633.2">This is because they represent the correlation between the variable and itself, which, of course, forms a perfect line, making the correlation </span><span><span class="kobospan" id="kobo.634.1">perfectly positive!</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.635.1">The matrix is symmetrical across the diagonal. </span><span class="kobospan" id="kobo.635.2">This is true for any correlation matrix made </span><span><span class="kobospan" id="kobo.636.1">in pandas.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.637.1">There are a few caveats to trusting the correlation </span><a id="_idIndexMarker445" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker446" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.638.1">coefficient. </span><span class="kobospan" id="kobo.638.2">One is that, in general, a correlation will attempt to measure a linear relationship between variables. </span><span class="kobospan" id="kobo.638.3">This means that if there is no visible correlation revealed by this measure, it does not mean that there is no relationship between the variables, only that there is no line of best fit that goes through the lines easily. </span><span class="kobospan" id="kobo.638.4">There might be a </span><em class="italic"><span class="kobospan" id="kobo.639.1">non-linear</span></em><span class="kobospan" id="kobo.640.1"> relationship that defines the </span><span><span class="kobospan" id="kobo.641.1">two variables.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.642.1">It is important to realize that causation is not implied by correlation. </span><span class="kobospan" id="kobo.642.2">Just because there is a weak negative correlation between these two variables does not necessarily mean that your overall happiness decreases as the number of friends you keep on Facebook goes up. </span><span class="kobospan" id="kobo.642.3">This causation must be tested further and, in later chapters, we will attempt to do </span><span><span class="kobospan" id="kobo.643.1">just that.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.644.1">To sum up, we can use correlation to make hypotheses about the relationship between variables, but we will need to use more sophisticated statistical methods and machine learning algorithms to solidify these assumptions </span><span><span class="kobospan" id="kobo.645.1">and hypotheses.</span></span></p>
<h1 id="_idParaDest-106" class="calibre6"><span class="kobospan" id="kobo.646.1">The empirical </span><a id="_idTextAnchor210" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.647.1">rule</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.648.1">Recall that a normal distribution is defined as</span><a id="_idIndexMarker447" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker448" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.649.1"> having a specific probability distribution that resembles a bell curve. </span><span class="kobospan" id="kobo.649.2">In statistics, we love it when our data behaves </span><em class="italic"><span class="kobospan" id="kobo.650.1">normally</span></em><span class="kobospan" id="kobo.651.1">. </span><span class="kobospan" id="kobo.651.2">For example, we may have data that resembles a normal distribution, </span><span><span class="kobospan" id="kobo.652.1">like so:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer196">
<span class="kobospan" id="kobo.653.1"><img alt="Figure 7.11 – Graphical representation of normal distribution" src="image/B19488_07_11.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.654.1">Figure 7.11 – Graphical representation of normal distribution</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.655.1">The normal distribution serves as a guiding line for many branches of statistics and the basis for many statistical tests. </span><span class="kobospan" id="kobo.655.2">Shown here, data that follows this distribution lets us “expect” a certain number of datapoints to live within 1, 2, and 3 standard deviations from </span><span><span class="kobospan" id="kobo.656.1">the mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.657.1">The </span><strong class="bold"><span class="kobospan" id="kobo.658.1">empirical rule</span></strong><span class="kobospan" id="kobo.659.1"> states that we can expect a certain amount of data to live between sets of standard deviations. </span><span class="kobospan" id="kobo.659.2">Specifically, the empirical rule states the following for data that is </span><span><span class="kobospan" id="kobo.660.1">distributed normally:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.661.1">About 68% of the data falls within 1 </span><span><span class="kobospan" id="kobo.662.1">standard deviation</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.663.1">About 95% of the data falls within 2 </span><span><span class="kobospan" id="kobo.664.1">standard deviations</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.665.1">About 99.7% of the data falls within 3 </span><span><span class="kobospan" id="kobo.666.1">standard deviations</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.667.1">For example, let’s see whether our Facebook friends’ data holds up to this. </span><span class="kobospan" id="kobo.667.2">Let’s use our DataFrame to find the percentage of people that fall within 1, 2, and 3 standard deviations of the mean, </span><span><span class="kobospan" id="kobo.668.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.669.1">
finding the percentage of people within one standard deviation of the mean
within_1_std = df_scaled[(df_scaled['friends_scaled'] &lt;= 1) &amp; (df_scaled['friends_scaled'] &gt;= -1)].shape[0] within_1_std / float(df_scaled.shape[0])
0.75
finding the percentage of people within two standard deviations of the mean
within_2_std = df_scaled[(df_scaled['friends_scaled'] &lt;= 2) &amp; (df_scaled['friends_scaled'] &gt;= -2)].shape[0] within_2_std / float(df_scaled.shape[0])
0.916
finding the percentage of people within three standard deviations of the mean
within_3_std = df_scaled[(df_scaled['friends_scaled'] &lt;= 3) &amp; (df_scaled['friends_scaled'] &gt;= -3)].shape[0] within_3_std / float(df_scaled.shape[0])
1.0</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.670.1">We can see that our data does seem to follow </span><a id="_idIndexMarker449" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker450" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.671.1">the empirical rule. </span><span class="kobospan" id="kobo.671.2">About 75% of the people are within a single standard deviation of the mean. </span><span class="kobospan" id="kobo.671.3">About 92% of the people are within two standard deviations, and all of them are within three </span><span><span class="kobospan" id="kobo.672.1">standard deviations.</span></span></p>
<h2 id="_idParaDest-107" class="calibre7"><a id="_idTextAnchor211" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.673.1">Example – exam scores</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.674.1">Let’s say that we’re measuring the scores of an exam and the scores generally have a bell-shaped normal distribution. </span><span class="kobospan" id="kobo.674.2">The average result on the exam was 84% and the standard deviation was 6%. </span><span class="kobospan" id="kobo.674.3">We can say the following, with </span><span><span class="kobospan" id="kobo.675.1">approximate certainty:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.676.1">About 68% of the class scored between 78% and 90% because 78 is 6 units below 84, and 90 is 6 units </span><span><span class="kobospan" id="kobo.677.1">above 84</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.678.1">If we were asked what percentage of the class scored between 72% and 96%, we would notice that 72 is 2 standard deviations below the mean, and 96 is 2 standard deviations above the mean, so the empirical rule tells us that about 95% of the class scored in </span><span><span class="kobospan" id="kobo.679.1">that range</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.680.1">However, not all data is normally distributed, so we </span><a id="_idIndexMarker451" class="pcalibre calibre4 pcalibre1"/><a id="_idIndexMarker452" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.681.1">can’t always use the empirical rule. </span><span class="kobospan" id="kobo.681.2">We have another theorem that helps us analyze any kind of distribution. </span><span class="kobospan" id="kobo.681.3">In the next chapter, we will go into depth about when we can assume a normal distribution. </span><span class="kobospan" id="kobo.681.4">This is because many statistical tests and hypotheses require the underlying data to come from a normally </span><span><span class="kobospan" id="kobo.682.1">distributed population.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.683.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.684.1">Previously, when we standardized our data to the z-score, we did not require an assumption of </span><span><span class="kobospan" id="kobo.685.1">normal distribution.</span></span></p>
<h1 id="_idParaDest-108" class="calibre6"><a id="_idTextAnchor212" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.686.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.687.1">In this</span><a id="_idTextAnchor213" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.688.1"> chapter, we covered many of the basic statistics required for most data scientists – everything from how we obtain/sample data to how to standardize data according to the z-score and applications of the empirical rule. </span><span class="kobospan" id="kobo.688.2">We also reviewed how to take samples for data analysis. </span><span class="kobospan" id="kobo.688.3">In addition, we reviewed various statistical measures, such as the mean and standard deviation, that help </span><span><span class="kobospan" id="kobo.689.1">describe data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.690.1">In the next chapter, we will look at much more advanced applications of statistics. </span><span class="kobospan" id="kobo.690.2">One thing that we will consider is how to use hypothesis tests on data that we can assume to be normal. </span><span class="kobospan" id="kobo.690.3">As we use these tests, we will also quantify our errors and identify the best practices to solve </span><span><span class="kobospan" id="kobo.691.1">these errors.</span></span></p>
</div>
</body></html>