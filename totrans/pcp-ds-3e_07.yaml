- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What Are the Chances? An Introduction to Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on the statistical knowledge required by any aspiring
    data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore ways of sampling and obtaining data without being affected by
    bias and then use measures of statistics to quantify and visualize our data. Using
    the z-score and the empirical rule, we will see how we can standardize data for
    the purposes of both graphing and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to obtain and sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The measures of center, variance, and relative standing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of data using the z-score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The empirical rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are statistics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This might seem like an odd question to ask, but I am frequently surprised
    by the number of people who cannot answer this simple and yet powerful question:
    what are statistics? Statistics are the numbers you always see on the news and
    in the paper. Statistics are useful when trying to prove a point or trying to
    scare someone, but what are they?'
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we need to back up for a minute and talk about why
    we even measure them in the first place. The goal of this field is to try to explain
    and model the world around us. To do that, we have to take a look at the population.
  prefs: []
  type: TYPE_NORMAL
- en: We can define a **population** as the entire pool of subjects of an experiment
    or a model.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, your population is who you care about. Who are you trying to talk
    about? If you are trying to test whether smoking leads to heart disease, your
    population would be the smokers of the world. If you are trying to study teenage
    drinking problems, your population would be all teenagers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine that you want to ask a question about your population. For example,
    if your population is all of your employees (assume that you have over 1,000 employees),
    perhaps you want to know what percentage of them enjoy traveling. The question
    is called a **parameter** – a numerical measurement describing a characteristic
    of a population. For example, if you ask all 1,000 employees and 100 of them enjoy
    traveling, the rate of travel enjoyment is 10%. The parameter here is 10%.
  prefs: []
  type: TYPE_NORMAL
- en: You probably can’t ask every single employee whether they enjoy traveling. What
    if you have over 10,000 employees? It would be very difficult to track everyone
    down in order to get your answer. When this happens, it’s impossible to figure
    out this parameter. In this case, we can *estimate* the parameter.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a *sample* of the population. We can define a sample of
    a population as a subset (not necessarily random) of the population. Perhaps ask
    200 of the 1,000 employees you have. Of these 200, suppose 26 enjoy traveling,
    making the rate 13%. Here, 13% is not a parameter because we didn’t get a chance
    to ask everyone. This 13% is an estimate of a parameter. Do you know what that’s
    called? That’s right, a **statistic**!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a statistic as a numerical measurement describing a characteristic
    of a sample of a population. A statistic is just an estimation of a parameter.
    It is a number that attempts to describe an entire population by describing a
    subset of that population. This is necessary because you can never hope to give
    a survey to every single teenager or to every single smoker in the world. That’s
    what the field of statistics is all about: taking samples of populations and running
    tests on these samples.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the next time you are given a statistic, just remember that number only
    represents a sample of that population, not the entire pool of subjects.
  prefs: []
  type: TYPE_NORMAL
- en: How do we obtain and sample data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If statistics is about taking samples of populations, it must be very important
    to know how we obtain these samples, and you’d be correct. Let’s focus on just
    a few of the many ways of obtaining and sampling data.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main ways of collecting data for our analysis: **observational**
    and **experimentation**. Both these ways have their pros and cons, of course.
    They each produce different types of behavior and, therefore, warrant different
    types of analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Observational
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We might obtain data through observational means, which consists of measuring
    specific characteristics but not attempting to modify the subjects being studied.
    For example, if you had tracking software on your website that observes users’
    behavior on the website, such as length of time spent on certain pages and the
    rate of clicking on ads, all the while not affecting the user’s experience, then
    that would be an observational study.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the most common ways to get data because it’s just plain easy.
    All you have to do is observe and collect data. Observational studies are also
    limited in the types of data that can be collected. This is because the observer
    (you) is not in control of the environment. You may only watch and collect natural
    behavior. If you are looking to induce a certain type of behavior, an observational
    study would not be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **experiment** consists of a treatment and the observation of its effect
    on the subjects. Subjects in an experiment are called **experimental units**.
    This is usually how most scientific labs collect data. They will put people into
    two or more groups (usually just two) and call them the control and the experimental
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: The control group is exposed to a certain environment and then observed. The
    experimental group is exposed to a different environment and then observed. The
    experimenter then aggregates data from both groups and makes a decision about
    which environment was more favorable (favorable is a quality that the experimenter
    gets to decide).
  prefs: []
  type: TYPE_NORMAL
- en: In a marketing example, imagine that we expose half of our users to a certain
    landing page with certain images and a certain style (*website A*), and we measure
    whether or not they sign up for the service. Then, we expose the other half to
    a different landing page, different images, and different styles (*website B*)
    and again measure whether or not they sign up. We can then decide which of the
    two sites performed better and should be used going further. This, specifically,
    is called an *A/B test*. Let’s see an example in Python!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s suppose we run the preceding test and obtain the following results as
    a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, each object in the list result represents a subject (person). Each person
    then has the following two attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Which website they were exposed to, represented by a single character
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not they converted (**0** for no and **1** for yes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can then aggregate and come up with the following results table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we create these two lists that will eventually hold each individual conversion
    value as Booleans (`0` or `1`), we will iterate all of our results of the test
    and add them to the appropriate list, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, each list contains a series of `1` and `0` values.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that **1** represents a user actually converting to the site after
    seeing that web page, and **0** represents a user seeing the page and leaving
    before signing up/converting.
  prefs: []
  type: TYPE_NORMAL
- en: To get the total number of people exposed to website `A`, we can use the `len()`
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use Python to illustrate the elements of our two lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To count the number of people who converted, we can use the sum of the list,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we subtract the length of the lists and the sum of the list, we are left
    with the number of people who did *not* convert for each site, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can aggregate and summarize our results in the following table, which represents
    our experiment on website conversion testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Did not** **sign up** | **Signed up** |'
  prefs: []
  type: TYPE_TB
- en: '| **Website A** | 134 | 54 |'
  prefs: []
  type: TYPE_TB
- en: '| **Website B** | 110 | 48 |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – The results of our A/B test
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quickly drum up some descriptive statistics. We can say that the website
    conversion rates for the two websites are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conversion for website A*: *154 /(154+34) = .**288*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conversion for website B*: *48/(110+48)= .3*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not much difference, but different nonetheless. Even though B has a higher conversion
    rate, can we really say that version B significantly converts better? Not yet.
    To test the *statistical significance* of such a result, a hypothesis test should
    be used. These tests will be covered in depth in the next chapter, where we will
    revisit this exact same example and finish it using a proper statistical test.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember how statistics are the result of measuring a sample of a population.
    Well, we should talk about two very common ways to decide who gets the honor of
    being in the sample that we measure. We will discuss the main type of sampling,
    called random sampling, which is the most common way to decide our sample sizes
    and our sample members.
  prefs: []
  type: TYPE_NORMAL
- en: Probability sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Probability sampling is a way of sampling from a population, in which every
    person has a known probability of being chosen, but that probability *might* be
    a different value than another user has. The simplest (and probably the most common)
    probability sampling method is **random sampling**.
  prefs: []
  type: TYPE_NORMAL
- en: Random sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that we are running an A/B test and we need to figure out who will
    be in Group A and who will be in Group B. The following are three suggestions
    from our data team:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Separate users based on location*: Users on the West Coast are placed in Group
    A, while users on the East Coast are placed in Group B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Separate users based on the time of day they visit the site*: Users who visit
    between 7 p.m. and 4 a.m. are group A, while the rest are placed in group B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Make it completely random*: Every new user has a 50/50 chance of being placed
    in either group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two are valid options for choosing samples and are fairly simple
    to implement, but they both have one fundamental flaw: they are both at risk of
    introducing a sampling bias.'
  prefs: []
  type: TYPE_NORMAL
- en: A sampling bias occurs when the way the sample is obtained systematically favors
    some other outcome over the target outcome. This can occur in various ways, such
    as using an unrepresentative sample, selecting participants based on certain criteria,
    or using biased sampling methods. When a sample is biased, it can lead to incorrect
    or misleading conclusions about the population being studied, and therefore it
    is important to ensure that sampling methods are appropriate and unbiased in order
    to obtain accurate and reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: It is not difficult to see why choosing the first or second option from the
    preceding list might introduce bias. If we choose our groups based on where they
    live or what time they log in, we are priming our experiment incorrectly and,
    now, we have much less control over the results.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we are at risk of introducing a **confounding factor** into our
    analysis, which is bad news.
  prefs: []
  type: TYPE_NORMAL
- en: A **confounding factor** is a variable that we are not directly measuring but
    that connects the variables that are being measured. Basically, a confounding
    factor is like the missing element in our analysis that is invisible but affects
    our results.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the first option is not taking into account the potential confounding
    factor of *geographical taste*. For example, if website A is unappealing, in general,
    to West Coast users, it will affect your results drastically.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the second option might introduce a temporal (time-based) confounding
    factor. What if website B is better viewed in a night-time environment (which
    was reserved for A), and users react negatively to the style purely because the
    time of day when they view it? These are both factors that we want to avoid, so
    we should go with the third option, which is a random sample.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While sampling bias can cause confounding, it is a different concept than confounding.
    The first and second options were both sampling biases because we chose the samples
    incorrectly, and were also examples of confounding factors because there was a
    third variable in each case that affected our decision.
  prefs: []
  type: TYPE_NORMAL
- en: A random sample is chosen such that every single member of a population has
    an equal chance of being chosen as any other member.
  prefs: []
  type: TYPE_NORMAL
- en: This is probably one of the easiest and most convenient ways to decide who will
    be a part of your sample. Everyone has the exact same chance of being in any particular
    group. Random sampling is an effective way of reducing the impact of confounding
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: Unequal probability sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall that I previously said that a probability sampling might have different
    probabilities for different potential sample members. But what if this actually
    introduced problems? Suppose we are interested in measuring the happiness level
    of our employees. We already know that we can’t ask every single member of staff
    because that would be silly and exhausting. So, we need to take a sample. Our
    data team suggests random sampling, and at first, everyone high-fives because
    they feel very smart and statistical. But then someone asks a seemingly harmless
    question: does anyone know the percentage of men/women who work here?'
  prefs: []
  type: TYPE_NORMAL
- en: The high-fives stop and the room goes silent.
  prefs: []
  type: TYPE_NORMAL
- en: This question is extremely important because sex is likely to be a confounding
    factor. The team looks into it and discovers a split of 75% men and 25% women
    in the company.
  prefs: []
  type: TYPE_NORMAL
- en: This means that if we introduce a random sample, our sample will likely have
    a similar split and thus favor the results for men and not women. To combat this,
    we can opt to include more women than men in our survey in order to make the split
    of our sample less biased toward men.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, introducing a favoring system in our random sampling seems
    like a bad idea; however, alleviating unequal sampling and, therefore, working
    to remove systematic bias among gender, race, disability, and so on is much more
    pertinent. A simple random sample, where everyone has the same chance as everyone
    else, is very likely to drown out the voices and opinions of minority population
    members. Therefore, it can be okay to introduce such a favoring system in your
    sampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How do we measure statistics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have our sample, it’s time to quantify our results. Suppose we wish
    to generalize the happiness of our employees or we want to figure out whether
    salaries in the company are very different from person to person.
  prefs: []
  type: TYPE_NORMAL
- en: These are some common ways of measuring our results.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of center
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Measures of the center are how we define the middle, or center, of a dataset.
    We do this because sometimes we wish to make generalizations about data values.
    For example, perhaps we’re curious about what the average rainfall in Seattle
    is or what the median height of European males is. It’s a way to generalize a
    large set of data so that it’s easier to convey to someone.
  prefs: []
  type: TYPE_NORMAL
- en: A measure of center is a value in the *middle* of a dataset. This can mean different
    things to different people. Who’s to say where the middle of a dataset is? There
    are so many different ways of defining the center of data. Let’s take a look at
    a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **arithmetic mean** of a dataset is found by adding up all of the values
    and then dividing it by the number of data values. This is likely the most common
    way to define the center of data, but can be flawed! Suppose we wish to find the
    mean of the following numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Simple enough; our average is `14.25` and all of our values are fairly close
    to it. But what if we introduce a new value: `31`?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This greatly affects the mean because the arithmetic mean is sensitive to outliers.
    The new value, `31`, is almost twice as large as the rest of the numbers, and
    therefore skews the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another, and sometimes better, measure of the center is the median. The **median**
    is the number found in the middle of the dataset when it is sorted in order, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note how the introduction of `31` using the median did not affect the median
    of the dataset greatly. This is because the median is less sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: When working with datasets with many outliers, it is sometimes more useful to
    use the median of the dataset, while if your data does not have many outliers
    and the datapoints are mostly close to one another, then the mean is likely a
    better option.
  prefs: []
  type: TYPE_NORMAL
- en: But how can we tell if the data is spread out? Well, we will have to introduce
    a new type of statistic.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of variation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Measures of the center are used to quantify the middle of the data, but now
    we will explore ways of measuring how to *spread out* the data we collect is.
    This is a useful way to identify whether our data has many outliers lurking inside.
    Let’s start with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we take a random sample of 24 of our friends on Facebook and record
    how many friends each of them has on Facebook. Here’s the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The average of the values in this list is just over `789`. So, we could say
    that according to this sample, the average Facebook friend has 789 friends. But
    what about the person who only has 89 friends or the person who has over 1,600
    friends? In fact, not many of these numbers are really that close to 789.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, how about we use the median? The median generally is not as affected
    by outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The median is `769.5`, which is fairly close to the mean. Hmm, good try, but
    still, it doesn’t really account for how drastically different a lot of these
    datapoints are. This is what statisticians call measuring the variation of data.
    Let’s start by introducing the most basic measure of variation: the range. The
    range is simply the maximum value minus the minimum value, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The range tells us how far away the two most extreme values are. Now, the range
    isn’t typically widely used but it does have some pertinent applications. Sometimes,
    we wish to know just how spread apart the outliers are. This is most useful in
    scientific and safety measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a car company wants to measure how long it takes for an airbag to deploy.
    Knowing the average of that time is nice, but they also really want to know how
    spread apart the slowest time is versus the fastest time. This literally could
    be the difference between life and death.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting back to the Facebook example, `1684` is our range, but I’m not quite
    sure it’s saying very much about our data. Now, let’s take a look at the most
    commonly used measure of variation, the **standard deviation**.
  prefs: []
  type: TYPE_NORMAL
- en: I’m sure many of you have heard this term thrown around a lot and it might even
    incite a degree of fear, but what does it really mean? In essence, standard deviation,
    denoted by *s* when we are working with a sample of a population, measures by
    how much data values deviate from the arithmetic mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s basically a way to see how spread out the data is. There is a general
    formula to calculate the standard deviation, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Formula for standard deviation](img/B19488_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Formula for standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: The formula for standard deviation might seem scary at first, but with time
    and practice, it will become a familiar friend, helping illustrate how spread
    out data really is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at each of the elements in this formula in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: '*s* is our sample’s standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is each individual data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover></mrow></math>](img/132.png) is the mean of the
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n* is the number of datapoints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you freak out and close this book at the seemingly complicated equation,
    let’s break it down. For each value in the sample, we will take that value, subtract
    the arithmetic mean from it, square the difference, and, once we’ve added up every
    single point this way, we will divide the entire thing by *n*, the number of points
    in the sample. Finally, we take a square root of everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without going into an in-depth analysis of the formula, think about it this
    way: it’s basically derived from the distance formula. Essentially, what the standard
    deviation is calculating is a sort of average distance of the data values from
    the arithmetic mean.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a closer look at the formula, you will see that it actually makes
    sense:'
  prefs: []
  type: TYPE_NORMAL
- en: By taking *x*-![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover></mrow></math>](img/133.png), you are finding the
    literal difference between the value and the mean of the sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By squaring the result, (*x*-![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover></mrow></math>](img/132.png))2, we are putting a
    greater penalty on outliers because squaring a large error only makes it much
    larger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By dividing by the number of items in the sample, we are taking (literally)
    the average squared distance between each point and the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By taking the square root of the answer, we are putting the number in terms
    that we can understand. For example, by squaring the number of friends minus the
    mean, we changed our units to friends squared, which makes no sense. Taking the
    square root puts our units back to just “friends.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go back to our Facebook example for a visualization and further explanation
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by calculating the standard deviation – a few of them, in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the arithmetic mean of the data was around `789`, so we’ll use `789`
    as the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by taking the difference between each data value and the mean, squaring
    it, adding them all up, dividing it by one less than the number of values, and
    then taking its square root. This would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Representation of an example of calculating the standard deviation
    of a series of data](img/B19488_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Representation of an example of calculating the standard deviation
    of a series of data
  prefs: []
  type: TYPE_NORMAL
- en: An example of calculating the standard deviation of a series of data has us
    subtracting each item in the list by the average of the list, squaring that sum,
    adding it all up, dividing by the number of items, and finally, taking the square
    root of that whole thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we can take the Python approach and do all this programmatically
    (which is usually preferred):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: What the number `425` represents is the spread of data. You could say that 425
    is a kind of average distance the data values are from the mean. What this means,
    in simple words, is that this data is pretty spread out.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our standard deviation is about `425`. This means that the number of friends
    that these people have on Facebook doesn’t seem to be close to a single number
    and that’s quite evident when we plot the data in a bar graph, and also plot the
    mean as well as the visualizations of the standard deviation. In the following
    plot, every person will be represented by a single bar in the bar chart, and the
    height of the bars represent the number of friends that the individuals have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the chart that we get:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.3 – Plotting each of our datapoints as a bar in a bar graph with\
    \ a line showing the average value (blue, in the middle)\uFEFF, the average minus\
    \ 1 standard deviation (red, below)\uFEFF, and the average plus 1 standard deviation\
    \ (green, the line above)](img/B19488_07_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Plotting each of our datapoints as a bar in a bar graph with a
    line showing the average value (blue, in the middle), the average minus 1 standard
    deviation (red, below), and the average plus 1 standard deviation (green, the
    line above)
  prefs: []
  type: TYPE_NORMAL
- en: The blue line in the center is drawn at the mean (789), the red line near the
    bottom is drawn at the mean minus the standard deviation (789 - 425 = 364), and
    finally, the green line toward the top is drawn at the mean plus the standard
    deviation (789 + 425 = 1,214).
  prefs: []
  type: TYPE_NORMAL
- en: Note how most of the data lives between the green and the red lines while the
    outliers live outside the lines. There are three people who have friend counts
    below the red line and three people who have a friend count above the green line.
    This is a common way to discuss outliers in data – through standard deviations.
    It’s often used as a unit distance. You can say, for example, that in the realm
    of this “friends” data, the value of 1,214 is “one standard deviation above the
    mean” or that 1,639 is “two standard deviations above the mean."
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to mention that the units for standard deviation are, in fact,
    the same units as the data’s units. So, in this example, we would say that the
    standard deviation is 425 friends on Facebook.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Another measure of variation is the variance, as described in the previous chapter.
    The variance is simply the standard deviation squared.
  prefs: []
  type: TYPE_NORMAL
- en: So, now we know that the standard deviation and variance are good for checking
    how spread out our data is, and that we can use it along with the mean to create
    a kind of range that a lot of our data lies in. But what if we want to compare
    the spread of two different datasets, maybe even with completely different units?
    That’s where the coefficient of variation comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: The coefficient of variation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **coefficient of variation** is defined as the ratio of the data’s standard
    deviation to its mean.
  prefs: []
  type: TYPE_NORMAL
- en: This ratio (which, by the way, is only helpful if we’re working in the ratio
    level of measurement, where the division is allowed and is meaningful) is a way
    to standardize the standard deviation, which makes it easier to compare across
    datasets. We use this measure frequently when attempting to compare means, and
    it spreads across populations that exist at different scales.
  prefs: []
  type: TYPE_NORMAL
- en: Example – employee salaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we look at the mean and standard deviation of employees’ salaries in the
    same company but among different departments, we see that, at first glance, it
    may be tough to compare the standard deviations because they are on such different
    scales:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Glancing at the means and standard deviations of salaries across
    departments](img/B19488_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Glancing at the means and standard deviations of salaries across
    departments
  prefs: []
  type: TYPE_NORMAL
- en: Glancing at the means and standard deviations of salaries across departments
    can be challenging without scaling them to the same scale as one another using
    the coefficient of variation (the final column). Only then can we see that the
    spread of salaries at the executive level is somewhat larger than the spread in
    the other departments
  prefs: []
  type: TYPE_NORMAL
- en: This is especially true when the mean salary of one department is $25,000, while
    another department has a mean salary in the six-figure area.
  prefs: []
  type: TYPE_NORMAL
- en: However, if we look at the last column, which is our coefficient of variation,
    it becomes clearer that the people in the executive department may be getting
    paid more but they are also getting wildly different salaries. This is probably
    because the CEO is earning way more than an office manager, who is still in the
    executive department, which makes the data very spread out.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, everyone in the mailroom, while not making as much money,
    is making just about the same as everyone else in the mailroom, which is why their
    coefficient of variation is only 8%.
  prefs: []
  type: TYPE_NORMAL
- en: With measures of variation, we can begin to answer big questions, such as how
    to spread out this data or how we can come up with a good range that most of the
    data falls into.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of relative standing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can combine both the measures of centers and variations to create measures
    of relative standing. **Measures of variation** measure where particular data
    values are positioned, relative to the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by learning a very important value in statistics, the z-score.
    The z-score is a way of telling us how far away a single data value is from the
    mean. Think back to the previous section where I was referring to datapoints being
    a certain number of standard deviations away from the mean. The z-score of an
    *x* data value exactly the calculation of this, the formula for which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>z</mi><mo>=</mo><mfrac><mrow><mi>X</mi><mo>−</mo><mover><mi>X</mi><mo
    stretchy="true">‾</mo></mover></mrow><mi>s</mi></mfrac></mrow></mrow></math>](img/135.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s break down this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X* is the data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>X</mi><mo
    stretchy="true">‾</mo></mover></mrow></math>](img/136.png)is the mean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* is the standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that the standard deviation was (sort of) an average distance of the
    data from the mean, and the z-score is an individualized value for each particular
    data point. We can find the z-score of a data value by subtracting it from the
    mean and dividing it by the standard deviation. The output is the standardized
    distance a value is from a mean. We use the z-score all over statistics. It is
    a very effective way of normalizing data that exists on very different scales,
    and also to put data in the context of its mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take our previous data on the numbers of friends on Facebook and standardize
    the data to the z-score. For each data point, we will find its z-score by applying
    the preceding formula. We will take each individual, subtract the average-friends
    number from the value, and divide that by the standard deviation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s plot these z-scores on a bar chart. The following chart shows the
    same individuals from our previous example using friends on Facebook, but instead
    of the bar height revealing the raw number of friends, now each bar is the z-score
    of the number of friends they have on Facebook. If we plot the z-scores, we’ll
    notice a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The z scores of our data](img/B19488_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The z scores of our data
  prefs: []
  type: TYPE_NORMAL
- en: The z scores of our data quickly show us which datapoints are below and above
    the average and by how many standard deviations
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we have negative values (meaning that the data point is below
    the mean). The bars’ lengths no longer represent the raw number of friends, but
    the degree to which that friend count differs from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: This chart makes it very easy to pick out the individuals with much lower and
    higher friends on average. For example, the individual at index 0 has fewer friends
    on average (they had 109 friends where the average was 789).
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to graph the standard deviations? Recall that we earlier plotted
    three horizontal lines: one at the mean, one at the mean plus the standard deviation
    (*x+s*), and one at the mean minus the standard deviation (*x-s*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plug these values into the formula for the z-score, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>Z</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>o</mi><mi>f</mi><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover></mrow><mi>s</mi></mfrac><mo>=</mo><mfrac><mn>0</mn><mi>s</mi></mfrac><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/137.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>Z</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>o</mi><mi>f</mi><mo>(</mo><mi
    mathvariant="normal">x</mi><mo>+</mo><mi mathvariant="normal">s</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>+</mo><mi>s</mi><mo>)</mo><mo>−</mo><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover></mrow><mi>s</mi></mfrac><mo>=</mo><mfrac><mi>s</mi><mi>s</mi></mfrac><mo>=</mo><mn>1</mn></mrow></mrow></mrow></math>](img/138.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>Z</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>o</mi><mi>f</mi><mo>(</mo><mi
    mathvariant="normal">x</mi><mo>−</mo><mi mathvariant="normal">s</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mi>s</mi><mo>)</mo><mo>−</mo><mover><mi>x</mi><mo
    stretchy="true">‾</mo></mover></mrow><mi>s</mi></mfrac><mo>=</mo><mfrac><mrow><mo>−</mo><mi>s</mi></mrow><mi>s</mi></mfrac><mo>=</mo><mo>−</mo><mn>1</mn></mrow></mrow></mrow></math>](img/139.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is no coincidence! When we standardize the data using the z-score, our
    standard deviations become the metric of choice. Let’s plot a new graph with the
    standard deviations added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is adding the following three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: A blue line at *y = 0* that represents zero standard deviations away from the
    mean (which is on the *x* axis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A green line that represents one standard deviation above the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A red line that represents one standard deviation below the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the graph we get as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Our z-score graph with lines at 1 and -1](img/B19488_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Our z-score graph with lines at 1 and -1
  prefs: []
  type: TYPE_NORMAL
- en: Our z-score graph with lines at 1 and -1 parallels *Figure 7**.3*, where we
    had values within 1 standard deviation of the mean shown between the green and
    red (top and bottom) lines
  prefs: []
  type: TYPE_NORMAL
- en: The colors of the lines match up with the lines drawn in the earlier graph of
    the raw friend count. If you look carefully, you’ll see the same people still
    fall outside of the green and the red lines. Namely, the same three people still
    fall below the red (lower) line, and the same three people fall above the green
    (upper) line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Z-scores are an effective way to *standardize* data. This means that we can
    put the entire set on the same scale. For example, if we also measure each person’s
    general happiness scale (which is between 0 and 1), we might have a dataset similar
    to the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Representing data with two columns, one for the number of friends,
    the other for happiness measures between 0 and 1](img/B19488_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Representing data with two columns, one for the number of friends,
    the other for happiness measures between 0 and 1
  prefs: []
  type: TYPE_NORMAL
- en: These datapoints are on two different dimensions, each with a very different
    scale. The friend count can be in the thousands while our happiness score is stuck
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remedy this (and for some statistical/machine learning modeling, this practice
    will become essential), we can simply standardize the dataset using a prebuilt
    standardization package in scikit-learn, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will scale both the friends and happiness columns simultaneously,
    thus revealing the z-score for each column. It is important to note that when
    running the preceding code, the preprocessing module in `sklearn` does the following
    things separately for each column:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the mean of the column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the standard deviation of the column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the *z*-score function to each element in the column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result is two columns, as shown, that exist on the same scale as each other
    even if they were not previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Using the z-score, standardizing each column to be on the same
    scale](img/B19488_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Using the z-score, standardizing each column to be on the same
    scale
  prefs: []
  type: TYPE_NORMAL
- en: Each of these numbers now has the same unit – *standard deviations*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can plot friends and happiness on the same scale and the graph will
    at least be readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Plotting scaled data](img/B19488_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Plotting scaled data
  prefs: []
  type: TYPE_NORMAL
- en: Plotting scaled data is often much easier to read than unscaled data because
    it’s much easier to compare data when it is all in the same unit
  prefs: []
  type: TYPE_NORMAL
- en: Now, our data is standardized to the z-score and this scatter plot is fairly
    easily interpretable! In later chapters, this practice of standardization will
    not only make our data more interpretable but will also be an essential part of
    our model optimization. Many machine learning algorithms require us to have standardized
    columns as they are reliant on the notion of scale.
  prefs: []
  type: TYPE_NORMAL
- en: The insightful part – correlations in data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we discuss the difference between having data and having
    actionable insights about your data. Having data is only one step in achieving
    a successful data science operation. Being able to obtain, clean, and plot data
    helps to tell the story that the data has to offer but cannot reveal the moral
    of the story. In order to take this entire example one step further, we will look
    at the relationship between having friends on Facebook and happiness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In subsequent chapters, we will look at a specific machine learning algorithm
    that attempts to find relationships between quantitative features, called **linear
    regression**, but we do not have to wait until then to begin to form hypotheses.
    We have a sample of people, a measure of their online social presence, and their
    reported happiness. The question of the day here is this: can we find a relationship
    between the number of friends on Facebook and overall happiness?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, obviously, this is a big question and should be treated respectfully.
    Experiments to answer this question should be conducted in a laboratory setting,
    but we can begin to form a hypothesis about this question. Given the nature of
    our data, we really only have the following three options for a hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a positive association between the number of online friends and happiness
    (as one goes up, so does the other)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a negative association between them (as the number of friends goes
    up, your happiness goes down)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no association between the variables (as one changes, the other doesn’t
    really change that much)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we use basic statistics to form a hypothesis about this question? I say
    we can! But first, we must introduce a concept called **correlation**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Correlation coefficients** are a quantitative measure that describes the
    strength of association/relationship between two variables.'
  prefs: []
  type: TYPE_NORMAL
- en: The correlation between the two sets of data tells us about how they move together.
    The hope is to understand whether changing one value might help us predict the
    other. This concept is not only interesting but also one of the core assumptions
    that many machine learning models make on data. For many prediction algorithms
    to work, they rely on the fact that there is some sort of relationship between
    the variables being looked at. The learning algorithms then exploit this relationship
    in order to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few things to note about the standard correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: It will lie between -1 and 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The greater the absolute value (closer to -1 or 1), the stronger the relationship
    between the variables:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strongest correlation is -1 or 1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The weakest correlation is 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A positive correlation means that as one variable increases, the other one tends
    to increase as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A negative correlation means that as one variable increases, the other one tends
    to decrease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can use pandas to quickly show us correlation coefficients between every
    feature and every other feature in the DataFrame, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Correlation between friends and happiness](img/B19488_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Correlation between friends and happiness
  prefs: []
  type: TYPE_NORMAL
- en: The correlation between friends and happiness is about -0.2, which says that,
    according to this data, an increase in 1 friend tends to lead to a reduction of
    happiness by 0.2 units
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding table shows the correlation between **friends** and **happiness**.
    Note the first two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal of the matrix is filled with positive 1s. This is because they
    represent the correlation between the variable and itself, which, of course, forms
    a perfect line, making the correlation perfectly positive!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix is symmetrical across the diagonal. This is true for any correlation
    matrix made in pandas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few caveats to trusting the correlation coefficient. One is that,
    in general, a correlation will attempt to measure a linear relationship between
    variables. This means that if there is no visible correlation revealed by this
    measure, it does not mean that there is no relationship between the variables,
    only that there is no line of best fit that goes through the lines easily. There
    might be a *non-linear* relationship that defines the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to realize that causation is not implied by correlation. Just
    because there is a weak negative correlation between these two variables does
    not necessarily mean that your overall happiness decreases as the number of friends
    you keep on Facebook goes up. This causation must be tested further and, in later
    chapters, we will attempt to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, we can use correlation to make hypotheses about the relationship
    between variables, but we will need to use more sophisticated statistical methods
    and machine learning algorithms to solidify these assumptions and hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: The empirical rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that a normal distribution is defined as having a specific probability
    distribution that resembles a bell curve. In statistics, we love it when our data
    behaves *normally*. For example, we may have data that resembles a normal distribution,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Graphical representation of normal distribution](img/B19488_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Graphical representation of normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution serves as a guiding line for many branches of statistics
    and the basis for many statistical tests. Shown here, data that follows this distribution
    lets us “expect” a certain number of datapoints to live within 1, 2, and 3 standard
    deviations from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **empirical rule** states that we can expect a certain amount of data to
    live between sets of standard deviations. Specifically, the empirical rule states
    the following for data that is distributed normally:'
  prefs: []
  type: TYPE_NORMAL
- en: About 68% of the data falls within 1 standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About 95% of the data falls within 2 standard deviations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About 99.7% of the data falls within 3 standard deviations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s see whether our Facebook friends’ data holds up to this.
    Let’s use our DataFrame to find the percentage of people that fall within 1, 2,
    and 3 standard deviations of the mean, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can see that our data does seem to follow the empirical rule. About 75% of
    the people are within a single standard deviation of the mean. About 92% of the
    people are within two standard deviations, and all of them are within three standard
    deviations.
  prefs: []
  type: TYPE_NORMAL
- en: Example – exam scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s say that we’re measuring the scores of an exam and the scores generally
    have a bell-shaped normal distribution. The average result on the exam was 84%
    and the standard deviation was 6%. We can say the following, with approximate
    certainty:'
  prefs: []
  type: TYPE_NORMAL
- en: About 68% of the class scored between 78% and 90% because 78 is 6 units below
    84, and 90 is 6 units above 84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we were asked what percentage of the class scored between 72% and 96%, we
    would notice that 72 is 2 standard deviations below the mean, and 96 is 2 standard
    deviations above the mean, so the empirical rule tells us that about 95% of the
    class scored in that range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, not all data is normally distributed, so we can’t always use the empirical
    rule. We have another theorem that helps us analyze any kind of distribution.
    In the next chapter, we will go into depth about when we can assume a normal distribution.
    This is because many statistical tests and hypotheses require the underlying data
    to come from a normally distributed population.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Previously, when we standardized our data to the z-score, we did not require
    an assumption of normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered many of the basic statistics required for most data
    scientists – everything from how we obtain/sample data to how to standardize data
    according to the z-score and applications of the empirical rule. We also reviewed
    how to take samples for data analysis. In addition, we reviewed various statistical
    measures, such as the mean and standard deviation, that help describe data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at much more advanced applications of statistics.
    One thing that we will consider is how to use hypothesis tests on data that we
    can assume to be normal. As we use these tests, we will also quantify our errors
    and identify the best practices to solve these errors.
  prefs: []
  type: TYPE_NORMAL
