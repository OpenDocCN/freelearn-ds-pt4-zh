<html><head></head><body>
<div id="_idContainer224" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-109"><a id="_idTextAnchor214" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-110" class="calibre6"><a id="_idTextAnchor215" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">Advanced Statistics</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we are concerned with making inferences about entire populations based on certain samples of data. </span><span class="kobospan" id="kobo.3.2">We will be using hypothesis tests along with different estimation tests in order to gain a better understanding of populations, given samples </span><span><span class="kobospan" id="kobo.4.1">of data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.5.1">The key topics that we will cover in this chapter are </span><span><span class="kobospan" id="kobo.6.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span><span class="kobospan" id="kobo.7.1">Point estimates</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.8.1">Confidence intervals</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.9.1">The central </span><span><span class="kobospan" id="kobo.10.1">limit theorem</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.11.1">Hypothesis testing</span></span></li>
</ul>
<h1 id="_idParaDest-111" class="calibre6"><a id="_idTextAnchor216" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.12.1">Understanding point estimates</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.13.1">Recall that, in the previous</span><a id="_idIndexMarker453" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.14.1"> chapter, we mentioned how difficult it is to obtain a population parameter; so, we had to use sample data to calculate a statistic that was an estimate of a parameter. </span><span class="kobospan" id="kobo.14.2">When we make these estimates, we call them </span><span><strong class="bold"><span class="kobospan" id="kobo.15.1">point estimates</span></strong></span><span><span class="kobospan" id="kobo.16.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.17.1">A point estimate is an estimate of a population parameter based on </span><span><span class="kobospan" id="kobo.18.1">sample data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.19.1">We use point estimates to estimate things such as population means, variances, and other statistics. </span><span class="kobospan" id="kobo.19.2">To obtain these estimates, we simply apply the function that we wish to measure for our population to a sample of the data. </span><span class="kobospan" id="kobo.19.3">For example, suppose there is a company of 9,000 employees and we are interested in ascertaining the average length of breaks taken by employees in a single day. </span><span class="kobospan" id="kobo.19.4">As we probably cannot ask every single person, we will take a sample of the 9,000 people and take a mean of the sample. </span><span class="kobospan" id="kobo.19.5">This sample mean</span><a id="_idIndexMarker454" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.20.1"> will be our point estimate. </span><span class="kobospan" id="kobo.20.2">We will use the probability distribution, known as the Poisson distribution, to randomly generate 9,000 answers to the question </span><em class="italic"><span class="kobospan" id="kobo.21.1">For how many minutes in a day do you usually take breaks?</span></em><span class="kobospan" id="kobo.22.1"> This will represent our </span><em class="italic"><span class="kobospan" id="kobo.23.1">population</span></em><span class="kobospan" id="kobo.24.1">. </span><span class="kobospan" id="kobo.24.2">Remember, from </span><a href="B19488_06.xhtml#_idTextAnchor170" class="pcalibre calibre4 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.25.1">Chapter 6</span></em></span></a><span class="kobospan" id="kobo.26.1">, </span><em class="italic"><span class="kobospan" id="kobo.27.1">Advanced Probability</span></em><span class="kobospan" id="kobo.28.1">, that the Poisson random variable is used when we know the average value of an event and wish to model a distribution </span><span><span class="kobospan" id="kobo.29.1">around it.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.30.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.31.1">I set a random seed in order to encourage reproducibility (this allows us to get the same random numbers </span><span><span class="kobospan" id="kobo.32.1">each time).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.33.1">We will take a sample</span><a id="_idIndexMarker455" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.34.1"> of 100 employees (using the Python random sample method) and find</span><a id="_idIndexMarker456" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.35.1"> a point estimate of a mean (called a </span><span><strong class="bold"><span class="kobospan" id="kobo.36.1">sample mean</span></strong></span><span><span class="kobospan" id="kobo.37.1">).</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.38.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.39.1">Note that this is just over 1% of </span><span><span class="kobospan" id="kobo.40.1">our population.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.41.1">Compare our sample mean (the mean of the sample of 100 employees) to our </span><span><span class="kobospan" id="kobo.42.1">population mean.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.43.1">Let’s take a look at the </span><span><span class="kobospan" id="kobo.44.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.45.1">
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import poisson
np.random.seed(1234)
# represents 3000 people who take about a 60 minute break
long_breaks = stats.poisson.rvs(mu=60, size=3000)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.46.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.47.1">long_breaks</span></strong><span class="kobospan" id="kobo.48.1"> variable represents 3,000 answers to the question, </span><em class="italic"><span class="kobospan" id="kobo.49.1">How many minutes on average do you take breaks for?</span></em><span class="kobospan" id="kobo.50.1">, and these answers will be on the longer side. </span><span class="kobospan" id="kobo.50.2">Let’s see a visualization of this distribution, </span><span><span class="kobospan" id="kobo.51.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.52.1">
pd.Series(long_breaks).hist()</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer198">
<span class="kobospan" id="kobo.53.1"><img alt="Figure 8.1 – The histogram of our longer break times with a known average of 60 minutes" src="image/B19488_08_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.54.1">Figure 8.1 – The histogram of our longer break times with a known average of 60 minutes</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.55.1">We can see that our average of 60 minutes is to the left of the distribution. </span><span class="kobospan" id="kobo.55.2">Also, because we only sampled 3,000 people, our bars are at their highest at around </span><span><span class="kobospan" id="kobo.56.1">700–800 people.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.57.1">Now, let’s model 6,000 people</span><a id="_idIndexMarker457" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.58.1"> who take, on average, about 15 minutes’ worth </span><span><span class="kobospan" id="kobo.59.1">of breaks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.60.1">Let’s again use the Poisson distribution to simulate 6,000 people, </span><span><span class="kobospan" id="kobo.61.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.62.1">
# represents 6000 people who take about a 15 minute break
short_breaks = stats.poisson.rvs(mu=15, size=6000)
pd.Series(short_breaks).hist()</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer199">
<span class="kobospan" id="kobo.63.1"><img alt="Figure 8.2 – The histogram of our shorter break times with a known average of 15 minutes" src="image/B19488_08_02.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.64.1">Figure 8.2 – The histogram of our shorter break times with a known average of 15 minutes</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.65.1">Okay, so we have a distribution</span><a id="_idIndexMarker458" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.66.1"> for the people who take longer breaks and a distribution for the people who take shorter breaks. </span><span class="kobospan" id="kobo.66.2">Again, note how our average break length of 15 minutes falls to the left-hand side of the distribution, and note that the tallest bar is for about </span><span><span class="kobospan" id="kobo.67.1">1,600 people:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.68.1">
breaks = np.concatenate((long_breaks, short_breaks))
# put the two arrays together to get our "population" of 9000 people</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.69.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.70.1">breaks</span></strong><span class="kobospan" id="kobo.71.1"> variable is the amalgamation of all the 9,000 employees, both long and short break-takers. </span><span class="kobospan" id="kobo.71.2">Let’s see the entire distribution of people in a </span><span><span class="kobospan" id="kobo.72.1">single visualization:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.73.1">
pd.Series(breaks).hist()</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer200">
<span class="kobospan" id="kobo.74.1"><img alt="Figure 8.3 – The histogram of our two types of break-takers" src="image/B19488_08_03.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.75.1">Figure 8.3 – The histogram of our two types of break-takers</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.76.1">We can see we have two humps. </span><span class="kobospan" id="kobo.76.2">On the left, we have</span><a id="_idIndexMarker459" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.77.1"> our larger hump of people who take about a 15-minute break, and on the right, we have a smaller hump of people who take longer breaks. </span><span class="kobospan" id="kobo.77.2">Later on, we will investigate this </span><span><span class="kobospan" id="kobo.78.1">graph further.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.79.1">We can find the total average break length by running the </span><span><span class="kobospan" id="kobo.80.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.81.1">
breaks.mean()
# 29.99 minutes is our parameter.</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.82.1">Our average company break length is about 40 minutes. </span><span class="kobospan" id="kobo.82.2">Remember that our population is the entire company’s employee size of 9,000 people, and our parameter is 40 minutes. </span><span class="kobospan" id="kobo.82.3">In the real world, our goal would be to estimate the population parameter because we would not have the resources to ask every single employee in a survey their average break length for many reasons. </span><span class="kobospan" id="kobo.82.4">Instead, we will use a </span><span><span class="kobospan" id="kobo.83.1">point estimate.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.84.1">So, to make our point, we want to simulate a world where we ask 100 random people about the length of their breaks. </span><span class="kobospan" id="kobo.84.2">To do this, let’s take a random sample of 100 employees out of the 9,000 employees we simulated, </span><span><span class="kobospan" id="kobo.85.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.86.1">
sample_breaks = np.random.choice(a = breaks, size=100)
# taking a sample of 100 employees</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.87.1">Now, let’s take the mean of the sample and subtract it from the population mean and see how far off </span><span><span class="kobospan" id="kobo.88.1">we were:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.89.1">
breaks.mean() - sample_breaks.mean()
# difference between means is 0.699 minutes, not bad!</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.90.1">This is extremely interesting because, with only about 1% of our population (100 out of 9,000), we were able to get within 1 minute of our population parameter and get a very accurate estimate of our population mean. </span><span><span class="kobospan" id="kobo.91.1">Not bad!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.92.1">We calculated a point estimate for the mean, but we can also do this for proportion parameters. </span><span class="kobospan" id="kobo.92.2">By proportion, I am referring to a ratio of two </span><span><span class="kobospan" id="kobo.93.1">quantitative values.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">Let’s suppose that in a company</span><a id="_idIndexMarker460" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.95.1"> of 10,000 people, our employees are 20% white, 10% black, 10% Hispanic, 30% Asian, and 30% identify as other. </span><span class="kobospan" id="kobo.95.2">We will take a sample of 1,000 employees and see whether their race proportions </span><span><span class="kobospan" id="kobo.96.1">are similar:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.97.1">
employee_races = (["white"]*2000) + (["black"]*1000) +\
(["hispanic"]*1000) + (["asian"]*3000) +\
(["other"]*3000)</span></pre>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.98.1">employee_races</span></strong><span class="kobospan" id="kobo.99.1"> represents our employee population. </span><span class="kobospan" id="kobo.99.2">For example, in our company of 10,000 people, 2,000 people are white (20%) and 3,000 people are </span><span><span class="kobospan" id="kobo.100.1">Asian (30%).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.101.1">Let’s take a random sample of 1,000 people, </span><span><span class="kobospan" id="kobo.102.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.103.1">
import random
demo_sample = random.sample(employee_races, 1000) # Sample 1000 value
for race in set(demo_sample):print( race + " proportion estimate:" )
print( demo_sample.count(race)/1000. </span><span class="kobospan1" id="kobo.103.2">)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.104.1">The output obtained would be </span><span><span class="kobospan" id="kobo.105.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.106.1">
hispanic proportion estimate:
0.103
white proportion estimate:
0.192
other proportion estimate:
0.288
black proportion estimate:
0.1
asian proportion estimate:
0.317</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.107.1">We can see that the race proportion</span><a id="_idIndexMarker461" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.108.1"> estimates are very close to the underlying population’s proportions. </span><span class="kobospan" id="kobo.108.2">For example, we got 10.3% for </span><strong class="source-inline"><span class="kobospan" id="kobo.109.1">hispanic</span></strong><span class="kobospan" id="kobo.110.1"> in our sample and the population proportion for </span><strong class="source-inline"><span class="kobospan" id="kobo.111.1">hispanic</span></strong> <span><span class="kobospan" id="kobo.112.1">was</span><a id="_idTextAnchor217" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.113.1"> 10%.</span></span></p>
<h1 id="_idParaDest-112" class="calibre6"><a id="_idTextAnchor218" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.114.1">Sampling distributions</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.115.1">In </span><a href="B19488_07.xhtml#_idTextAnchor192" class="pcalibre calibre4 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.116.1">Chapter 7</span></em></span></a><span class="kobospan" id="kobo.117.1">, </span><em class="italic"><span class="kobospan" id="kobo.118.1">What Are the Chances? </span><span class="kobospan" id="kobo.118.2">An Introduction to Statistics</span></em><span class="kobospan" id="kobo.119.1">, we mentioned how much</span><a id="_idIndexMarker462" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.120.1"> we love it when data follows the normal distribution. </span><span class="kobospan" id="kobo.120.2">One of the reasons for this is that many statistical tests (including the ones we will use in this chapter) rely on data that follows a normal pattern, and for the most part, a lot of real-world data is not normal (surprised?). </span><span class="kobospan" id="kobo.120.3">Take our employee break data, for example—you might think I was just being fancy creating data using the Poisson distribution, but I had a reason for this. </span><span class="kobospan" id="kobo.120.4">I specifically wanted non-normal data, </span><span><span class="kobospan" id="kobo.121.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.122.1">
pd.DataFrame(breaks).hist(bins=50,range=(5,100))</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer201">
<span class="kobospan" id="kobo.123.1"><img alt="Figure 8.4 – The histogram of our break-takers with a larger number of bins, showing more granularity" src="image/B19488_08_04.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.124.1">Figure 8.4 – The histogram of our break-takers with a larger number of bins, showing more granularity</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.125.1">As you can see, our data is definitely</span><a id="_idIndexMarker463" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.126.1"> not following a normal distribution; it appears to be bimodal, which means that there are two peaks of break times, at around 25 and 70 minutes. </span><span class="kobospan" id="kobo.126.2">As our data is not normal, many of the most popular statistics tests may not apply; however, if we follow the given procedure, we can create normal data! </span><span class="kobospan" id="kobo.126.3">Think I’m crazy? </span><span class="kobospan" id="kobo.126.4">Well, see </span><span><span class="kobospan" id="kobo.127.1">for yourself.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.128.1">First off, we will need to utilize what is known as a sampling distribution, which is a distribution of point estimates of several samples of the same size. </span><span class="kobospan" id="kobo.128.2">Our procedure for creating a sampling distribution will be </span><span><span class="kobospan" id="kobo.129.1">the following:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.130.1">Take 500 different samples of the break times of the size of </span><span><span class="kobospan" id="kobo.131.1">100 each.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.132.1">Take a histogram of these 500 different point estimates (revealing </span><span><span class="kobospan" id="kobo.133.1">their distribution).</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.134.1">The number of elements in the sample (100) was arbitrary but large enough to be a representative sample of the population. </span><span class="kobospan" id="kobo.134.2">The number of samples I took (500) was also arbitrary, but large enough to ensure that our data would converge to a </span><span><span class="kobospan" id="kobo.135.1">normal distribution:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.136.1">
point_estimates = []
for x in range(500): # Generate 500 samples
# take a sample of 100 points
sample = np.random.choice(a=breaks, size=100)
# add the sample mean to our list of point estimates
point_estimates.append( sample.mean() )
# look at the distribution of our sample means
pd.DataFrame(point_estimates).hist()</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer202">
<span class="kobospan" id="kobo.137.1"><img alt="Figure 8.5 – The distribution of sample means becomes much more normally distributed, a blessing of the central limit theorem" src="image/B19488_08_05.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.138.1">Figure 8.5 – The distribution of sample means becomes much more normally distributed, a blessing of the central limit theorem</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.139.1">Behold! </span><span class="kobospan" id="kobo.139.2">The sampling distribution</span><a id="_idIndexMarker464" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.140.1"> of the sample mean appears to be normal even though we took data from an underlying bimodal population distribution. </span><span class="kobospan" id="kobo.140.2">It is important to note that the bars in this histogram represent the average break length of 500 samples of employees, where each sample has 100 people in it. </span><span class="kobospan" id="kobo.140.3">In other words, a sampling distribution is a distribution of several </span><span><span class="kobospan" id="kobo.141.1">point estimates.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.142.1">Our data converged to a normal distribution because of something called the central limit theorem, which states that the sampling </span><a id="_idIndexMarker465" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.143.1">distribution (the distribution of point estimates) will approach a normal distribution as we increase the number of </span><span><span class="kobospan" id="kobo.144.1">samples taken.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.145.1">What’s more, as we take more and more samples, the mean of the sampling distribution will approach the true population mean, </span><span><span class="kobospan" id="kobo.146.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.147.1">
breaks.mean() - np.array(point_estimates).mean()
# .042 minutes difference</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.148.1">This is actually a very exciting</span><a id="_idIndexMarker466" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.149.1"> result because it means that we can get even closer than a single point estimate by taking multiple point estimates and utilizing the central </span><span><span class="kobospan" id="kobo.150.1">limit theorem!</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.151.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.152.1">In general, as we increase the number of samples taken, our estimate will get closer to the parameter (</span><span><span class="kobospan" id="kobo.153.1">actual</span><a id="_idTextAnchor219" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.154.1"> value).</span></span></p>
<h1 id="_idParaDest-113" class="calibre6"><a id="_idTextAnchor220" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.155.1">Confidence intervals</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.156.1">While point estimates are okay, estimates</span><a id="_idIndexMarker467" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.157.1"> of a population parameter and sampling distributions are even better. </span><span class="kobospan" id="kobo.157.2">There are the following two main issues with </span><span><span class="kobospan" id="kobo.158.1">these approaches:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.159.1">Single point estimates are very prone to error (due to sampling bias among </span><span><span class="kobospan" id="kobo.160.1">other things)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.161.1">Taking multiple samples of a certain size for sampling distributions might not be feasible, and may sometimes be even more infeasible than actually finding the </span><span><span class="kobospan" id="kobo.162.1">population parameter</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.163.1">For these reasons</span><a id="_idIndexMarker468" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.164.1"> and more, we may turn to a concept known as the </span><strong class="bold"><span class="kobospan" id="kobo.165.1">confidence interval</span></strong><span class="kobospan" id="kobo.166.1"> to </span><span><span class="kobospan" id="kobo.167.1">find statistics.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.168.1">A confidence interval is a range of values based on a point estimate that contains the true population parameter at some </span><span><span class="kobospan" id="kobo.169.1">confidence level.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.170.1">Confidence is an important concept in advanced statistics. </span><span class="kobospan" id="kobo.170.2">Its meaning is sometimes misconstrued. </span><span class="kobospan" id="kobo.170.3">Informally, a confidence level does not represent a </span><em class="italic"><span class="kobospan" id="kobo.171.1">probability of being correct</span></em><span class="kobospan" id="kobo.172.1">; instead, it represents the frequency at which the obtained answer will be accurate. </span><span class="kobospan" id="kobo.172.2">For example, if you want to have a 95% chance of capturing the true population parameter using only a single point estimate, you have to set your confidence level </span><span><span class="kobospan" id="kobo.173.1">to 95%.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.174.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.175.1">Higher confidence levels result in wider (larger) confidence intervals in order to be </span><span><span class="kobospan" id="kobo.176.1">more sure.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.177.1">Calculating a confidence interval</span><a id="_idIndexMarker469" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.178.1"> involves finding a point estimate and then incorporating a margin of error to create a range. </span><span class="kobospan" id="kobo.178.2">The margin of error is a value that represents our certainty that our point estimate is accurate and is based on our desired confidence level, the variance of the data, and how big our sample is. </span><span class="kobospan" id="kobo.178.3">There are many ways to calculate confidence intervals; for the purpose of brevity and simplicity, we will look at a single way of taking the confidence interval of a population mean. </span><span class="kobospan" id="kobo.178.4">For this confidence interval, we need </span><span><span class="kobospan" id="kobo.179.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.180.1">A point estimate. </span><span class="kobospan" id="kobo.180.2">For this, we will take our sample mean of break lengths from our </span><span><span class="kobospan" id="kobo.181.1">previous example.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.182.1">An estimate of the population standard deviation, which represents the variance in the data. </span><span class="kobospan" id="kobo.182.2">This is calculated by taking the sample standard deviation (the standard deviation of the sample data) and dividing that number by the square root of the </span><span><span class="kobospan" id="kobo.183.1">population size.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.184.1">The degrees of freedom (which is the sample size - </span><span><span class="kobospan" id="kobo.185.1">1).</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.186.1">Obtaining these numbers might seem arbitrary but, trust me, there is a reason for all of them. </span><span class="kobospan" id="kobo.186.2">However, again for simplicity, I will use prebuilt Python modules, as shown, to calculate our confidence interval and then demonstrate </span><span><span class="kobospan" id="kobo.187.1">its value:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.188.1">
import math
sample_size = 100
# the size of the sample we wish to take
sample = np.random.choice(a= breaks, size = sample_size)
a sample of sample_size taken from the 9,000 breaks population from before
sample_mean = sample.mean()
# the sample mean of the break lengths sample
         sample_stdev = sample.std()
# sample standard deviation
sigma = sample_stdev/math.sqrt(sample_size)
# population standard deviation estimate
stats.t.interval(confidence = 0.95, # Confidence level 95%
df= sample_size - 1, # Degrees of freedom
loc = sample_mean, # Sample mean
scale = sigma) # Standard deviation
# (24.28, 33.14)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.189.1">To reiterate, this range of values (from </span><strong class="source-inline"><span class="kobospan" id="kobo.190.1">24.28</span></strong><span class="kobospan" id="kobo.191.1"> to </span><strong class="source-inline"><span class="kobospan" id="kobo.192.1">33.14</span></strong><span class="kobospan" id="kobo.193.1">) represents a confidence interval for the average break time with </span><span><span class="kobospan" id="kobo.194.1">95% confidence.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.195.1">We already know that our population</span><a id="_idIndexMarker470" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.196.1"> parameter is </span><strong class="source-inline"><span class="kobospan" id="kobo.197.1">29.99</span></strong><span class="kobospan" id="kobo.198.1">, and note that the interval includes the population mean </span><span><span class="kobospan" id="kobo.199.1">of </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.200.1">29.99</span></strong></span><span><span class="kobospan" id="kobo.201.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.202.1">As mentioned earlier that the confidence level is not a percentage of the accuracy of our interval but the percent chance that the interval will even contain the population parameter </span><span><span class="kobospan" id="kobo.203.1">at all.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.204.1">To better understand the confidence level, let’s take 10,000 confidence intervals and see how often our population mean falls in the interval. </span><span class="kobospan" id="kobo.204.2">First, let’s make a function, as illustrated, that makes a single confidence interval from our </span><span><span class="kobospan" id="kobo.205.1">breaks data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.206.1">
# function to make confidence interval
def makeConfidenceInterval():
sample_size = 100
sample = np.random.choice(a= breaks, size = sample_size)
sample_mean = sample.mean()  # sample mean
sample_stdev = sample.std()
# sample standard deviation
sigma = sample_stdev/math.sqrt(sample_size)
# population Standard deviation estimate
return stats.t.interval(confidence = 0.95, df= sample_size - 1, loc = sample_mean, scale = sigma)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.207.1">Now that we have a function</span><a id="_idIndexMarker471" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.208.1"> that will create a single confidence interval, let’s create a procedure that will test the probability that a single confidence interval will contain the true population </span><span><span class="kobospan" id="kobo.209.1">parameter, </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.210.1">29.99</span></strong></span><span><span class="kobospan" id="kobo.211.1">:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.212.1">Take </span><strong class="source-inline1"><span class="kobospan" id="kobo.213.1">10,000</span></strong><span class="kobospan" id="kobo.214.1"> confidence intervals of the </span><span><span class="kobospan" id="kobo.215.1">sample mean.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.216.1">Count the number of times that the population parameter falls into our </span><span><span class="kobospan" id="kobo.217.1">confidence intervals.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.218.1">Output the ratio of the number of times the parameter fell into the interval </span><span><span class="kobospan" id="kobo.219.1">by 10,000:</span></span></li>
</ol>
<pre class="source-code"><span class="kobospan1" id="kobo.220.1">
breaks_mean = breaks.mean()
times_in_interval = 0
n = 10_000
for i in range(n):
    interval = makeConfidenceInterval()
    if breaks_mean &gt;= interval[0] and breaks_mean &lt;= interval[1]:  # if 29.99 falls in the interval
        times_in_interval += 1
print(times_in_interval / n)
# 0.9465</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.221.1">Success! </span><span class="kobospan" id="kobo.221.2">We see that about 95% of our confidence intervals contained our actual population mean. </span><span class="kobospan" id="kobo.221.3">Estimating population parameters through point estimates and confidence intervals is a relatively simple and powerful form of </span><span><span class="kobospan" id="kobo.222.1">statistical inference.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.223.1">Let’s also take a quick look at how the size of confidence intervals changes as we change our confidence level. </span><span class="kobospan" id="kobo.223.2">Let’s calculate confidence intervals for multiple confidence levels and look at how large the intervals are by looking at the difference between the two numbers. </span><span class="kobospan" id="kobo.223.3">Our hypothesis will be that as we make our confidence level larger, we will likely see larger confidence intervals to be sure that we catch the true </span><span><span class="kobospan" id="kobo.224.1">population parameter:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.225.1">
for confidence in (.5, .8, .85, .9, .95, .99):
confidence_interval = stats.t.interval(confidence = confidence, df= sample_size - 1, loc = sample_mean, scale = sigma)
length_of_interval = round(confidence_interval[1] - confidence_interval[0], 2)
# the length of the confidence interval
print( "confidence {0} has a interval of size {1}".format(confidence, length_of_interval))
confidence 0.5 has a interval of size 2.95
confidence 0.8 has a interval of size 5.63
confidence 0.85 has a interval of size 6.33
confidence 0.9 has a interval of size 7.24
confidence 0.95 has a interval of size 8.65
confidence 0.99 has a interval of size 11.45</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.226.1">We can see that as we wish to be </span><em class="italic"><span class="kobospan" id="kobo.227.1">more confident</span></em><span class="kobospan" id="kobo.228.1"> in our interval, our interval expands in order </span><span><span class="kobospan" id="kobo.229.1">to compensate.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.230.1">Next, we will take our concept </span><a id="_idIndexMarker472" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.231.1">of confidence levels and look at statistical hypothesis testing in order to both expand on these topics and create (usually) even more powerful </span><span><span class="kobospan" id="kobo.232.1">statistic</span><a id="_idTextAnchor221" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.233.1">al inferences.</span></span></p>
<h1 id="_idParaDest-114" class="calibre6"><a id="_idTextAnchor222" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.234.1">Hypothesis tests</span></h1>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.235.1">Hypothesis tests</span></strong><span class="kobospan" id="kobo.236.1"> are one of the most widely used tests </span><a id="_idIndexMarker473" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.237.1">in statistics. </span><span class="kobospan" id="kobo.237.2">They come in many forms; however, all of them have the same </span><span><span class="kobospan" id="kobo.238.1">basic purpose.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.239.1">A hypothesis test is a statistical test that is used to ascertain whether we are allowed to assume that a certain condition is true for the entire population, given a data sample. </span><span class="kobospan" id="kobo.239.2">Basically, a hypothesis test is a test for a certain hypothesis that we have about an entire population. </span><span class="kobospan" id="kobo.239.3">The result of the test then tells us whether we should believe the hypothesis or reject it for an </span><span><span class="kobospan" id="kobo.240.1">alternative one.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.241.1">You can think of the hypothesis </span><a id="_idIndexMarker474" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.242.1">test’s framework to determine whether the observed sample data deviates from what was to be expected from the population itself. </span><span class="kobospan" id="kobo.242.2">Now, this sounds like a difficult task but, luckily, Python comes to the rescue and includes built-in libraries to conduct these </span><span><span class="kobospan" id="kobo.243.1">tests easily.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">A hypothesis test generally</span><a id="_idIndexMarker475" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.245.1"> looks at two opposing hypotheses about a population. </span><span class="kobospan" id="kobo.245.2">We call them the </span><strong class="bold"><span class="kobospan" id="kobo.246.1">null hypothesis</span></strong><span class="kobospan" id="kobo.247.1"> and the </span><strong class="bold"><span class="kobospan" id="kobo.248.1">alternative hypothesis</span></strong><span class="kobospan" id="kobo.249.1">. </span><span class="kobospan" id="kobo.249.2">The null hypothesis is the statement being tested and is the default correct </span><a id="_idIndexMarker476" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.250.1">answer; it is our starting point and our original hypothesis. </span><span class="kobospan" id="kobo.250.2">The alternative hypothesis is the statement that opposes the null hypothesis. </span><span class="kobospan" id="kobo.250.3">Our test will tell us which hypothesis we should trust and which we </span><span><span class="kobospan" id="kobo.251.1">should reject.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.252.1">Based on sample data from a population, a hypothesis test determines whether or not to reject the null hypothesis. </span><span class="kobospan" id="kobo.252.2">We usually use a </span><em class="italic"><span class="kobospan" id="kobo.253.1">p</span></em><span class="kobospan" id="kobo.254.1"> value (which is based on our significance level) to make </span><span><span class="kobospan" id="kobo.255.1">this conclusion.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.256.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.257.1">A very common misconception is that statistical hypothesis tests are designed to select the more likely of the two hypotheses. </span><span class="kobospan" id="kobo.257.2">This is incorrect. </span><span class="kobospan" id="kobo.257.3">A hypothesis test will default to the null hypothesis until there is enough data to support the </span><span><span class="kobospan" id="kobo.258.1">alternative hypothesis.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.259.1">The following are some examples of questions you can answer with a </span><span><span class="kobospan" id="kobo.260.1">hypothesis test:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.261.1">Does the mean break time of employees differ from </span><span><span class="kobospan" id="kobo.262.1">40 minutes?</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.263.1">Is there a difference between people who interacted with website A and people who interacted with website B (</span><span><span class="kobospan" id="kobo.264.1">A/B testing)?</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.265.1">Does a sample of coffee beans vary significantly in taste from the entire po</span><a id="_idTextAnchor223" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.266.1">pulation </span><span><span class="kobospan" id="kobo.267.1">of beans?</span></span></li>
</ul>
<h2 id="_idParaDest-115" class="calibre7"><a id="_idTextAnchor224" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.268.1">Conducting a hypothesis test</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.269.1">There are multiple types of hypothesis</span><a id="_idIndexMarker477" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.270.1"> tests out there, and among them are dozens of different procedures and metrics. </span><span class="kobospan" id="kobo.270.2">Nonetheless, there are five basic steps that most hypothesis tests follow, which are </span><span><span class="kobospan" id="kobo.271.1">as follows:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.272.1">Specify </span><span><span class="kobospan" id="kobo.273.1">the hypotheses:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.274.1">Here, we formulate our two hypotheses: the null and </span><span><span class="kobospan" id="kobo.275.1">the alternative.</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.276.1">We usually use the notation of </span><span class="kobospan" id="kobo.277.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/140.png" class="calibre151"/></span><span class="kobospan" id="kobo.278.1">to represent the null hypothesis and </span><span class="kobospan" id="kobo.279.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/141.png" class="calibre152"/></span><span class="kobospan" id="kobo.280.1"> to represent our </span><span><span class="kobospan" id="kobo.281.1">alternative hypothesis.</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.282.1">Determine the sample size for the </span><span><span class="kobospan" id="kobo.283.1">test sample:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.284.1">This calculation depends on the chosen test. </span><span class="kobospan" id="kobo.284.2">Usually, we have to determine a proper sample size in order to utilize theorems, such as the central limit theorem, and assume the normality </span><span><span class="kobospan" id="kobo.285.1">of data.</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.286.1">Choose a significance level (usually called alpha </span><span><span class="kobospan" id="kobo.287.1">or </span></span><span><em class="italic"><span class="kobospan" id="kobo.288.1">α</span></em></span><span><span class="kobospan" id="kobo.289.1">):</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.290.1">A significance level of 0.05 </span><span><span class="kobospan" id="kobo.291.1">is common.</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.292.1">Collect </span><span><span class="kobospan" id="kobo.293.1">the data:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.294.1">Collect a sample of data to conduct </span><span><span class="kobospan" id="kobo.295.1">the test.</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.296.1">Decide whether to reject or fail to reject the </span><span><span class="kobospan" id="kobo.297.1">null hypothesis:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.298.1">This step changes slightly based on the type of test being used. </span><span class="kobospan" id="kobo.298.2">The final result will either yield a rejection of the null hypothesis in favor of the alternative or fail to reject the </span><span><span class="kobospan" id="kobo.299.1">null hypothesis.</span></span></li></ul></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.300.1">In this chapter, we will look at the following three types of </span><span><span class="kobospan" id="kobo.301.1">hypothesis tests:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span><span class="kobospan" id="kobo.302.1">One-sample t-tests</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.303.1">Chi-square goodness </span><span><span class="kobospan" id="kobo.304.1">of fit</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.305.1">Chi-square test </span><span><span class="kobospan" id="kobo.306.1">for association/independence</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.307.1">There are many more tests. </span><span class="kobospan" id="kobo.307.2">However, these three are a great combination of distinct, simple, and powerful tests. </span><span class="kobospan" id="kobo.307.3">One of the biggest things to consider when choosing which test we should implement is the type of data we are working with—specifically, whether we are dealing with continuous or categorical data. </span><span class="kobospan" id="kobo.307.4">In order to truly see the effects of a hypothesis, I suggest </span><a id="_idIndexMarker478" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.308.1">we dive right into an example. </span><span class="kobospan" id="kobo.308.2">First, let’s look at the use of </span><em class="italic"><span class="kobospan" id="kobo.309.1">t</span></em><span class="kobospan" id="kobo.310.1">-tests to deal w</span><a id="_idTextAnchor225" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.311.1">ith </span><span><span class="kobospan" id="kobo.312.1">continuous data.</span></span></p>
<h2 id="_idParaDest-116" class="calibre7"><a id="_idTextAnchor226" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.313.1">One-sample t-tests</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.314.1">The one-sample </span><em class="italic"><span class="kobospan" id="kobo.315.1">t</span></em><span class="kobospan" id="kobo.316.1">-test is a statistical</span><a id="_idIndexMarker479" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.317.1"> test used to determine whether a quantitative (numerical) data sample differs significantly from another dataset (the population or another sample). </span><span class="kobospan" id="kobo.317.2">Suppose, in our previous employee break time example, we look specifically at the engineering department’s break times, </span><span><span class="kobospan" id="kobo.318.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.319.1">
long_breaks_in_engineering = stats.poisson.rvs(loc=10, mu=55, size=100)
short_breaks_in_engineering = stats.poisson.rvs(loc=10, mu=15, size=300)
engineering_breaks = np.concatenate((long_breaks_in_engineering, short_breaks_in_engineering))
print(breaks.mean())
# 29.99
print(engineering_breaks.mean())
# 34.825</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.320.1">Note that I took the same approach as making the original break times but with the following </span><span><span class="kobospan" id="kobo.321.1">two differences:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.322.1">I took a smaller sample from the Poisson distribution (to simulate that we took a sample of 400 people from the </span><span><span class="kobospan" id="kobo.323.1">engineering department)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.324.1">Instead of using </span><strong class="source-inline1"><span class="kobospan" id="kobo.325.1">mu</span></strong><span class="kobospan" id="kobo.326.1"> of </span><strong class="source-inline1"><span class="kobospan" id="kobo.327.1">60</span></strong><span class="kobospan" id="kobo.328.1"> as before, I used </span><strong class="source-inline1"><span class="kobospan" id="kobo.329.1">55</span></strong><span class="kobospan" id="kobo.330.1"> to simulate the fact that the engineering department’s break behavior isn’t exactly the same as the company’s behavior as </span><span><span class="kobospan" id="kobo.331.1">a whole</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.332.1">It is easy to see that there seems to be a difference (of over 5 minutes) between the engineering department and the company as a whole. </span><span class="kobospan" id="kobo.332.2">We usually don’t have the entire population and the population parameters at our disposal, but I have them simulated in order for the example to work. </span><span class="kobospan" id="kobo.332.3">So, even though we (the omniscient readers) can see a difference, we will assume that we know nothing of these population parameters and, instead, rely </span><a id="_idIndexMarker480" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.333.1">on a statistical test in order to ascert</span><a id="_idTextAnchor227" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.334.1">ain </span><span><span class="kobospan" id="kobo.335.1">these differences.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.336.1">Example of a one-sample t-test</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.337.1">Our objective here is to ascertain</span><a id="_idIndexMarker481" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.338.1"> whether there is a difference between the overall population’s (company employees) break times and the break times of employees in the </span><span><span class="kobospan" id="kobo.339.1">engineering department.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.340.1">Let’s now conduct a </span><em class="italic"><span class="kobospan" id="kobo.341.1">t</span></em><span class="kobospan" id="kobo.342.1">-test at a 95% confidence level in order to find a difference (or not!). </span><span class="kobospan" id="kobo.342.2">Technically speaking, this test will tell us whether the sample comes from the same distribu</span><a id="_idTextAnchor228" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.343.1">tion as </span><span><span class="kobospan" id="kobo.344.1">the population.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.345.1">Assumptions of the one-sample t-test</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.346.1">Before diving into the five steps, we</span><a id="_idIndexMarker482" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.347.1"> must first acknowledge that </span><em class="italic"><span class="kobospan" id="kobo.348.1">t</span></em><span class="kobospan" id="kobo.349.1">-tests must satisfy the following two conditions to </span><span><span class="kobospan" id="kobo.350.1">work properly:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.351.1">The population distribution should be normal, or the sample should be large (</span><em class="italic"><span class="kobospan" id="kobo.352.1">n</span></em><span class="kobospan" id="kobo.353.1"> ≥ </span><span><span class="kobospan" id="kobo.354.1">30)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.355.1">In order to make the assumption that the sample is independently, randomly sampled, it is sufficient to enforce that the population size should be at least 10 times larger than the sample size (</span><em class="italic"><span class="kobospan" id="kobo.356.1">10n</span></em><span class="kobospan" id="kobo.357.1"> &lt; </span><span><em class="italic"><span class="kobospan" id="kobo.358.1">N</span></em></span><span><span class="kobospan" id="kobo.359.1">)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.360.1">Note that our test requires that either the underlying data be normal (which we know is not true for us), or that the sample size is at least 30 points large. </span><span class="kobospan" id="kobo.360.2">For the </span><em class="italic"><span class="kobospan" id="kobo.361.1">t</span></em><span class="kobospan" id="kobo.362.1">-test, this condition is sufficient to assume normality. </span><span class="kobospan" id="kobo.362.2">This test also requires independence, which is satisfied by taking a sufficiently small sample. </span><span class="kobospan" id="kobo.362.3">Sounds weird, right? </span><span class="kobospan" id="kobo.362.4">The basic idea is that our sample must be large enough to assume normality (through conclusions similar to the central limit theorem) but small enough to be independent of </span><span><span class="kobospan" id="kobo.363.1">the population.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.364.1">Now, let’s follow our </span><span><span class="kobospan" id="kobo.365.1">five steps:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.366.1">Specify </span><span><span class="kobospan" id="kobo.367.1">the hypotheses.</span></span><p class="calibre3"><span class="kobospan" id="kobo.368.1">We will let </span><span class="kobospan" id="kobo.369.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre153"/></span><span class="kobospan" id="kobo.370.1">= the engineering department take breaks the same as the company as </span><span><span class="kobospan" id="kobo.371.1">a whole.</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.372.1">If we let this be the company average, we may write </span><span><span class="kobospan" id="kobo.373.1">the following:</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.374.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre153"/></span><span class="kobospan" id="kobo.375.1">:</span><em class="italic"><span class="kobospan" id="kobo.376.1">(engineering takes breaks the same length as </span></em><span><em class="italic"><span class="kobospan" id="kobo.377.1">everyone else)</span></em></span></p></li>
</ol>
<p class="callout-heading"><span class="kobospan" id="kobo.378.1">Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.379.1">Note how this is our null, or default, hypothesis. </span><span class="kobospan" id="kobo.379.2">It is what we would assume, given no data. </span><span class="kobospan" id="kobo.379.3">What we would like to show is the </span><span><span class="kobospan" id="kobo.380.1">alternative hypothesis.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.381.1">Now that we actually have</span><a id="_idIndexMarker483" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.382.1"> some options for our alternative, we could either say that the engineering mean (let’s call it that) is lower than the company average, higher than the company average, or just flat-out different (higher or lower) from the </span><span><span class="kobospan" id="kobo.383.1">company average:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.384.1">If we wish to answer the question, </span><em class="italic"><span class="kobospan" id="kobo.385.1">Is the sample mean different from the company average?</span></em><span class="kobospan" id="kobo.386.1">, then this is called a </span><strong class="bold"><span class="kobospan" id="kobo.387.1">two-tailed test</span></strong><span class="kobospan" id="kobo.388.1"> and our alternative hypothesis</span><a id="_idIndexMarker484" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.389.1"> would be </span><span><span class="kobospan" id="kobo.390.1">as follows:</span></span></li>
</ul>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.391.1">Ha:(engineering takes breaks of different lengths than the rest of </span></em><span><em class="italic"><span class="kobospan" id="kobo.392.1">the company)</span></em></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.393.1">If we want to find out whether the sample</span><a id="_idIndexMarker485" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.394.1"> mean is lower than the company average or the sample mean is higher than the company average, then we are dealing with a </span><strong class="bold"><span class="kobospan" id="kobo.395.1">one-tailed test</span></strong><span class="kobospan" id="kobo.396.1"> and our alternative hypothesis would be one of the </span><span><span class="kobospan" id="kobo.397.1">following hypotheses:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.398.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/144.png" class="calibre154"/></span><span class="kobospan" id="kobo.399.1">:</span><em class="italic"><span class="kobospan" id="kobo.400.1">(engineering takes </span></em><span><em class="italic"><span class="kobospan" id="kobo.401.1">longer breaks)</span></em></span></li><li class="calibre14"><span class="kobospan" id="kobo.402.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/144.png" class="calibre154"/></span><span class="kobospan" id="kobo.403.1">:</span><em class="italic"><span class="kobospan" id="kobo.404.1">(engineering takes </span></em><span><em class="italic"><span class="kobospan" id="kobo.405.1">shorter breaks)</span></em></span></li></ul></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.406.1">The difference between one and two tails is the difference of dividing a number later on by two or not. </span><span class="kobospan" id="kobo.406.2">The process remains completely unchanged for both. </span><span class="kobospan" id="kobo.406.3">For this example, let’s choose the two-tailed test. </span><span class="kobospan" id="kobo.406.4">So, we are testing for whether or not this sample of the engineering department’s average break times is different from the </span><span><span class="kobospan" id="kobo.407.1">company average.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.408.1">Our test will end in one of two possible conclusions: we will either reject the null hypothesis, which means that the engineering department’s break times are different from the company average, or we will fail to reject the null hypothesis, which means that there wasn’t enough evidence in the sample to support rejecting </span><span><span class="kobospan" id="kobo.409.1">the null.</span></span></p>
<ol class="calibre13">
<li value="2" class="calibre14"><span class="kobospan" id="kobo.410.1">Determine the sample</span><a id="_idIndexMarker486" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.411.1"> size for the </span><span><span class="kobospan" id="kobo.412.1">test sample.</span></span><p class="calibre3"><span class="kobospan" id="kobo.413.1">As mentioned earlier, most tests (including this one) make the assumption that either the underlying data is normal or that our sample is in the </span><span><span class="kobospan" id="kobo.414.1">right range:</span></span></p><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.415.1">The sample is at least 30 points (it </span><span><span class="kobospan" id="kobo.416.1">is 400)</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.417.1">The sample is less than 10% of the population (which would be </span><span><span class="kobospan" id="kobo.418.1">900 people)</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.419.1">Choose a significance level (usually called alpha or </span><em class="italic"><span class="kobospan" id="kobo.420.1">α</span></em><span class="kobospan" id="kobo.421.1">). </span><span class="kobospan" id="kobo.421.2">We will choose a 95% significance level, which means that our alpha would actually be </span><em class="italic"><span class="kobospan" id="kobo.422.1">1 - .95 = .</span></em><span><em class="italic"><span class="kobospan" id="kobo.423.1">05</span></em></span><span><span class="kobospan" id="kobo.424.1">.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.425.1">Collect the data. </span><span class="kobospan" id="kobo.425.2">This is generated through the two </span><span><span class="kobospan" id="kobo.426.1">Poisson distributions.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.427.1">Decide whether to reject or fail to reject the null hypothesis. </span><span class="kobospan" id="kobo.427.2">As mentioned before, this step varies based on the test used. </span><span class="kobospan" id="kobo.427.3">For a one-sample </span><em class="italic"><span class="kobospan" id="kobo.428.1">t</span></em><span class="kobospan" id="kobo.429.1">-test, we must calculate two numbers: the test statistic and our </span><em class="italic"><span class="kobospan" id="kobo.430.1">p</span></em><span class="kobospan" id="kobo.431.1"> value. </span><span class="kobospan" id="kobo.431.2">Luckily, we can do this in one line </span><span><span class="kobospan" id="kobo.432.1">in Python.</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.433.1">A test statistic is a value that is derived from sample data during a type of hypothesis test. </span><span class="kobospan" id="kobo.433.2">They are used to determine whether or not to reject the </span><span><span class="kobospan" id="kobo.434.1">null hypothesis.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.435.1">The test statistic is used to compare the observed data with what is expected under the null hypothesis. </span><span class="kobospan" id="kobo.435.2">The test statistic is used in conjunction with the </span><span><em class="italic"><span class="kobospan" id="kobo.436.1">p</span></em></span><span><span class="kobospan" id="kobo.437.1"> value.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.438.1">The </span><em class="italic"><span class="kobospan" id="kobo.439.1">p</span></em><span class="kobospan" id="kobo.440.1"> value is the probability that the observed data occurred this way </span><span><span class="kobospan" id="kobo.441.1">by chance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.442.1">When the data shows very strong evidence against the null hypothesis, the test statistic becomes large (either positive or negative) and the </span><em class="italic"><span class="kobospan" id="kobo.443.1">p</span></em><span class="kobospan" id="kobo.444.1"> value usually becomes very small, which means that our test is showing powerful results and what is happening is probably not happening </span><span><span class="kobospan" id="kobo.445.1">by chance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.446.1">In the case of a </span><em class="italic"><span class="kobospan" id="kobo.447.1">t</span></em><span class="kobospan" id="kobo.448.1">-test, a </span><em class="italic"><span class="kobospan" id="kobo.449.1">t</span></em><span class="kobospan" id="kobo.450.1"> value is our test statistic, </span><span><span class="kobospan" id="kobo.451.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.452.1">
t_statistic, p_value = stats.ttest_1samp(a= engineering_breaks, popmean= breaks.mean())</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.453.1">We input the </span><strong class="source-inline"><span class="kobospan" id="kobo.454.1">engineering_breaks</span></strong><span class="kobospan" id="kobo.455.1"> variable (which holds 400 break times) and the population mean (</span><strong class="source-inline"><span class="kobospan" id="kobo.456.1">popmean</span></strong><span class="kobospan" id="kobo.457.1">), and we obtain the </span><span><span class="kobospan" id="kobo.458.1">following numbers:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.459.1">
t_statistic == -5.742
p_value == .00000018</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.460.1">The test result shows that the </span><em class="italic"><span class="kobospan" id="kobo.461.1">t</span></em><span class="kobospan" id="kobo.462.1"> value is </span><strong class="source-inline"><span class="kobospan" id="kobo.463.1">-5.742</span></strong><span class="kobospan" id="kobo.464.1">. </span><span class="kobospan" id="kobo.464.2">This is a standardized metric that reveals the deviation of the sample</span><a id="_idIndexMarker487" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.465.1"> mean from the null hypothesis. </span><span class="kobospan" id="kobo.465.2">The </span><em class="italic"><span class="kobospan" id="kobo.466.1">p</span></em><span class="kobospan" id="kobo.467.1"> value is what gives us our final answer. </span><span class="kobospan" id="kobo.467.2">Our </span><em class="italic"><span class="kobospan" id="kobo.468.1">p</span></em><span class="kobospan" id="kobo.469.1"> value tells us how often our result would appear by chance. </span><span class="kobospan" id="kobo.469.2">So, for example, if our </span><em class="italic"><span class="kobospan" id="kobo.470.1">p</span></em><span class="kobospan" id="kobo.471.1"> value was </span><strong class="source-inline"><span class="kobospan" id="kobo.472.1">.06</span></strong><span class="kobospan" id="kobo.473.1">, then that would mean we should expect to observe this data by chance about 6% of the time. </span><span class="kobospan" id="kobo.473.2">This means that about 6% of samples would yield results </span><span><span class="kobospan" id="kobo.474.1">like this.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.475.1">We are interested in how our </span><em class="italic"><span class="kobospan" id="kobo.476.1">p</span></em><span class="kobospan" id="kobo.477.1"> value compares to our </span><span><span class="kobospan" id="kobo.478.1">significance level:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.479.1">If the </span><em class="italic"><span class="kobospan" id="kobo.480.1">p</span></em><span class="kobospan" id="kobo.481.1"> value is less than the significance level, then we can reject the </span><span><span class="kobospan" id="kobo.482.1">null hypothesis</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.483.1">If the </span><em class="italic"><span class="kobospan" id="kobo.484.1">p</span></em><span class="kobospan" id="kobo.485.1"> value is greater than the significance level, then we failed to reject the </span><span><span class="kobospan" id="kobo.486.1">null hypothesis</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.487.1">Our </span><em class="italic"><span class="kobospan" id="kobo.488.1">p</span></em><span class="kobospan" id="kobo.489.1"> value is way lower than </span><strong class="source-inline"><span class="kobospan" id="kobo.490.1">.05</span></strong><span class="kobospan" id="kobo.491.1"> (our chosen significance level), which means that we may reject our null hypothesis in favor of the alternative. </span><span class="kobospan" id="kobo.491.2">This means that the engineering department seems to take different break lengths from the company as </span><span><span class="kobospan" id="kobo.492.1">a whole!</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.493.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.494.1">The use of </span><em class="italic"><span class="kobospan" id="kobo.495.1">p</span></em><span class="kobospan" id="kobo.496.1"> values is controversial. </span><span class="kobospan" id="kobo.496.2">Many journals have actually banned the use of </span><em class="italic"><span class="kobospan" id="kobo.497.1">p</span></em><span class="kobospan" id="kobo.498.1"> values in tests for significance. </span><span class="kobospan" id="kobo.498.2">This is because of the nature of the value. </span><span class="kobospan" id="kobo.498.3">Suppose our </span><em class="italic"><span class="kobospan" id="kobo.499.1">p</span></em><span class="kobospan" id="kobo.500.1"> value came out to </span><strong class="source-inline1"><span class="kobospan" id="kobo.501.1">.04</span></strong><span class="kobospan" id="kobo.502.1">. </span><span class="kobospan" id="kobo.502.2">This means that 4% of the time, our data just randomly happened to appear this way and is not significant in any way. </span><span class="kobospan" id="kobo.502.3">4% is not that small of a percentage! </span><span class="kobospan" id="kobo.502.4">For this reason, many people are switching to different statistical tests. </span><span class="kobospan" id="kobo.502.5">However, that does not mean that </span><em class="italic"><span class="kobospan" id="kobo.503.1">p</span></em><span class="kobospan" id="kobo.504.1"> values are useless. </span><span class="kobospan" id="kobo.504.2">It merely means that we must be careful and aware of what the number is </span><span><span class="kobospan" id="kobo.505.1">telling us.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.506.1">There are many other types of </span><em class="italic"><span class="kobospan" id="kobo.507.1">t</span></em><span class="kobospan" id="kobo.508.1">-tests, including one-tailed tests (mentioned before) and paired tests as well </span><a id="_idIndexMarker488" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.509.1">as two-sample </span><em class="italic"><span class="kobospan" id="kobo.510.1">t</span></em><span class="kobospan" id="kobo.511.1">-tests (both not mentioned yet). </span><span class="kobospan" id="kobo.511.2">These procedures can be readily found in statistics literature; however, we should look at something very important—what</span><a id="_idTextAnchor229" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.512.1"> happens when we get </span><span><span class="kobospan" id="kobo.513.1">it wrong.</span></span></p>
<h2 id="_idParaDest-117" class="calibre7"><a id="_idTextAnchor230" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.514.1">Type I and Type II errors</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.515.1">In the realm of statistical hypothesis testing, two kinds of errors can occur: Type I and Type II errors. </span><span class="kobospan" id="kobo.515.2">These errors are synonymous with the concepts of false positives and false negatives, respectively. </span><span class="kobospan" id="kobo.515.3">Grasping these concepts is crucial as they underscore the potential limitations and risks associated with inferential statistics, where conclusions about a population are drawn from </span><span><span class="kobospan" id="kobo.516.1">sample data.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.517.1">Type I errors explained</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.518.1">A Type I error is often referred to as a </span><strong class="bold"><span class="kobospan" id="kobo.519.1">false positive</span></strong><span class="kobospan" id="kobo.520.1">. </span><span class="kobospan" id="kobo.520.2">Imagine you have an app</span><a id="_idIndexMarker489" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.521.1"> on your phone designed</span><a id="_idIndexMarker490" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.522.1"> to identify bird songs from snippets of music it hears. </span><span class="kobospan" id="kobo.522.2">If the app indicates it recognizes a bird from the ambient noise when there is no bird nearby, it has made a Type I error—it has alerted you to a “hit” when there was none. </span><span class="kobospan" id="kobo.522.3">In statistical hypothesis testing, this error occurs when we reject a true null hypothesis. </span><span class="kobospan" id="kobo.522.4">The null hypothesis usually represents a default position or a statement of no effect—for example, the assumption that a new drug has no effect on </span><span><span class="kobospan" id="kobo.523.1">a disease.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.524.1">When setting up a hypothesis test, we determine the significance level (denoted as </span><em class="italic"><span class="kobospan" id="kobo.525.1">α</span></em><span class="kobospan" id="kobo.526.1">), which defines the threshold for how much evidence is required to reject the null hypothesis. </span><span class="kobospan" id="kobo.526.2">Commonly, a 5% significance level is used, meaning there is a 5% chance of rejecting the null hypothesis when it is actually true. </span><span class="kobospan" id="kobo.526.3">This 5% is the risk we are willing to take of making a Type </span><span><span class="kobospan" id="kobo.527.1">I error.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.528.1">Type II errors explained</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.529.1">Conversely, a Type II error is when we overlook</span><a id="_idIndexMarker491" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.530.1"> something significant—a false negative. </span><span class="kobospan" id="kobo.530.2">Consider a medical test designed to detect a disease. </span><span class="kobospan" id="kobo.530.3">If the test results come back negative when the person has the disease, the test has made a Type II error. </span><span class="kobospan" id="kobo.530.4">In the context of hypothesis testing, this error occurs when the null hypothesis is not rejected despite being false. </span><span class="kobospan" id="kobo.530.5">For example, we might conclude that the new drug has no effect on a disease when, in fact, </span><span><span class="kobospan" id="kobo.531.1">it does.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.532.1">The likelihood of a Type II error is represented by </span><em class="italic"><span class="kobospan" id="kobo.533.1">β</span></em><span class="kobospan" id="kobo.534.1">, and it is inversely related to the significance level </span><em class="italic"><span class="kobospan" id="kobo.535.1">α</span></em><span class="kobospan" id="kobo.536.1">. </span><span class="kobospan" id="kobo.536.2">As we lower the risk of committing a Type I error by choosing a smaller </span><em class="italic"><span class="kobospan" id="kobo.537.1">α</span></em><span class="kobospan" id="kobo.538.1"> (for example, setting a higher confidence level such as 99%), we inadvertently increase the risk of a Type II error. </span><span class="kobospan" id="kobo.538.2">This is because requiring stronger evidence to reject the null hypothesis (i.e., a higher confidence level) can make it harder to detect an </span><span><span class="kobospan" id="kobo.539.1">actual effect.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.540.1">Balancing these errors is a critical part of designing experiments and interpreting statistical results. </span><span class="kobospan" id="kobo.540.2">Researchers must decide on an acceptable balance between the risks of Type I and Type II errors, often based on the context of the research and the potential conse</span><a id="_idTextAnchor231" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.541.1">quences of </span><span><span class="kobospan" id="kobo.542.1">incorrect conclusions.</span></span></p>
<h2 id="_idParaDest-118" class="calibre7"><a id="_idTextAnchor232" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.543.1">Hypothesis testing for categorical variables</span></h2>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.544.1">T</span></em><span class="kobospan" id="kobo.545.1">-tests (among other tests) are hypothesis tests</span><a id="_idIndexMarker492" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.546.1"> that work to compare </span><a id="_idIndexMarker493" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.547.1">and contrast quantitative variables and underlying population distributions. </span><span class="kobospan" id="kobo.547.2">In this section, we will explore two new tests, both of which serve to</span><a id="_idIndexMarker494" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.548.1"> explore qualitative data. </span><span class="kobospan" id="kobo.548.2">They are both a form of test called </span><strong class="bold"><span class="kobospan" id="kobo.549.1">chi-square tests</span></strong><span class="kobospan" id="kobo.550.1">. </span><span class="kobospan" id="kobo.550.2">These two tests will perform the following two tasks </span><span><span class="kobospan" id="kobo.551.1">for us:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.552.1">Determine whether a sample of categorical variables is taken from a specific population (similar to </span><span><span class="kobospan" id="kobo.553.1">the </span></span><span><em class="italic"><span class="kobospan" id="kobo.554.1">t</span></em></span><span><span class="kobospan" id="kobo.555.1">-test)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.556.1">Determine whether two variables affect each othe</span><a id="_idTextAnchor233" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.557.1">r and are associated with </span><span><span class="kobospan" id="kobo.558.1">each other</span></span></li>
</ul>
<h2 id="_idParaDest-119" class="calibre7"><a id="_idTextAnchor234" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.559.1">Chi-square goodness of fit test</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.560.1">The one-sample </span><em class="italic"><span class="kobospan" id="kobo.561.1">t</span></em><span class="kobospan" id="kobo.562.1">-test was used to check </span><a id="_idIndexMarker495" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.563.1">whether a sample mean differed from the population mean. </span><span class="kobospan" id="kobo.563.2">The chi-square goodness of fit test is very similar to the one-sample </span><em class="italic"><span class="kobospan" id="kobo.564.1">t</span></em><span class="kobospan" id="kobo.565.1">-test in that it tests whether the distribution of the sample data matches an expected distribution, while the big difference is that it tests for </span><span><span class="kobospan" id="kobo.566.1">categorical variables.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.567.1">For example, a chi-square goodness</span><a id="_idIndexMarker496" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.568.1"> of fit test would be used to see whether the race demographics of your company match that of the entire city of the U.S. </span><span class="kobospan" id="kobo.568.2">population. </span><span class="kobospan" id="kobo.568.3">It can also be used to see whether users of your website show similar characteristics to average </span><span><span class="kobospan" id="kobo.569.1">internet users.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.570.1">As we are working with categorical data, we have to be careful because categories such as “male,” “female,” or “other” don’t have any mathematical meaning. </span><span class="kobospan" id="kobo.570.2">Therefore, we must consider counts of the variables rather than the actual </span><span><span class="kobospan" id="kobo.571.1">variables themselves.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.572.1">In general, we use the chi-square goodness</span><a id="_idIndexMarker497" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.573.1"> of fit test in the </span><span><span class="kobospan" id="kobo.574.1">following cases:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.575.1">We want to analyze one categorical variable from </span><span><span class="kobospan" id="kobo.576.1">one population</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.577.1">We want to determine whether a variable fits a specified or </span><span><span class="kobospan" id="kobo.578.1">expected distribution</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.579.1">In a chi-square test, we comp</span><a id="_idTextAnchor235" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.580.1">are what is observed to what </span><span><span class="kobospan" id="kobo.581.1">we expect.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.582.1">Assumptions of the chi-square goodness of fit test</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.583.1">There are two usual assumptions</span><a id="_idIndexMarker498" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.584.1"> of this test, </span><span><span class="kobospan" id="kobo.585.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.586.1">All the expected counts are at </span><span><span class="kobospan" id="kobo.587.1">least </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.588.1">5</span></strong></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.589.1">Individual observations are independent and the population should be at least 10 times as large as the sample (</span><em class="italic"><span class="kobospan" id="kobo.590.1">10n</span></em><span class="kobospan" id="kobo.591.1"> &lt; </span><span><em class="italic"><span class="kobospan" id="kobo.592.1">N</span></em></span><span><span class="kobospan" id="kobo.593.1">)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.594.1">The second assumption should look familiar to the </span><em class="italic"><span class="kobospan" id="kobo.595.1">t</span></em><span class="kobospan" id="kobo.596.1">-test; however, the first assumption should look foreign. </span><span class="kobospan" id="kobo.596.2">Expected counts are something we haven’t talked about yet but are </span><span><span class="kobospan" id="kobo.597.1">about to!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.598.1">When formulating our null and alternative hypotheses for this test, we consider a default distribution of categorical variables. </span><span class="kobospan" id="kobo.598.2">For example, if we have a die and we are testing whether or not the outcomes are coming from a fair die, our hypothesis might look </span><span><span class="kobospan" id="kobo.599.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.600.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre153"/></span><span class="kobospan" id="kobo.601.1">: The specified distribution of the categorical variable </span><span><span class="kobospan" id="kobo.602.1">is correct.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.603.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/147.png" class="calibre155"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.604.1">Our alternative hypothesis is quite simple, </span><span><span class="kobospan" id="kobo.605.1">as shown:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.606.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/148.png" class="calibre156"/></span><span class="kobospan" id="kobo.607.1">: The specified distribution of the categorical variable is not correct. </span><span class="kobospan" id="kobo.607.2">At least one of the pi values is </span><span><span class="kobospan" id="kobo.608.1">not correct.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.609.1">In the </span><em class="italic"><span class="kobospan" id="kobo.610.1">t</span></em><span class="kobospan" id="kobo.611.1">-test, we used our test statistic (the </span><em class="italic"><span class="kobospan" id="kobo.612.1">t</span></em><span class="kobospan" id="kobo.613.1"> value) to find our </span><em class="italic"><span class="kobospan" id="kobo.614.1">p</span></em><span class="kobospan" id="kobo.615.1"> value. </span><span class="kobospan" id="kobo.615.2">In a chi-square test, our test statistic is, well, </span><span><span class="kobospan" id="kobo.616.1">a chi-square:</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.617.1">Test Statistic: χ2 =</span></em><em class="italic"><span class="kobospan" id="kobo.618.1">
over k categories</span></em></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.619.1">Degrees of Freedom = k − 1</span></em></p>
<p class="calibre3"><span class="kobospan" id="kobo.620.1">A critical value is when we use </span><span class="kobospan" id="kobo.621.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;χ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/149.png" class="calibre157"/></span><span class="kobospan" id="kobo.622.1">as well as our degrees of freedom and our significance level, and then reject</span><a id="_idIndexMarker499" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.623.1"> the null hypothesis if the </span><em class="italic"><span class="kobospan" id="kobo.624.1">p</span></em><span class="kobospan" id="kobo.625.1"> value is below our significance level (the same </span><span><span class="kobospan" id="kobo.626.1">as before).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.627.1">Let’s look </span><a id="_idTextAnchor236" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.628.1">at an example to understand </span><span><span class="kobospan" id="kobo.629.1">this further.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.630.1">Example of a chi-square test for goodness of fit</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.631.1">The CDC categorizes</span><a id="_idIndexMarker500" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.632.1"> adult BMIs into four classes: </span><strong class="source-inline"><span class="kobospan" id="kobo.633.1">Under/Normal</span></strong><span class="kobospan" id="kobo.634.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.635.1">Over</span></strong><span class="kobospan" id="kobo.636.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.637.1">Obesity</span></strong><span class="kobospan" id="kobo.638.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.639.1">Extreme Obesity</span></strong><span class="kobospan" id="kobo.640.1">. </span><span class="kobospan" id="kobo.640.2">A 2009 survey showed the distribution for adults in the US to be 31.2%, 33.1%, 29.4%, and 6.3%, respectively. </span><span class="kobospan" id="kobo.640.3">A total of 500 adults were randomly sampled and their BMI categories </span><span><span class="kobospan" id="kobo.641.1">were recorded.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer213">
<span class="kobospan" id="kobo.642.1"><img alt="Figure 8.6 – The raw numbers of people who fall under each BMI category" src="image/B19488_08_06.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.643.1">Figure 8.6 – The raw numbers of people who fall under each BMI category</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.644.1">Is there evidence to suggest that BMI trends have changed since 2009? </span><span class="kobospan" id="kobo.644.2">Let’s test at the </span><strong class="source-inline"><span class="kobospan" id="kobo.645.1">0.05</span></strong> <span><span class="kobospan" id="kobo.646.1">significance level:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.647.1">First, let’s calculate our expected values. </span><span class="kobospan" id="kobo.647.2">In a sample of 500, we expect 156 to be </span><strong class="bold"><span class="kobospan" id="kobo.648.1">Under/Normal</span></strong><span class="kobospan" id="kobo.649.1"> (that’s 31.2% of 500), and we fill in the remaining boxes in the </span><span><span class="kobospan" id="kobo.650.1">same way:</span></span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer214">
<span class="kobospan" id="kobo.651.1"><img alt="Figure 8.7 – The same raw numbers as the previous figure with an added row representing the “Expected” values in each category given a 2009 survey" src="image/B19488_08_07.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.652.1">Figure 8.7 – The same raw numbers as the previous figure with an added row representing the “Expected” values in each category given a 2009 survey</span></p>
<ol class="calibre13">
<li value="2" class="calibre14"><span class="kobospan" id="kobo.653.1">Now, let’s check that the conditions</span><a id="_idIndexMarker501" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.654.1"> for our test </span><span><span class="kobospan" id="kobo.655.1">are satisfied:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.656.1">All of the expected counts are greater </span><span><span class="kobospan" id="kobo.657.1">than five</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.658.1">Each observation is independent and our population is very large (much more than 10 times of </span><span><span class="kobospan" id="kobo.659.1">500 people)</span></span></li></ul></li>
<li class="calibre14"><span class="kobospan" id="kobo.660.1">Next, we will carry out a goodness of fit test. </span><span class="kobospan" id="kobo.660.2">We will set our null and </span><span><span class="kobospan" id="kobo.661.1">alternative hypotheses:</span></span><ul class="calibre16"><li class="calibre14"><span class="kobospan" id="kobo.662.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre153"/></span><span class="kobospan" id="kobo.663.1">: The 2009 BMI distribution is </span><span><span class="kobospan" id="kobo.664.1">still accurate.</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.665.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/151.png" class="calibre158"/></span><span class="kobospan" id="kobo.666.1">: The 2009 BMI distribution is no longer accurate (at least one of the proportions is different now). </span><span class="kobospan" id="kobo.666.2">We can calculate our test statistic </span><span><span class="kobospan" id="kobo.667.1">by hand:</span></span></li></ul></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer217">
<span class="kobospan" id="kobo.668.1"><img alt="Figure 8.8 – Walking through the calculation for our test statistic. Keep reading for the Python code to do it for you!" src="image/B19488_08_08.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.669.1">Figure 8.8 – Walking through the calculation for our test statistic. </span><span class="kobospan" id="kobo.669.2">Keep reading for the Python code to do it for you!</span></p>
<ol class="calibre13">
<li value="4" class="calibre14"><span class="kobospan" id="kobo.670.1">Alternatively, we can use our handy-dandy Python skills, </span><span><span class="kobospan" id="kobo.671.1">like so:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.672.1">
observed = [102, 178, 186, 34]</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.673.1">
expected = [156, 165.5, 147, 31.5]</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.674.1">
chi_squared, p_value = stats.chisquare(f_obs= observed, f_exp= expected)</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.675.1">
chi_squared, p_value</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.676.1">
#(30.1817679275599, 1.26374310311106e-06)</span></pre></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.677.1">Our </span><em class="italic"><span class="kobospan" id="kobo.678.1">p</span></em><span class="kobospan" id="kobo.679.1"> value is lower than .05; therefore, we may</span><a id="_idIndexMarker502" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.680.1"> reject the null hypothesis in favor of the fact that the BMI trends today a</span><a id="_idTextAnchor237" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.681.1">re different from what they were </span><span><span class="kobospan" id="kobo.682.1">in 2009.</span></span></p>
<h2 id="_idParaDest-120" class="calibre7"><a id="_idTextAnchor238" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.683.1">Chi-square test for association/independence</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.684.1">Independence as a concept</span><a id="_idIndexMarker503" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.685.1"> in probability is when knowing the value of one variable tells you nothing about the value of another. </span><span class="kobospan" id="kobo.685.2">For example, we might expect that the country and the month you were born in are independent. </span><span class="kobospan" id="kobo.685.3">However, knowing which type of phone you use might indicate your creativity levels. </span><span class="kobospan" id="kobo.685.4">Those variables might not </span><span><span class="kobospan" id="kobo.686.1">be independent.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.687.1">The chi-square test for association/independence helps us ascertain whether two categorical variables are independent of one another. </span><span class="kobospan" id="kobo.687.2">The test for independence is commonly used to determine whether variables such as education levels or tax brackets vary based on demographic factors, such as gender, race, and religion. </span><span class="kobospan" id="kobo.687.3">Let’s look back at an example posed in the preceding chapter, the A/B </span><span><span class="kobospan" id="kobo.688.1">split test.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.689.1">Recall that we ran a test and exposed half of our users to a certain landing page (</span><strong class="bold"><span class="kobospan" id="kobo.690.1">Website A</span></strong><span class="kobospan" id="kobo.691.1">), exposed the other half to a different landing page (</span><strong class="bold"><span class="kobospan" id="kobo.692.1">Website B</span></strong><span class="kobospan" id="kobo.693.1">), and then measured the sign-up rates for both sites. </span><span class="kobospan" id="kobo.693.2">We obtained the </span><span><span class="kobospan" id="kobo.694.1">following results:</span></span></p>
<table class="no-table-style" id="table001-5">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.695.1">Did not </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.696.1">sign up</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.697.1">Signed up</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.698.1">Website A</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.699.1">134</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.700.1">54</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.701.1">Website B</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.702.1">110</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.703.1">48</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.704.1">Figure 8.9 – Results of our A/B test</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.705.1">We calculated website conversions but what we really want to know is whether there is a difference between the two variables: </span><em class="italic"><span class="kobospan" id="kobo.706.1">which website was the user exposed to? </span><span class="kobospan" id="kobo.706.2">And did the user sign up?</span></em><span class="kobospan" id="kobo.707.1"> For this, we will use our </span><span><span class="kobospan" id="kobo.708.1">chi-square test.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor239" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.709.1">Assumptions of the chi-square independence test</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.710.1">There are the following</span><a id="_idIndexMarker504" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.711.1"> two assumptions of </span><span><span class="kobospan" id="kobo.712.1">this test:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.713.1">All expected counts are at </span><span><span class="kobospan" id="kobo.714.1">least </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.715.1">5</span></strong></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.716.1">Individual observations are independent and the population should be at least 10 times as large as the sample (</span><em class="italic"><span class="kobospan" id="kobo.717.1">10n</span></em><span class="kobospan" id="kobo.718.1"> &lt; </span><span><em class="italic"><span class="kobospan" id="kobo.719.1">N</span></em></span><span><span class="kobospan" id="kobo.720.1">)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.721.1">Note that they are exactly the same as the last </span><span><span class="kobospan" id="kobo.722.1">chi-square test.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.723.1">Let’s set up </span><span><span class="kobospan" id="kobo.724.1">our hypotheses:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.725.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre153"/></span><span class="kobospan" id="kobo.726.1">: There is no association between two categorical variables in the population </span><span><span class="kobospan" id="kobo.727.1">of interest</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.728.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre153"/></span><span class="kobospan" id="kobo.729.1">: Two categorical variables are independent in the population </span><span><span class="kobospan" id="kobo.730.1">of interest</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.731.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/144.png" class="calibre154"/></span><span class="kobospan" id="kobo.732.1">: There is an association between two categorical variables in the population </span><span><span class="kobospan" id="kobo.733.1">of interest</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.734.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/144.png" class="calibre154"/></span><span class="kobospan" id="kobo.735.1">: Two categorical variables are not independent in the population </span><span><span class="kobospan" id="kobo.736.1">of interest</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.737.1">You might notice that we are missing something important here. </span><span class="kobospan" id="kobo.737.2">Where are the expected counts? </span><span class="kobospan" id="kobo.737.3">Earlier, we had a prior distribution to compare our observed results to but now we do not. </span><span class="kobospan" id="kobo.737.4">For this reason, we will have to create some. </span><span class="kobospan" id="kobo.737.5">We can use the following formula to calculate the expected values for each value. </span><span class="kobospan" id="kobo.737.6">In each cell of the table, we can use </span><span><span class="kobospan" id="kobo.738.1">the following:</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.739.1">Expected Count = to calculate our chi-square test statistic and our degrees </span></em><span><em class="italic"><span class="kobospan" id="kobo.740.1">of freedom</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.741.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Test&lt;/mtext&gt;&lt;mtext&gt;Statistic:&lt;/mtext&gt;&lt;msup&gt;&lt;mi&gt;χ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mtext&gt;Observed&lt;/mtext&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mtext&gt;Expected&lt;/mtext&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mtext&gt;Expected&lt;/mtext&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/156.png" class="calibre159"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.742.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Degrees&lt;/mtext&gt;&lt;mtext&gt;of&lt;/mtext&gt;&lt;mtext&gt;Freedom:&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/157.png" class="calibre160"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.743.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.744.1">r</span></em><span class="kobospan" id="kobo.745.1"> is the number of rows and </span><em class="italic"><span class="kobospan" id="kobo.746.1">c</span></em><span class="kobospan" id="kobo.747.1"> is the number of columns. </span><span class="kobospan" id="kobo.747.2">Of course, as before, when we calculate our </span><em class="italic"><span class="kobospan" id="kobo.748.1">p</span></em><span class="kobospan" id="kobo.749.1"> value, we will reject the null hypothesis if that </span><em class="italic"><span class="kobospan" id="kobo.750.1">p</span></em><span class="kobospan" id="kobo.751.1"> value is less than the significance level. </span><span class="kobospan" id="kobo.751.2">Let’s use some built-in Python methods, as shown, in order to quickly get </span><span><span class="kobospan" id="kobo.752.1">our results:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.753.1">
observed = np.array([[134, 54],[110, 48]])
# built a 2x2 matrix as seen in the table above
chi_squared, p_value, degrees_of_freedom, matrix = stats.chi2_contingency(observed= observed)
chi_squared, p_value
# (0.04762692369491045, 0.82724528704422262)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.754.1">We can see that our </span><em class="italic"><span class="kobospan" id="kobo.755.1">p</span></em><span class="kobospan" id="kobo.756.1"> value is quite</span><a id="_idIndexMarker505" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.757.1"> large; therefore, we fail to reject the null hypothesis and we cannot say for sure that seeing a particular website has any effect on whether or not a user signs up. </span><span class="kobospan" id="kobo.757.2">There is no association between </span><span><span class="kobospan" id="kobo.758.1">these variables</span><a id="_idTextAnchor240" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.759.1">.</span></span></p>
<h1 id="_idParaDest-121" class="calibre6"><a id="_idTextAnchor241" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.760.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.761.1">In this chapter, we looked at different statistical tests, including chi-square and </span><em class="italic"><span class="kobospan" id="kobo.762.1">t</span></em><span class="kobospan" id="kobo.763.1">-tests, as well as point estimates and confidence intervals, in order to ascertain population parameters based on sample data. </span><span class="kobospan" id="kobo.763.2">We were able to find that even with small samples of data, we can make powerful assumptions about the underlying population as </span><span><span class="kobospan" id="kobo.764.1">a whole.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.765.1">Using the concepts reviewed in this chapter, data scientists will be able to make inferences about entire datasets based on certain samples of data. </span><span class="kobospan" id="kobo.765.2">In addition, they will be able to use hypothesis tests to gain a better understanding of full datasets, given samples </span><span><span class="kobospan" id="kobo.766.1">of data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.767.1">Statistics is a very wide and expansive subject that cannot truly be covered in a single chapter; however, our understanding of the subject will allow us to carry on and talk more about how we can use statistics and probability in order to communicate our ideas through data science in the </span><span><span class="kobospan" id="kobo.768.1">next chapter.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.769.1">In the next chapter, we will discuss different ways of communicating results from data analysis including various presentation styles as well as </span><span><span class="kobospan" id="kobo.770.1">visualization techniques.</span></span></p>
</div>
</body></html>