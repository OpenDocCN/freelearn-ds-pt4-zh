<html><head></head><body>
<div id="_idContainer255" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-140"><a id="_idTextAnchor272" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-141" class="calibre6"><a id="_idTextAnchor273" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">How to Tell if Your Toaster is Learning – Machine Learning Essentials</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">It seems as though every time we hear about the next great start-up or turn on the news, we hear something about a revolutionary piece of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">machine learning</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">ML</span></strong><span class="kobospan" id="kobo.7.1">) or </span><strong class="bold"><span class="kobospan" id="kobo.8.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">AI</span></strong><span class="kobospan" id="kobo.11.1">) technology and how it will change the way we live. </span><span class="kobospan" id="kobo.11.2">This chapter focuses on ML as a practical part of data science. </span><span class="kobospan" id="kobo.11.3">We will cover the following topics in </span><span><span class="kobospan" id="kobo.12.1">this chapter:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.13.1">Defining different types of ML, along with examples of </span><span><span class="kobospan" id="kobo.14.1">each kind</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.15.1">Regression </span><span><span class="kobospan" id="kobo.16.1">and classification</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.17.1">What is ML, and how is it used in </span><span><span class="kobospan" id="kobo.18.1">data science?</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.19.1">The differences between ML and statistical modeling and how ML is a broad category of </span><span><span class="kobospan" id="kobo.20.1">the latter</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.21.1">An Introduction to </span><span><span class="kobospan" id="kobo.22.1">Linear Regression</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.23.1">Our aim in this chapter will be to utilize statistics, probability, and algorithmic thinking in order to understand and apply essential ML skills to practical industries, such as marketing. </span><span class="kobospan" id="kobo.23.2">Examples will include predicting star ratings of restaurant reviews, predicting the presence of a disease, spam email detection, and much more. </span><span class="kobospan" id="kobo.23.3">This chapter focuses on ML as a whole and as a single statistical model. </span><span class="kobospan" id="kobo.23.4">The subsequent chapters will deal with many more models, some of which are much </span><span><span class="kobospan" id="kobo.24.1">more complex.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.25.1">We will also turn our focus on metrics, which tell us how effective our models are. </span><span class="kobospan" id="kobo.25.2">We will use metrics in order to conclude results and make predictions </span><span><span class="kobospan" id="kobo.26.1">using ML.</span></span></p>
<h1 id="_idParaDest-142" class="calibre6"><a id="_idTextAnchor274" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.27.1">Introducing ML</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.28.1">In </span><a href="B19488_01.xhtml#_idTextAnchor015" class="pcalibre calibre4 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.29.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.30.1">, </span><em class="italic"><span class="kobospan" id="kobo.31.1">Data Science Terminology</span></em><span class="kobospan" id="kobo.32.1">, we defined ML as giving computers the </span><a id="_idIndexMarker562" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.33.1">ability to learn from data without being given explicit rules by a programmer. </span><span class="kobospan" id="kobo.33.2">This definition still holds true. </span><span class="kobospan" id="kobo.33.3">ML is concerned with the ability to ascertain certain patterns (signals) out of data, even if the data has inherent errors in </span><span><span class="kobospan" id="kobo.34.1">it (noise).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.35.1">ML models are able to learn from data without the explicit direction of a human. </span><span class="kobospan" id="kobo.35.2">That is the main difference between ML models and classical </span><span><span class="kobospan" id="kobo.36.1">non-ML algorithms.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.37.1">Classical algorithms are told directly by a human how to find the best answer in a complex system, and the algorithm then achieves these best solutions, often working faster and more efficiently than a human. </span><span class="kobospan" id="kobo.37.2">However, the bottleneck here is that the human has to first come up with the best solution in order to tell the algorithm what to do. </span><span class="kobospan" id="kobo.37.3">In ML, the model is not told the best solution and, instead, is given several examples of the problem and told to figure out the best solution </span><span><span class="kobospan" id="kobo.38.1">for itself.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.39.1">ML is just another tool in the belt of a data scientist. </span><span class="kobospan" id="kobo.39.2">It is on the same level as statistical tests (chi-square or t-tests) or uses basic probability or statistics to estimate population parameters. </span><span class="kobospan" id="kobo.39.3">ML is often regarded as the only thing data scientists know how to do, and this is simply untrue. </span><span class="kobospan" id="kobo.39.4">A true data scientist is able to recognize when ML is applicable and, more importantly, when it </span><span><span class="kobospan" id="kobo.40.1">is not.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.41.1">ML is a game of correlations and relationships. </span><span class="kobospan" id="kobo.41.2">Most ML algorithms in existence are concerned with finding and/or exploiting relationships between datasets (often represented as columns in a </span><strong class="source-inline"><span class="kobospan" id="kobo.42.1">pandas</span></strong><span class="kobospan" id="kobo.43.1"> DataFrame). </span><span class="kobospan" id="kobo.43.2">Once ML algorithms can pinpoint certain correlations, the model can either use these relationships to predict future observations or generalize the data to reveal </span><span><span class="kobospan" id="kobo.44.1">interesting patterns.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.45.1">Perhaps a great way to explain ML is to offer an example of a problem coupled with two possible solutions: one using an ML algorithm and the othe</span><a id="_idTextAnchor275" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.46.1">r utilizing a </span><span><span class="kobospan" id="kobo.47.1">non-ML algorithm.</span></span></p>
<h2 id="_idParaDest-143" class="calibre7"><a id="_idTextAnchor276" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.48.1">Example – facial recognition</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.49.1">This problem is, on </span><a id="_idIndexMarker563" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.50.1">its face (pun intended), quite simple: given a picture of a face, who does it belong to? </span><span class="kobospan" id="kobo.50.2">However, let’s consider a slightly simpler task. </span><span class="kobospan" id="kobo.50.3">Suppose you wish to implement a home security system that recognizes who is entering your house. </span><span class="kobospan" id="kobo.50.4">Most likely, during the day, your house will be empty most of the time, and facial recognition will kick in only if there is a person in the shot. </span><span class="kobospan" id="kobo.50.5">This is exactly the question I propose we try to solve – given a photo, is there a face in it to </span><span><span class="kobospan" id="kobo.51.1">even recognize?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.52.1">Given this task definition, I propose the following </span><span><span class="kobospan" id="kobo.53.1">two solutions:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.54.1">A non-ML algorithm that will define a face as having a roundish structure, two eyes, hair, nose, and so on. </span><span class="kobospan" id="kobo.54.2">The algorithm then looks for these hardcoded features in the photo and returns whether or not it was able to find any of </span><span><span class="kobospan" id="kobo.55.1">these features.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.56.1">An ML algorithm that will work a bit differently. </span><span class="kobospan" id="kobo.56.2">The model will only be given several pictures of faces and non-faces that are labeled as such. </span><span class="kobospan" id="kobo.56.3">From the examples (called training sets), it would figure out its own definition of </span><span><span class="kobospan" id="kobo.57.1">a face.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.58.1">The ML version </span><a id="_idIndexMarker564" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.59.1">of the solution is never told what a face is; it is merely given several examples – some with faces, and some without. </span><span class="kobospan" id="kobo.59.2">It is then up to the ML model to figure out the difference between the two. </span><span class="kobospan" id="kobo.59.3">Once it figures this out, it uses this information to take in a picture and predict whether or not there is a face in the new picture. </span><span class="kobospan" id="kobo.59.4">For example, to train the system, we might have the following images denoted in </span><span><em class="italic"><span class="kobospan" id="kobo.60.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.61.1">.1</span></em></span><span><span class="kobospan" id="kobo.62.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer239">
<span class="kobospan" id="kobo.63.1"><img alt="Figure 10.1 – Input images for training an ML model" src="image/B19488_10_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.64.1">Figure 10.1 – Input images for training an ML model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.65.1">The model will then figure out the difference between the pictures labeled as </span><em class="italic"><span class="kobospan" id="kobo.66.1">Face</span></em><span class="kobospan" id="kobo.67.1"> and t</span><a id="_idTextAnchor277" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.68.1">he pictures labeled as </span><em class="italic"><span class="kobospan" id="kobo.69.1">No Face</span></em><span class="kobospan" id="kobo.70.1"> and be able to use that difference to find faces in future photos. </span><span class="kobospan" id="kobo.70.2">Because</span><a id="_idIndexMarker565" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.71.1"> the promise of ML – learning simply from data and without explicit human intervention – is so alluring, many people might believe that ML is perfect, but it </span><span><span class="kobospan" id="kobo.72.1">simply isn’t.</span></span></p>
<h2 id="_idParaDest-144" class="calibre7"><a id="_idTextAnchor278" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.73.1">ML isn’t perfect</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.74.1">There are many </span><a id="_idIndexMarker566" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.75.1">caveats of ML. </span><span class="kobospan" id="kobo.75.2">Many are specific to different models being implemented, but some assumptions are universal for any </span><span><span class="kobospan" id="kobo.76.1">ML model:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.77.1">The data used, for the most part, is preprocessed and cleaned using the methods outlined in the earlier chapters. </span><span class="kobospan" id="kobo.77.2">Almost no ML model will tolerate extremely dirty/incomplete data with missing values or </span><span><span class="kobospan" id="kobo.78.1">categorical values.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.79.1">Each row of a cleaned dataset represents a single observation of the environment we are trying </span><span><span class="kobospan" id="kobo.80.1">to model.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.81.1">The data as a whole should be representative of the task we are solving. </span><span class="kobospan" id="kobo.81.2">This might sound obvious, but in so many cases, people use data to train an ML model that is close to but not exactly related to the task. </span><span class="kobospan" id="kobo.81.3">This is often seen in criminal justice examples where people might use arrest data to train a model to predict criminality but, of course, arrests are not the same as convicting someone of </span><span><span class="kobospan" id="kobo.82.1">a crime.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.83.1">If our goal is to find relationships between variables, then there is an assumption that there is some kind of relationship between these variables. </span><span class="kobospan" id="kobo.83.2">Again, this seems obvious, but if a human putting the data together is biased and “believes” there is a relationship between the data, then they might incorrectly judge</span><a id="_idIndexMarker567" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.84.1"> an ML model to be more powerful than it </span><span><span class="kobospan" id="kobo.85.1">actually is.</span></span><p class="calibre3"><span class="kobospan" id="kobo.86.1">This assumption is particularly important. </span><span class="kobospan" id="kobo.86.2">Many ML models take this assumption very seriously. </span><span class="kobospan" id="kobo.86.3">These models are not able to communicate that there might not be </span><span><span class="kobospan" id="kobo.87.1">a relationship.</span></span></p></li>
<li class="calibre14"><span class="kobospan" id="kobo.88.1">ML models are generally considered semi-automatic, which means that intelligent decisions by humans are </span><span><span class="kobospan" id="kobo.89.1">still needed.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.90.1">The machine is very smart but has a hard time putting things into context. </span><span class="kobospan" id="kobo.90.2">The output of most models is a series of numbers and metrics attempting to quantify how well the model did. </span><span class="kobospan" id="kobo.90.3">It is up to a human to put these metrics into perspective and communicate the results to </span><span><span class="kobospan" id="kobo.91.1">an audience.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.92.1">Most ML models are sensitive to noisy data. </span><span class="kobospan" id="kobo.92.2">This means that the models get confused when you include data that doesn’t make sense. </span><span class="kobospan" id="kobo.92.3">For example, if you are attempting to find relationships between economic data around the world and one of your columns relates to puppy adoption rates in the capital city, that information is likely not relevant and will confuse </span><span><span class="kobospan" id="kobo.93.1">the model.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">These assumptions will come up again and again when dealing with ML. </span><span class="kobospan" id="kobo.94.2">They are all too important and</span><a id="_idIndexMarker568" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.95.1"> are often ignored by novice </span><span><span class="kobospan" id="kobo.96.1">data scientists.</span></span></p>
<h2 id="_idParaDest-145" class="calibre7"><a id="_idTextAnchor279" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.97.1">How does ML work?</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.98.1">Each flavor of ML </span><a id="_idIndexMarker569" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.99.1">and each individual model works in very different ways, exploiting different parts of mathematics and data science. </span><span class="kobospan" id="kobo.99.2">However, in general, ML works by taking in data, finding relationships within the data, and giving as output what the model learned, as illustrated in </span><span><em class="italic"><span class="kobospan" id="kobo.100.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.101.1">.2</span></em></span><span><span class="kobospan" id="kobo.102.1">.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer240">
<span class="kobospan" id="kobo.103.1"><img alt="Figure 10.2 – An overview of ML models taking in input data, learning signals, and identifying patterns in order to produce a meaningful and interpretable output" src="image/B19488_10_02.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.104.1">Figure 10.2 – An overview of ML models taking in input data, learning signals, and identifying patterns in order to produce a meaningful and interpretable output</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.105.1">As we explore different types of ML models, we will see ho</span><a id="_idTextAnchor280" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.106.1">w they manipulate data differently and come up with different outputs for </span><span><span class="kobospan" id="kobo.107.1">different applications.</span></span></p>
<h1 id="_idParaDest-146" class="calibre6"><a id="_idTextAnchor281" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.108.1">Types of ML</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.109.1">There are many ways to </span><a id="_idIndexMarker570" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.110.1">segment ML and dive deeper. </span><span class="kobospan" id="kobo.110.2">In </span><a href="B19488_01.xhtml#_idTextAnchor015" class="pcalibre calibre4 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.111.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.112.1">, </span><em class="italic"><span class="kobospan" id="kobo.113.1">Data Science Terminology</span></em><span class="kobospan" id="kobo.114.1">, I mentioned statistical and probabilistic models. </span><span class="kobospan" id="kobo.114.2">These models utilize statistics and probability, which we’ve seen in the previous chapters, in order to find relationships between data and make predictions. </span><span class="kobospan" id="kobo.114.3">In this chapter, we will implement both types of models. </span><span class="kobospan" id="kobo.114.4">In the following chapter, we will see ML outside the rigid mathematical world of statistics/probability. </span><span class="kobospan" id="kobo.114.5">You can segment ML models by different characteristics, including </span><span><span class="kobospan" id="kobo.115.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.116.1">The types of data organic structures they </span><a id="_idIndexMarker571" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.117.1">utilize (tree, graph, or </span><strong class="bold"><span class="kobospan" id="kobo.118.1">neural </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.119.1">network</span></strong></span><span><span class="kobospan" id="kobo.120.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.121.1">NN</span></strong></span><span><span class="kobospan" id="kobo.122.1">))</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.123.1">The field of mathematics they are most related to (statistical </span><span><span class="kobospan" id="kobo.124.1">or probabilistic)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.125.1"> The level of computation required to </span><a id="_idIndexMarker572" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.126.1">train (</span><strong class="bold"><span class="kobospan" id="kobo.127.1">deep </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.128.1">learning</span></strong></span><span><span class="kobospan" id="kobo.129.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.130.1">DL</span></strong></span><span><span class="kobospan" id="kobo.131.1">))</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.132.1">Branching off from the top level of ML, there are the following </span><span><span class="kobospan" id="kobo.133.1">three subsets:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.134.1">Supervised </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.135.1">learning</span></strong></span><span><span class="kobospan" id="kobo.136.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.137.1">SL</span></strong></span><span><span class="kobospan" id="kobo.138.1">)</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.139.1">Unsupervised</span><a id="_idTextAnchor282" class="pcalibre calibre4 pcalibre1"/> </strong><span><strong class="bold"><span class="kobospan" id="kobo.140.1">learning</span></strong></span><span><span class="kobospan" id="kobo.141.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.142.1">UL</span></strong></span><span><span class="kobospan" id="kobo.143.1">)</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.144.1">Reinforcement </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.145.1">learning</span></strong></span><span><span class="kobospan" id="kobo.146.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.147.1">RL</span></strong></span><span><span class="kobospan" id="kobo.148.1">)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.149.1">Let’s go into each one of these one by one. </span><span class="kobospan" id="kobo.149.2">Our next chapter will include multiple examples of the </span><a id="_idIndexMarker573" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.150.1">first two, with the third one being slightly out of the scope of our introductory book. </span><span class="kobospan" id="kobo.150.2">You can always find more resources in our </span><span><span class="kobospan" id="kobo.151.1">code base!</span></span></p>
<h2 id="_idParaDest-147" class="calibre7"><a id="_idTextAnchor283" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.152.1">SL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.153.1">Simply put, </span><strong class="bold"><span class="kobospan" id="kobo.154.1">SL</span></strong><span class="kobospan" id="kobo.155.1"> finds </span><a id="_idIndexMarker574" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.156.1">associations between features</span><a id="_idIndexMarker575" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.157.1"> of a dataset (independent variables) and a target (dependent) variable. </span><span class="kobospan" id="kobo.157.2">For example, SL models might try to find the association between a person’s health features (heart rate, weight, and so on) and that person’s risk of having a heart attack (the target variable). </span><span class="kobospan" id="kobo.157.3">These associations allow supervised models to make predictions based on past</span><a id="_idIndexMarker576" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.158.1"> examples. </span><strong class="bold"><span class="kobospan" id="kobo.159.1">Supervised ML</span></strong><span class="kobospan" id="kobo.160.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.161.1">SML</span></strong><span class="kobospan" id="kobo.162.1">) models are often called predictive analytics models, named for their ability to predict the future based on the past. </span><span class="kobospan" id="kobo.162.2">This is often the first thing that comes to people’s minds when they hear the term </span><em class="italic"><span class="kobospan" id="kobo.163.1">ML</span></em><span class="kobospan" id="kobo.164.1">, but it in no way encompasses the realm </span><span><span class="kobospan" id="kobo.165.1">of ML.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.166.1">SML requires a certain type of data </span><a id="_idIndexMarker577" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.167.1">called </span><strong class="bold"><span class="kobospan" id="kobo.168.1">labeled data</span></strong><span class="kobospan" id="kobo.169.1"> – data that acts as full, correct, and complete examples of the features and target variable. </span><span><em class="italic"><span class="kobospan" id="kobo.170.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.171.1">.1</span></em><span class="kobospan" id="kobo.172.1"> shows a snippet of labeled data. </span><span class="kobospan" id="kobo.172.2">The goal is to let our model learn by giving it historical examples that are labeled with the </span><span><span class="kobospan" id="kobo.173.1">correct answer.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.174.1">Recall the facial recognition example. </span><span class="kobospan" id="kobo.174.2">That is an SL model because we are training our model with the previous pictures labeled as either </span><em class="italic"><span class="kobospan" id="kobo.175.1">face</span></em><span class="kobospan" id="kobo.176.1"> or </span><em class="italic"><span class="kobospan" id="kobo.177.1">not face</span></em><span class="kobospan" id="kobo.178.1">, and then asking the model to predict whether or not a new picture has a face </span><span><span class="kobospan" id="kobo.179.1">in it.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.180.1">First, let us separate the data into two distinct parts, </span><span><span class="kobospan" id="kobo.181.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.182.1">The features, which are the columns that will be used to make our prediction. </span><span class="kobospan" id="kobo.182.2">These are sometimes called predictors, input values, variables, and </span><span><span class="kobospan" id="kobo.183.1">independent variables.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.184.1">The response, which is the column that we wish to predict. </span><span class="kobospan" id="kobo.184.2">This is sometimes called outcome, label, target, and </span><span><span class="kobospan" id="kobo.185.1">dependent variable.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.186.1">SL attempts to find a relationship between features and responses in order to make a prediction. </span><span class="kobospan" id="kobo.186.2">The idea is that, in the future, a data observation will present itself, and we will only know the predictors. </span><span class="kobospan" id="kobo.186.3">The model will then have to use the features to make an accurate prediction of the response value. </span><span><em class="italic"><span class="kobospan" id="kobo.187.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.188.1">.3</span></em><span class="kobospan" id="kobo.189.1"> shows a visualization of how we generally use </span><a id="_idIndexMarker578" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.190.1">supervised models: we train (fit) them</span><a id="_idIndexMarker579" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.191.1"> using labeled training data and use the result to predict unseen cases (features without the response) to make </span><span><span class="kobospan" id="kobo.192.1">final predictions:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer241">
<span class="kobospan" id="kobo.193.1"><img alt="Figure 10.3 – Supervised models are fit using labeled training data and are then used to make predictions fro﻿m unseen cases" src="image/B19488_10_03.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.194.1">Figure 10.3 – Supervised models are fit using labeled training data and are then used to make predictions fro</span><a id="_idTextAnchor284" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.195.1">m unseen cases</span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.196.1">Example – heart attack prediction</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.197.1">Suppose we wish</span><a id="_idIndexMarker580" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.198.1"> to predict whether someone will have a heart attack within a year. </span><span class="kobospan" id="kobo.198.2">To predict this, we are given that person’s cholesterol level, blood pressure, height, smoking habits, and perhaps more. </span><span class="kobospan" id="kobo.198.3">From this data, we must ascertain the likelihood of a heart attack. </span><span class="kobospan" id="kobo.198.4">Suppose, to make this prediction, we look at previous patients and their medical history. </span><span class="kobospan" id="kobo.198.5">As these are previous patients, we know not only their predictors (cholesterol, blood pressure, and so on) but also if they actually had a heart attack (because it </span><span><span class="kobospan" id="kobo.199.1">already happened!).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.200.1">This is an SML problem because we are doing </span><span><span class="kobospan" id="kobo.201.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.202.1">We are making a prediction </span><span><span class="kobospan" id="kobo.203.1">about someone</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.204.1">We are using historical training data to find relationships between medical variables and </span><span><span class="kobospan" id="kobo.205.1">heart attacks</span></span></li>
</ul>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.206.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.207.1">.4</span></em><span class="kobospan" id="kobo.208.1"> shows a basic outline of how SML </span><a id="_idTextAnchor285" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.209.1">models </span><span><span class="kobospan" id="kobo.210.1">use data:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer242">
<span class="kobospan" id="kobo.211.1"><img alt="Figure 10.4 – An SML model uses predictors and a response from data in order to learn relationships between them, usually in order to make future predictions given predictors without the response" src="image/B19488_10_04.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.212.1">Figure 10.4 – An SML model uses predictors and a response from data in order to learn relationships between them, usually in order to make future predictions given predictors without the response</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.213.1">The hope here is that a patient will walk in sometime in the future and our model will be able to identify whether or not the patient is at risk for a heart attack based on their conditions (just like a </span><span><span class="kobospan" id="kobo.214.1">doctor would!).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.215.1">As the model sees more and more diverse and representative labeled data, it should adjust itself in order to match the correct labels outlined in the training data. </span><span class="kobospan" id="kobo.215.2">We can then use different metrics (explained more in the next chapter) to pinpoint exactly how well our SML model is doing and how it can better adjust itself. </span><span class="kobospan" id="kobo.215.3">One of the largest obstacles associated with SML is obtaining diverse and representative labeled data, which can be very difficult to get hold of. </span><span class="kobospan" id="kobo.215.4">Suppose we wish to predict heart attacks; we might need thousands of patients along with all of their medical information and years’ worth of follow-up records for each person, which could be a nightmare to obtain. </span><span class="kobospan" id="kobo.215.5">In short, supervised models use historical labeled data in order to make predictions about the future from </span><a id="_idIndexMarker581" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.216.1">predefined features. </span><span class="kobospan" id="kobo.216.2">Some possible applications for SL include </span><span><span class="kobospan" id="kobo.217.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.218.1">Stock price predictions</span></strong><span class="kobospan" id="kobo.219.1">: Historical trading volume and movements could be features along with social media sentiment, while the future price could be </span><span><span class="kobospan" id="kobo.220.1">a target</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.221.1">Weather predictions</span></strong><span class="kobospan" id="kobo.222.1">: Using past meteorological data, such as temperature, humidity, and wind speed, to forecast future </span><span><span class="kobospan" id="kobo.223.1">weather conditions</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.224.1">Disease diagnosis</span></strong><span class="kobospan" id="kobo.225.1">: Medical imaging and patient history can be used to predict the presence or absence of </span><span><span class="kobospan" id="kobo.226.1">a disease</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.227.1">Facial recognition</span></strong><span class="kobospan" id="kobo.228.1">: Features extracted from images of faces to </span><span><span class="kobospan" id="kobo.229.1">identify individuals</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.230.1">Email filtering</span></strong><span class="kobospan" id="kobo.231.1">: Using characteristics of emails to classify them as spam or </span><span><span class="kobospan" id="kobo.232.1">not spam</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.233.1">Credit scoring</span></strong><span class="kobospan" id="kobo.234.1">: Historical financial behavior data to </span><span><span class="kobospan" id="kobo.235.1">predict creditworthiness</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.236.1">Each of these applications relies on a labeled dataset that includes historical data points and a target variable that the model is trying to predict. </span><span class="kobospan" id="kobo.236.2">The quality and quantity of the labeled data are crucial in SL as they directly impact the model’s ability to learn and generalize to new, unseen data. </span><span class="kobospan" id="kobo.236.3">When designing an SL model, it is important to consider the features that will be used. </span><span class="kobospan" id="kobo.236.4">Features should be relevant, informative, and non-redundant to ensure the model performs effectively. </span><span class="kobospan" id="kobo.236.5">Additionally, the choice of algorithm depends on the nature of the task (regression, classification), the size and dimensionality of the dataset, and the computational </span><span><span class="kobospan" id="kobo.237.1">efficiency required.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.238.1">By carefully preparing </span><a id="_idIndexMarker582" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.239.1">the dataset and selecting the right features and model, SL can provide powerful predictive insights across various fields, from financ</span><a id="_idTextAnchor286" class="pcalibre calibre4 pcalibre1"/><a id="_idTextAnchor287" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.240.1">e </span><span><span class="kobospan" id="kobo.241.1">to healthcare.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.242.1">Types of SL models</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.243.1">There are, in general, two</span><a id="_idIndexMarker583" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.244.1"> types of SL models: </span><strong class="bold"><span class="kobospan" id="kobo.245.1">regression</span></strong><span class="kobospan" id="kobo.246.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.247.1">classification</span></strong><span class="kobospan" id="kobo.248.1"> models. </span><span class="kobospan" id="kobo.248.2">The difference between the two is quite simple and lies in the natu</span><a id="_idTextAnchor288" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.249.1">re of the </span><span><span class="kobospan" id="kobo.250.1">response variable.</span></span></p>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.251.1">Regression</span></strong><span class="kobospan" id="kobo.252.1"> models </span><a id="_idIndexMarker584" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.253.1">attempt to predict a continuous response. </span><span class="kobospan" id="kobo.253.2">This means that the response can take on a range of infinite values. </span><span class="kobospan" id="kobo.253.3">Consider the </span><span><span class="kobospan" id="kobo.254.1">following examples:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.255.1">House pricing</span></strong><span class="kobospan" id="kobo.256.1">: Where the value to be predicted is the cost of a house based on features such as square footage, number of bedrooms, </span><span><span class="kobospan" id="kobo.257.1">and location</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.258.1">Temperature forecasting</span></strong><span class="kobospan" id="kobo.259.1">: Where the model predicts the temperature for future days or hours based on historical </span><span><span class="kobospan" id="kobo.260.1">weather data</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.261.1">Stock market price prediction</span></strong><span class="kobospan" id="kobo.262.1">: Where the continuous response could be the future price of a stock based on its historical performance and other </span><span><span class="kobospan" id="kobo.263.1">economic indicators</span></span></li>
</ul>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.264.1">Classification</span></strong><span class="kobospan" id="kobo.265.1"> models, on</span><a id="_idIndexMarker585" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.266.1"> the other hand, predict categorical responses. </span><span class="kobospan" id="kobo.266.2">These responses are discrete and have a finite number of values, often referred to as classes or categories. </span><span class="kobospan" id="kobo.266.3">Here are </span><span><span class="kobospan" id="kobo.267.1">some examples:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.268.1">Email spam detection</span></strong><span class="kobospan" id="kobo.269.1">: The model classifies emails as either “spam” or “not spam” based on content, sender information, and </span><span><span class="kobospan" id="kobo.270.1">other attributes</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.271.1">Medical diagnosis</span></strong><span class="kobospan" id="kobo.272.1">: A model might classify patient outcomes based on test results, predicting categories such as “disease” or “</span><span><span class="kobospan" id="kobo.273.1">no disease”</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.274.1">Image recognition</span></strong><span class="kobospan" id="kobo.275.1">: Classifying images into predefined categories such as “cat," “dog," “car," and so on, based on pixel data </span><span><span class="kobospan" id="kobo.276.1">and patterns</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.277.1">Both regression and classification tasks use a similar process of learning from historical data, but their applications and evaluation metrics differ due to the nat</span><a id="_idTextAnchor289" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.278.1">ure of their output. </span><span class="kobospan" id="kobo.278.2">Our earlier heart attack example is classification because the question w</span><a id="_idTextAnchor290" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.279.1">as: Will this person have a heart attack within a year? </span><span class="kobospan" id="kobo.279.2">This has only two possible answers: </span><em class="italic"><span class="kobospan" id="kobo.280.1">Yes</span></em><span class="kobospan" id="kobo.281.1"> or </span><em class="italic"><span class="kobospan" id="kobo.282.1">No</span></em><span class="kobospan" id="kobo.283.1">. </span><span class="kobospan" id="kobo.283.2">We will see a </span><a id="_idIndexMarker586" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.284.1">full example of regression later in this chapter and full examples of both classification and regression in the </span><span><span class="kobospan" id="kobo.285.1">next chapter.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.286.1">Deciding between classification and regression</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.287.1">Sometimes, it</span><a id="_idIndexMarker587" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.288.1"> can be tricky to decide whether or not you should use classification or regression. </span><span class="kobospan" id="kobo.288.2">Consider that we are interested in the weather outside. </span><span class="kobospan" id="kobo.288.3">We could ask the question, </span><em class="italic"><span class="kobospan" id="kobo.289.1">How hot is it outside?</span></em><span class="kobospan" id="kobo.290.1"> In this case, your answer is on a continuous scale, and some possible answers are 60.7 degrees or 98 degrees. </span><span class="kobospan" id="kobo.290.2">However, as an exercise, go and ask 10 people what the temperature is outside. </span><span class="kobospan" id="kobo.290.3">I guarantee you that someone (if not most people) will not answer in some exact degrees but will bucket their answer and say something like </span><em class="italic"><span class="kobospan" id="kobo.291.1">It’s in </span></em><span><em class="italic"><span class="kobospan" id="kobo.292.1">the 60s</span></em></span><span><span class="kobospan" id="kobo.293.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.294.1">We might wish to consider this problem as a classification problem, where the response variable is no longer in exact degrees but is in a bucket. </span><span class="kobospan" id="kobo.294.2">There would only be a finite number of buckets in theory, making the model perhaps learn the differences between 60s and 70s a </span><span><span class="kobospan" id="kobo.295.1">bit better.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.296.1">What if we aren’t predicting anything, but instead we wanted to use ML to simply better understand and interpret our data? </span><span class="kobospan" id="kobo.296.2">That’s where UL </span><a id="_idTextAnchor291" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.297.1">comes in </span><span><span class="kobospan" id="kobo.298.1">very handy.</span></span></p>
<h2 id="_idParaDest-148" class="calibre7"><a id="_idTextAnchor292" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.299.1">UL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.300.1">The second </span><a id="_idIndexMarker588" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.301.1">type of ML on our list does not </span><a id="_idIndexMarker589" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.302.1">deal with making predictions but has a much more open objective. </span><strong class="bold"><span class="kobospan" id="kobo.303.1">UL</span></strong><span class="kobospan" id="kobo.304.1"> takes in a set of predictors and utilizes relationships between the predictors in order to accomplish tasks such as </span><span><span class="kobospan" id="kobo.305.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.306.1">It reduces the dimension of the data by condensing variables together. </span><span class="kobospan" id="kobo.306.2">An example of this would be file compression. </span><span class="kobospan" id="kobo.306.3">Compression works by utilizing patterns in the data and representing the data in a smaller format. </span><span class="kobospan" id="kobo.306.4">This is referred to</span><a id="_idIndexMarker590" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.307.1"> as </span><span><strong class="bold"><span class="kobospan" id="kobo.308.1">dimension reduction</span></strong></span><span><span class="kobospan" id="kobo.309.1">.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.310.1">It finds groups of observations that behave similarly and groups them together. </span><span class="kobospan" id="kobo.310.2">This is</span><a id="_idIndexMarker591" class="pcalibre calibre4 pcalibre1"/> <span><span class="kobospan" id="kobo.311.1">called </span></span><span><strong class="bold"><span class="kobospan" id="kobo.312.1">clustering</span></strong></span><span><span class="kobospan" id="kobo.313.1">.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.314.1">Both of these are examples of UL because they do not attempt to find a relationship between predictors and a specific response and therefore are not used to make predictions of any kind. </span><span class="kobospan" id="kobo.314.2">Unsupervised models, instead, are utilized to find organizations and representations of the data that were </span><span><span class="kobospan" id="kobo.315.1">previously unknown.</span></span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.316.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.317.1">.5</span></em><span class="kobospan" id="kobo.318.1"> gives a </span><a id="_idIndexMarker592" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.319.1">representation of</span><a id="_idIndexMarker593" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.320.1"> a </span><span><span class="kobospan" id="kobo.321.1">cluster analysis:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer243">
<span class="kobospan" id="kobo.322.1"><img alt="Figure 10.5 – Cluster analysis groups together similar data points to add a layer of interpretation on top of raw data" src="image/B19488_10_05.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.323.1">Figure 10.5 – Cluster analysis groups together similar data points to add a layer of interpretation on top of raw data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.324.1">The model will recognize that each uniquely colored cluster of observations is similar to another but different from the </span><span><span class="kobospan" id="kobo.325.1">other clusters.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.326.1">A big advantage of UL is that it does not require labeled data, which means that it is much easier to get data that complies with UL models. </span><span class="kobospan" id="kobo.326.2">Of course, a drawback to this is that we lose all predictive power because the response variable holds the information to make predictions and, without it, our model will be hopeless in making any sort </span><span><span class="kobospan" id="kobo.327.1">of predictions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.328.1">A big drawback is that it is difficult to see how well we are doing. </span><span class="kobospan" id="kobo.328.2">In a regression or classification problem, we can easily tell how well our models are predicting by comparing our models’ answers to the actual answers. </span><span class="kobospan" id="kobo.328.3">For example, if our supervised model predicts rain and it is sunny outside, the prediction is incorrect. </span><span class="kobospan" id="kobo.328.4">If our supervised model predicts the price will go up by 1 dollar and it goes up by 99 cents, our prediction is very close! </span><span class="kobospan" id="kobo.328.5">In unsupervised modeling, this concept is foreign because we have no answer to compare our models to. </span><span class="kobospan" id="kobo.328.6">Unsupervised models merely suggest differences and similarities that</span><a id="_idIndexMarker594" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.329.1"> then require a human’s</span><a id="_idIndexMarker595" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.330.1"> interpretation, as visualized in </span><span><em class="italic"><span class="kobospan" id="kobo.331.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.332.1">.6</span></em></span><span><span class="kobospan" id="kobo.333.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer244">
<span class="kobospan" id="kobo.334.1"><img alt="Figure 10.6 – Unsupervised models don’t have a notion of a “target” and instead focus on adding a layer of structure on top of raw data" src="image/B19488_10_06.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.335.1">Figure 10.6 – Unsupervised models don’t have a notion of a “target” and instead focus on adding a layer of structure on top of raw data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.336.1">In short, the main goal of unsupervised models is to find similarities and differences between data observations and use these comparisons to add structure on top of otherwise raw and </span><span><span class="kobospan" id="kobo.337.1">unstructured data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.338.1">Moving in a very different direction is our final type of ML. </span><span class="kobospan" id="kobo.338.2">To be honest with you, we don’t have the </span><a id="_idIndexMarker596" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.339.1">time nor the pages in this</span><a id="_idIndexMarker597" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.340.1"> book to cover our next topic with the respect it deserves, but it merits a place in our </span><span><span class="kobospan" id="kobo.341.1">text regardless.</span></span></p>
<h2 id="_idParaDest-149" class="calibre7"><a id="_idTextAnchor293" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.342.1">RL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.343.1">In </span><strong class="bold"><span class="kobospan" id="kobo.344.1">RL</span></strong><span class="kobospan" id="kobo.345.1">, algorithms, referred </span><a id="_idIndexMarker598" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.346.1">to as agents, learn </span><a id="_idIndexMarker599" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.347.1">to make decisions by interacting with an environment. </span><span class="kobospan" id="kobo.347.2">The agent selects an action to take based on its current state and then receives a reward or penalty based on the outcome of that action. </span><span class="kobospan" id="kobo.347.3">The goal is to learn a policy—a mapping from states to actions—that maximizes the cumulative reward </span><span><span class="kobospan" id="kobo.348.1">over time.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.349.1">This type of ML is quite distinct from SL as it does not rely on labeled input/output pairs and does not require explicit correction of suboptimal actions. </span><span class="kobospan" id="kobo.349.2">Instead, it focuses on finding a balance between exploration (trying new actions) and exploitation (using known information to maximize </span><span><span class="kobospan" id="kobo.350.1">the reward).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.351.1">RL has been successfully applied in various domains, including </span><span><span class="kobospan" id="kobo.352.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.353.1">Game playing</span></strong><span class="kobospan" id="kobo.354.1">: AI agents </span><a id="_idIndexMarker600" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.355.1">are trained to play and excel at complex games, such as Go, chess, and various video games, often surpassing human </span><span><span class="kobospan" id="kobo.356.1">expert performance</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.357.1">Robotics</span></strong><span class="kobospan" id="kobo.358.1">: Robots learn to perform tasks such as walking, picking up objects, or navigating through challenging terrain through trial </span><span><span class="kobospan" id="kobo.359.1">and error</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.360.1">Autonomous vehicles</span></strong><span class="kobospan" id="kobo.361.1">: RL is used to develop systems that can make real-time driving decisions in dynamic and </span><span><span class="kobospan" id="kobo.362.1">unpredictable environments</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.363.1">OpenAI’s pioneering work in using </span><strong class="bold"><span class="kobospan" id="kobo.364.1">RL with human feedback</span></strong><span class="kobospan" id="kobo.365.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.366.1">RLHF</span></strong><span class="kobospan" id="kobo.367.1">) has been instrumental in developing AI models such as ChatGPT. </span><span class="kobospan" id="kobo.367.2">By incorporating human preferences, these models are trained to generate responses that are not only relevant but also aligned with human values, enhancing their helpfulness and minimizing </span><span><span class="kobospan" id="kobo.368.1">potential harm.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.369.1">A typical flow for an RL problem would look something </span><span><span class="kobospan" id="kobo.370.1">like this:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.371.1">The agent receives state </span><em class="italic"><span class="kobospan" id="kobo.372.1">S</span></em><span class="kobospan" id="kobo.373.1"> from </span><span><span class="kobospan" id="kobo.374.1">the environment.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.375.1">The agent takes action </span><em class="italic"><span class="kobospan" id="kobo.376.1">A</span></em><span class="kobospan" id="kobo.377.1"> based on </span><span><span class="kobospan" id="kobo.378.1">policy </span></span><span><em class="italic"><span class="kobospan" id="kobo.379.1">π</span></em></span><span><span class="kobospan" id="kobo.380.1">.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.381.1">The environment presents a new state </span><em class="italic"><span class="kobospan" id="kobo.382.1">S</span></em><span class="kobospan" id="kobo.383.1"> and reward </span><em class="italic"><span class="kobospan" id="kobo.384.1">R</span></em><span class="kobospan" id="kobo.385.1"> to </span><span><span class="kobospan" id="kobo.386.1">the agent.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.387.1">The reward informs the agent of the </span><span><span class="kobospan" id="kobo.388.1">action’s effectiveness.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.389.1">The agent </span><a id="_idIndexMarker601" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.390.1">updates policy </span><em class="italic"><span class="kobospan" id="kobo.391.1">π</span></em><span class="kobospan" id="kobo.392.1"> to</span><a id="_idIndexMarker602" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.393.1"> increase </span><span><span class="kobospan" id="kobo.394.1">future rewards.</span></span></li>
</ol>
<h2 id="_idParaDest-150" class="calibre7"><a id="_idTextAnchor294" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.395.1">Overview of the types of ML</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.396.1">Of the three </span><a id="_idIndexMarker603" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.397.1">types of ML – SL, UL, and RL – we can imagine the world of ML as something like the depiction in </span><span><em class="italic"><span class="kobospan" id="kobo.398.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.399.1">.7</span></em></span><span><span class="kobospan" id="kobo.400.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer245">
<span class="kobospan" id="kobo.401.1"><img alt="Figure 10.7 – Our family tree of ML has three main branches: SL, UL, and RL" src="image/B19488_10_07.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.402.1">Figure 10.7 – Our family tree of ML has three main branches: SL, UL, and RL</span></p>
<h2 id="_idParaDest-151" class="calibre7"><a id="_idTextAnchor295" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.403.1">ML paradigms – pros and cons</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.404.1">As we now know, ML can be broadly classified into three categories, each with its own set of advantages </span><span><span class="kobospan" id="kobo.405.1">and disadvantages.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.406.1">SML</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.407.1">This method </span><a id="_idIndexMarker604" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.408.1">leverages the </span><a id="_idIndexMarker605" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.409.1">relationships between input predictors and the output response variable to predict future </span><span><span class="kobospan" id="kobo.410.1">data observations.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.411.1">The advantages </span><a id="_idIndexMarker606" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.412.1">of it are </span><span><span class="kobospan" id="kobo.413.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.414.1">Enables predictive analysis for </span><span><span class="kobospan" id="kobo.415.1">future events</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.416.1">Quantifies the relationships and effects </span><span><span class="kobospan" id="kobo.417.1">between variables</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.418.1">Provides insights into how variables interact and influence </span><span><span class="kobospan" id="kobo.419.1">each other</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.420.1">Let’s see </span><span><span class="kobospan" id="kobo.421.1">the disadvantage:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.422.1">Dependent </span><a id="_idIndexMarker607" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.423.1">on the availability of labeled data, which can be scarce and expensive </span><span><span class="kobospan" id="kobo.424.1">to procure</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.425.1">Unsupervised ML (UML)</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.426.1">This approach</span><a id="_idIndexMarker608" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.427.1"> discovers patterns by finding similarities and differences between data points without using </span><span><span class="kobospan" id="kobo.428.1">labeled responses.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.429.1">The advantages of it are </span><span><span class="kobospan" id="kobo.430.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.431.1">Identifies</span><a id="_idIndexMarker609" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.432.1"> subtle correlations that may not be evident to </span><span><span class="kobospan" id="kobo.433.1">human analysts</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.434.1">Serves as a valuable preprocessing step for SL, transforming raw data into </span><span><span class="kobospan" id="kobo.435.1">structured clusters</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.436.1">Utilizes unlabeled data, which is generally more abundant </span><span><span class="kobospan" id="kobo.437.1">and accessible</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.438.1">Here are </span><span><span class="kobospan" id="kobo.439.1">the disadvantages:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.440.1">Lacks direct </span><a id="_idIndexMarker610" class="pcalibre calibre4 pcalibre1"/><span><span class="kobospan" id="kobo.441.1">predictive capabilities</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.442.1">Validation of the model’s efficacy is challenging and highly reliant on </span><span><span class="kobospan" id="kobo.443.1">human judgment</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.444.1">RL</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.445.1">RL employs</span><a id="_idIndexMarker611" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.446.1"> a system of rewards to train agents to take optimal actions within </span><span><span class="kobospan" id="kobo.447.1">their environments.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.448.1">The advantages of it are </span><span><span class="kobospan" id="kobo.449.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.450.1">Capable of </span><a id="_idIndexMarker612" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.451.1">developing complex AI behaviors through intricate </span><span><span class="kobospan" id="kobo.452.1">reward systems</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.453.1">Adaptable to a wide range of environments, including </span><span><span class="kobospan" id="kobo.454.1">real-world scenarios</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.455.1">The disadvantages are </span><span><span class="kobospan" id="kobo.456.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.457.1">Initial behavior</span><a id="_idIndexMarker613" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.458.1"> can be unpredictable as the agent learns from </span><span><span class="kobospan" id="kobo.459.1">its mistakes</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.460.1">Learning can be slow, as the agent may take time to discern beneficial actions from </span><span><span class="kobospan" id="kobo.461.1">detrimental ones</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.462.1">There is a risk of the agent becoming overly cautious, limiting its actions to avoid </span><span><span class="kobospan" id="kobo.463.1">negative</span></span><span><a id="_idIndexMarker614" class="pcalibre calibre4 pcalibre1"/></span><span><span class="kobospan" id="kobo.464.1"> outcomes</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.465.1">E</span><a id="_idTextAnchor296" class="pcalibre calibre4 pcalibre1"/><a id="_idTextAnchor297" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.466.1">nough </span><a id="_idIndexMarker615" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.467.1">talk – let’s look at our first </span><span><span class="kobospan" id="kobo.468.1">ML code!</span></span></p>
<h1 id="_idParaDest-152" class="calibre6"><a id="_idTextAnchor298" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.469.1">Predicting continuous variables with linear regression</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.470.1">We will finally</span><a id="_idIndexMarker616" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.471.1"> explore our first true ML </span><a id="_idIndexMarker617" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.472.1">model! </span><span class="kobospan" id="kobo.472.2">Linear regression is a form of regression, which means that it is an ML model that attempts to find a relationship between predictors and a response variable, and that response variable is – you guessed it –continuous! </span><span class="kobospan" id="kobo.472.3">This notion is synonymous with making a </span><em class="italic"><span class="kobospan" id="kobo.473.1">line of best fit</span></em><span class="kobospan" id="kobo.474.1">. </span><span class="kobospan" id="kobo.474.2">While linear regressions are no longer a state-of-the-art ML algorithm, the path behind it can be a bit tricky and it will serve as an excellent entry point </span><span><span class="kobospan" id="kobo.475.1">for us.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.476.1">In the case of linear regression, we will attempt to find a linear relationship between our predictors and our response variable. </span><span class="kobospan" id="kobo.476.2">Formally, we wish to solve a formula of the </span><span><span class="kobospan" id="kobo.477.1">following format:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.478.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/158.png" class="calibre161"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.479.1">Let’s look at the constituents of </span><span><span class="kobospan" id="kobo.480.1">this formula:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.481.1">y is our </span><span><span class="kobospan" id="kobo.482.1">response variable</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.483.1">x</span><span class="subscript"><span class="kobospan1" id="kobo.484.1">i</span></span><span class="kobospan" id="kobo.485.1"> is our i</span><span class="superscript"><span class="kobospan1" id="kobo.486.1">th</span></span><span class="kobospan" id="kobo.487.1"> variable (i</span><span class="superscript"><span class="kobospan1" id="kobo.488.1">th</span></span><span class="kobospan" id="kobo.489.1"> column or </span><span><span class="kobospan" id="kobo.490.1">i</span></span><span><span class="superscript"><span class="kobospan1" id="kobo.491.1">th</span></span></span><span><span class="kobospan" id="kobo.492.1"> predictor)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.493.1">B</span><span class="subscript"><span class="kobospan1" id="kobo.494.1">0</span></span><span class="kobospan" id="kobo.495.1"> is </span><span><span class="kobospan" id="kobo.496.1">the intercept</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.497.1">B</span><span class="subscript"><span class="kobospan1" id="kobo.498.1">i</span></span><span class="kobospan" id="kobo.499.1"> is the coefficient for the </span><span><span class="kobospan" id="kobo.500.1">x</span></span><span><span class="subscript"><span class="kobospan1" id="kobo.501.1">i</span></span></span><span><span class="kobospan" id="kobo.502.1"> term</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.503.1">Let’s take a look at some data before we go in depth. </span><span class="kobospan" id="kobo.503.2">This dataset is publicly available and attempts to predict the number of bikes needed on a particular day for a </span><span><span class="kobospan" id="kobo.504.1">bike-sharing program:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.505.1">
# read the data and set the datetime as the index
# taken from Kaggle: https://www.kaggle.com/c/bike-sharing-demand/data
import pandas as pd
import matplotlib.pyplot as plt
url ='https://raw.githubusercontent.com/justmarkham/DAT8/master/data/bikeshare.csv'
bikes = pd.read_csv(url) bikes.head()</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.506.1">Our data can </span><a id="_idIndexMarker618" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.507.1">be</span><a id="_idIndexMarker619" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.508.1"> seen in </span><span><em class="italic"><span class="kobospan" id="kobo.509.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.510.1">.8</span></em></span><span><span class="kobospan" id="kobo.511.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer247">
<span class="kobospan" id="kobo.512.1"><img alt="Figure 10.8 – The first five rows (the head) of our bike-share data" src="image/B19488_10_08.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.513.1">Figure 10.8 – The first five rows (the head) of our bike-share data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.514.1">We can see that every row represents a single hour of bike usage. </span><span class="kobospan" id="kobo.514.2">In this case, we are interested in predicting the </span><strong class="source-inline"><span class="kobospan" id="kobo.515.1">count</span></strong><span class="kobospan" id="kobo.516.1"> value, which represents the total number of bikes rented in the period of </span><span><span class="kobospan" id="kobo.517.1">that hour.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.518.1">Let’s use the </span><strong class="source-inline"><span class="kobospan" id="kobo.519.1">seaborn</span></strong><span class="kobospan" id="kobo.520.1"> module to draw ourselves a line of best fit using only the </span><strong class="source-inline"><span class="kobospan" id="kobo.521.1">temp</span></strong><span class="kobospan" id="kobo.522.1"> feature, </span><span><span class="kobospan" id="kobo.523.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.524.1">
import seaborn as sns #using seaborn to get a line of best fit
sns.lmplot(x='temp', y='count', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2})</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.525.1">The output of this code can be seen in </span><span><em class="italic"><span class="kobospan" id="kobo.526.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.527.1">.9</span></em></span><span><span class="kobospan" id="kobo.528.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer248">
<span class="kobospan" id="kobo.529.1"><img alt="Figure 10.9 – Our first line of best fit showing us the relationship between the temperature and the number of bike shares" src="image/B19488_10_09.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.530.1">Figure 10.9 – Our first line of best fit showing us the relationship between the temperature and the number of bike shares</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.531.1">This line in the graph attempts to visualize and quantify the relationship between </span><strong class="source-inline"><span class="kobospan" id="kobo.532.1">temp</span></strong><span class="kobospan" id="kobo.533.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.534.1">count</span></strong><span class="kobospan" id="kobo.535.1">. </span><span class="kobospan" id="kobo.535.2">To make a prediction, we simply find a given temperature and then see where the line would predict</span><a id="_idIndexMarker620" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.536.1"> the </span><a id="_idIndexMarker621" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.537.1">count. </span><span class="kobospan" id="kobo.537.2">For example, if the temperature is 20 degrees (Celsius, mind you), then our line would predict that about 200 bikes will be rented. </span><span class="kobospan" id="kobo.537.3">If the temperature is above 40°C, then more than 400 bikes will </span><span><span class="kobospan" id="kobo.538.1">be needed!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.539.1">It appears that as </span><strong class="source-inline"><span class="kobospan" id="kobo.540.1">temp</span></strong><span class="kobospan" id="kobo.541.1"> goes up, our </span><strong class="source-inline"><span class="kobospan" id="kobo.542.1">count</span></strong><span class="kobospan" id="kobo.543.1"> value also goes up. </span><span class="kobospan" id="kobo.543.2">Let’s see if our correlation value, which quantifies a linear relationship between variables, also matches </span><span><span class="kobospan" id="kobo.544.1">this notion:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.545.1">
bikes[['count', 'temp']].corr() # 0.3944</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.546.1">There is a (weak) positive correlation between the two variables, which makes sense considering our line of best fit! </span><span class="kobospan" id="kobo.546.2">Let’s now use </span><strong class="source-inline"><span class="kobospan" id="kobo.547.1">pandas</span></strong><span class="kobospan" id="kobo.548.1"> to create a variable for our features (</span><strong class="source-inline"><span class="kobospan" id="kobo.549.1">X</span></strong><span class="kobospan" id="kobo.550.1">) and another one for our </span><span><span class="kobospan" id="kobo.551.1">target (</span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.552.1">y</span></strong></span><span><span class="kobospan" id="kobo.553.1">):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.554.1">
# create X and y
feature_cols = ['temp'] # a list of the predictors
X = bikes[feature_cols] # subsetting our data to only the predictors
y = bikes['count'] # our response variable</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.555.1">Our </span><strong class="source-inline"><span class="kobospan" id="kobo.556.1">X</span></strong><span class="kobospan" id="kobo.557.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.558.1">y</span></strong><span class="kobospan" id="kobo.559.1"> variables represent our predictors and our response variable. </span><span class="kobospan" id="kobo.559.2">Then, we will import our ML module, </span><strong class="source-inline"><span class="kobospan" id="kobo.560.1">scikit-learn</span></strong><span class="kobospan" id="kobo.561.1">, </span><span><span class="kobospan" id="kobo.562.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.563.1">
# import scikit-learn, our machine learning module from sklearn.linear_model import LinearRegression</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.564.1">Finally, we will fit our model to the predictors and the response variable, </span><span><span class="kobospan" id="kobo.565.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.566.1">
linreg = LinearRegression() #instantiate a new model linreg.fit(X, y) #fit the model to our data
# print the coefficients print(linreg.intercept_) print(linreg.coef_) 6.04621295962 # our Beta_0
[ 9.17054048] # our beta parameters</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.567.1">Let’s attempt to </span><span><span class="kobospan" id="kobo.568.1">interpret this:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.569.1">B0 (6.04)</span></em><span class="kobospan" id="kobo.570.1"> is the value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.571.1">y</span></strong><span class="kobospan" id="kobo.572.1"> when </span><strong class="source-inline1"><span class="kobospan" id="kobo.573.1">X</span></strong><span class="kobospan" id="kobo.574.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.575.1">0</span></strong></li>
<li class="calibre14"><span class="kobospan" id="kobo.576.1">It is the estimation of bikes that will be rented when the temperature </span><span><span class="kobospan" id="kobo.577.1">is 0°C</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.578.1">So, at 0°C, six bikes are predicted to be in use (</span><span><span class="kobospan" id="kobo.579.1">it’s cold!)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.580.1">Sometimes, it might not make sense to interpret the intercept at all because there might not be a concept of </span><a id="_idIndexMarker622" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.581.1">zero in </span><a id="_idIndexMarker623" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.582.1">some cases. </span><span class="kobospan" id="kobo.582.2">Recall the levels of data. </span><span class="kobospan" id="kobo.582.3">Not all levels have this notion of zero. </span><span class="kobospan" id="kobo.582.4">Our target variable does have the inherent notion of no bikes; so, we </span><span><span class="kobospan" id="kobo.583.1">are safe.</span></span></p>
<h2 id="_idParaDest-153" class="calibre7"><a id="_idTextAnchor299" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.584.1">Correlation versus causation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.585.1">In the context of linear </span><a id="_idIndexMarker624" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.586.1">regression, coefficients represent the strength and direction of the relationship between the predictor variables and the response variable. </span><span class="kobospan" id="kobo.586.2">However, this statistical relationship should not be confused </span><span><span class="kobospan" id="kobo.587.1">with causation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.588.1">The coefficient </span><em class="italic"><span class="kobospan" id="kobo.589.1">B1</span></em><span class="kobospan" id="kobo.590.1">, with a value of 9.17 in our previous code snippet, indicates the average change in the dependent variable (number of bikes rented) for each one-unit change in the independent variable (temperature in °C). </span><span class="kobospan" id="kobo.590.2">Concretely, this means </span><span><span class="kobospan" id="kobo.591.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.592.1">For every 1°C increase in temperature, there is an associated average increase of approximately 9 bikes </span><span><span class="kobospan" id="kobo.593.1">in rentals</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.594.1">The positive sign of this coefficient suggests a direct relationship: as temperature increases, so do </span><span><span class="kobospan" id="kobo.595.1">bike rentals</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.596.1">Yet, despite the apparent association indicated by </span><em class="italic"><span class="kobospan" id="kobo.597.1">B1</span></em><span class="kobospan" id="kobo.598.1">, we must be cautious. </span><span class="kobospan" id="kobo.598.2">This is a correlation, which means it only indicates that two variables move together—it does not imply that one causes the other to change. </span><span class="kobospan" id="kobo.598.3">A negative coefficient would have suggested an inverse relationship: as temperature rises, bike rentals would decrease. </span><span class="kobospan" id="kobo.598.4">But again, this would not </span><a id="_idIndexMarker625" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.599.1">confirm that temperature changes cause changes in </span><span><span class="kobospan" id="kobo.600.1">bike rentals.</span></span></p>
<h2 id="_idParaDest-154" class="calibre7"><a id="_idTextAnchor300" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.601.1">Causation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.602.1">To make a claim </span><a id="_idIndexMarker626" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.603.1">of causation, we would need a controlled experimental design or additional statistical techniques that account for confounding variables and establish a causal link. </span><span class="kobospan" id="kobo.603.2">Without such evidence, our findings from regression analysis should be presented as correlational insights, which highlight patterns that may warrant further investigation but do not </span><span><span class="kobospan" id="kobo.604.1">confirm causality.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.605.1">Therefore, while our temperature coefficient </span><em class="italic"><span class="kobospan" id="kobo.606.1">B1</span></em><span class="kobospan" id="kobo.607.1"> suggests a correlation between warm weather and increased bike rentals, we cannot conclude that warm weather causes more people to rent bikes without a deeper </span><span><span class="kobospan" id="kobo.608.1">causal analysis.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.609.1">Now that we are confident in our interpretations of our correlational findings, let’s use </span><strong class="source-inline"><span class="kobospan" id="kobo.610.1">scikit-learn</span></strong><span class="kobospan" id="kobo.611.1"> to make </span><span><span class="kobospan" id="kobo.612.1">some predictions:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.613.1">
linreg.predict(20) # a temperatureof 20 degrees wo</span><a id="_idTextAnchor301" class="pcalibre pcalibre1 calibre162"/><span class="kobospan1" id="kobo.614.1">uld lead our model to predict 189.46 bikes to be in use</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.615.1">This means that roughly 189 bikes will likely be rented if the temperature </span><span><span class="kobospan" id="kobo.616.1">is 20°C.</span></span></p>
<h2 id="_idParaDest-155" class="calibre7"><a id="_idTextAnchor302" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.617.1">Adding more predictors</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.618.1">Of course, temperature</span><a id="_idIndexMarker627" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.619.1"> is not the only thing that will help us predict the number of bikes. </span><span class="kobospan" id="kobo.619.2">Adding more predictors to the model is as simple as telling the linear regression model in </span><strong class="source-inline"><span class="kobospan" id="kobo.620.1">scikit-learn</span></strong> <span><span class="kobospan" id="kobo.621.1">about them!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.622.1">Before we do, we should look at the data dictionary provided to us to make more sense of some </span><span><span class="kobospan" id="kobo.623.1">more features:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.624.1">season</span></strong><span class="kobospan" id="kobo.625.1">: </span><strong class="source-inline1"><span class="kobospan" id="kobo.626.1">1</span></strong><span class="kobospan" id="kobo.627.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.628.1">spring</span></strong><span class="kobospan" id="kobo.629.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.630.1">2</span></strong><span class="kobospan" id="kobo.631.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.632.1">summer</span></strong><span class="kobospan" id="kobo.633.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.634.1">3</span></strong><span class="kobospan" id="kobo.635.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.636.1">fall</span></strong><span class="kobospan" id="kobo.637.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.638.1">4</span></strong><span class="kobospan" id="kobo.639.1"> = </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.640.1">winter</span></strong></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.641.1">holiday</span></strong><span class="kobospan" id="kobo.642.1">: Whether the day is considered </span><span><span class="kobospan" id="kobo.643.1">a holiday</span></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.644.1">workingday</span></strong><span class="kobospan" id="kobo.645.1">: Whether the day is a weekend </span><span><span class="kobospan" id="kobo.646.1">or holiday</span></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.647.1">weather</span></strong><span class="kobospan" id="kobo.648.1">: </span><strong class="source-inline1"><span class="kobospan" id="kobo.649.1">1</span></strong><span class="kobospan" id="kobo.650.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.651.1">Clear, Few clouds, Partly cloudy</span></strong><span class="kobospan" id="kobo.652.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.653.1">2</span></strong><span class="kobospan" id="kobo.654.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.655.1">Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist</span></strong><span class="kobospan" id="kobo.656.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.657.1">3</span></strong><span class="kobospan" id="kobo.658.1"> = </span><strong class="source-inline1"><span class="kobospan" id="kobo.659.1">Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds</span></strong><span class="kobospan" id="kobo.660.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.661.1">4 = Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.662.1">Fog</span></strong></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.663.1">temp</span></strong><span class="kobospan" id="kobo.664.1">: The temperature </span><span><span class="kobospan" id="kobo.665.1">in °C</span></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.666.1">atemp</span></strong><span class="kobospan" id="kobo.667.1">: The “feels like” temperature, taking wind speeds </span><span><span class="kobospan" id="kobo.668.1">into consideration</span></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.669.1">humidity</span></strong><span class="kobospan" id="kobo.670.1">: </span><span><span class="kobospan" id="kobo.671.1">Relative humidity</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.672.1">Now, let’s create our linear regression model using more features. </span><span class="kobospan" id="kobo.672.2">As before, we will first create a list holding the features we wish to look at, create our features and our response </span><a id="_idIndexMarker628" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.673.1">datasets (</span><strong class="source-inline"><span class="kobospan" id="kobo.674.1">X</span></strong><span class="kobospan" id="kobo.675.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.676.1">y</span></strong><span class="kobospan" id="kobo.677.1">), and then fit our linear regression. </span><span class="kobospan" id="kobo.677.2">Once we fit our regression model, we will take a look at the model’s coefficients in order to see how our features are interacting with </span><span><span class="kobospan" id="kobo.678.1">our response:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.679.1">
# create a list of features
feature_cols = ['temp', 'season', 'weather', 'humidity'] # create X and y
X = bikes[feature_cols] y = bikes['count']
# instantiate and fit linreg = LinearRegression() linreg.fit(X, y)
# pair the feature names with the coefficients result = zip(feature_cols, linreg.coef_) resultSet = set(result)
print(resultSet)
his gives us the following output:
[('temp', 7.8648249924774403),
('season', 22.538757532466754),
('weather', 6.6703020359238048),
('humidity', -3.1188733823964974)]</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.680.1">And this is what </span><span><span class="kobospan" id="kobo.681.1">that means:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.682.1">Holding all other predictors constant, a 1-unit increase in temperature is associated with a rental increase of </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.683.1">7.86</span></strong></span><span><span class="kobospan" id="kobo.684.1"> bikes</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.685.1">Holding all other predictors constant, a 1-unit increase in season is associated with a rental increase of </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.686.1">22.5</span></strong></span><span><span class="kobospan" id="kobo.687.1"> bikes</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.688.1">Holding all other predictors constant, a 1-unit increase in weather is associated with a rental increase of </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.689.1">6.67</span></strong></span><span><span class="kobospan" id="kobo.690.1"> bikes</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.691.1">Holding all other predictors constant, a 1-unit increase in humidity is associated with a rental decrease of </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.692.1">3.12</span></strong></span><span><span class="kobospan" id="kobo.693.1"> bikes</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.694.1">This </span><span><span class="kobospan" id="kobo.695.1">is interesting.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.696.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.697.1">Note that, as </span><strong class="source-inline1"><span class="kobospan" id="kobo.698.1">weather</span></strong><span class="kobospan" id="kobo.699.1"> goes up (meaning that the weather is getting closer to overcast), the bike demand goes up, as is the case when the season variables increase (meaning that we are approaching winter). </span><span class="kobospan" id="kobo.699.2">This is not what I was expecting at </span><span><span class="kobospan" id="kobo.700.1">all, frankly!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.701.1">While these individual correlations are helpful in many ways, it is crucial to identify metrics to judge</span><a id="_idIndexMarker629" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.702.1"> our ML system as a whole. </span><span class="kobospan" id="kobo.702.2">Usually, we think about metrics in terms of the task we are performing. </span><span class="kobospan" id="kobo.702.3">Certain metrics are useful for classification, while other metrics are more useful </span><span><span class="kobospan" id="kobo.703.1">for regression.</span></span></p>
<h2 id="_idParaDest-156" class="calibre7"><a id="_idTextAnchor303" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.704.1">Regression metrics</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.705.1">There are usually </span><a id="_idIndexMarker630" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.706.1">three main metrics when using regression ML models. </span><span class="kobospan" id="kobo.706.2">They are </span><span><span class="kobospan" id="kobo.707.1">as follows:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.708.1">Mean Absolute Error (MAE)</span></strong><span class="kobospan" id="kobo.709.1">: This is the average of the absolute errors between the</span><a id="_idIndexMarker631" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.710.1"> predicted values and the actual values. </span><span class="kobospan" id="kobo.710.2">It’s calculated by taking the sum of the absolute values of the errors (the differences between the predicted values </span><span class="kobospan" id="kobo.711.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="image/159.png" class="calibre163"/></span><span class="kobospan" id="kobo.712.1"> and the actual values </span><span class="kobospan" id="kobo.713.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/160.png" class="calibre164"/></span><span class="kobospan" id="kobo.714.1">) and then dividing by the number of </span><span><span class="kobospan" id="kobo.715.1">observations n:</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.716.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;MAE&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/161.png" class="calibre165"/></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.717.1">Mean Squared Error (MSE)</span></strong><span class="kobospan" id="kobo.718.1">: This is the average of the squares of the errors between the </span><a id="_idIndexMarker632" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.719.1">predicted values and the actual values. </span><span class="kobospan" id="kobo.719.2">It’s computed by squaring each error, summing these squares, and then dividing by the number </span><span><span class="kobospan" id="kobo.720.1">of observations:</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.721.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;MSE&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/162.png" class="calibre166"/></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.722.1">Root MSE (RMSE)</span></strong><span class="kobospan" id="kobo.723.1">: This is the </span><a id="_idIndexMarker633" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.724.1">square root</span><a id="_idIndexMarker634" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.725.1"> of the MSE. </span><span class="kobospan" id="kobo.725.2">It’s obtained by taking the square root of the average of the squared differences between the predicted values and the actual values. </span><span class="kobospan" id="kobo.725.3">RMSE is useful because it scales the errors to the original units of the output variable and can be more interpretable </span><span><span class="kobospan" id="kobo.726.1">than MSE:</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.727.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;RMSE&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/163.png" class="calibre167"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.728.1">These metrics are crucial in evaluating the performance of regression models, with each having its own advantages. </span><span class="kobospan" id="kobo.728.2">MAE provides a straightforward average-error magnitude, MSE penalizes larger errors more heavily, and RMSE is particularly sensitive to large errors due to the squaring of </span><span><span class="kobospan" id="kobo.729.1">the errors.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.730.1">Let’s take a look at implementations </span><span><span class="kobospan" id="kobo.731.1">in Python:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.732.1">
# example true and predicted response values true = [9, 6, 7, 6]
pred = [8, 7, 7, 12]
# note that each value in the last represents a single prediction for a model
# So we are comparing four predictions to four actual answers
# calculate these metrics by hand! </span><span class="kobospan1" id="kobo.732.2">from sklearn import metrics
import numpy as np
print('MAE:', metrics.mean_absolute_error(true, pred)) print('MSE:', metrics.mean_squared_error(true, pred)) print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.733.1">Here, the output </span><a id="_idIndexMarker635" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.734.1">would be </span><span><span class="kobospan" id="kobo.735.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.736.1">
MAE: 2.0
MSE: 9.5
RMSE: 3.08220700148</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.737.1">Let’s use RMSE to ascertain which columns are helping and which are hindering. </span><span class="kobospan" id="kobo.737.2">Let’s start with only using temperature. </span><span class="kobospan" id="kobo.737.3">Note that our procedure will be </span><span><span class="kobospan" id="kobo.738.1">as follows:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.739.1">Create our </span><strong class="source-inline1"><span class="kobospan" id="kobo.740.1">X</span></strong><span class="kobospan" id="kobo.741.1"> and our </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.742.1">y</span></strong></span><span><span class="kobospan" id="kobo.743.1"> variables.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.744.1">Fit a linear </span><span><span class="kobospan" id="kobo.745.1">regression model.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.746.1">Use the model to make a list of predictions based </span><span><span class="kobospan" id="kobo.747.1">on </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.748.1">X</span></strong></span><span><span class="kobospan" id="kobo.749.1">.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.750.1">Calculate the RMSE between the predictions and the </span><span><span class="kobospan" id="kobo.751.1">actual values.</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.752.1">Let’s take a look at </span><span><span class="kobospan" id="kobo.753.1">the code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.754.1">
from sklearn import metrics
# import metrics from scikit-learn
feature_cols = ['temp'] # create X and y
X = bikes[feature_cols] linreg = LinearRegression() linreg.fit(X, y)
y_pred = linreg.predict(X) np.sqrt(metrics.mean_squared_error(y, y_pred)) # RMSE # Can be interpreted loosely as an average error #166.45</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.755.1">Now, let’s try it using temperature and humidity, </span><span><span class="kobospan" id="kobo.756.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.757.1">
feature_cols = ['temp', 'humidity'] # create X and y
X = bikes[feature_cols] linreg = LinearRegression() linreg.fit(X, y)
y_pred = linreg.predict(X) np.sqrt(metrics.mean_squared_error(y, y_pred)) # RMSE # 157.79</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.758.1">It got better! </span><span class="kobospan" id="kobo.758.2">Let’s </span><a id="_idIndexMarker636" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.759.1">try using even more predictors, </span><span><span class="kobospan" id="kobo.760.1">as illustrated:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.761.1">
feature_cols = ['temp', 'humidity', 'season', 'holiday', 'workingday', 'windspeed', 'atemp']
# create X and y
X = bikes[feature_cols] linreg = LinearRegression() linreg.fit(X, y)
y_pred = linreg.predict(X) np.sqrt(metrics.mean_squared_error(y, y_pred)) # RMSE # 155.75</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.762.1">Even better! </span><span class="kobospan" id="kobo.762.2">At first, this seems like a major triumph, but there is actually a hidden danger here. </span><span class="kobospan" id="kobo.762.3">Note that we are training the line to fit </span><strong class="source-inline"><span class="kobospan" id="kobo.763.1">X</span></strong><span class="kobospan" id="kobo.764.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.765.1">y</span></strong><span class="kobospan" id="kobo.766.1"> and then asking it to predict </span><strong class="source-inline"><span class="kobospan" id="kobo.767.1">X</span></strong><span class="kobospan" id="kobo.768.1"> again! </span><span class="kobospan" id="kobo.768.2">This is actually a huge mistake in ML because it can lead to overfitting, which means that our model is merely memorizing the data and regurgitating it back </span><span><span class="kobospan" id="kobo.769.1">to us.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.770.1">Imagine that you are a student, and you walk into the first day of class and the teacher says that the final exam is very difficult in this class. </span><span class="kobospan" id="kobo.770.2">In order to prepare you, she gives you practice test after practice test after practice test. </span><span class="kobospan" id="kobo.770.3">The day of the final exam arrives, and you are shocked to find out that every question on the exam is exactly the same as in the practice test! </span><span class="kobospan" id="kobo.770.4">Luckily, you did them so many times that you remember the answer and get 100% on </span><span><span class="kobospan" id="kobo.771.1">the exam.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.772.1">The same thing applies here, more or less. </span><span class="kobospan" id="kobo.772.2">By fitting and predicting on the same data, the model is memorizing the data and getting better at it. </span><span class="kobospan" id="kobo.772.3">A great way to combat this </span><strong class="bold"><span class="kobospan" id="kobo.773.1">overfitting</span></strong><span class="kobospan" id="kobo.774.1"> problem</span><a id="_idIndexMarker637" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.775.1"> is to use the train/test approach to fit ML models, which works as </span><span><span class="kobospan" id="kobo.776.1">illustrated next.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.777.1">Essentially, we will take the </span><span><span class="kobospan" id="kobo.778.1">following steps:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.779.1">Split up the </span><a id="_idIndexMarker638" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.780.1">dataset into two parts: a training and a </span><span><span class="kobospan" id="kobo.781.1">test set.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.782.1">Fit our model on the training set and then test it on the test set, just like in school, where the teacher would teach from one set of notes and then test us on different (but </span><span><span class="kobospan" id="kobo.783.1">similar) questions.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.784.1">Once our model is good enough (based on our metrics), we turn our model’s attention toward the </span><span><span class="kobospan" id="kobo.785.1">entire dataset.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.786.1">Our model awaits new data previously unseen </span><span><span class="kobospan" id="kobo.787.1">by anyone.</span></span><p class="calibre3"><span class="kobospan" id="kobo.788.1">This can be visualized in </span><span><em class="italic"><span class="kobospan" id="kobo.789.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.790.1">.10</span></em></span><span><span class="kobospan" id="kobo.791.1">:</span></span></p></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer254">
<span class="kobospan" id="kobo.792.1"><img alt="Figure 10.10 – Splitting our data up into a training and testing set helps us properly evaluate our model’s ability to predict unseen data" src="image/B19488_10_10.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.793.1">Figure 10.10 – Splitting our data up into a training and testing set helps us properly evaluate our model’s ability to predict unseen data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.794.1">The goal here is to minimize the out-of-sample errors of our model, which are errors our model has on data that it has never seen before. </span><span class="kobospan" id="kobo.794.2">This is important because the main idea (usually) of a supervised model is to predict outcomes for new data. </span><span class="kobospan" id="kobo.794.3">If our model is unable to generalize from our training data and use that to predict unseen cases, then our model isn’t </span><span><span class="kobospan" id="kobo.795.1">very good.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.796.1">The preceding diagram outlines a simple way of ensuring that our model can effectively ingest the training data and use it to predict data points that the model itself has never seen. </span><span class="kobospan" id="kobo.796.2">Of course, as data scientists, we know that the test set also has answers attached to it, but the model doesn’t </span><span><span class="kobospan" id="kobo.797.1">know that.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.798.1">All of this might sound complicated, but luckily, the </span><strong class="source-inline"><span class="kobospan" id="kobo.799.1">scikit-learn</span></strong><span class="kobospan" id="kobo.800.1"> package has a built-in method to </span><a id="_idIndexMarker639" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.801.1">do this, </span><span><span class="kobospan" id="kobo.802.1">as shown:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.803.1">
from sklearn.cross_validation import train_test_split
# function that splits data into training and testing sets
# setting our overall data X, and y
feature_cols = ['temp']
X = bikes[feature_cols]
y = bikes['count']
# Note that in this example, we are attempting to find an association between only the temperature of the day and the number of bike rentals.
</span><span class="kobospan1" id="kobo.803.2">X_train, X_test, y_train, y_test = train_test_split(X, y) # split the data into training and testing sets
# X_train and y_train will be used to train the model # X_test and y_test will be used to test the model
# Remember that all four of these variables are just subsets of the overall X and y.
</span><span class="kobospan1" id="kobo.803.3">linreg = LinearRegression() # instantiate the model
linreg.fit(X_train, y_train)
# fit the model to our training set
y_pred = linreg.predict(X_test) # predict our testing set
np.sqrt(metrics.mean_squared_error(y_test, y_pred)) # RMSE # Calculate our metric:
# == 166.91</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.804.1">In other words, our </span><strong class="source-inline"><span class="kobospan" id="kobo.805.1">train_test_split</span></strong><span class="kobospan" id="kobo.806.1"> function is ensuring that the metrics we are looking at are more honest estimates of our </span><span><span class="kobospan" id="kobo.807.1">sample performance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.808.1">Now, let’s try again with more predictors, </span><span><span class="kobospan" id="kobo.809.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.810.1">
feature_cols = ['temp', 'workingday']
X = bikes[feature_cols]
y = bikes['count']
X_train, X_test, y_train, y_test = train_test_split(X, y) # Pick a new random training and test set
linreg = LinearRegression() linreg.fit(X_train, y_train) y_pred = linreg.predict(X_test) # fit and predict
np.sqrt(metrics.mean_squared_error(y_test, y_pred)) # 166.95</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.811.1">Our model </span><a id="_idIndexMarker640" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.812.1">actually got worse with that addition! </span><span class="kobospan" id="kobo.812.2">This implies that </span><strong class="source-inline"><span class="kobospan" id="kobo.813.1">workingday</span></strong><span class="kobospan" id="kobo.814.1"> might not be very predictive of our response, the bike </span><span><span class="kobospan" id="kobo.815.1">rental count.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.816.1">All of this is well and good, and we can keep adding and removing features to lower our RMSE, but how well is our model really doing at predicting rather than just guessing? </span><span class="kobospan" id="kobo.816.2">We have an RMSE of around 167 bikes, but is that good? </span><span class="kobospan" id="kobo.816.3">What do we compare it to? </span><span class="kobospan" id="kobo.816.4">One way to discover this is to evaluate the </span><span><span class="kobospan" id="kobo.817.1">null model.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.818.1">The </span><strong class="bold"><span class="kobospan" id="kobo.819.1">null model</span></strong><span class="kobospan" id="kobo.820.1"> in SML </span><a id="_idIndexMarker641" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.821.1">represents effectively guessing the expected outcome over and over, and seeing how you did. </span><span class="kobospan" id="kobo.821.2">For example, in regression, if we always guess the average number of hourly bike rentals, then how well would that </span><span><span class="kobospan" id="kobo.822.1">model do?</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.823.1">First, let’s get the average hourly bike rental, </span><span><span class="kobospan" id="kobo.824.1">as shown:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.825.1">
average_bike_rental = bikes['count'].mean() average_bike_rental</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.826.1">
# 191.57</span></pre><p class="calibre3"><span class="kobospan" id="kobo.827.1">This means that, overall, in this dataset, regardless of weather, time, day of the week, humidity, and everything else, the average number of bikes that go out every hour is </span><span><span class="kobospan" id="kobo.828.1">about 192.</span></span></p></li>
<li class="calibre14"><span class="kobospan" id="kobo.829.1">Let’s make a fake prediction list, wherein every single guess is 191.57. </span><span class="kobospan" id="kobo.829.2">Let’s make this guess for every single hour, </span><span><span class="kobospan" id="kobo.830.1">as follows:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.831.1">
num_rows = bikes.shape[0] num_rows</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.832.1">
# 10886</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.833.1">
null_model_predictions = [average_bike_rental] * num_rows null_model_predictions</span></pre><p class="calibre3"><span class="kobospan" id="kobo.834.1">The output is </span><a id="_idIndexMarker642" class="pcalibre calibre4 pcalibre1"/><span><span class="kobospan" id="kobo.835.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.836.1">
[191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.837.1">
191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.838.1">
191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.839.1">
191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.840.1">
...</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.841.1">
191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.842.1">
191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.843.1">
191.57413191254824,</span></pre><pre class="source-code"><span class="kobospan1" id="kobo.844.1">
191.57413191254824]</span></pre><p class="calibre3"><span class="kobospan" id="kobo.845.1">So, we have </span><strong class="source-inline"><span class="kobospan" id="kobo.846.1">10,886</span></strong><span class="kobospan" id="kobo.847.1"> values, all of which are the average hourly bike rental number. </span><span class="kobospan" id="kobo.847.2">Let’s see what the RMSE would be if our model only ever guessed the expected value of the average hourly bike </span><span><span class="kobospan" id="kobo.848.1">rental count:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.849.1">
np.sqrt(metrics.mean_squared_error(y, null_model_predictions))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.850.1">The output is </span><span><span class="kobospan" id="kobo.851.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.852.1">
181.13613</span></pre></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.853.1">This means that by simply guessing the average value over and over again, our RMSE would be 181 bikes. </span><span class="kobospan" id="kobo.853.2">So, even with one or two features, we can beat it! </span><span class="kobospan" id="kobo.853.3">Beating the null model is a kind of </span><a id="_idIndexMarker643" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.854.1">baseline in ML. </span><span class="kobospan" id="kobo.854.2">If you think about it, why go thro</span><a id="_idTextAnchor304" class="pcalibre calibre4 pcalibre1"/><a id="_idTextAnchor305" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.855.1">ugh any effort at all if your ML is not even better than </span><span><span class="kobospan" id="kobo.856.1">just guessing?</span></span></p>
<h1 id="_idParaDest-157" class="calibre6"><a id="_idTextAnchor306" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.857.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.858.1">In this chapter, we looked at ML and its different subcategories. </span><span class="kobospan" id="kobo.858.2">We explored SL, UL, and RL strategies and looked at situations where each one would come </span><span><span class="kobospan" id="kobo.859.1">in handy.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.860.1">Looking into linear regression, we were able to find relationships between predictors and a continuous response variable. </span><span class="kobospan" id="kobo.860.2">Through the train/test split, we were able to help avoid overfitting our ML models and get a more generalized prediction. </span><span class="kobospan" id="kobo.860.3">We were able to use metrics, such as RMSE, to evaluate our models </span><span><span class="kobospan" id="kobo.861.1">as well.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.862.1">In the next few chapters, we will be taking a much deeper dive into many more ML models and, along the way, we will learn new metrics, new validation techniques, and – more importantly – new ways of applying data science to </span><span><span class="kobospan" id="kobo.863.1">the world.</span></span></p>
</div>
</body></html>