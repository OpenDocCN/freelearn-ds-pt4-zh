- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to Tell if Your Toaster is Learning – Machine Learning Essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It seems as though every time we hear about the next great start-up or turn
    on the news, we hear something about a revolutionary piece of **machine learning**
    (**ML**) or **artificial intelligence** (**AI**) technology and how it will change
    the way we live. This chapter focuses on ML as a practical part of data science.
    We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining different types of ML, along with examples of each kind
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ML, and how is it used in data science?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between ML and statistical modeling and how ML is a broad category
    of the latter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Introduction to Linear Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our aim in this chapter will be to utilize statistics, probability, and algorithmic
    thinking in order to understand and apply essential ML skills to practical industries,
    such as marketing. Examples will include predicting star ratings of restaurant
    reviews, predicting the presence of a disease, spam email detection, and much
    more. This chapter focuses on ML as a whole and as a single statistical model.
    The subsequent chapters will deal with many more models, some of which are much
    more complex.
  prefs: []
  type: TYPE_NORMAL
- en: We will also turn our focus on metrics, which tell us how effective our models
    are. We will use metrics in order to conclude results and make predictions using
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B19488_01.xhtml#_idTextAnchor015), *Data Science Terminology*,
    we defined ML as giving computers the ability to learn from data without being
    given explicit rules by a programmer. This definition still holds true. ML is
    concerned with the ability to ascertain certain patterns (signals) out of data,
    even if the data has inherent errors in it (noise).
  prefs: []
  type: TYPE_NORMAL
- en: ML models are able to learn from data without the explicit direction of a human.
    That is the main difference between ML models and classical non-ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Classical algorithms are told directly by a human how to find the best answer
    in a complex system, and the algorithm then achieves these best solutions, often
    working faster and more efficiently than a human. However, the bottleneck here
    is that the human has to first come up with the best solution in order to tell
    the algorithm what to do. In ML, the model is not told the best solution and,
    instead, is given several examples of the problem and told to figure out the best
    solution for itself.
  prefs: []
  type: TYPE_NORMAL
- en: ML is just another tool in the belt of a data scientist. It is on the same level
    as statistical tests (chi-square or t-tests) or uses basic probability or statistics
    to estimate population parameters. ML is often regarded as the only thing data
    scientists know how to do, and this is simply untrue. A true data scientist is
    able to recognize when ML is applicable and, more importantly, when it is not.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a game of correlations and relationships. Most ML algorithms in existence
    are concerned with finding and/or exploiting relationships between datasets (often
    represented as columns in a `pandas` DataFrame). Once ML algorithms can pinpoint
    certain correlations, the model can either use these relationships to predict
    future observations or generalize the data to reveal interesting patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps a great way to explain ML is to offer an example of a problem coupled
    with two possible solutions: one using an ML algorithm and the other utilizing
    a non-ML algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Example – facial recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This problem is, on its face (pun intended), quite simple: given a picture
    of a face, who does it belong to? However, let’s consider a slightly simpler task.
    Suppose you wish to implement a home security system that recognizes who is entering
    your house. Most likely, during the day, your house will be empty most of the
    time, and facial recognition will kick in only if there is a person in the shot.
    This is exactly the question I propose we try to solve – given a photo, is there
    a face in it to even recognize?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this task definition, I propose the following two solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: A non-ML algorithm that will define a face as having a roundish structure, two
    eyes, hair, nose, and so on. The algorithm then looks for these hardcoded features
    in the photo and returns whether or not it was able to find any of these features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ML algorithm that will work a bit differently. The model will only be given
    several pictures of faces and non-faces that are labeled as such. From the examples
    (called training sets), it would figure out its own definition of a face.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ML version of the solution is never told what a face is; it is merely given
    several examples – some with faces, and some without. It is then up to the ML
    model to figure out the difference between the two. Once it figures this out,
    it uses this information to take in a picture and predict whether or not there
    is a face in the new picture. For example, to train the system, we might have
    the following images denoted in *Figure 10**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Input images for training an ML model](img/B19488_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Input images for training an ML model
  prefs: []
  type: TYPE_NORMAL
- en: The model will then figure out the difference between the pictures labeled as
    *Face* and the pictures labeled as *No Face* and be able to use that difference
    to find faces in future photos. Because the promise of ML – learning simply from
    data and without explicit human intervention – is so alluring, many people might
    believe that ML is perfect, but it simply isn’t.
  prefs: []
  type: TYPE_NORMAL
- en: ML isn’t perfect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many caveats of ML. Many are specific to different models being implemented,
    but some assumptions are universal for any ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: The data used, for the most part, is preprocessed and cleaned using the methods
    outlined in the earlier chapters. Almost no ML model will tolerate extremely dirty/incomplete
    data with missing values or categorical values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row of a cleaned dataset represents a single observation of the environment
    we are trying to model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data as a whole should be representative of the task we are solving. This
    might sound obvious, but in so many cases, people use data to train an ML model
    that is close to but not exactly related to the task. This is often seen in criminal
    justice examples where people might use arrest data to train a model to predict
    criminality but, of course, arrests are not the same as convicting someone of
    a crime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If our goal is to find relationships between variables, then there is an assumption
    that there is some kind of relationship between these variables. Again, this seems
    obvious, but if a human putting the data together is biased and “believes” there
    is a relationship between the data, then they might incorrectly judge an ML model
    to be more powerful than it actually is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This assumption is particularly important. Many ML models take this assumption
    very seriously. These models are not able to communicate that there might not
    be a relationship.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ML models are generally considered semi-automatic, which means that intelligent
    decisions by humans are still needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The machine is very smart but has a hard time putting things into context. The
    output of most models is a series of numbers and metrics attempting to quantify
    how well the model did. It is up to a human to put these metrics into perspective
    and communicate the results to an audience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most ML models are sensitive to noisy data. This means that the models get confused
    when you include data that doesn’t make sense. For example, if you are attempting
    to find relationships between economic data around the world and one of your columns
    relates to puppy adoption rates in the capital city, that information is likely
    not relevant and will confuse the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These assumptions will come up again and again when dealing with ML. They are
    all too important and are often ignored by novice data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: How does ML work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each flavor of ML and each individual model works in very different ways, exploiting
    different parts of mathematics and data science. However, in general, ML works
    by taking in data, finding relationships within the data, and giving as output
    what the model learned, as illustrated in *Figure 10**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – An overview of ML models taking in input data, learning signals,
    and identifying patterns in order to produce a meaningful and interpretable output](img/B19488_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – An overview of ML models taking in input data, learning signals,
    and identifying patterns in order to produce a meaningful and interpretable output
  prefs: []
  type: TYPE_NORMAL
- en: As we explore different types of ML models, we will see how they manipulate
    data differently and come up with different outputs for different applications.
  prefs: []
  type: TYPE_NORMAL
- en: Types of ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many ways to segment ML and dive deeper. In [*Chapter 1*](B19488_01.xhtml#_idTextAnchor015),
    *Data Science Terminology*, I mentioned statistical and probabilistic models.
    These models utilize statistics and probability, which we’ve seen in the previous
    chapters, in order to find relationships between data and make predictions. In
    this chapter, we will implement both types of models. In the following chapter,
    we will see ML outside the rigid mathematical world of statistics/probability.
    You can segment ML models by different characteristics, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The types of data organic structures they utilize (tree, graph, or **neural**
    **network** (**NN**))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field of mathematics they are most related to (statistical or probabilistic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The level of computation required to train (**deep** **learning** (**DL**))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Branching off from the top level of ML, there are the following three subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised** **learning** (**SL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised** **learning** (**UL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement** **learning** (**RL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go into each one of these one by one. Our next chapter will include multiple
    examples of the first two, with the third one being slightly out of the scope
    of our introductory book. You can always find more resources in our code base!
  prefs: []
  type: TYPE_NORMAL
- en: SL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply put, **SL** finds associations between features of a dataset (independent
    variables) and a target (dependent) variable. For example, SL models might try
    to find the association between a person’s health features (heart rate, weight,
    and so on) and that person’s risk of having a heart attack (the target variable).
    These associations allow supervised models to make predictions based on past examples.
    **Supervised ML** (**SML**) models are often called predictive analytics models,
    named for their ability to predict the future based on the past. This is often
    the first thing that comes to people’s minds when they hear the term *ML*, but
    it in no way encompasses the realm of ML.
  prefs: []
  type: TYPE_NORMAL
- en: SML requires a certain type of data called **labeled data** – data that acts
    as full, correct, and complete examples of the features and target variable. *Figure
    10**.1* shows a snippet of labeled data. The goal is to let our model learn by
    giving it historical examples that are labeled with the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the facial recognition example. That is an SL model because we are training
    our model with the previous pictures labeled as either *face* or *not face*, and
    then asking the model to predict whether or not a new picture has a face in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us separate the data into two distinct parts, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The features, which are the columns that will be used to make our prediction.
    These are sometimes called predictors, input values, variables, and independent
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response, which is the column that we wish to predict. This is sometimes
    called outcome, label, target, and dependent variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SL attempts to find a relationship between features and responses in order
    to make a prediction. The idea is that, in the future, a data observation will
    present itself, and we will only know the predictors. The model will then have
    to use the features to make an accurate prediction of the response value. *Figure
    10**.3* shows a visualization of how we generally use supervised models: we train
    (fit) them using labeled training data and use the result to predict unseen cases
    (features without the response) to make final predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 10.3 – Supervised models are fit using labeled training data and are\
    \ then used to make predictions fro\uFEFFm unseen cases](img/B19488_10_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Supervised models are fit using labeled training data and are
    then used to make predictions from unseen cases
  prefs: []
  type: TYPE_NORMAL
- en: Example – heart attack prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we wish to predict whether someone will have a heart attack within a
    year. To predict this, we are given that person’s cholesterol level, blood pressure,
    height, smoking habits, and perhaps more. From this data, we must ascertain the
    likelihood of a heart attack. Suppose, to make this prediction, we look at previous
    patients and their medical history. As these are previous patients, we know not
    only their predictors (cholesterol, blood pressure, and so on) but also if they
    actually had a heart attack (because it already happened!).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an SML problem because we are doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We are making a prediction about someone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are using historical training data to find relationships between medical
    variables and heart attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 10**.4* shows a basic outline of how SML models use data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – An SML model uses predictors and a response from data in order
    to learn relationships between them, usually in order to make future predictions
    given predictors without the response](img/B19488_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – An SML model uses predictors and a response from data in order
    to learn relationships between them, usually in order to make future predictions
    given predictors without the response
  prefs: []
  type: TYPE_NORMAL
- en: The hope here is that a patient will walk in sometime in the future and our
    model will be able to identify whether or not the patient is at risk for a heart
    attack based on their conditions (just like a doctor would!).
  prefs: []
  type: TYPE_NORMAL
- en: 'As the model sees more and more diverse and representative labeled data, it
    should adjust itself in order to match the correct labels outlined in the training
    data. We can then use different metrics (explained more in the next chapter) to
    pinpoint exactly how well our SML model is doing and how it can better adjust
    itself. One of the largest obstacles associated with SML is obtaining diverse
    and representative labeled data, which can be very difficult to get hold of. Suppose
    we wish to predict heart attacks; we might need thousands of patients along with
    all of their medical information and years’ worth of follow-up records for each
    person, which could be a nightmare to obtain. In short, supervised models use
    historical labeled data in order to make predictions about the future from predefined
    features. Some possible applications for SL include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stock price predictions**: Historical trading volume and movements could
    be features along with social media sentiment, while the future price could be
    a target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weather predictions**: Using past meteorological data, such as temperature,
    humidity, and wind speed, to forecast future weather conditions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disease diagnosis**: Medical imaging and patient history can be used to predict
    the presence or absence of a disease'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial recognition**: Features extracted from images of faces to identify
    individuals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Email filtering**: Using characteristics of emails to classify them as spam
    or not spam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Credit scoring**: Historical financial behavior data to predict creditworthiness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these applications relies on a labeled dataset that includes historical
    data points and a target variable that the model is trying to predict. The quality
    and quantity of the labeled data are crucial in SL as they directly impact the
    model’s ability to learn and generalize to new, unseen data. When designing an
    SL model, it is important to consider the features that will be used. Features
    should be relevant, informative, and non-redundant to ensure the model performs
    effectively. Additionally, the choice of algorithm depends on the nature of the
    task (regression, classification), the size and dimensionality of the dataset,
    and the computational efficiency required.
  prefs: []
  type: TYPE_NORMAL
- en: By carefully preparing the dataset and selecting the right features and model,
    SL can provide powerful predictive insights across various fields, from finance
    to healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: Types of SL models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are, in general, two types of SL models: **regression** and **classification**
    models. The difference between the two is quite simple and lies in the nature
    of the response variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** models attempt to predict a continuous response. This means
    that the response can take on a range of infinite values. Consider the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**House pricing**: Where the value to be predicted is the cost of a house based
    on features such as square footage, number of bedrooms, and location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature forecasting**: Where the model predicts the temperature for future
    days or hours based on historical weather data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stock market price prediction**: Where the continuous response could be the
    future price of a stock based on its historical performance and other economic
    indicators'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification** models, on the other hand, predict categorical responses.
    These responses are discrete and have a finite number of values, often referred
    to as classes or categories. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Email spam detection**: The model classifies emails as either “spam” or “not
    spam” based on content, sender information, and other attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: A model might classify patient outcomes based on test
    results, predicting categories such as “disease” or “no disease”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image recognition**: Classifying images into predefined categories such as
    “cat," “dog," “car," and so on, based on pixel data and patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both regression and classification tasks use a similar process of learning
    from historical data, but their applications and evaluation metrics differ due
    to the nature of their output. Our earlier heart attack example is classification
    because the question was: Will this person have a heart attack within a year?
    This has only two possible answers: *Yes* or *No*. We will see a full example
    of regression later in this chapter and full examples of both classification and
    regression in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Deciding between classification and regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, it can be tricky to decide whether or not you should use classification
    or regression. Consider that we are interested in the weather outside. We could
    ask the question, *How hot is it outside?* In this case, your answer is on a continuous
    scale, and some possible answers are 60.7 degrees or 98 degrees. However, as an
    exercise, go and ask 10 people what the temperature is outside. I guarantee you
    that someone (if not most people) will not answer in some exact degrees but will
    bucket their answer and say something like *It’s in* *the 60s*.
  prefs: []
  type: TYPE_NORMAL
- en: We might wish to consider this problem as a classification problem, where the
    response variable is no longer in exact degrees but is in a bucket. There would
    only be a finite number of buckets in theory, making the model perhaps learn the
    differences between 60s and 70s a bit better.
  prefs: []
  type: TYPE_NORMAL
- en: What if we aren’t predicting anything, but instead we wanted to use ML to simply
    better understand and interpret our data? That’s where UL comes in very handy.
  prefs: []
  type: TYPE_NORMAL
- en: UL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second type of ML on our list does not deal with making predictions but
    has a much more open objective. **UL** takes in a set of predictors and utilizes
    relationships between the predictors in order to accomplish tasks such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the dimension of the data by condensing variables together. An example
    of this would be file compression. Compression works by utilizing patterns in
    the data and representing the data in a smaller format. This is referred to as
    **dimension reduction**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It finds groups of observations that behave similarly and groups them together.
    This is called **clustering**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these are examples of UL because they do not attempt to find a relationship
    between predictors and a specific response and therefore are not used to make
    predictions of any kind. Unsupervised models, instead, are utilized to find organizations
    and representations of the data that were previously unknown.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.5* gives a representation of a cluster analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Cluster analysis groups together similar data points to add
    a layer of interpretation on top of raw data](img/B19488_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Cluster analysis groups together similar data points to add a
    layer of interpretation on top of raw data
  prefs: []
  type: TYPE_NORMAL
- en: The model will recognize that each uniquely colored cluster of observations
    is similar to another but different from the other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: A big advantage of UL is that it does not require labeled data, which means
    that it is much easier to get data that complies with UL models. Of course, a
    drawback to this is that we lose all predictive power because the response variable
    holds the information to make predictions and, without it, our model will be hopeless
    in making any sort of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A big drawback is that it is difficult to see how well we are doing. In a regression
    or classification problem, we can easily tell how well our models are predicting
    by comparing our models’ answers to the actual answers. For example, if our supervised
    model predicts rain and it is sunny outside, the prediction is incorrect. If our
    supervised model predicts the price will go up by 1 dollar and it goes up by 99
    cents, our prediction is very close! In unsupervised modeling, this concept is
    foreign because we have no answer to compare our models to. Unsupervised models
    merely suggest differences and similarities that then require a human’s interpretation,
    as visualized in *Figure 10**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Unsupervised models don’t have a notion of a “target” and instead
    focus on adding a layer of structure on top of raw data](img/B19488_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Unsupervised models don’t have a notion of a “target” and instead
    focus on adding a layer of structure on top of raw data
  prefs: []
  type: TYPE_NORMAL
- en: In short, the main goal of unsupervised models is to find similarities and differences
    between data observations and use these comparisons to add structure on top of
    otherwise raw and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Moving in a very different direction is our final type of ML. To be honest with
    you, we don’t have the time nor the pages in this book to cover our next topic
    with the respect it deserves, but it merits a place in our text regardless.
  prefs: []
  type: TYPE_NORMAL
- en: RL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In **RL**, algorithms, referred to as agents, learn to make decisions by interacting
    with an environment. The agent selects an action to take based on its current
    state and then receives a reward or penalty based on the outcome of that action.
    The goal is to learn a policy—a mapping from states to actions—that maximizes
    the cumulative reward over time.
  prefs: []
  type: TYPE_NORMAL
- en: This type of ML is quite distinct from SL as it does not rely on labeled input/output
    pairs and does not require explicit correction of suboptimal actions. Instead,
    it focuses on finding a balance between exploration (trying new actions) and exploitation
    (using known information to maximize the reward).
  prefs: []
  type: TYPE_NORMAL
- en: 'RL has been successfully applied in various domains, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Game playing**: AI agents are trained to play and excel at complex games,
    such as Go, chess, and various video games, often surpassing human expert performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robotics**: Robots learn to perform tasks such as walking, picking up objects,
    or navigating through challenging terrain through trial and error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous vehicles**: RL is used to develop systems that can make real-time
    driving decisions in dynamic and unpredictable environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s pioneering work in using **RL with human feedback** (**RLHF**) has
    been instrumental in developing AI models such as ChatGPT. By incorporating human
    preferences, these models are trained to generate responses that are not only
    relevant but also aligned with human values, enhancing their helpfulness and minimizing
    potential harm.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical flow for an RL problem would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent receives state *S* from the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent takes action *A* based on policy *π*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The environment presents a new state *S* and reward *R* to the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reward informs the agent of the action’s effectiveness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent updates policy *π* to increase future rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overview of the types of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Of the three types of ML – SL, UL, and RL – we can imagine the world of ML
    as something like the depiction in *Figure 10**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Our family tree of ML has three main branches: SL, UL, and
    RL](img/B19488_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7 – Our family tree of ML has three main branches: SL, UL, and RL'
  prefs: []
  type: TYPE_NORMAL
- en: ML paradigms – pros and cons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we now know, ML can be broadly classified into three categories, each with
    its own set of advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: SML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method leverages the relationships between input predictors and the output
    response variable to predict future data observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of it are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Enables predictive analysis for future events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantifies the relationships and effects between variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides insights into how variables interact and influence each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see the disadvantage:'
  prefs: []
  type: TYPE_NORMAL
- en: Dependent on the availability of labeled data, which can be scarce and expensive
    to procure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised ML (UML)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach discovers patterns by finding similarities and differences between
    data points without using labeled responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of it are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifies subtle correlations that may not be evident to human analysts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serves as a valuable preprocessing step for SL, transforming raw data into structured
    clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizes unlabeled data, which is generally more abundant and accessible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Lacks direct predictive capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation of the model’s efficacy is challenging and highly reliant on human
    judgment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RL employs a system of rewards to train agents to take optimal actions within
    their environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of it are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Capable of developing complex AI behaviors through intricate reward systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptable to a wide range of environments, including real-world scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initial behavior can be unpredictable as the agent learns from its mistakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning can be slow, as the agent may take time to discern beneficial actions
    from detrimental ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a risk of the agent becoming overly cautious, limiting its actions
    to avoid negative outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enough talk – let’s look at our first ML code!
  prefs: []
  type: TYPE_NORMAL
- en: Predicting continuous variables with linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will finally explore our first true ML model! Linear regression is a form
    of regression, which means that it is an ML model that attempts to find a relationship
    between predictors and a response variable, and that response variable is – you
    guessed it –continuous! This notion is synonymous with making a *line of best
    fit*. While linear regressions are no longer a state-of-the-art ML algorithm,
    the path behind it can be a bit tricky and it will serve as an excellent entry
    point for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of linear regression, we will attempt to find a linear relationship
    between our predictors and our response variable. Formally, we wish to solve a
    formula of the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">β</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/158.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s look at the constituents of this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: y is our response variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: xi is our ith variable (ith column or ith predictor)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B0 is the intercept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi is the coefficient for the xi term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at some data before we go in depth. This dataset is publicly
    available and attempts to predict the number of bikes needed on a particular day
    for a bike-sharing program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data can be seen in *Figure 10**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The first five rows (the head) of our bike-share data](img/B19488_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – The first five rows (the head) of our bike-share data
  prefs: []
  type: TYPE_NORMAL
- en: We can see that every row represents a single hour of bike usage. In this case,
    we are interested in predicting the `count` value, which represents the total
    number of bikes rented in the period of that hour.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the `seaborn` module to draw ourselves a line of best fit using only
    the `temp` feature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code can be seen in *Figure 10**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Our first line of best fit showing us the relationship between
    the temperature and the number of bike shares](img/B19488_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Our first line of best fit showing us the relationship between
    the temperature and the number of bike shares
  prefs: []
  type: TYPE_NORMAL
- en: This line in the graph attempts to visualize and quantify the relationship between
    `temp` and `count`. To make a prediction, we simply find a given temperature and
    then see where the line would predict the count. For example, if the temperature
    is 20 degrees (Celsius, mind you), then our line would predict that about 200
    bikes will be rented. If the temperature is above 40°C, then more than 400 bikes
    will be needed!
  prefs: []
  type: TYPE_NORMAL
- en: 'It appears that as `temp` goes up, our `count` value also goes up. Let’s see
    if our correlation value, which quantifies a linear relationship between variables,
    also matches this notion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a (weak) positive correlation between the two variables, which makes
    sense considering our line of best fit! Let’s now use `pandas` to create a variable
    for our features (`X`) and another one for our target (`y`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `X` and `y` variables represent our predictors and our response variable.
    Then, we will import our ML module, `scikit-learn`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will fit our model to the predictors and the response variable,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s attempt to interpret this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*B0 (6.04)* is the value of **y** when **X** = **0**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is the estimation of bikes that will be rented when the temperature is 0°C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, at 0°C, six bikes are predicted to be in use (it’s cold!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, it might not make sense to interpret the intercept at all because
    there might not be a concept of zero in some cases. Recall the levels of data.
    Not all levels have this notion of zero. Our target variable does have the inherent
    notion of no bikes; so, we are safe.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation versus causation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of linear regression, coefficients represent the strength and
    direction of the relationship between the predictor variables and the response
    variable. However, this statistical relationship should not be confused with causation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The coefficient *B1*, with a value of 9.17 in our previous code snippet, indicates
    the average change in the dependent variable (number of bikes rented) for each
    one-unit change in the independent variable (temperature in °C). Concretely, this
    means the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For every 1°C increase in temperature, there is an associated average increase
    of approximately 9 bikes in rentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The positive sign of this coefficient suggests a direct relationship: as temperature
    increases, so do bike rentals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yet, despite the apparent association indicated by *B1*, we must be cautious.
    This is a correlation, which means it only indicates that two variables move together—it
    does not imply that one causes the other to change. A negative coefficient would
    have suggested an inverse relationship: as temperature rises, bike rentals would
    decrease. But again, this would not confirm that temperature changes cause changes
    in bike rentals.'
  prefs: []
  type: TYPE_NORMAL
- en: Causation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make a claim of causation, we would need a controlled experimental design
    or additional statistical techniques that account for confounding variables and
    establish a causal link. Without such evidence, our findings from regression analysis
    should be presented as correlational insights, which highlight patterns that may
    warrant further investigation but do not confirm causality.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, while our temperature coefficient *B1* suggests a correlation between
    warm weather and increased bike rentals, we cannot conclude that warm weather
    causes more people to rent bikes without a deeper causal analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are confident in our interpretations of our correlational findings,
    let’s use `scikit-learn` to make some predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This means that roughly 189 bikes will likely be rented if the temperature is
    20°C.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, temperature is not the only thing that will help us predict the number
    of bikes. Adding more predictors to the model is as simple as telling the linear
    regression model in `scikit-learn` about them!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do, we should look at the data dictionary provided to us to make
    more sense of some more features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**season**: **1** = **spring**, **2** = **summer**, **3** = **fall**, and **4**
    = **winter**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**holiday**: Whether the day is considered a holiday'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**workingday**: Whether the day is a weekend or holiday'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weather**: **1** = **Clear, Few clouds, Partly cloudy**, **2** = **Mist +
    Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist**, **3** = **Light Snow,
    Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds**,
    and **4 = Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow +** **Fog**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**temp**: The temperature in °C'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**atemp**: The “feels like” temperature, taking wind speeds into consideration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**humidity**: Relative humidity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s create our linear regression model using more features. As before,
    we will first create a list holding the features we wish to look at, create our
    features and our response datasets (`X` and `y`), and then fit our linear regression.
    Once we fit our regression model, we will take a look at the model’s coefficients
    in order to see how our features are interacting with our response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is what that means:'
  prefs: []
  type: TYPE_NORMAL
- en: Holding all other predictors constant, a 1-unit increase in temperature is associated
    with a rental increase of **7.86** bikes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holding all other predictors constant, a 1-unit increase in season is associated
    with a rental increase of **22.5** bikes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holding all other predictors constant, a 1-unit increase in weather is associated
    with a rental increase of **6.67** bikes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holding all other predictors constant, a 1-unit increase in humidity is associated
    with a rental decrease of **3.12** bikes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that, as **weather** goes up (meaning that the weather is getting closer
    to overcast), the bike demand goes up, as is the case when the season variables
    increase (meaning that we are approaching winter). This is not what I was expecting
    at all, frankly!
  prefs: []
  type: TYPE_NORMAL
- en: While these individual correlations are helpful in many ways, it is crucial
    to identify metrics to judge our ML system as a whole. Usually, we think about
    metrics in terms of the task we are performing. Certain metrics are useful for
    classification, while other metrics are more useful for regression.
  prefs: []
  type: TYPE_NORMAL
- en: Regression metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are usually three main metrics when using regression ML models. They
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Absolute Error (MAE)**: This is the average of the absolute errors between
    the predicted values and the actual values. It’s calculated by taking the sum
    of the absolute values of the errors (the differences between the predicted values
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/159.png)
    and the actual values ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/160.png))
    and then dividing by the number of observations n:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>MAE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/161.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Mean Squared Error (MSE)**: This is the average of the squares of the errors
    between the predicted values and the actual values. It’s computed by squaring
    each error, summing these squares, and then dividing by the number of observations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>MSE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/162.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Root MSE (RMSE)**: This is the square root of the MSE. It’s obtained by taking
    the square root of the average of the squared differences between the predicted
    values and the actual values. RMSE is useful because it scales the errors to the
    original units of the output variable and can be more interpretable than MSE:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>RMSE</mtext><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true">ˆ</mo></mover></mrow></mfenced><mn>2</mn></msup></mrow></mrow></msqrt></mrow></mrow></math>](img/163.png)'
  prefs: []
  type: TYPE_IMG
- en: These metrics are crucial in evaluating the performance of regression models,
    with each having its own advantages. MAE provides a straightforward average-error
    magnitude, MSE penalizes larger errors more heavily, and RMSE is particularly
    sensitive to large errors due to the squaring of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at implementations in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use RMSE to ascertain which columns are helping and which are hindering.
    Let’s start with only using temperature. Note that our procedure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create our **X** and our **y** variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a linear regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to make a list of predictions based on **X**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the RMSE between the predictions and the actual values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s try it using temperature and humidity, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It got better! Let’s try using even more predictors, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Even better! At first, this seems like a major triumph, but there is actually
    a hidden danger here. Note that we are training the line to fit `X` and `y` and
    then asking it to predict `X` again! This is actually a huge mistake in ML because
    it can lead to overfitting, which means that our model is merely memorizing the
    data and regurgitating it back to us.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you are a student, and you walk into the first day of class and
    the teacher says that the final exam is very difficult in this class. In order
    to prepare you, she gives you practice test after practice test after practice
    test. The day of the final exam arrives, and you are shocked to find out that
    every question on the exam is exactly the same as in the practice test! Luckily,
    you did them so many times that you remember the answer and get 100% on the exam.
  prefs: []
  type: TYPE_NORMAL
- en: The same thing applies here, more or less. By fitting and predicting on the
    same data, the model is memorizing the data and getting better at it. A great
    way to combat this **overfitting** problem is to use the train/test approach to
    fit ML models, which works as illustrated next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, we will take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split up the dataset into two parts: a training and a test set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit our model on the training set and then test it on the test set, just like
    in school, where the teacher would teach from one set of notes and then test us
    on different (but similar) questions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once our model is good enough (based on our metrics), we turn our model’s attention
    toward the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our model awaits new data previously unseen by anyone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be visualized in *Figure 10**.10*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Splitting our data up into a training and testing set helps
    us properly evaluate our model’s ability to predict unseen data](img/B19488_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Splitting our data up into a training and testing set helps us
    properly evaluate our model’s ability to predict unseen data
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to minimize the out-of-sample errors of our model, which are
    errors our model has on data that it has never seen before. This is important
    because the main idea (usually) of a supervised model is to predict outcomes for
    new data. If our model is unable to generalize from our training data and use
    that to predict unseen cases, then our model isn’t very good.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram outlines a simple way of ensuring that our model can effectively
    ingest the training data and use it to predict data points that the model itself
    has never seen. Of course, as data scientists, we know that the test set also
    has answers attached to it, but the model doesn’t know that.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this might sound complicated, but luckily, the `scikit-learn` package
    has a built-in method to do this, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In other words, our `train_test_split` function is ensuring that the metrics
    we are looking at are more honest estimates of our sample performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try again with more predictors, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Our model actually got worse with that addition! This implies that `workingday`
    might not be very predictive of our response, the bike rental count.
  prefs: []
  type: TYPE_NORMAL
- en: All of this is well and good, and we can keep adding and removing features to
    lower our RMSE, but how well is our model really doing at predicting rather than
    just guessing? We have an RMSE of around 167 bikes, but is that good? What do
    we compare it to? One way to discover this is to evaluate the null model.
  prefs: []
  type: TYPE_NORMAL
- en: The **null model** in SML represents effectively guessing the expected outcome
    over and over, and seeing how you did. For example, in regression, if we always
    guess the average number of hourly bike rentals, then how well would that model
    do?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s get the average hourly bike rental, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This means that, overall, in this dataset, regardless of weather, time, day
    of the week, humidity, and everything else, the average number of bikes that go
    out every hour is about 192.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s make a fake prediction list, wherein every single guess is 191.57\. Let’s
    make this guess for every single hour, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, we have `10,886` values, all of which are the average hourly bike rental
    number. Let’s see what the RMSE would be if our model only ever guessed the expected
    value of the average hourly bike rental count:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This means that by simply guessing the average value over and over again, our
    RMSE would be 181 bikes. So, even with one or two features, we can beat it! Beating
    the null model is a kind of baseline in ML. If you think about it, why go through
    any effort at all if your ML is not even better than just guessing?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at ML and its different subcategories. We explored
    SL, UL, and RL strategies and looked at situations where each one would come in
    handy.
  prefs: []
  type: TYPE_NORMAL
- en: Looking into linear regression, we were able to find relationships between predictors
    and a continuous response variable. Through the train/test split, we were able
    to help avoid overfitting our ML models and get a more generalized prediction.
    We were able to use metrics, such as RMSE, to evaluate our models as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few chapters, we will be taking a much deeper dive into many more
    ML models and, along the way, we will learn new metrics, new validation techniques,
    and – more importantly – new ways of applying data science to the world.
  prefs: []
  type: TYPE_NORMAL
