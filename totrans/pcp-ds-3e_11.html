<html><head></head><body>
<div id="_idContainer301" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-158"><a id="_idTextAnchor307" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-159" class="calibre6"><a id="_idTextAnchor308" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">Predictions Don’t Grow on Trees, or Do They?</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">Our goal in this chapter is to see and apply concepts learned from previous chapters in order to construct and use modern learning algorithms to glean insights and make predictions on real datasets. </span><span class="kobospan" id="kobo.3.2">While we explore the following algorithms, we should always remember that we are constantly keeping our metrics </span><span><span class="kobospan" id="kobo.4.1">in mind.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.5.1">In this chapter, we will be looking at the following </span><span><span class="kobospan" id="kobo.6.1">ML algorithms:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.7.1">Performing naïve </span><span><span class="kobospan" id="kobo.8.1">Bayes classification</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.9.1">Understanding </span><span><span class="kobospan" id="kobo.10.1">decision trees</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.11.1">Diving deep into </span><strong class="bold"><span class="kobospan" id="kobo.12.1">unsupervised </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.13.1">learning</span></strong></span><span><span class="kobospan" id="kobo.14.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.15.1">UL</span></strong></span><span><span class="kobospan" id="kobo.16.1">)</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.17.1">k-means clustering</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.18.1">Feature extraction and </span><strong class="bold"><span class="kobospan" id="kobo.19.1">principal component </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.20.1">analysis</span></strong></span><span><span class="kobospan" id="kobo.21.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.22.1">PCA</span></strong></span><span><span class="kobospan" id="kobo.23.1">)</span></span></li>
</ul>
<h1 id="_idParaDest-160" class="calibre6"><a id="_idTextAnchor309" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.24.1">Performing naïve Bayes classification</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.25.1">Let’s get </span><a id="_idIndexMarker644" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.26.1">right into it! </span><span class="kobospan" id="kobo.26.2">Let’s begin with </span><strong class="bold"><span class="kobospan" id="kobo.27.1">naïve Bayes</span></strong><span class="kobospan" id="kobo.28.1"> classification. </span><span class="kobospan" id="kobo.28.2">This ML model relies heavily on results from previous chapters, specifically with </span><span><span class="kobospan" id="kobo.29.1">Bayes’ theorem:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.30.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/164.png" class="calibre168"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.31.1">Let’s look a little closer at the specific features of </span><span><span class="kobospan" id="kobo.32.1">this formula:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.33.1">P(H)</span></em><span class="kobospan" id="kobo.34.1"> is the</span><a id="_idIndexMarker645" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.35.1"> probability of the hypothesis before we observe the data, called the</span><em class="italic"><span class="kobospan" id="kobo.36.1"> prior probability</span></em><span class="kobospan" id="kobo.37.1">, or </span><span><span class="kobospan" id="kobo.38.1">just </span></span><span><em class="italic"><span class="kobospan" id="kobo.39.1">prior</span></em></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.40.1">P(H|D)</span></em><span class="kobospan" id="kobo.41.1"> is what we want to compute: the probability of the hypothesis after we observe the data, called </span><span><span class="kobospan" id="kobo.42.1">the </span></span><span><em class="italic"><span class="kobospan" id="kobo.43.1">posterior</span></em></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.44.1">P(D|H)</span></em><span class="kobospan" id="kobo.45.1"> is the probability of the data under the given hypothesis, called </span><span><span class="kobospan" id="kobo.46.1">the </span></span><span><em class="italic"><span class="kobospan" id="kobo.47.1">likelihood</span></em></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.48.1">P(D)</span></em><span class="kobospan" id="kobo.49.1"> is the probability </span><a id="_idIndexMarker646" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.50.1">of the data under any hypothesis, called the </span><span><em class="italic"><span class="kobospan" id="kobo.51.1">normalizing constant</span></em></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.52.1">Naïve Bayes classification is a classification model, and therefore a supervised model. </span><span class="kobospan" id="kobo.52.2">Given this, what kind of data do we need – labeled or </span><span><span class="kobospan" id="kobo.53.1">unlabeled data?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.54.1">(Insert </span><em class="italic"><span class="kobospan" id="kobo.55.1">Jeopardy</span></em> <span><span class="kobospan" id="kobo.56.1">music here)</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.57.1">If you answered </span><em class="italic"><span class="kobospan" id="kobo.58.1">labeled data</span></em><span class="kobospan" id="kobo.59.1">, then you’re well on your way to becoming a </span><span><span class="kobospan" id="kobo.60.1">data scientist!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.61.1">Suppose we have a dataset with </span><em class="italic"><span class="kobospan" id="kobo.62.1">n features</span></em><span class="kobospan" id="kobo.63.1">, </span><em class="italic"><span class="kobospan" id="kobo.64.1">(x1, x2, …, xn)</span></em><span class="kobospan" id="kobo.65.1">, and a </span><em class="italic"><span class="kobospan" id="kobo.66.1">class label</span></em><span class="kobospan" id="kobo.67.1">, </span><em class="italic"><span class="kobospan" id="kobo.68.1">C</span></em><span class="kobospan" id="kobo.69.1">. </span><span class="kobospan" id="kobo.69.2">For example, let’s take some data involving spam text classification. </span><span class="kobospan" id="kobo.69.3">Our data would consist of rows of individual text samples and columns of both our features and our class labels. </span><span class="kobospan" id="kobo.69.4">Our features would be words and phrases that are contained within the text samples, and our class</span><a id="_idIndexMarker647" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.70.1"> labels are simply </span><em class="italic"><span class="kobospan" id="kobo.71.1">spam</span></em><span class="kobospan" id="kobo.72.1"> or </span><em class="italic"><span class="kobospan" id="kobo.73.1">not spam</span></em><span class="kobospan" id="kobo.74.1">. </span><span class="kobospan" id="kobo.74.2">In this scenario, I will replace the not-spam class with an easier-to-say word, ham. </span><span class="kobospan" id="kobo.74.3">Let’s take a look at the following code snippet to better understand our spam and </span><span><span class="kobospan" id="kobo.75.1">ham data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.76.1">
import pandas as pd import sklearn
df = pd.read_table('..data/sms.tsv',
sep='\t', header=None, names=['label', 'msg'])
df</span></pre>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.77.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.78.1">.1</span></em><span class="kobospan" id="kobo.79.1"> is a sample of text data in a </span><span><span class="kobospan" id="kobo.80.1">row-column format:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer257">
<span class="kobospan" id="kobo.81.1"><img alt="Figure 11.1 – A sample of our spam versus not spam (ham) messages" src="image/B19488_11_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.82.1">Figure 11.1 – A sample of our spam versus not spam (ham) messages</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.83.1">Let’s do some preliminary statistics to see what we are dealing with. </span><span class="kobospan" id="kobo.83.2">Let’s see the difference in</span><a id="_idIndexMarker648" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.84.1"> the number of ham and spam messages at </span><span><span class="kobospan" id="kobo.85.1">our disposal:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.86.1">
df.label.value_counts().plot(kind="bar")</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.87.1">This gives us a bar chart, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.88.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.89.1">.2</span></em></span><span><span class="kobospan" id="kobo.90.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer258">
<span class="kobospan" id="kobo.91.1"><img alt="Figure 11.2 – The distribution of ham versus spam" src="image/B19488_11_02.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.92.1">Figure 11.2 – The distribution of ham versus spam</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.93.1">Because we are dealing with classification, it would help to itemize some of the metrics we will be using</span><a id="_idIndexMarker649" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.94.1"> to evaluate </span><span><span class="kobospan" id="kobo.95.1">our model.</span></span></p>
<h2 id="_idParaDest-161" class="calibre7"><a id="_idTextAnchor310" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.96.1">Classification metrics</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.97.1">When evaluating </span><a id="_idIndexMarker650" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.98.1">classification models, different metrics are used compared to regression models. </span><span class="kobospan" id="kobo.98.2">These metrics help to understand how well the model is performing, especially in terms of correctly predicting different classes. </span><span class="kobospan" id="kobo.98.3">Let’s look at what </span><span><span class="kobospan" id="kobo.99.1">they are:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.100.1">Accuracy</span></strong><span class="kobospan" id="kobo.101.1">: This is the most intuitive performance measure, and it is simply a ratio of correctly predicted observations to the total observations. </span><span class="kobospan" id="kobo.101.2">It’s suitable for binary and multiclass </span><span><span class="kobospan" id="kobo.102.1">classification problems:</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.103.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Accuracy&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;Number&lt;/mtext&gt;&lt;mtext&gt;of&lt;/mtext&gt;&lt;mtext&gt;correct&lt;/mtext&gt;&lt;mtext&gt;predictions&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/165.png" class="calibre169"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.104.1">2.</span><strong class="bold"><span class="kobospan" id="kobo.105.1">	Precision (best for binary classification – with only two classes)</span></strong><span class="kobospan" id="kobo.106.1">: Also known as positive predictive value, this metric helps to answer the question: “What proportion of positive identifications was </span><span><span class="kobospan" id="kobo.107.1">actually correct?”</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.108.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Precision&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/166.png" class="calibre170"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.109.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.110.1">TP</span></em><span class="kobospan" id="kobo.111.1"> is the number of true positives (predicted positive and the prediction was correct), and </span><em class="italic"><span class="kobospan" id="kobo.112.1">FP</span></em><span class="kobospan" id="kobo.113.1"> is the number of false positives (predicted positive but the prediction </span><span><span class="kobospan" id="kobo.114.1">was incorrect).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.115.1">3.	</span><strong class="bold"><span class="kobospan" id="kobo.116.1">Recall (Sensitivity) (best for binary classification – with only two classes)</span></strong><span class="kobospan" id="kobo.117.1">: This metric helps to answer the question: “What proportion of actual positives was </span><span><span class="kobospan" id="kobo.118.1">identified correctly?”</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.119.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Recall&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/167.png" class="calibre171"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.120.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.121.1">TP</span></em><span class="kobospan" id="kobo.122.1"> (predicted positive and the prediction was correct) is the number of true positives, and </span><em class="italic"><span class="kobospan" id="kobo.123.1">FN</span></em><span class="kobospan" id="kobo.124.1"> is the number of false negatives (predicted negative but the prediction </span><span><span class="kobospan" id="kobo.125.1">was incorrect).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.126.1">4.	</span><strong class="bold"><span class="kobospan" id="kobo.127.1">F1 Score</span></strong><span class="kobospan" id="kobo.128.1">: The </span><em class="italic"><span class="kobospan" id="kobo.129.1">F1</span></em><span class="kobospan" id="kobo.130.1"> Score is the weighted average of precision and recall. </span><span class="kobospan" id="kobo.130.2">Therefore, this score takes both false positives and false negatives into account. </span><span class="kobospan" id="kobo.130.3">It is a good way to show that a classifier has a good value for both precision </span><span><span class="kobospan" id="kobo.131.1">and recall:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.132.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;Precision&lt;/mtext&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;Recall&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Precision&lt;/mtext&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;Recall&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/168.png" class="calibre172"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.133.1">These metrics are crucial for understanding the behavior of classification models, especially in</span><a id="_idIndexMarker651" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.134.1"> domains where the costs of false positives and false negatives are </span><span><span class="kobospan" id="kobo.135.1">very different.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.136.1">So, we have way more </span><em class="italic"><span class="kobospan" id="kobo.137.1">ham</span></em><span class="kobospan" id="kobo.138.1"> messages than we do </span><em class="italic"><span class="kobospan" id="kobo.139.1">spam</span></em><span class="kobospan" id="kobo.140.1">. </span><span class="kobospan" id="kobo.140.2">Because this is a classification problem, it would be very useful to know</span><a id="_idIndexMarker652" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.141.1"> our </span><strong class="bold"><span class="kobospan" id="kobo.142.1">null accuracy rate</span></strong><span class="kobospan" id="kobo.143.1">, which is the percentage chance of predicting a single row correctly if we keep guessing the most common class, </span><strong class="source-inline"><span class="kobospan" id="kobo.144.1">ham</span></strong><span class="kobospan" id="kobo.145.1">. </span><span class="kobospan" id="kobo.145.2">Here’s how we </span><span><span class="kobospan" id="kobo.146.1">do that:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.147.1">
df.label.value_counts() / df.shape[0]
ham0.865937
spam 0.134063</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.148.1">So, if we blindly guessed </span><em class="italic"><span class="kobospan" id="kobo.149.1">ham</span></em><span class="kobospan" id="kobo.150.1">, we would be correct about 87% of the time, but we can do better </span><span><span class="kobospan" id="kobo.151.1">than that.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.152.1">If we have a set of classes, C, and features, </span><span class="kobospan" id="kobo.153.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/169.png" class="calibre173"/></span><span class="kobospan" id="kobo.154.1">, then we can use Bayes’ theorem to predict the probability that a single row belongs to class C, using the </span><span><span class="kobospan" id="kobo.155.1">following formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.156.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;class&lt;/mtext&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;class&lt;/mtext&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;class&lt;/mtext&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/170.png" class="calibre174"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.157.1">Let’s look at this formula in a little </span><span><span class="kobospan" id="kobo.158.1">more detail:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.159.1">P(class C | { </span></em><em class="italic"><span class="kobospan" id="kobo.160.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/171.png" class="calibre175"/></span></em><em class="italic"><span class="kobospan" id="kobo.161.1"> })</span></em><span class="kobospan" id="kobo.162.1">: The posterior probability is the probability that the row belongs to class C given the </span><span><span class="kobospan" id="kobo.163.1">features </span></span><span><em class="italic"><span class="kobospan" id="kobo.164.1">{xi}</span></em></span><span><span class="kobospan" id="kobo.165.1">.</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.166.1">P({ </span></em><em class="italic"><span class="kobospan" id="kobo.167.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/172.png" class="calibre176"/></span></em><em class="italic"><span class="kobospan" id="kobo.168.1"> } | class C)</span></em><span class="kobospan" id="kobo.169.1">: This is the likelihood that we would observe these features given that the row was in </span><span><span class="kobospan" id="kobo.170.1">class </span></span><span><em class="italic"><span class="kobospan" id="kobo.171.1">C</span></em></span><span><span class="kobospan" id="kobo.172.1">.</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.173.1">P(class C)</span></em><span class="kobospan" id="kobo.174.1">: This is the prior probability. </span><span class="kobospan" id="kobo.174.2">It is the probability that the data point belongs to class </span><em class="italic"><span class="kobospan" id="kobo.175.1">C</span></em><span class="kobospan" id="kobo.176.1"> before we see </span><span><span class="kobospan" id="kobo.177.1">any data.</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.178.1">P({ </span></em><em class="italic"><span class="kobospan" id="kobo.179.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/173.png" class="calibre177"/></span></em><em class="italic"><span class="kobospan" id="kobo.180.1"> })</span></em><span class="kobospan" id="kobo.181.1">: This is our </span><span><span class="kobospan" id="kobo.182.1">normalization constant.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.183.1">For example, imagine we have an email with three words: </span><em class="italic"><span class="kobospan" id="kobo.184.1">send cash now</span></em><span class="kobospan" id="kobo.185.1">. </span><span class="kobospan" id="kobo.185.2">We’ll use naïve Bayes to</span><a id="_idIndexMarker653" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.186.1"> classify the email as either being spam </span><span><span class="kobospan" id="kobo.187.1">or ham:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.188.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/174.png" class="calibre178"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.189.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;ham&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;ham&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;ham&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/175.png" class="calibre179"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.190.1">We are concerned with the difference of these two numbers. </span><span class="kobospan" id="kobo.190.2">We can use the following criteria to classify any single </span><span><span class="kobospan" id="kobo.191.1">text sample:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.192.1">If </span><em class="italic"><span class="kobospan" id="kobo.193.1">P(spam | send cash now)</span></em><span class="kobospan" id="kobo.194.1"> is larger than </span><em class="italic"><span class="kobospan" id="kobo.195.1">P(ham | send cash now)</span></em><span class="kobospan" id="kobo.196.1">, then we will classify the text </span><span><span class="kobospan" id="kobo.197.1">as spam</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.198.1">If </span><em class="italic"><span class="kobospan" id="kobo.199.1">P(ham | send cash now)</span></em><span class="kobospan" id="kobo.200.1"> is larger than </span><em class="italic"><span class="kobospan" id="kobo.201.1">P(spam | send cash now)</span></em><span class="kobospan" id="kobo.202.1">, then we will label the </span><span><span class="kobospan" id="kobo.203.1">text ham</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.204.1">Because both equations have </span><em class="italic"><span class="kobospan" id="kobo.205.1">P (send money now)</span></em><span class="kobospan" id="kobo.206.1"> in the denominator, we can ignore them. </span><span class="kobospan" id="kobo.206.2">So, now we are concerned with </span><span><span class="kobospan" id="kobo.207.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.208.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;VS&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.125em&quot; /&gt;&lt;mtext&gt;ham&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;ham&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/176.png" class="calibre180"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.209.1">Let’s work out the numbers in </span><span><span class="kobospan" id="kobo.210.1">this equation:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.211.1">P(spam) = </span><span><span class="kobospan" id="kobo.212.1">0.134063</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.213.1">P(ham) = </span><span><span class="kobospan" id="kobo.214.1">0.865937</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.215.1">P(send cash now | spam) = ???</span></li>
<li class="calibre14"><span class="kobospan" id="kobo.216.1">P(send cash now | ham) = ???</span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.217.1">The final two likelihoods might seem like they would not be so difficult to calculate. </span><span class="kobospan" id="kobo.217.2">All we have to do is count the number of spam messages that include the send </span><span><span class="kobospan" id="kobo.218.1">money, right?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.219.1">Now, phrase and divide that by the total number of </span><span><span class="kobospan" id="kobo.220.1">spam messages:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.221.1">
df.msg = df.msg.apply(lambda x:x.lower())
# make all strings lower case so we can search easier
df[df.msg.str.contains('send cash now')].shape # == (0, 2)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.222.1">Oh no! </span><span class="kobospan" id="kobo.222.2">There are </span><a id="_idIndexMarker654" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.223.1">none! </span><span class="kobospan" id="kobo.223.2">There are literally zero texts with the exact phrase </span><em class="italic"><span class="kobospan" id="kobo.224.1">send cash now</span></em><span class="kobospan" id="kobo.225.1">. </span><span class="kobospan" id="kobo.225.2">The hidden problem here is that this phrase is very specific, and we can’t assume that we will have enough data in the world to have seen this exact phrase many </span><span><span class="kobospan" id="kobo.226.1">times before.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.227.1">Instead, we can make a naïve assumption in our Bayes’ theorem. </span><span class="kobospan" id="kobo.227.2">If we assume that the features (words) are conditionally independent (meaning that no word affects the existence of another word), then we can rewrite </span><span><span class="kobospan" id="kobo.228.1">the formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.229.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/177.png" class="calibre181"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.230.1">And here’s what it looks like done </span><span><span class="kobospan" id="kobo.231.1">in Python:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.232.1">
spams = df[df.label == 'spam']
for word in ['send', 'cash', 'now']:
print( word, spams[spams.msg.str.contains(word)].shape[0] / float(spams.shape[0]))</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.233.1">Printing out the conditional </span><span><span class="kobospan" id="kobo.234.1">probabilities yields:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.235.1">P(send|spam) = </span><span><span class="kobospan" id="kobo.236.1">0.096</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.237.1">P(cash|spam) = </span><span><span class="kobospan" id="kobo.238.1">0.091</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.239.1">P(now|spam) = </span><span><span class="kobospan" id="kobo.240.1">0.280</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.241.1">With this, we can calculate </span><span><span class="kobospan" id="kobo.242.1">the following:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.243.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;send&lt;/mtext&gt;&lt;mtext&gt;cash&lt;/mtext&gt;&lt;mtext&gt;now&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;spam&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;0.096&lt;/mn&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mn&gt;0.091&lt;/mn&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mn&gt;0.280&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mn&gt;0.134&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.00032&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/178.png" class="calibre182"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">Repeating the same procedure for ham gives us </span><span><span class="kobospan" id="kobo.245.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.246.1">P(send|ham) = </span><span><span class="kobospan" id="kobo.247.1">0.03</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.248.1">P(cash|ham) = </span><span><span class="kobospan" id="kobo.249.1">0.003</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.250.1">P(now|ham) = </span><span><span class="kobospan" id="kobo.251.1">0.109</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.252.1">The fact that these numbers are both very low is not as important as the fact that the spam probability is much larger than the ham calculation. </span><span class="kobospan" id="kobo.252.2">If we do the calculations, we get that the </span><em class="italic"><span class="kobospan" id="kobo.253.1">send cash now</span></em><span class="kobospan" id="kobo.254.1"> probability for spam is 38 times bigger than for spam! </span><span class="kobospan" id="kobo.254.2">Doing this means that we can classify </span><em class="italic"><span class="kobospan" id="kobo.255.1">send cash now</span></em><span class="kobospan" id="kobo.256.1"> as spam! </span><span><span class="kobospan" id="kobo.257.1">Simple, right?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.258.1">Let’s use </span><a id="_idIndexMarker655" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.259.1">Python to implement a naïve Bayes classifier without having to do all of these </span><span><span class="kobospan" id="kobo.260.1">calculations ourselves.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.261.1">First, let’s revisit the count vectorizer in scikit-learn, which turns text into numerical data for us. </span><span class="kobospan" id="kobo.261.2">Let’s assume that we will train on three documents (sentences), in the following </span><span><span class="kobospan" id="kobo.262.1">code snippet:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.263.1">
# simple count vectorizer example
from sklearn.feature_extraction.text import CountVectorizer # start with a simple example
train_simple = ['call you tonight',
'Call me a cab',
'please call me... </span><span class="kobospan1" id="kobo.263.2">PLEASE 44!']
# learn the 'vocabulary' of the training data vect = CountVectorizer()
train_simple_dtm = vect.fit_transform(train_simple) pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())</span></pre>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.264.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.265.1">.3</span></em><span class="kobospan" id="kobo.266.1"> demonstrates the feature vectors learned from </span><span><span class="kobospan" id="kobo.267.1">our dataset:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer273">
<span class="kobospan" id="kobo.268.1"><img alt="Figure 11.3 – The first five rows of our SMS dataset after breaking up each text into a count of unique words" src="image/B19488_11_03.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.269.1">Figure 11.3 – The first five rows of our SMS dataset after breaking up each text into a count of unique words</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.270.1">Note that each row represents one of the three documents (sentences), each column represents one of the words present in the documents, and each cell contains the number of times each word appears in </span><span><span class="kobospan" id="kobo.271.1">each document.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.272.1">We can then use</span><a id="_idIndexMarker656" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.273.1"> the count vectorizer to transform new incoming test documents to conform with our training set (the </span><span><span class="kobospan" id="kobo.274.1">three sentences):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.275.1">
# transform testing data into a document-term matrix (using existing vocabulary, notice don't is missing)
test_simple = ["please don't call me"] test_simple_dtm = vect.transform(test_simple) test_simple_dtm.toarray()
pd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())</span></pre>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.276.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.277.1">.4</span></em><span class="kobospan" id="kobo.278.1"> is shown </span><span><span class="kobospan" id="kobo.279.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer274">
<span class="kobospan" id="kobo.280.1"><img alt="Figure 11.4 – Representation of the “please don’t call me” SMS in the same vocabulary as our training data" src="image/B19488_11_04.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.281.1">Figure 11.4 – Representation of the “please don’t call me” SMS in the same vocabulary as our training data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.282.1">Note how, in our test sentence, we had a new word – namely, </span><em class="italic"><span class="kobospan" id="kobo.283.1">don’t</span></em><span class="kobospan" id="kobo.284.1">. </span><span class="kobospan" id="kobo.284.2">When we vectorized it, because we hadn’t seen that word previously in our training data, the vectorizer simply ignored it. </span><span class="kobospan" id="kobo.284.3">This is important and incentivizes data scientists to obtain as much data as possible for their </span><span><span class="kobospan" id="kobo.285.1">training sets.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.286.1">Now, let’s do this for our </span><span><span class="kobospan" id="kobo.287.1">actual data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.288.1">
# split into training and testing sets
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.msg, df.label, random_state=1)
# instantiate the vectorizer vect = CountVectorizer()
# learn vocabulary and create document-term matrix in a single step train_dtm = vect.fit_transform(X_train)
train_dtm</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.289.1">The following is</span><a id="_idIndexMarker657" class="pcalibre calibre4 pcalibre1"/> <span><span class="kobospan" id="kobo.290.1">the output:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.291.1">
&lt;4179x7456 sparse matrix of type '&lt;class 'numpy.int64'&gt;' with 55209 stored elements in Compressed Sparse Row format&gt;</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.292.1">Note that the format is in a sparse matrix, meaning the matrix is large and full of zeros. </span><span class="kobospan" id="kobo.292.2">There is a special format to deal with objects such as this. </span><span class="kobospan" id="kobo.292.3">Take a look at the number </span><span><span class="kobospan" id="kobo.293.1">of columns.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.294.1">There are 7,456 words. </span><span class="kobospan" id="kobo.294.2">That’s a lot! </span><span class="kobospan" id="kobo.294.3">This means that in our training set, there are 7,456 unique words to look at. </span><span class="kobospan" id="kobo.294.4">We can now transform our test data to conform to </span><span><span class="kobospan" id="kobo.295.1">our vocabulary:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.296.1">
# transform testing data into a document-term matrix test_dtm = vect.transform(X_test)
test_dtm</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.297.1">The output is </span><span><span class="kobospan" id="kobo.298.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.299.1">
&lt;1393x7456 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 17604 stored elements in Compressed Sparse Row format&gt;</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.300.1">Note that we have the same exact number of columns because it is conforming to our test set to be exactly the same vocabulary as before. </span><span class="kobospan" id="kobo.300.2">No more, </span><span><span class="kobospan" id="kobo.301.1">no less.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.302.1">Now, let’s build a naïve Bayes model (similar to the linear </span><span><span class="kobospan" id="kobo.303.1">regression process):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.304.1">
## MODEL BUILDING WITH NAIVE BAYES
# train a Naive Bayes model using train_dtm from sklearn.naive_bayes import MultinomialNB # import our model
nb = MultinomialNB()
# instantiate our model
nb.fit(train_dtm, y_train)
# fit it to our training set</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.305.1">Now, the </span><strong class="source-inline"><span class="kobospan" id="kobo.306.1">nb</span></strong><span class="kobospan" id="kobo.307.1"> variable holds our fitted model. </span><span class="kobospan" id="kobo.307.2">The training phase of the model involves computing the likelihood function, which is the conditional probability of each feature given </span><span><span class="kobospan" id="kobo.308.1">each class:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.309.1">
# make predictions on test data using test_dtm preds = nb.predict(test_dtm)
preds</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.310.1">The output is </span><span><span class="kobospan" id="kobo.311.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.312.1">
array(['ham', 'ham', 'ham', ..., 'ham', 'spam', 'ham'], dtype='|S4')</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.313.1">The prediction </span><a id="_idIndexMarker658" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.314.1">phase of the model involves computing the posterior probability of each class given the observed features and choosing the class with the </span><span><span class="kobospan" id="kobo.315.1">highest probability.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.316.1">We will use </span><strong class="source-inline"><span class="kobospan" id="kobo.317.1">sklearn</span></strong><span class="kobospan" id="kobo.318.1">’s built-in accuracy and confusion matrix to look at how well our naïve Bayes models </span><span><span class="kobospan" id="kobo.319.1">are performing:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.320.1">
# compare predictions to true labels from sklearn import metrics
print metrics.accuracy_score(y_test, preds) print metrics.confusion_matrix(y_test, preds)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.321.1">The output is </span><span><span class="kobospan" id="kobo.322.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.323.1">
accuracy == 0.988513998564
confusion matrix ==
[[12035]
[11174]]</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.324.1">First off, our accuracy is great! </span><span class="kobospan" id="kobo.324.2">Compared to our null accuracy, which was 87%, 99% is a </span><span><span class="kobospan" id="kobo.325.1">fantastic improvement.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.326.1">Now to our confusion matrix. </span><span class="kobospan" id="kobo.326.2">From before, we know that each row represents actual values while columns represent predicted values, so the top-left value, 1,203, represents our true negatives. </span><span class="kobospan" id="kobo.326.3">But what is negative and positive? </span><span class="kobospan" id="kobo.326.4">We gave the model the spam and ham strings as our classes, not positive </span><span><span class="kobospan" id="kobo.327.1">and negative.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.328.1">We can use </span><span><span class="kobospan" id="kobo.329.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.330.1">
nb.classes_</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.331.1">The output is </span><span><span class="kobospan" id="kobo.332.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.333.1">
array(['ham', 'spam'])</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.334.1">We can then line up the indices so that 1,203 refers to true ham predictions and 174 refers to true spam predictions. </span><span class="kobospan" id="kobo.334.2">There were also five false spam classifications, meaning that five messages were predicted as spam but were actually ham, as well as 11 false ham classifications. </span><span class="kobospan" id="kobo.334.3">In summary, naïve Bayes classification uses Bayes’ theorem in order to fit posterior probabilities of classes so that data points are correctly labeled as belonging to the </span><span><span class="kobospan" id="kobo.335.1">proper class.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.336.1">Every ML model has its own set of unique properties and advantages or disadvantages for use with different types of data. </span><span class="kobospan" id="kobo.336.2">The naïve Bayes classifier, for example, is known for its speed and efficiency. </span><span class="kobospan" id="kobo.336.3">It is particularly fast when fitting to training data and when making predictions on test data. </span><span class="kobospan" id="kobo.336.4">This is due to its assumption of feature independence, which simplifies</span><a id="_idIndexMarker659" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.337.1"> the calculations involved in </span><span><span class="kobospan" id="kobo.338.1">probability estimation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.339.1">However, this same assumption can also be seen as a limitation. </span><span class="kobospan" id="kobo.339.2">In reality, features often do exhibit some level of dependency, and the naïve Bayes classifier may oversimplify complex relationships in data. </span><span class="kobospan" id="kobo.339.3">Moreover, it is based on the assumption that the form of the data distribution (often assumed to be Gaussian) holds true, which might not always be the case in </span><span><span class="kobospan" id="kobo.340.1">real-world scenarios.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.341.1">Despite these limitations, naïve Bayes can perform exceptionally well with appropriate data and is particularly useful for text</span><a id="_idIndexMarker660" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.342.1"> classification tasks, such as spam </span><a id="_idIndexMarker661" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.343.1">filtering and </span><strong class="bold"><span class="kobospan" id="kobo.344.1">sentiment </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.345.1">analysis</span></strong></span><span><span class="kobospan" id="kobo.346.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.347.1">SA</span></strong></span><span><span class="kobospan" id="kobo.348.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.349.1">Another widely used ML technique is the decision tree. </span><span class="kobospan" id="kobo.349.2">Decision trees are a </span><strong class="bold"><span class="kobospan" id="kobo.350.1">supervised learning</span></strong><span class="kobospan" id="kobo.351.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.352.1">SL</span></strong><span class="kobospan" id="kobo.353.1">) method used for classification and regression. </span><span class="kobospan" id="kobo.353.2">They are intuitive and easy to interpret since they mimic human decision-making more closely than </span><span><span class="kobospan" id="kobo.354.1">othe</span><a id="_idTextAnchor311" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.355.1">r</span></span><span><a id="_idIndexMarker662" class="pcalibre calibre4 pcalibre1"/></span><span><span class="kobospan" id="kobo.356.1"> algorithms.</span></span></p>
<h1 id="_idParaDest-162" class="calibre6"><a id="_idTextAnchor312" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.357.1">Understanding decision trees</span></h1>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.358.1">Decision trees</span></strong><span class="kobospan" id="kobo.359.1"> are</span><a id="_idIndexMarker663" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.360.1"> supervised models that can either perform regression or classification. </span><span class="kobospan" id="kobo.360.2">They are a flowchart-like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (for classification) or a value (for regression). </span><span class="kobospan" id="kobo.360.3">One of the primary advantages of decision trees is their simplicity; they do not require any complex mathematical formulations, making them easier to understand </span><span><span class="kobospan" id="kobo.361.1">and visualize.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.362.1">The goal of a decision tree is to split the data in a manner that maximizes the purity of the nodes resulting from those splits. </span><span class="kobospan" id="kobo.362.2">In the context of a classification problem, “purity” refers to how homogeneous the nodes are with respect to the target variable. </span><span class="kobospan" id="kobo.362.3">A perfectly pure node would contain instances of only a </span><span><span class="kobospan" id="kobo.363.1">single class.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.364.1">Decision trees achieve this by using measures of impurity, such as the Gini index or entropy (more on that soon), to evaluate potential splits. </span><span class="kobospan" id="kobo.364.2">A good split is one that most effectively separates the data into nodes with high purity, meaning that it increases the homogeneity of the nodes with respect to the </span><span><span class="kobospan" id="kobo.365.1">target variable.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.366.1">The process involves </span><span><span class="kobospan" id="kobo.367.1">the following:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.368.1">Selecting the best attribute to split the data based on a specific criterion (such as Gini </span><span><span class="kobospan" id="kobo.369.1">or entropy).</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.370.1">Partitioning the dataset into subsets that contain instances with similar values for </span><span><span class="kobospan" id="kobo.371.1">that attribute.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.372.1">Repeating this process recursively for each derived subset until the stopping criteria are met (which could be a maximum depth of the tree, a minimum number of instances in a node, or the achievement of a node with </span><span><span class="kobospan" id="kobo.373.1">high purity).</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.374.1">This recursive partitioning makes decision trees a powerful and interpretable modeling technique for classification and </span><span><span class="kobospan" id="kobo.375.1">regression tasks.</span></span></p>
<h2 id="_idParaDest-163" class="calibre7"><a id="_idTextAnchor313" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.376.1">Measuring purity</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.377.1">The </span><strong class="bold"><span class="kobospan" id="kobo.378.1">Gini index</span></strong><span class="kobospan" id="kobo.379.1"> is a measure</span><a id="_idIndexMarker664" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.380.1"> of inequality among values of </span><a id="_idIndexMarker665" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.381.1">a frequency distribution (for example, levels of income). </span><span class="kobospan" id="kobo.381.2">In the context of ML and decision trees, it measures the impurity of a node with the </span><span><span class="kobospan" id="kobo.382.1">following formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.383.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Gini&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/179.png" class="calibre183"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.384.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.385.1">D</span></em><span class="kobospan" id="kobo.386.1"> is the dataset, </span><em class="italic"><span class="kobospan" id="kobo.387.1">J</span></em><span class="kobospan" id="kobo.388.1"> is the number of classes, and </span><em class="italic"><span class="kobospan" id="kobo.389.1">pi</span></em><span class="kobospan" id="kobo.390.1"> is the probability of class </span><em class="italic"><span class="kobospan" id="kobo.391.1">i</span></em><span class="kobospan" id="kobo.392.1"> in the </span><span><span class="kobospan" id="kobo.393.1">dataset </span></span><span><em class="italic"><span class="kobospan" id="kobo.394.1">D</span></em></span><span><span class="kobospan" id="kobo.395.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.396.1">Entropy, on the other hand, is a measure from information theory that quantifies the amount of uncertainty or randomness in the data. </span><span class="kobospan" id="kobo.396.2">It’s used in the construction of decision trees to represent the impurity of a dataset with the </span><span><span class="kobospan" id="kobo.397.1">following formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.398.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Entropy&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/180.png" class="calibre184"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.399.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.400.1">pi</span></em><span class="kobospan" id="kobo.401.1"> is the probability of class </span><em class="italic"><span class="kobospan" id="kobo.402.1">i</span></em><span class="kobospan" id="kobo.403.1"> within the </span><span><span class="kobospan" id="kobo.404.1">dataset </span></span><span><em class="italic"><span class="kobospan" id="kobo.405.1">D</span></em></span><span><span class="kobospan" id="kobo.406.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.407.1">Both the Gini index and entropy are used to choose where to split the data when building a decision tree. </span><span class="kobospan" id="kobo.407.2">The choice between using the Gini index and entropy often depends on the specific dataset and the preferences of the modeler, as they can lead to slightly different trees. </span><span class="kobospan" id="kobo.407.3">In practice, the difference in the trees generated by these two methods is often </span><span><span class="kobospan" id="kobo.408.1">very small.</span></span></p>
<h2 id="_idParaDest-164" class="calibre7"><a id="_idTextAnchor314" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.409.1">Exploring the Titanic dataset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.410.1">The </span><em class="italic"><span class="kobospan" id="kobo.411.1">Titanic</span></em><span class="kobospan" id="kobo.412.1"> dataset is </span><a id="_idIndexMarker666" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.413.1">truly a classic in the field of data</span><a id="_idIndexMarker667" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.414.1"> science, often used to illustrate the fundamentals of ML. </span><span class="kobospan" id="kobo.414.2">It details the tragic sinking of the RMS Titanic, one of the most infamous shipwrecks in history. </span><span class="kobospan" id="kobo.414.3">This dataset serves as a rich source of demographic and travel information about the passengers, which can be utilized to model and predict </span><span><span class="kobospan" id="kobo.415.1">survival outcomes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.416.1">Through the lens of this dataset, we can apply statistical analysis and predictive modeling to understand factors that may have influenced the chances of survival. </span><span class="kobospan" id="kobo.416.2">For instance, consider a subset of only 25 passengers from the </span><em class="italic"><span class="kobospan" id="kobo.417.1">Titanic</span></em><span class="kobospan" id="kobo.418.1"> dataset. </span><span class="kobospan" id="kobo.418.2">Out of 25, 10 of these individuals survived the disaster, while 15 did not. </span><span class="kobospan" id="kobo.418.3">By examining attributes such as age, gender, class, and fare paid, we can begin to construct a predictive model that estimates the likelihood of survival for each passenger in </span><span><span class="kobospan" id="kobo.419.1">similar circumstances.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.420.1">We first calculate the </span><em class="italic"><span class="kobospan" id="kobo.421.1">Gini index</span></em><span class="kobospan" id="kobo.422.1"> before </span><span><span class="kobospan" id="kobo.423.1">doing anything.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.424.1">In this example, overall classes are </span><strong class="source-inline"><span class="kobospan" id="kobo.425.1">survived</span></strong><span class="kobospan" id="kobo.426.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.427.1">died</span></strong><span class="kobospan" id="kobo.428.1">, illustrated in the </span><span><span class="kobospan" id="kobo.429.1">following formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.430.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;G&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;survived&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;total&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;died&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;total&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt;G&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;25&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;25&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.48&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/181.png" class="calibre185"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.431.1">This Gini index of 0.48 indicates the level of impurity in the dataset. </span><span class="kobospan" id="kobo.431.2">The value suggests a moderate</span><a id="_idIndexMarker668" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.432.1"> separation </span><a id="_idIndexMarker669" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.433.1">between the classes, with some degree of mixture between the </span><em class="italic"><span class="kobospan" id="kobo.434.1">survived</span></em><span class="kobospan" id="kobo.435.1"> and </span><em class="italic"><span class="kobospan" id="kobo.436.1">died</span></em><span class="kobospan" id="kobo.437.1"> categories within this group </span><span><span class="kobospan" id="kobo.438.1">of passengers.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.439.1">If we were to make a split in the dataset based on a certain feature, we would calculate the Gini index for each resulting subset. </span><span class="kobospan" id="kobo.439.2">The goal is to choose a split that minimizes the Gini index, thus increasing the purity of the subsets with respect to the target variable, which in this case is survival on </span><span><span class="kobospan" id="kobo.440.1">the Titanic.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.441.1">Now, let’s consider a potential split on gender. </span><span class="kobospan" id="kobo.441.2">We first calculate the Gini index for each given gender, as seen in </span><span><em class="italic"><span class="kobospan" id="kobo.442.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.443.1">.5</span></em></span><span><span class="kobospan" id="kobo.444.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer278">
<span class="kobospan" id="kobo.445.1"><img alt="Figure 11.5 – Calculating impurity of our dataset on gender" src="image/B19488_11_05.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.446.1">Figure 11.5 – Calculating impurity of our dataset on gender</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.447.1">The following formula calculates the Gini index for male and female, </span><span><span class="kobospan" id="kobo.448.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.449.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Gini&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;13&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.23&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/182.png" class="calibre186"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.450.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Gini&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.32&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/183.png" class="calibre187"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.451.1">Once we</span><a id="_idIndexMarker670" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.452.1"> have </span><a id="_idIndexMarker671" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.453.1">the Gini index for each gender, we then calculate the overall Gini index for the split on gender, </span><span><span class="kobospan" id="kobo.454.1">as follows:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.455.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Gini&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mtext&gt;Gini&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.23&lt;/mml:mn&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;0.32&lt;/mml:mn&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.27&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/184.png" class="calibre188"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.456.1">So, the </span><em class="italic"><span class="kobospan" id="kobo.457.1">Gini coefficient</span></em><span class="kobospan" id="kobo.458.1"> for splitting on </span><em class="italic"><span class="kobospan" id="kobo.459.1">gender</span></em><span class="kobospan" id="kobo.460.1"> is </span><em class="italic"><span class="kobospan" id="kobo.461.1">0.27</span></em><span class="kobospan" id="kobo.462.1">. </span><span class="kobospan" id="kobo.462.2">We then follow this procedure for three potential splits (shown in </span><span><em class="italic"><span class="kobospan" id="kobo.463.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.464.1">.6</span></em></span><span><span class="kobospan" id="kobo.465.1">):</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.466.1">Gender (male </span><span><span class="kobospan" id="kobo.467.1">or female)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.468.1">Number of siblings on board (0 </span><span><span class="kobospan" id="kobo.469.1">or 1+)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.470.1">Class (first and second </span><span><span class="kobospan" id="kobo.471.1">versus third)</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer282">
<span class="kobospan" id="kobo.472.1"><img alt="Figure 11.6 – Calculating the resulting Gini coefficient for multiple splits on our dataset to decide which one to use for our decision tree" src="image/B19488_11_06.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.473.1">Figure 11.6 – Calculating the resulting Gini coefficient for multiple splits on our dataset to decide which one to use for our decision tree</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.474.1">In this example, we would choose the gender to split on as it has the </span><em class="italic"><span class="kobospan" id="kobo.475.1">lowest </span></em><span><em class="italic"><span class="kobospan" id="kobo.476.1">Gini index</span></em></span><span><span class="kobospan" id="kobo.477.1">!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.478.1">Before we get to some more code, we need to think about how to deal with categorical features that </span><a id="_idIndexMarker672" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.479.1">are not numerically encoded. </span><span class="kobospan" id="kobo.479.2">ML </span><a id="_idIndexMarker673" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.480.1">algorithms require numerical inputs, and most datasets will have at least one feature that is </span><span><span class="kobospan" id="kobo.481.1">not numerical.</span></span></p>
<h2 id="_idParaDest-165" class="calibre7"><a id="_idTextAnchor315" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.482.1">Dummy variables</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.483.1">Dummy</span><a id="_idIndexMarker674" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.484.1"> variables are</span><a id="_idIndexMarker675" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.485.1"> used when we are hoping to convert a categorical feature into a quantitative one. </span><span class="kobospan" id="kobo.485.2">Remember that we have two types of categorical features: nominal and ordinal. </span><span class="kobospan" id="kobo.485.3">Ordinal features have natural order among them, while nominal data </span><span><span class="kobospan" id="kobo.486.1">does not.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.487.1">Encoding qualitative (nominal) data using separate columns is called making dummy variables, and it works by turning each unique category of a nominal column into its own column that is either </span><strong class="source-inline"><span class="kobospan" id="kobo.488.1">true</span></strong> <span><span class="kobospan" id="kobo.489.1">or </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.490.1">false</span></strong></span><span><span class="kobospan" id="kobo.491.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.492.1">For example, if we had a column for someone’s college major and we wished to plug that information into linear or logistic regression, we couldn’t because they only take in numbers! </span><span class="kobospan" id="kobo.492.2">So, for each row, we had new columns that represent the single nominal column. </span><span class="kobospan" id="kobo.492.3">In this case, we have four unique majors: computer science, engineering, business, and literature. </span><span class="kobospan" id="kobo.492.4">We end up with three new columns (we omit computer science as it is not necessary and can be inferred if all of the other three majors are 0). </span><span><em class="italic"><span class="kobospan" id="kobo.493.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.494.1">.7</span></em><span class="kobospan" id="kobo.495.1"> shows us </span><span><span class="kobospan" id="kobo.496.1">an example:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer283">
<span class="kobospan" id="kobo.497.1"><img alt="Figure 11.7 – Creating dummy variables for a single feature involves creating a new binary feature for each option except for one, which can be inferred by having all 0s in the rest of the features" src="image/B19488_11_07.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.498.1">Figure 11.7 – Creating dummy variables for a single feature involves creating a new binary feature for each option except for one, which can be inferred by having all 0s in the rest of the features</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.499.1">Note that the first row has a 0 in all of the columns, which means that this person did not major in</span><a id="_idIndexMarker676" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.500.1"> engineering, did not major in business, and did not </span><a id="_idIndexMarker677" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.501.1">major in literature. </span><span class="kobospan" id="kobo.501.2">The second person has a single 1 in the </span><strong class="source-inline"><span class="kobospan" id="kobo.502.1">Engineering</span></strong><span class="kobospan" id="kobo.503.1"> column as that is the major </span><span><span class="kobospan" id="kobo.504.1">they studied.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.505.1">We are going to need to make some dummy variables using pandas as we use scikit-learn’s built-in decision tree function in order to build a </span><span><span class="kobospan" id="kobo.506.1">decision tree:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.507.1">
# read in the data
titanic = pd.read_csv('short_titanic.csv')
# encode female as 0 and male as 1
titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})
# fill in the missing values for age with the median age titanic.Age.fillna(titanic.Age.median(), inplace=True)
# create a DataFrame of dummy variables for Embarked
embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked') embarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)
# concatenate the original DataFrame and the dummy DataFrame titanic = pd.concat([titanic, embarked_dummies], axis=1)
# define X and y
feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S'] X = titanic[feature_cols]
y = titanic.Survived X.head()</span></pre>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.508.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.509.1">.8</span></em><span class="kobospan" id="kobo.510.1"> shows what our dataset looks like after our preceding code block. </span><span class="kobospan" id="kobo.510.2">Note that we are going to use class, sex, age, and dummy variables for city embarked as </span><span><span class="kobospan" id="kobo.511.1">our features:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer284">
<span class="kobospan" id="kobo.512.1"><img alt="Figure 11.8 – Our Titanic dataset after creating dummy variables for Embarked" src="image/B19488_11_08.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.513.1">Figure 11.8 – Our Titanic dataset after creating dummy variables for Embarked</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.514.1">Now, we</span><a id="_idIndexMarker678" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.515.1"> can</span><a id="_idIndexMarker679" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.516.1"> fit our decision </span><span><span class="kobospan" id="kobo.517.1">tree classifier:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.518.1">
# fit a classification tree with max_depth=3 on all data from sklearn.tree import DecisionTreeClassifier treeclf = DecisionTreeClassifier(max_depth=3, random_state=1) treeclf.fit(X, y)</span></pre>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.519.1">max_depth</span></strong><span class="kobospan" id="kobo.520.1"> is a hyperparameter that limits the depth of our tree. </span><span class="kobospan" id="kobo.520.2">It means that, for any data point, our tree is only able to ask up to three questions and create three splits. </span><span class="kobospan" id="kobo.520.3">We can output our tree into a visual format, and we will obtain the result seen in </span><span><em class="italic"><span class="kobospan" id="kobo.521.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.522.1">.9</span></em></span><span><span class="kobospan" id="kobo.523.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer285">
<span class="kobospan" id="kobo.524.1"><img alt="Figure 11.9 – The decision tree produced with scikit-learn with the Gini coefficient calculated at each node" src="image/B19488_11_09.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.525.1">Figure 11.9 – The decision tree produced with scikit-learn with the Gini coefficient calculated at each node</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.526.1">We can </span><a id="_idIndexMarker680" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.527.1">notice a </span><a id="_idIndexMarker681" class="pcalibre calibre4 pcalibre1"/><span><span class="kobospan" id="kobo.528.1">few things:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.529.1">Sex</span></strong><span class="kobospan" id="kobo.530.1"> is the first split, meaning that sex is the most important determining factor of whether or not a person survived </span><span><span class="kobospan" id="kobo.531.1">the crash</span></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.532.1">Embarked_Q</span></strong><span class="kobospan" id="kobo.533.1"> was never used in </span><span><span class="kobospan" id="kobo.534.1">any split</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.535.1">For either classification or regression trees, we can also do something very interesting with decision trees, which is that we can output a number that represents each feature’s importance in the prediction of our data points (shown in </span><span><em class="italic"><span class="kobospan" id="kobo.536.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.537.1">.10</span></em></span><span><span class="kobospan" id="kobo.538.1">):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.539.1">
# compute the feature importances pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer286">
<span class="kobospan" id="kobo.540.1"><img alt="Figure 11.10 – Features that contributed most to the change in the Gini coefficient displayed as percentages adding up to 1; it’s no coincidence that our highest value (Sex) is also our first split" src="image/B19488_11_10.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.541.1">Figure 11.10 – Features that contributed most to the change in the Gini coefficient displayed as percentages adding up to 1; it’s no coincidence that our highest value (Sex) is also our first split</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.542.1">The importance scores are an average Gini index difference for each variable, with higher values corresponding to higher importance to the prediction. </span><span class="kobospan" id="kobo.542.2">We can use this information to select fewer features in the future. </span><span class="kobospan" id="kobo.542.3">For example, both of the embarked variables are very low in comparison to the rest of the features, so we may be a</span><a id="_idTextAnchor316" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.543.1">ble to say that they are not important in our prediction of life </span><span><span class="kobospan" id="kobo.544.1">or death.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.545.1">As we transition from the structured realm of SL, where the outcomes are known and the model learns from labeled data, we venture into the domain of UL. </span><span class="kobospan" id="kobo.545.2">Recall that UL algorithms uncover hidden patterns and intrinsic structures within data that isn’t explicitly labeled. </span><span class="kobospan" id="kobo.545.3">In the upcoming section, we will explore how unsupervised techniques can discern</span><a id="_idIndexMarker682" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.546.1"> underlying relationships in data and provide deeper insights </span><a id="_idIndexMarker683" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.547.1">without the guidance of a predefined outcome, and how they can complement the predictive models we’ve discussed </span><span><span class="kobospan" id="kobo.548.1">so far.</span></span></p>
<h1 id="_idParaDest-166" class="calibre6"><a id="_idTextAnchor317" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.549.1">Diving deep into UL</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.550.1">It’s time to see some </span><a id="_idIndexMarker684" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.551.1">examples of UL, given that we’ve spent some time on </span><span><em class="italic"><span class="kobospan" id="kobo.552.1">SL algorithms</span></em></span><span><span class="kobospan" id="kobo.553.1">.</span></span></p>
<h2 id="_idParaDest-167" class="calibre7"><a id="_idTextAnchor318" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.554.1">When to use UL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.555.1">There are many times</span><a id="_idIndexMarker685" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.556.1"> when UL can be appropriate. </span><span class="kobospan" id="kobo.556.2">Some very common examples include </span><span><span class="kobospan" id="kobo.557.1">the following:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.558.1">There is no clear response variable. </span><span class="kobospan" id="kobo.558.2">There is nothing that we are explicitly trying to predict or correlate to </span><span><span class="kobospan" id="kobo.559.1">other variables.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.560.1">To extract structure from data where no apparent structure or patterns exist (can be an </span><span><span class="kobospan" id="kobo.561.1">SL problem).</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.562.1">When an unsupervised concept </span><a id="_idIndexMarker686" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.563.1">called </span><strong class="bold"><span class="kobospan" id="kobo.564.1">feature extraction</span></strong><span class="kobospan" id="kobo.565.1"> is used. </span><span class="kobospan" id="kobo.565.2">Feature extraction is the process of creating new features from existing ones. </span><span class="kobospan" id="kobo.565.3">These new features can be even stronger than the </span><span><span class="kobospan" id="kobo.566.1">original features.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.567.1">The first tends to be the most common reason that data scientists choose to use UL. </span><span class="kobospan" id="kobo.567.2">This case arises frequently when we are working with data and we are not explicitly trying to predict any of the columns, and we merely wish to find patterns of similar (and dissimilar) groups of points. </span><span class="kobospan" id="kobo.567.3">The second option comes into play even if we are explicitly attempting to use a supervised model to predict a </span><span><span class="kobospan" id="kobo.568.1">response variable.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.569.1">Sometimes, simple </span><strong class="bold"><span class="kobospan" id="kobo.570.1">exploratory data analysis</span></strong><span class="kobospan" id="kobo.571.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.572.1">EDA</span></strong><span class="kobospan" id="kobo.573.1">) might </span><a id="_idIndexMarker687" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.574.1">not produce any clear patterns in the data in the few dimensions that humans can imagine, whereas a machine might pick up on data points behaving similarly to each other in </span><span><span class="kobospan" id="kobo.575.1">greater dimensions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.576.1">The third common reason to use UL is to extract new features from features that already exist. </span><span class="kobospan" id="kobo.576.2">This </span><a id="_idIndexMarker688" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.577.1">process (lovingly called feature extraction) might produce features that can be used in a future supervised model o</span><a id="_idTextAnchor319" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.578.1">r that can be used for presentation purposes (marketing </span><span><span class="kobospan" id="kobo.579.1">or otherwise).</span></span></p>
<h2 id="_idParaDest-168" class="calibre7"><a id="_idTextAnchor320" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.580.1">k-means clustering</span></h2>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.581.1">k-means clustering</span></strong><span class="kobospan" id="kobo.582.1"> is our </span><a id="_idIndexMarker689" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.583.1">first example</span><a id="_idIndexMarker690" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.584.1"> of an </span><strong class="bold"><span class="kobospan" id="kobo.585.1">unsupervised ML</span></strong><span class="kobospan" id="kobo.586.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.587.1">UML</span></strong><span class="kobospan" id="kobo.588.1">) model. </span><span class="kobospan" id="kobo.588.2">Remember – this</span><a id="_idIndexMarker691" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.589.1"> means that we are not making predictions. </span><span class="kobospan" id="kobo.589.2">We are trying instead to extract structure from seemingly </span><span><span class="kobospan" id="kobo.590.1">unstructured data.</span></span></p>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.591.1">Clustering</span></strong><span class="kobospan" id="kobo.592.1"> is a</span><a id="_idIndexMarker692" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.593.1"> family of UML models that attempt to group data points into clusters with centroids. </span><span class="kobospan" id="kobo.593.2">The concept of </span><strong class="bold"><span class="kobospan" id="kobo.594.1">similarity</span></strong><span class="kobospan" id="kobo.595.1"> is central to the definition of a cluster, and therefore to cluster analysis. </span><span class="kobospan" id="kobo.595.2">In general, greater similarity between points leads to better clustering. </span><span class="kobospan" id="kobo.595.3">In most cases, we turn data into points in </span><em class="italic"><span class="kobospan" id="kobo.596.1">n</span></em><span class="kobospan" id="kobo.597.1">-dimensional space and use the distance between these points as a form of similarity. </span><span class="kobospan" id="kobo.597.2">The </span><strong class="bold"><span class="kobospan" id="kobo.598.1">centroid</span></strong><span class="kobospan" id="kobo.599.1"> of the </span><a id="_idIndexMarker693" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.600.1">cluster is then usually the average of each dimension (column) for each data point in each cluster. </span><span class="kobospan" id="kobo.600.2">So, for example, the centroid of the red cluster is the result of taking the average value of each column of each red </span><span><span class="kobospan" id="kobo.601.1">data point.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.602.1">The purpose of cluster analysis is to enhance our understanding of a dataset by dividing the data into groups. </span><span class="kobospan" id="kobo.602.2">Clustering provides a layer of abstraction from individual data points. </span><span class="kobospan" id="kobo.602.3">The goal is to extract and enhance the natural structure of the data. </span><span class="kobospan" id="kobo.602.4">There are many kinds of classification procedures. </span><span class="kobospan" id="kobo.602.5">For our class, we will be focusing on k-means clustering, which is one of the most popular </span><span><span class="kobospan" id="kobo.603.1">clustering algorithms.</span></span></p>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.604.1">k-means</span></strong><span class="kobospan" id="kobo.605.1"> is an iterative method that partitions a dataset into </span><em class="italic"><span class="kobospan" id="kobo.606.1">k</span></em><span class="kobospan" id="kobo.607.1"> clusters. </span><span class="kobospan" id="kobo.607.2">It works in </span><span><span class="kobospan" id="kobo.608.1">four steps:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.609.1">Choose </span><em class="italic"><span class="kobospan" id="kobo.610.1">k</span></em><span class="kobospan" id="kobo.611.1"> initial centroids (note that </span><em class="italic"><span class="kobospan" id="kobo.612.1">k</span></em><span class="kobospan" id="kobo.613.1"> is </span><span><span class="kobospan" id="kobo.614.1">an input).</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.615.1">For each </span><a id="_idIndexMarker694" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.616.1">point, assign the point to</span><a id="_idIndexMarker695" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.617.1"> the </span><span><span class="kobospan" id="kobo.618.1">nearest cent</span><a id="_idTextAnchor321" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.619.1">roid.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.620.1">Recalculate the </span><span><span class="kobospan" id="kobo.621.1">centroid positions.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.622.1">Repeat </span><em class="italic"><span class="kobospan" id="kobo.623.1">steps 2</span></em><span class="kobospan" id="kobo.624.1"> and </span><em class="italic"><span class="kobospan" id="kobo.625.1">3</span></em><span class="kobospan" id="kobo.626.1"> until the stopping criteria </span><span><span class="kobospan" id="kobo.627.1">are met.</span></span></li>
</ol>
<h3 class="calibre8"><span class="kobospan" id="kobo.628.1">An illustrative example of clustering</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.629.1">Imagine that we </span><a id="_idIndexMarker696" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.630.1">have data points in a two-dimensional space, as seen in </span><span><em class="italic"><span class="kobospan" id="kobo.631.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.632.1">.11</span></em></span><span><span class="kobospan" id="kobo.633.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer287">
<span class="kobospan" id="kobo.634.1"><img alt="Figure 11.11 – A mock dataset to be clustered" src="image/B19488_11_11.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.635.1">Figure 11.11 – A mock dataset to be clustered</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.636.1">Each dot is colored gray to assume no prior grouping before applying the k-means algorithm. </span><span class="kobospan" id="kobo.636.2">The goal here is to eventually color in each dot and create groupings (clusters), as illustrated in </span><span><em class="italic"><span class="kobospan" id="kobo.637.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.638.1">.12</span></em></span><span><span class="kobospan" id="kobo.639.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer288">
<span class="kobospan" id="kobo.640.1"><img alt="Figure 11.12 – Step 1: k-means clustering begins by placing random centroids" src="image/B19488_11_12.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.641.1">Figure 11.12 – Step 1: k-means clustering begins by placing random centroids</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.642.1">We </span><a id="_idIndexMarker697" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.643.1">have (randomly) chosen </span><em class="italic"><span class="kobospan" id="kobo.644.1">three centroids</span></em><span class="kobospan" id="kobo.645.1"> (</span><em class="italic"><span class="kobospan" id="kobo.646.1">red</span></em><span class="kobospan" id="kobo.647.1">, </span><em class="italic"><span class="kobospan" id="kobo.648.1">blue</span></em><span class="kobospan" id="kobo.649.1">, </span><span><span class="kobospan" id="kobo.650.1">and </span></span><span><em class="italic"><span class="kobospan" id="kobo.651.1">yellow</span></em></span><span><span class="kobospan" id="kobo.652.1">).</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.653.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.654.1">Most k-means algorithms place random initial centroids, but there exist other pre-computed methods to place initial centroids. </span><span class="kobospan" id="kobo.654.2">For now, random </span><span><span class="kobospan" id="kobo.655.1">is fine.</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.656.1">Step 2</span></em><span class="kobospan" id="kobo.657.1"> has been applied in </span><span><em class="italic"><span class="kobospan" id="kobo.658.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.659.1">.13</span></em><span class="kobospan" id="kobo.660.1">. </span><span class="kobospan" id="kobo.660.2">For each data point, we found the most similar </span><span><span class="kobospan" id="kobo.661.1">centroid (closest):</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer289">
<span class="kobospan" id="kobo.662.1"><img alt="Figure 11.13 – Step 2: For each point, assign the point to the nearest centroid" src="image/B19488_11_13.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.663.1">Figure 11.13 – Step 2: For each point, assign the point to the nearest centroid</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.664.1">We then apply </span><em class="italic"><span class="kobospan" id="kobo.665.1">step 3</span></em><span class="kobospan" id="kobo.666.1"> in </span><span><em class="italic"><span class="kobospan" id="kobo.667.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.668.1">.14</span></em></span><span><span class="kobospan" id="kobo.669.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer290">
<span class="kobospan" id="kobo.670.1"><img alt="Figure 11.14 – Step 3: Recalculate the centroid positions" src="image/B19488_11_14.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.671.1">Figure 11.14 – Step 3: Recalculate the centroid positions</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.672.1">This is </span><em class="italic"><span class="kobospan" id="kobo.673.1">step 3</span></em><span class="kobospan" id="kobo.674.1"> and the crux of k-means. </span><span class="kobospan" id="kobo.674.2">Note that we have physically moved the centroids to be</span><a id="_idIndexMarker698" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.675.1"> the actual center of each cluster. </span><span class="kobospan" id="kobo.675.2">We have, for each color, computed the average point and made that point the new centroid. </span><span class="kobospan" id="kobo.675.3">For example, suppose the three red data points had the following coordinates: </span><em class="italic"><span class="kobospan" id="kobo.676.1">(1, 3)</span></em><span class="kobospan" id="kobo.677.1">, </span><em class="italic"><span class="kobospan" id="kobo.678.1">(2, 5)</span></em><span class="kobospan" id="kobo.679.1">, and </span><em class="italic"><span class="kobospan" id="kobo.680.1">(3, 4)</span></em><span class="kobospan" id="kobo.681.1">. </span><span class="kobospan" id="kobo.681.2">The </span><em class="italic"><span class="kobospan" id="kobo.682.1">center (red cross)</span></em><span class="kobospan" id="kobo.683.1"> would be calculated </span><span><span class="kobospan" id="kobo.684.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.685.1">
# centroid calculation import numpy as np
red_point1 = np.array([1, 3]) red_point2 = np.array([2, 5]) red_point3 = np.array([3, 4])
red_center = (red_point1 + red_point2 + red_point3) / 3.
</span><span class="kobospan1" id="kobo.685.2">red_center
# array([ 2., 4.])</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.686.1">That is, the </span><em class="italic"><span class="kobospan" id="kobo.687.1">(2, 4)</span></em><span class="kobospan" id="kobo.688.1"> point would be the coordinates of the preceding </span><span><span class="kobospan" id="kobo.689.1">red cross.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.690.1">We continue with our algorithm by repeating </span><em class="italic"><span class="kobospan" id="kobo.691.1">step 2</span></em><span class="kobospan" id="kobo.692.1">. </span><span class="kobospan" id="kobo.692.2">Here is the first part where we find the closest center for each point. </span><span class="kobospan" id="kobo.692.3">Note a big change – the point in the bottom left used to be a yellow point but has changed to be a red cluster point because the yellow cluster moved</span><a id="_idIndexMarker699" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.693.1"> closer to its </span><span><span class="kobospan" id="kobo.694.1">yellow constituents:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer291">
<span class="kobospan" id="kobo.695.1"><img alt="Figure 11.15 – Repeating step 2; note the data point on the lower left was yellow in the previous step and is now red" src="image/B19488_11_15.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.696.1">Figure 11.15 – Repeating step 2; note the data point on the lower left was yellow in the previous step and is now red</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.697.1">If we follow </span><em class="italic"><span class="kobospan" id="kobo.698.1">step 3</span></em><span class="kobospan" id="kobo.699.1"> again, we get the result shown in </span><span><em class="italic"><span class="kobospan" id="kobo.700.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.701.1">.16</span></em></span><span><span class="kobospan" id="kobo.702.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer292">
<span class="kobospan" id="kobo.703.1"><img alt="Figure 11.16 – Step 3 again" src="image/B19488_11_16.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.704.1">Figure 11.16 – Step 3 again</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.705.1">Here, we recalculate once more the centroids for each cluster (</span><em class="italic"><span class="kobospan" id="kobo.706.1">step 3</span></em><span class="kobospan" id="kobo.707.1">). </span><span class="kobospan" id="kobo.707.2">Note that the blue center did not move at all, while the yellow and red centers </span><span><span class="kobospan" id="kobo.708.1">both moved.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.709.1">Because we have</span><a id="_idIndexMarker700" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.710.1"> reached a </span><strong class="bold"><span class="kobospan" id="kobo.711.1">stopping criterion</span></strong><span class="kobospan" id="kobo.712.1"> (clusters do not move if we repeat </span><em class="italic"><span class="kobospan" id="kobo.713.1">steps 2</span></em><span class="kobospan" id="kobo.714.1"> and </span><em class="italic"><span class="kobospan" id="kobo.715.1">3</span></em><span class="kobospan" id="kobo.716.1">), we </span><a id="_idIndexMarker701" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.717.1">finalize our algorithm and we have our three clusters, which is t</span><a id="_idTextAnchor322" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.718.1">he final result of the </span><span><span class="kobospan" id="kobo.719.1">k-means algorithm.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.720.1">An illustrative example – beer!</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.721.1">Let’s run a cluster analysis on a new dataset outlining different beers with different characteristics. </span><span class="kobospan" id="kobo.721.2">We know that there are many types of beer, but I wonder if we could possibly group beers into different categories based on different </span><span><span class="kobospan" id="kobo.722.1">quantitative features.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.723.1">Let’s try! </span><span class="kobospan" id="kobo.723.2">Let’s import a dataset of just a few types of beer and visualize a few rows in </span><span><em class="italic"><span class="kobospan" id="kobo.724.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.725.1">.17</span></em></span><span><span class="kobospan" id="kobo.726.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.727.1">
# import the beer dataset url = '../data/beer.txt'
beer = pd.read_csv(url, sep=' ')
beer.head()</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer293">
<span class="kobospan" id="kobo.728.1"><img alt="Figure 11.17 – The first five rows of our beer dataset" src="image/B19488_11_17.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.729.1">Figure 11.17 – The first five rows of our beer dataset</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.730.1">Our dataset has 20 beers with 5 columns: </span><strong class="source-inline"><span class="kobospan" id="kobo.731.1">name</span></strong><span class="kobospan" id="kobo.732.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.733.1">calories</span></strong><span class="kobospan" id="kobo.734.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.735.1">sodium</span></strong><span class="kobospan" id="kobo.736.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.737.1">alcohol</span></strong><span class="kobospan" id="kobo.738.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.739.1">cost</span></strong><span class="kobospan" id="kobo.740.1">. </span><span class="kobospan" id="kobo.740.2">In clustering (as with almost all ML models), we like quantitative features, so we will ignore the name of the beer in </span><span><span class="kobospan" id="kobo.741.1">our clustering:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.742.1">
# define X
X = beer.drop('name', axis=1)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.743.1">Now, we will</span><a id="_idIndexMarker702" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.744.1"> perform k-means clustering </span><span><span class="kobospan" id="kobo.745.1">using </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.746.1">scikit-learn</span></strong></span><span><span class="kobospan" id="kobo.747.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.748.1">
# K-means with 3 clusters
from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=1) km.fit(X)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.749.1">Our k-means algorithm has run the algorithm on our data points and come up with </span><span><span class="kobospan" id="kobo.750.1">three clusters:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.751.1">
# save the cluster labels and sort by cluster beer['cluster'] = km.labels_</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.752.1">We can take a look at the center of each cluster by using </span><strong class="source-inline"><span class="kobospan" id="kobo.753.1">groupby</span></strong><span class="kobospan" id="kobo.754.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.755.1">mean</span></strong><span class="kobospan" id="kobo.756.1"> statements (visualized in </span><span><em class="italic"><span class="kobospan" id="kobo.757.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.758.1">.18</span></em></span><span><span class="kobospan" id="kobo.759.1">):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.760.1">
# calculate the mean of each feature for each cluster beer.groupby('cluster').mean()</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer294">
<span class="kobospan" id="kobo.761.1"><img alt="Figure 11.18 – Our found clusters for the beer dataset with k=3" src="image/B19488_11_18.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.762.1">Figure 11.18 – Our found clusters for the beer dataset with k=3</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.763.1">On inspection, we can see that </span><em class="italic"><span class="kobospan" id="kobo.764.1">cluster 0</span></em><span class="kobospan" id="kobo.765.1"> has, on average, a higher calorie, sodium, and alcohol content and costs more. </span><span class="kobospan" id="kobo.765.2">These might be considered heavier beers. </span><em class="italic"><span class="kobospan" id="kobo.766.1">Cluster 2</span></em><span class="kobospan" id="kobo.767.1"> has on average a very low alcohol content and very few calories. </span><span class="kobospan" id="kobo.767.2">These are probably light beers. </span><em class="italic"><span class="kobospan" id="kobo.768.1">Cluster 1</span></em><span class="kobospan" id="kobo.769.1"> is somewhere in </span><span><span class="kobospan" id="kobo.770.1">the middle.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.771.1">Let’s use</span><a id="_idIndexMarker703" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.772.1"> Python to make a graph to see this in more detail, as seen in </span><span><em class="italic"><span class="kobospan" id="kobo.773.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.774.1">.19</span></em></span><span><span class="kobospan" id="kobo.775.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.776.1">
import matplotlib.pyplot as plt
%matplotlib inline
# save the DataFrame of cluster centers centers = beer.groupby('cluster').mean() # create a "colors" array for plotting
colors = np.array(['red', 'green', 'blue', 'yellow'])
# scatter plot of calories versus alcohol, colored by cluster (0=red, 1=green, 2=blue)
plt.scatter(beer.calories, beer.alcohol, c=colors[list(beer.cluster)], s=50)
# cluster centers, marked by "+"
plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black')
# add labels plt.x</span><a id="_idTextAnchor323" class="pcalibre pcalibre1 calibre162"/><span class="kobospan1" id="kobo.777.1">label('calories') plt.ylabel('alcohol')</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer295">
<span class="kobospan" id="kobo.778.1"><img alt="Figure 11.19 – Our cluster analysis visualized using two dimensions of our dataset" src="image/B19488_11_19.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.779.1">Figure 11.19 – Our cluster analysis visualized using two dimensions of our dataset</span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.780.1">Choosing an optimal number for K and cluster validation</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.781.1">A big part of k-means</span><a id="_idIndexMarker704" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.782.1"> clustering is knowing the optimal number of clusters. </span><span class="kobospan" id="kobo.782.2">If we knew this number ahead of time, then that might defeat the purpose of even using UL. </span><span class="kobospan" id="kobo.782.3">So, we need a way to evaluate the output of our cluster analysis. </span><span class="kobospan" id="kobo.782.4">The problem here is that, because we are not performing any kind of prediction, we cannot gauge how right the algorithm is at predict</span><a id="_idTextAnchor324" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.783.1">ions. </span><span class="kobospan" id="kobo.783.2">Metrics such as accuracy and RMSE go right out of the window. </span><span class="kobospan" id="kobo.783.3">Luckily, we do have a pretty useful metric to help optimize our cluster analyses, called the </span><span><span class="kobospan" id="kobo.784.1">Silhouette Coefficient.</span></span></p>
<h2 id="_idParaDest-169" class="calibre7"><a id="_idTextAnchor325" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.785.1">The Silhouette Coefficient</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.786.1">The </span><strong class="bold"><span class="kobospan" id="kobo.787.1">Silhouette Coefficient</span></strong><span class="kobospan" id="kobo.788.1"> is a</span><a id="_idIndexMarker705" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.789.1"> common metric </span><a id="_idIndexMarker706" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.790.1">for evaluating clustering performance in situations when the true cluster assignments are not known. </span><span class="kobospan" id="kobo.790.2">The Silhouette Coefficient is a measure used to assess the quality of clusters created by a clustering algorithm. </span><span class="kobospan" id="kobo.790.3">It quantifies how similar an object is to its own cluster (cohesion) compared to other clusters (separation). </span><span class="kobospan" id="kobo.790.4">The Silhouette Coefficient for a single data point is calculated using the </span><span><span class="kobospan" id="kobo.791.1">following formula:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.792.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/185.png" class="calibre189"/></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.793.1">Here, the </span><span><span class="kobospan" id="kobo.794.1">following applies:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.795.1">a</span></em><span class="kobospan" id="kobo.796.1"> is the mean distance between a sample and all other points in the same class or cluster. </span><span class="kobospan" id="kobo.796.2">It represents the cohesion of </span><span><span class="kobospan" id="kobo.797.1">the cluster.</span></span></li>
<li class="calibre14"><em class="italic"><span class="kobospan" id="kobo.798.1">b</span></em><span class="kobospan" id="kobo.799.1"> is the mean distance between a sample and all other points in the next nearest cluster. </span><span class="kobospan" id="kobo.799.2">It represents the separation from the nearest cluster that the sample is not a </span><span><span class="kobospan" id="kobo.800.1">part of.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.801.1">It ranges from </span><em class="italic"><span class="kobospan" id="kobo.802.1">-1 (worst)</span></em><span class="kobospan" id="kobo.803.1"> to </span><em class="italic"><span class="kobospan" id="kobo.804.1">1 (best)</span></em><span class="kobospan" id="kobo.805.1">. </span><span class="kobospan" id="kobo.805.2">A global score is calculated by taking the mean score for all observations. </span><span class="kobospan" id="kobo.805.3">The Silhouette Coefficient is particularly useful for determining the effectiveness of a clustering algorithm because it takes into account both the compactness of the clusters and the separation between them. </span><span class="kobospan" id="kobo.805.4">In general, a Silhouette Coefficient of 1 is preferred, while a score of -1 is </span><span><span class="kobospan" id="kobo.806.1">not preferable:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.807.1">
# calculate Silhouette Coefficient for K=3 from sklearn import metrics metrics.silhouette_score(X, km.labels_)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.808.1">The output is </span><span><span class="kobospan" id="kobo.809.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.810.1">
0.67317750464557957</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.811.1">Let's try</span><a id="_idIndexMarker707" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.812.1"> calculating</span><a id="_idIndexMarker708" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.813.1"> the coefficient for multiple values of </span><strong class="source-inline"><span class="kobospan" id="kobo.814.1">K</span></strong><span class="kobospan" id="kobo.815.1"> to find the </span><span><span class="kobospan" id="kobo.816.1">best value:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.817.1">
# center and scale the data
from sklearn.preprocessing import StandardScaler scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# calculate SC for K=2 through K=19 k_range = range(2, 20)
scores = []
for k in k_range:
km = KMeans(n_clusters=k, random_state=1) km.fit(X_scaled) scores.append(metrics.silhouette_score(X, km.labels_))
# plot the results plt.plot(k_range, scores) plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient') plt.grid(True)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.818.1">So, it looks like our optimal number of beer clusters is 4! </span><span class="kobospan" id="kobo.818.2">This means that our k-means algorithm has determined that there seem to be four distinct types </span><span><span class="kobospan" id="kobo.819.1">of beer.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer297">
<span class="kobospan" id="kobo.820.1"><img alt="Figure 11.20 – The Silhouette Coefficient for a varying number of clusters" src="image/B19488_11_20.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.821.1">Figure 11.20 – The Silhouette Coefficient for a varying number of clusters</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.822.1">k-means is a popular algorithm because of its computational efficiency and simple and intuitive nature. </span><span class="kobospan" id="kobo.822.2">k-means, however, is highly scale-dependent and is not suitable for data with widely varying shapes and densities. </span><span class="kobospan" id="kobo.822.3">There are ways to combat this issue by scaling data using </span><span><strong class="source-inline"><span class="kobospan" id="kobo.823.1">scikit-learn</span></strong></span><span><span class="kobospan" id="kobo.824.1">’s </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.825.1">StandardScalar</span></strong></span><span><span class="kobospan" id="kobo.826.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.827.1">
# center and scale the data
from sklearn.preprocessing import StandardScaler scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# K-means with 3 clusters on scaled data km = KMeans(n_clusters=3, random_state=1) km.fit(X_scaled)</span></pre>
<p class="calibre3"><span><span class="kobospan" id="kobo.828.1">Easy!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.829.1">Now, let’s </span><a id="_idIndexMarker709" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.830.1">take a </span><a id="_idIndexMarker710" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.831.1">look at the third option in our reasons for using unsupervised methods: </span><span><em class="italic"><span class="kobospan" id="kobo.832.1">feature extraction</span></em></span><span><span class="kobospan" id="kobo.833.1">.</span></span></p>
<h1 id="_idParaDest-170" class="calibre6"><a id="_idTextAnchor326" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.834.1">Feature extraction and PCA</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.835.1">A common</span><a id="_idIndexMarker711" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.836.1"> problem when working with data, particularly when</span><a id="_idIndexMarker712" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.837.1"> it comes to ML, is having an overwhelming number of columns and not enough rows to handle such a quantity </span><span><span class="kobospan" id="kobo.838.1">of columns.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.839.1">A great example of this is when we were looking at the </span><em class="italic"><span class="kobospan" id="kobo.840.1">send cash now</span></em><span class="kobospan" id="kobo.841.1"> example in our naïve Bayes example earlier. </span><span class="kobospan" id="kobo.841.2">Remember we had literally 0 instances of texts with that exact phrase? </span><span class="kobospan" id="kobo.841.3">In that case, we turned to a naïve assumption that allowed us to extrapolate a probability for both of </span><span><span class="kobospan" id="kobo.842.1">our categories.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.843.1">The reason we had this problem in the first place is because of something called the </span><strong class="bold"><span class="kobospan" id="kobo.844.1">curse of dimensionality</span></strong><span class="kobospan" id="kobo.845.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.846.1">COD</span></strong><span class="kobospan" id="kobo.847.1">). </span><span class="kobospan" id="kobo.847.2">The</span><a id="_idIndexMarker713" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.848.1"> COD basically says that as we introduce new feature columns, we need exponentially more rows (data points) to consider the increased number </span><span><span class="kobospan" id="kobo.849.1">of possibilities.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.850.1">Consider an example where we attempt to use a learning model that utilizes the distance between points on a corpus of text that has 4,086 pieces of text and that the whole thing has been count-vectorized using </span><strong class="source-inline"><span class="kobospan" id="kobo.851.1">scikit-learn</span></strong><span class="kobospan" id="kobo.852.1">. </span><span class="kobospan" id="kobo.852.2">Now, let’s do an experiment. </span><span class="kobospan" id="kobo.852.3">I will first consider a single word as the only dimension of our text. </span><span class="kobospan" id="kobo.852.4">Then, I will count how many pieces of text are within 1 unit of each other. </span><span class="kobospan" id="kobo.852.5">For example, if 2 sentences both contain that word, they would be 0 units away and, similarly, if </span><a id="_idIndexMarker714" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.853.1">neither </span><a id="_idIndexMarker715" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.854.1">of them contains the word, they would be 0 units away from </span><span><span class="kobospan" id="kobo.855.1">one another:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.856.1">
d = 1
# Let's look for points within 1 unit of one another
X_first_word = X.iloc[:,:1]
# Only looking at the first column, but ALL of the rows
from sklearn.neighbors import NearestNeighbors
# this module will calculate for us distances between each point
neigh = NearestNeighbors(n_neighbors=4086)
neigh.fit(X_first_word)
# tell the module to calculate each distance between each point
A = neigh.kneighbors_graph(X_first_word, mode='distance').todense() # This matrix holds all distances (over 16 million of them)
num_points_within_d = (A &lt; d).sum()
# Count the number of pairs of points within 1 unit of distance, 16,258,504</span></pre>
<p class="callout-heading"><span class="kobospan" id="kobo.857.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.858.1">Note that we have </span><strong class="source-inline1"><span class="kobospan" id="kobo.859.1">16,695,396</span></strong><span class="kobospan" id="kobo.860.1"> (</span><strong class="source-inline1"><span class="kobospan" id="kobo.861.1">4086*4086</span></strong><span class="kobospan" id="kobo.862.1">) distances to </span><span><span class="kobospan" id="kobo.863.1">scan over.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.864.1">So, 16.2 million pairs of texts are within a single unit of distance. </span><span class="kobospan" id="kobo.864.2">Now, let’s try again with the first </span><span><span class="kobospan" id="kobo.865.1">two words:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.866.1">
X_first_two_words = X.iloc[:,:2]
neigh = NearestNeighbors(n_neighbors=4086) neigh.fit(X_first_two_words)
A = neigh.kneighbors_graph(X_first_two_words, mode='distance').todense() num_points_within_d = (A &lt; d).sum()
# num_points_within_d is now 16,161,970</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.867.1">By considering a single new column, we lost about 100,000 pairs of points that were within a single unit of distance. </span><span class="kobospan" id="kobo.867.2">This is because we are adding space in between them for every dimension that we add. </span><span class="kobospan" id="kobo.867.3">Let’s take</span><a id="_idIndexMarker716" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.868.1"> this </span><a id="_idIndexMarker717" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.869.1">test a step further and calculate this number for the first 100 words and then plot </span><span><span class="kobospan" id="kobo.870.1">the results:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.871.1">
num_columns = range(1, 100)
# Looking at the first 100 columns points = []
# We will be collecting the number of points within 1 unit for a graph
neigh = NearestNeighbors(n_neighbors=X.shape[0])
for subset in num_columns:
X_subset = X.iloc[:,:subset]
# look at the first column, then first two columns, then first three columns, etc
neigh.fit(X_subset)
A = neigh.kneighbors_graph(X_subset, mode='distance').todense() num_points_within_d = (A &lt; d).sum()
# calculate the number of points within 1 unit points.append(num_points_within_d)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.872.1">Now, let’s plot the number of points within 1 unit versus the number of dimensions we consider in </span><span><em class="italic"><span class="kobospan" id="kobo.873.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.874.1">.21</span></em></span><span><span class="kobospan" id="kobo.875.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer298">
<span class="kobospan" id="kobo.876.1"><img alt="Figure 11.21 – The COD says that as we increase the number of feature columns in our dataset, data points become further away from each other due to the increase in high-dimensional space" src="image/B19488_11_21.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.877.1">Figure 11.21 – The COD says that as we increase the number of feature columns in our dataset, data points become further away from each other due to the increase in high-dimensional space</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.878.1">Put another way, the COD states that as we increase the number of feature columns, we need exponentially more data to maintain the same level of model performance. </span><span class="kobospan" id="kobo.878.2">This is because, in high-dimensional spaces, even the nearest neighbors can be very far away from a given data point, making it difficult to create good predictions. </span><span class="kobospan" id="kobo.878.3">High dimensionality also increases the risk of overfitting as the model may start to fit to noise in the data rather than the </span><span><span class="kobospan" id="kobo.879.1">actual signal.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.880.1">Moreover, with</span><a id="_idIndexMarker718" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.881.1"> more</span><a id="_idIndexMarker719" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.882.1"> dimensions, the volume of the space increases so rapidly that the available data becomes sparse. </span><span class="kobospan" id="kobo.882.2">This sparsity is problematic for any method that requires statistical significance. </span><span class="kobospan" id="kobo.882.3">In order to obtain a reliable result, the amount of data needed to support the analysis often grows exponentially with </span><span><span class="kobospan" id="kobo.883.1">the dimensionality.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.884.1">We can see clearly that the number of points within a single unit of one another goes down dramatically as we introduce more and more columns. </span><span class="kobospan" id="kobo.884.2">And this is only the first </span><span><span class="kobospan" id="kobo.885.1">100 columns!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.886.1">All of this space that we add in by considering new columns makes it harder for the finite amount of points we have to stay happily within range of each other. </span><span class="kobospan" id="kobo.886.2">We would have to add more points in order to fill in this gap. </span><span class="kobospan" id="kobo.886.3">And that, my friends, is why we should consider using </span><span><span class="kobospan" id="kobo.887.1">dimension reduction.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.888.1">The COD is solved by either adding more data points (which is not always possible) or implementing dimension reduction. </span><strong class="bold"><span class="kobospan" id="kobo.889.1">Dimension reduction</span></strong><span class="kobospan" id="kobo.890.1"> is</span><a id="_idIndexMarker720" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.891.1"> simply the act of reducing the number of columns in our dataset and not the number of rows. </span><span class="kobospan" id="kobo.891.2">There are two ways of implementing </span><span><span class="kobospan" id="kobo.892.1">dimension reduction:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.893.1">Feature selection</span></strong><span class="kobospan" id="kobo.894.1">: This is</span><a id="_idIndexMarker721" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.895.1"> the act of creating a subset of our column features and only using the </span><span><span class="kobospan" id="kobo.896.1">best features</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.897.1">Feature extraction</span></strong><span class="kobospan" id="kobo.898.1">: This is the act of mathematically transforming our feature set into a new extracted </span><span><span class="kobospan" id="kobo.899.1">coordinate system</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.900.1">We are familiar with feature selection as the process of saying the </span><strong class="source-inline"><span class="kobospan" id="kobo.901.1">Embarked_Q</span></strong><span class="kobospan" id="kobo.902.1"> column is not helping our decision tree. </span><span class="kobospan" id="kobo.902.2">Let’s get rid of it and see how it performs. </span><span class="kobospan" id="kobo.902.3">It is literally when we (or the machine) make the decision to ignore </span><span><span class="kobospan" id="kobo.903.1">certain columns.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.904.1">Feature extraction is a </span><span><span class="kobospan" id="kobo.905.1">bit trickier.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.906.1">In </span><em class="italic"><span class="kobospan" id="kobo.907.1">feature extraction</span></em><span class="kobospan" id="kobo.908.1">, we are using usually fairly complicated mathematical formulas in order to obtain new super columns that are usually better than any single </span><span><span class="kobospan" id="kobo.909.1">original column.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.910.1">Our primary model for doing so is called </span><strong class="bold"><span class="kobospan" id="kobo.911.1">PCA</span></strong><span class="kobospan" id="kobo.912.1">. </span><span class="kobospan" id="kobo.912.2">PCA will extract a set number of super columns in order to represent our original data with much fewer columns. </span><span class="kobospan" id="kobo.912.3">Let’s take a concrete example. </span><span class="kobospan" id="kobo.912.4">Previously, I mentioned</span><a id="_idIndexMarker722" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.913.1"> some</span><a id="_idIndexMarker723" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.914.1"> text with 4,086 rows and over 18,000 columns. </span><span class="kobospan" id="kobo.914.2">That dataset is actually a set of </span><em class="italic"><span class="kobospan" id="kobo.915.1">Yelp</span></em> <span><span class="kobospan" id="kobo.916.1">online reviews:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.917.1">
url = '../data/yelp.csv'
yelp = pd.read_csv(url, encoding='unicode-escape')
# create a new DataFrame that only contains the 5-star and 1-star reviews yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]
# define X and y
X = yelp_best_worst.text
y = yelp_best_worst.stars == 5</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.918.1">Our goal is to predict whether or not a person gave a 5- or 1-star review based on the words they used in the review. </span><span class="kobospan" id="kobo.918.2">Let’s set a baseline with logistic regression and see how well we can predict this </span><span><span class="kobospan" id="kobo.919.1">binary category:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.920.1">
from sklearn.linear_model import LogisticRegression lr = LogisticRegression()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100) # Make our training and testing sets
vect = CountVectorizer(stop_words='english')
# Count the number of words but remove stop words like a, an, the, you, etc
X_train_dtm = vect.fit_transform(X_train) X_test_dtm = vect.transform(X_test)
# transform our text into document term matrices
lr.fit(X_train_dtm, y_train) # fit to our training set
lr.score(X_test_dtm, y_test) # score on our testing set</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.921.1">The output is </span><span><span class="kobospan" id="kobo.922.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.923.1">
0.91193737</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.924.1">So, by utilizing all of the words in our corpus, our model seems to have over a 91% accuracy. </span><span><span class="kobospan" id="kobo.925.1">Not bad!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.926.1">Let’s try only using the top 100 </span><span><span class="kobospan" id="kobo.927.1">used words:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.928.1">
vect = CountVectorizer(stop_words='english', max_features=100) # Only use the 100 most used words
X_train_dtm = vect.fit_transform(X_train) X_test_dtm = vect.transform(X_test) print( X_test_dtm.shape) # (1022, 100)
lr.fit(X_train_dtm, y_train) lr.score(X_test_dtm, y_test)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.929.1">The output is </span><span><span class="kobospan" id="kobo.930.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.931.1">
0.8816</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.932.1">Note how our training</span><a id="_idIndexMarker724" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.933.1"> and </span><a id="_idIndexMarker725" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.934.1">testing matrices have 100 columns. </span><span class="kobospan" id="kobo.934.2">This is because I told our vectorizer to only look at the top 100 words. </span><span class="kobospan" id="kobo.934.3">See also that our performance took a hit and is now down to 88% accuracy. </span><span class="kobospan" id="kobo.934.4">This makes sense because we are ignoring over 4,700 words in </span><span><span class="kobospan" id="kobo.935.1">our corpus.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.936.1">Now, let’s take a different approach. </span><span class="kobospan" id="kobo.936.2">Let’s import a PCA module and tell it to make us 100 new super columns and see how </span><span><span class="kobospan" id="kobo.937.1">that performs:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.938.1">
from sklearn import decomposition
# We will be creating 100 super columns
vect = CountVectorizer(stop_words='english') # Don't ignore any words
pca = decomposition.PCA(n_components=100) # instantate a pca object
X_train_dtm = vect.fit_transform(X_train).todense()
# A dense matrix is required to pass into PCA, does not affect the overall message
X_train_dtm = pca.fit_transform(X_train_dtm)
X_test_dtm = vect.transform(X_test).todense() X_test_dtm = pca.transform(X_test_dtm)
print( X_test_dtm.shape) # (1022, 100) lr.fit(X_train_dtm, y_train) lr.score(X_test_dtm, y_test)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.939.1">The output is </span><span><span class="kobospan" id="kobo.940.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.941.1">
.89628</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.942.1">Not only do our matrices still have 100 columns, but these columns are no longer words in our corpus. </span><span class="kobospan" id="kobo.942.2">They are complex transformations of columns and are 100 new columns. </span><span class="kobospan" id="kobo.942.3">Also, note that using 100 of these new columns gives us a better predictive performance than using the 100 </span><span><span class="kobospan" id="kobo.943.1">top words!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.944.1">Feature extraction is a great way to use mathematical formulas to extract brand-new columns that generally perform better than just selecting the best </span><span><span class="kobospan" id="kobo.945.1">ones beforehand.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.946.1">But how do we visualize these new super columns? </span><span class="kobospan" id="kobo.946.2">Well, I can think of no better way than to look at an example using</span><a id="_idIndexMarker726" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.947.1"> image </span><a id="_idIndexMarker727" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.948.1">analysis. </span><span class="kobospan" id="kobo.948.2">Specifically, let’s make facial recognition software. </span><span class="kobospan" id="kobo.948.3">OK? </span><span class="kobospan" id="kobo.948.4">OK. </span><span class="kobospan" id="kobo.948.5">Let’s begin by importing some faces given to us </span><span><span class="kobospan" id="kobo.949.1">by </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.950.1">scikit-learn</span></strong></span><span><span class="kobospan" id="kobo.951.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.952.1">
from sklearn.datasets import fetch_lfw_people
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
# introspect the images arrays to find the shapes (for plotting) n_samples, h, w = lfw_people.images.shape
# for machine learning we use the 2 data directly (as relative pixel # positions info is ignored by this model)
X = lfw_people.data
y = lfw_people.target n_features = X.shape[1] X.shape (1288, 1850)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.953.1">We have gathered 1,288 images of people’s faces, and each one has 1,850 features (pixels) that identify that person. </span><span class="kobospan" id="kobo.953.2">Here’s the code we used – an example of one of our faces can be seen in </span><span><em class="italic"><span class="kobospan" id="kobo.954.1">Figure 11</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.955.1">.22</span></em></span><span><span class="kobospan" id="kobo.956.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.957.1">
plt.imshow(X[100].reshape((h, w)), cmap=plt.cm.gray) lfw_people.target_names[y[100]] 'George W Bush'</span></pre>
<div class="calibre2">
<div class="img---figure" id="_idContainer299">
<span class="kobospan" id="kobo.958.1"><img alt="Figure 11.22 – A face from our dataset: George W. Bush" src="image/B19488_11_22.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.959.1">Figure 11.22 – A face from our dataset: George W. </span><span class="kobospan" id="kobo.959.2">Bush</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.960.1">Great! </span><span class="kobospan" id="kobo.960.2">To get a glimpse</span><a id="_idIndexMarker728" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.961.1"> at </span><a id="_idIndexMarker729" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.962.1">the type of dataset we are looking at, let’s look at a few </span><span><span class="kobospan" id="kobo.963.1">overall metrics:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.964.1">
# the label to predict is the id of the person target_names = lfw_people.target_names n_classes = target_names.shape[0]
print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes) Total dataset size:
---
n_samples: 1288
n_features: 1850
n_classes: 7</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.965.1">So, we have 1,288 images, 1,850 features, and 7 classes (people) to choose from. </span><span class="kobospan" id="kobo.965.2">Our goal is to make a classifier that will assign the person’s face a name based on the 1,850 pixels given </span><span><span class="kobospan" id="kobo.966.1">to us.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.967.1">Let’s take a baseline and see how a logistic regression (a classifier that is based on linear regression) performs on our data without doing anything to </span><span><span class="kobospan" id="kobo.968.1">our dataset.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.969.1">I know we haven’t formally introduced logistic regressions before, but they are a very lightweight classifier that works off of very similar assumptions as linear regressions from the last chapter. </span><span class="kobospan" id="kobo.969.2">All </span><a id="_idIndexMarker730" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.970.1">we </span><a id="_idIndexMarker731" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.971.1">need to know for now is that it </span><span><span class="kobospan" id="kobo.972.1">performs classification!</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.973.1">
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from time import time # for timing our work
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=1)
# get our training and test set
t0 = time()
# get the time now
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
# Predicting people's names on the test set
y_pred = logreg.predict(X_test)
print( accuracy_score(y_pred, y_test), "Accuracy") print( (time() - t0), "seconds" )</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.974.1">The output is </span><span><span class="kobospan" id="kobo.975.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.976.1">
0.810559006211 Accuracy
6.31762504578 seconds</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.977.1">So, within 6.3 seconds, we were able to get 81% on our test set. </span><span class="kobospan" id="kobo.977.2">Not </span><span><span class="kobospan" id="kobo.978.1">too bad.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.979.1">Now, let’s try this with our </span><span><span class="kobospan" id="kobo.980.1">decomposed faces:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.981.1">
# split into a training and testing set
from sklearn.cross_validation import train_test_split
# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled # dataset): unsupervised feature extraction / dimensionality reduction
n_components = 75
print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))
pca = decomposition.PCA(n_components=n_components, whiten=True).fit(X_train)
# This whiten parameter speeds up the computation of our extracted columns
# Projecting the input data on the eigenfaces orthonormal basis
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.982.1">The preceding code is collecting </span><strong class="source-inline"><span class="kobospan" id="kobo.983.1">75</span></strong><span class="kobospan" id="kobo.984.1"> extracted columns from our 1,850 unprocessed columns. </span><span class="kobospan" id="kobo.984.2">These are </span><a id="_idIndexMarker732" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.985.1">our</span><a id="_idIndexMarker733" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.986.1"> super faces. </span><span class="kobospan" id="kobo.986.2">Now, let’s plug in our newly extracted columns into our logistic regression </span><span><span class="kobospan" id="kobo.987.1">and compare:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.988.1">
t0 = time()
# Predicting people's names on the test set WITH PCA logreg.fit(X_train_pca, y_train)
y_pred = logreg.predict(X_test_pca)
print accuracy_score(y_pred, y_test), "Accuracy" print (time() - t0), "seconds"
0.82298136646 Accuracy
0.194181919098 seconds</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.989.1">Wow! </span><span class="kobospan" id="kobo.989.2">Not only was this entire calculation about 30 times faster than the unprocessed images, but the predictive performance also got better! </span><span class="kobospan" id="kobo.989.3">This shows us that PCA and feature extraction, in general, can help us all around when performing ML on complex datasets with many columns. </span><span class="kobospan" id="kobo.989.4">By searching for these patterns in the dataset and extracting new feature columns, we can speed up and enhance our </span><span><span class="kobospan" id="kobo.990.1">learning algorithms.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.991.1">Let’s look at one more interesting thing. </span><span class="kobospan" id="kobo.991.2">I mentioned before that one of the purposes of this example was to examine and visualize our eigenfaces, as they are called: our super columns. </span><span class="kobospan" id="kobo.991.3">I will </span><a id="_idIndexMarker734" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.992.1">not </span><a id="_idIndexMarker735" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.993.1">disappoint. </span><span class="kobospan" id="kobo.993.2">Let’s write some code that will show us our super columns as they would look to </span><span><span class="kobospan" id="kobo.994.1">us humans:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.995.1">
def plot_gallery(images, titles, n_row=3, n_col=4): """Helper function to plot a gallery of portraits""" plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
for i in range(n_row * n_col):
plt.subplot(n_row, n_col, i + 1) plt.imshow(images[i], cmap=plt.cm.gray) plt.title(titles[i], size=12)
# plot the gallery of the most significative eigenfaces eigenfaces = pca.components_.reshape((n_components, h, w))
eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles)
plt.show()</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.996.1">Warning: the</span><a id="_idIndexMarker736" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.997.1"> faces in </span><span><em class="italic"><span class="kobospan" id="kobo.998.1">Figure 11</span></em></span><em class="italic"><span class="kobospan" id="kobo.999.1">.23</span></em><span class="kobospan" id="kobo.1000.1"> are a </span><a id="_idIndexMarker737" class="pcalibre calibre4 pcalibre1"/><span><span class="kobospan" id="kobo.1001.1">bit creepy!</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer300">
<span class="kobospan" id="kobo.1002.1"><img alt="Figure 11.23 – Performing PCA on the pixels of our faces creates what is known as “eigenfaces” that represent features that our classifiers look for when trying to recognize faces" src="image/B19488_11_23.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1003.1">Figure 11.23 – Performing PCA on the pixels of our faces creates what is known as “eigenfaces” that represent features that our classifiers look for when trying to recognize faces</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.1004.1">Wow! </span><span class="kobospan" id="kobo.1004.2">A haunting and yet beautiful representation of what the data believes to be the most important features of a face. </span><span class="kobospan" id="kobo.1004.3">As we move from the top left (first super column) to the bottom, it is actually somewhat easy to see what the image is trying to tell us. </span><span class="kobospan" id="kobo.1004.4">The first super column looks like a very general face structure with eyes and nose and a mouth. </span><span class="kobospan" id="kobo.1004.5">It is almost saying “I represent the basic qualities of a face that all faces must have.” </span><span class="kobospan" id="kobo.1004.6">Our second super column directly to its right seems to be telling us about shadows in the image. </span><span class="kobospan" id="kobo.1004.7">The next one might be telling us that skin tone plays a role in detecting who this is, which might be why the third face is much darker than the </span><span><span class="kobospan" id="kobo.1005.1">first two.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.1006.1">Using feature extraction UL methods such as PCA can give us a very deep look into our data and reveal to us what the data believes to be the most important features, not just what we believe them to be. </span><span class="kobospan" id="kobo.1006.2">Feature extraction is a great preprocessing tool that can speed up our future learning methods, make them more powerful, and give us more insight into how the data believes it should be viewed. </span><span class="kobospan" id="kobo.1006.3">To sum up this section, we will list the pros </span><span><span class="kobospan" id="kobo.1007.1">and cons.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.1008.1">Here are some of the advantages of using </span><span><span class="kobospan" id="kobo.1009.1">feature extraction:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.1010.1">Our models become </span><span><span class="kobospan" id="kobo.1011.1">much faster</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.1012.1">Our predictive performance can </span><span><span class="kobospan" id="kobo.1013.1">become better</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.1014.1">It can give us insight into the extracted </span><span><span class="kobospan" id="kobo.1015.1">features (eigenfaces)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.1016.1">And here are some of the disadvantages of using </span><span><span class="kobospan" id="kobo.1017.1">feature extraction:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.1018.1">We lose some of the interpretability of our features as they are new mathematically derived columns, not our </span><span><span class="kobospan" id="kobo.1019.1">old ones</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.1020.1">We can lose </span><a id="_idIndexMarker738" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1021.1">predictive</span><a id="_idIndexMarker739" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1022.1"> performance because we are losing in</span><a id="_idTextAnchor327" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1023.1">formation as we extract </span><span><span class="kobospan" id="kobo.1024.1">fewer columns</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.1025.1">Let’s move on to the </span><span><span class="kobospan" id="kobo.1026.1">summary next.</span></span></p>
<h1 id="_idParaDest-171" class="calibre6"><a id="_idTextAnchor328" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1027.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.1028.1">Our exploration into the world of ML has revealed a vast landscape that extends well beyond the foundational techniques of linear and logistic regression. </span><span class="kobospan" id="kobo.1028.2">We delved into decision trees, which provide intuitive insights into data through their hierarchical structure. </span><span class="kobospan" id="kobo.1028.3">Naïve Bayes classification offered us a probabilistic perspective, showing how to make predictions under the assumption of feature independence. </span><span class="kobospan" id="kobo.1028.4">We ventured into dimensionality reduction, encountering techniques such as feature extraction, which help overcome the COD and reduce </span><span><span class="kobospan" id="kobo.1029.1">computational complexity.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.1030.1">k-means clustering introduced us to the realm of UL, where we learned to find hidden patterns and groupings in data without pre-labeled outcomes. </span><span class="kobospan" id="kobo.1030.2">Across these methods, we’ve seen how ML can tackle a plethora of complex problems, from predicting categorical outcomes to uncovering latent structures </span><span><span class="kobospan" id="kobo.1031.1">in data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.1032.1">Through practical examples, we’ve compared and contrasted SL, which relies on labeled data, with UL, which operates without explicit guidance on the output. </span><span class="kobospan" id="kobo.1032.2">This journey has equipped us with a deeper understanding of the various techniques and their appropriate applications within the broad and dynamic field of </span><span><span class="kobospan" id="kobo.1033.1">data science.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.1034.1">As we continue to harness the power of these algorithms, we are reminded of the importance of selecting the right model for the right task—a principle that remains central to the practice of effective </span><span><span class="kobospan" id="kobo.1035.1">data science.</span></span></p>
</div>
</body></html>