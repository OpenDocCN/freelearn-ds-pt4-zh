["```py\nimport pandas as pd import sklearn\ndf = pd.read_table('..data/sms.tsv',\nsep='\\t', header=None, names=['label', 'msg'])\ndf\n```", "```py\ndf.label.value_counts().plot(kind=\"bar\")\n```", "```py\ndf.label.value_counts() / df.shape[0]\nham0.865937\nspam 0.134063\n```", "```py\ndf.msg = df.msg.apply(lambda x:x.lower())\n# make all strings lower case so we can search easier\ndf[df.msg.str.contains('send cash now')].shape # == (0, 2)\n```", "```py\nspams = df[df.label == 'spam']\nfor word in ['send', 'cash', 'now']:\nprint( word, spams[spams.msg.str.contains(word)].shape[0] / float(spams.shape[0]))\n```", "```py\n# simple count vectorizer example\nfrom sklearn.feature_extraction.text import CountVectorizer # start with a simple example\ntrain_simple = ['call you tonight',\n'Call me a cab',\n'please call me... PLEASE 44!']\n# learn the 'vocabulary' of the training data vect = CountVectorizer()\ntrain_simple_dtm = vect.fit_transform(train_simple) pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())\n```", "```py\n# transform testing data into a document-term matrix (using existing vocabulary, notice don't is missing)\ntest_simple = [\"please don't call me\"] test_simple_dtm = vect.transform(test_simple) test_simple_dtm.toarray()\npd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())\n```", "```py\n# split into training and testing sets\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.msg, df.label, random_state=1)\n# instantiate the vectorizer vect = CountVectorizer()\n# learn vocabulary and create document-term matrix in a single step train_dtm = vect.fit_transform(X_train)\ntrain_dtm\n```", "```py\n<4179x7456 sparse matrix of type '<class 'numpy.int64'>' with 55209 stored elements in Compressed Sparse Row format>\n```", "```py\n# transform testing data into a document-term matrix test_dtm = vect.transform(X_test)\ntest_dtm\n```", "```py\n<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\nwith 17604 stored elements in Compressed Sparse Row format>\n```", "```py\n## MODEL BUILDING WITH NAIVE BAYES\n# train a Naive Bayes model using train_dtm from sklearn.naive_bayes import MultinomialNB # import our model\nnb = MultinomialNB()\n# instantiate our model\nnb.fit(train_dtm, y_train)\n# fit it to our training set\n```", "```py\n# make predictions on test data using test_dtm preds = nb.predict(test_dtm)\npreds\n```", "```py\narray(['ham', 'ham', 'ham', ..., 'ham', 'spam', 'ham'], dtype='|S4')\n```", "```py\n# compare predictions to true labels from sklearn import metrics\nprint metrics.accuracy_score(y_test, preds) print metrics.confusion_matrix(y_test, preds)\n```", "```py\naccuracy == 0.988513998564\nconfusion matrix ==\n[[12035]\n[11174]]\n```", "```py\nnb.classes_\n```", "```py\narray(['ham', 'spam'])\n```", "```py\n# read in the data\ntitanic = pd.read_csv('short_titanic.csv')\n# encode female as 0 and male as 1\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n# fill in the missing values for age with the median age titanic.Age.fillna(titanic.Age.median(), inplace=True)\n# create a DataFrame of dummy variables for Embarked\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked') embarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\n# concatenate the original DataFrame and the dummy DataFrame titanic = pd.concat([titanic, embarked_dummies], axis=1)\n# define X and y\nfeature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S'] X = titanic[feature_cols]\ny = titanic.Survived X.head()\n```", "```py\n# fit a classification tree with max_depth=3 on all data from sklearn.tree import DecisionTreeClassifier treeclf = DecisionTreeClassifier(max_depth=3, random_state=1) treeclf.fit(X, y)\n```", "```py\n# compute the feature importances pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})\n```", "```py\n# centroid calculation import numpy as np\nred_point1 = np.array([1, 3]) red_point2 = np.array([2, 5]) red_point3 = np.array([3, 4])\nred_center = (red_point1 + red_point2 + red_point3) / 3.\nred_center\n# array([ 2., 4.])\n```", "```py\n# import the beer dataset url = '../data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer.head()\n```", "```py\n# define X\nX = beer.drop('name', axis=1)\n```", "```py\n# K-means with 3 clusters\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=1) km.fit(X)\n```", "```py\n# save the cluster labels and sort by cluster beer['cluster'] = km.labels_\n```", "```py\n# calculate the mean of each feature for each cluster beer.groupby('cluster').mean()\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# save the DataFrame of cluster centers centers = beer.groupby('cluster').mean() # create a \"colors\" array for plotting\ncolors = np.array(['red', 'green', 'blue', 'yellow'])\n# scatter plot of calories versus alcohol, colored by cluster (0=red, 1=green, 2=blue)\nplt.scatter(beer.calories, beer.alcohol, c=colors[list(beer.cluster)], s=50)\n# cluster centers, marked by \"+\"\nplt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black')\n# add labels plt.xlabel('calories') plt.ylabel('alcohol')\n```", "```py\n# calculate Silhouette Coefficient for K=3 from sklearn import metrics metrics.silhouette_score(X, km.labels_)\n```", "```py\n0.67317750464557957\n```", "```py\n# center and scale the data\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n# calculate SC for K=2 through K=19 k_range = range(2, 20)\nscores = []\nfor k in k_range:\nkm = KMeans(n_clusters=k, random_state=1) km.fit(X_scaled) scores.append(metrics.silhouette_score(X, km.labels_))\n# plot the results plt.plot(k_range, scores) plt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Coefficient') plt.grid(True)\n```", "```py\n# center and scale the data\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n# K-means with 3 clusters on scaled data km = KMeans(n_clusters=3, random_state=1) km.fit(X_scaled)\n```", "```py\nd = 1\n# Let's look for points within 1 unit of one another\nX_first_word = X.iloc[:,:1]\n# Only looking at the first column, but ALL of the rows\nfrom sklearn.neighbors import NearestNeighbors\n# this module will calculate for us distances between each point\nneigh = NearestNeighbors(n_neighbors=4086)\nneigh.fit(X_first_word)\n# tell the module to calculate each distance between each point\nA = neigh.kneighbors_graph(X_first_word, mode='distance').todense() # This matrix holds all distances (over 16 million of them)\nnum_points_within_d = (A < d).sum()\n# Count the number of pairs of points within 1 unit of distance, 16,258,504\n```", "```py\nX_first_two_words = X.iloc[:,:2]\nneigh = NearestNeighbors(n_neighbors=4086) neigh.fit(X_first_two_words)\nA = neigh.kneighbors_graph(X_first_two_words, mode='distance').todense() num_points_within_d = (A < d).sum()\n# num_points_within_d is now 16,161,970\n```", "```py\nnum_columns = range(1, 100)\n# Looking at the first 100 columns points = []\n# We will be collecting the number of points within 1 unit for a graph\nneigh = NearestNeighbors(n_neighbors=X.shape[0])\nfor subset in num_columns:\nX_subset = X.iloc[:,:subset]\n# look at the first column, then first two columns, then first three columns, etc\nneigh.fit(X_subset)\nA = neigh.kneighbors_graph(X_subset, mode='distance').todense() num_points_within_d = (A < d).sum()\n# calculate the number of points within 1 unit points.append(num_points_within_d)\n```", "```py\nurl = '../data/yelp.csv'\nyelp = pd.read_csv(url, encoding='unicode-escape')\n# create a new DataFrame that only contains the 5-star and 1-star reviews yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n# define X and y\nX = yelp_best_worst.text\ny = yelp_best_worst.stars == 5\n```", "```py\nfrom sklearn.linear_model import LogisticRegression lr = LogisticRegression()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100) # Make our training and testing sets\nvect = CountVectorizer(stop_words='english')\n# Count the number of words but remove stop words like a, an, the, you, etc\nX_train_dtm = vect.fit_transform(X_train) X_test_dtm = vect.transform(X_test)\n# transform our text into document term matrices\nlr.fit(X_train_dtm, y_train) # fit to our training set\nlr.score(X_test_dtm, y_test) # score on our testing set\n```", "```py\n0.91193737\n```", "```py\nvect = CountVectorizer(stop_words='english', max_features=100) # Only use the 100 most used words\nX_train_dtm = vect.fit_transform(X_train) X_test_dtm = vect.transform(X_test) print( X_test_dtm.shape) # (1022, 100)\nlr.fit(X_train_dtm, y_train) lr.score(X_test_dtm, y_test)\n```", "```py\n0.8816\n```", "```py\nfrom sklearn import decomposition\n# We will be creating 100 super columns\nvect = CountVectorizer(stop_words='english') # Don't ignore any words\npca = decomposition.PCA(n_components=100) # instantate a pca object\nX_train_dtm = vect.fit_transform(X_train).todense()\n# A dense matrix is required to pass into PCA, does not affect the overall message\nX_train_dtm = pca.fit_transform(X_train_dtm)\nX_test_dtm = vect.transform(X_test).todense() X_test_dtm = pca.transform(X_test_dtm)\nprint( X_test_dtm.shape) # (1022, 100) lr.fit(X_train_dtm, y_train) lr.score(X_test_dtm, y_test)\n```", "```py\n.89628\n```", "```py\nfrom sklearn.datasets import fetch_lfw_people\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n# introspect the images arrays to find the shapes (for plotting) n_samples, h, w = lfw_people.images.shape\n# for machine learning we use the 2 data directly (as relative pixel # positions info is ignored by this model)\nX = lfw_people.data\ny = lfw_people.target n_features = X.shape[1] X.shape (1288, 1850)\n```", "```py\nplt.imshow(X[100].reshape((h, w)), cmap=plt.cm.gray) lfw_people.target_names[y[100]] 'George W Bush'\n```", "```py\n# the label to predict is the id of the person target_names = lfw_people.target_names n_classes = target_names.shape[0]\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)\nprint(\"n_features: %d\" % n_features)\nprint(\"n_classes: %d\" % n_classes) Total dataset size:\n---\nn_samples: 1288\nn_features: 1850\nn_classes: 7\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom time import time # for timing our work\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=1)\n# get our training and test set\nt0 = time()\n# get the time now\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n# Predicting people's names on the test set\ny_pred = logreg.predict(X_test)\nprint( accuracy_score(y_pred, y_test), \"Accuracy\") print( (time() - t0), \"seconds\" )\n```", "```py\n0.810559006211 Accuracy\n6.31762504578 seconds\n```", "```py\n# split into a training and testing set\nfrom sklearn.cross_validation import train_test_split\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled # dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 75\nprint(\"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0]))\npca = decomposition.PCA(n_components=n_components, whiten=True).fit(X_train)\n# This whiten parameter speeds up the computation of our extracted columns\n# Projecting the input data on the eigenfaces orthonormal basis\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n```", "```py\nt0 = time()\n# Predicting people's names on the test set WITH PCA logreg.fit(X_train_pca, y_train)\ny_pred = logreg.predict(X_test_pca)\nprint accuracy_score(y_pred, y_test), \"Accuracy\" print (time() - t0), \"seconds\"\n0.82298136646 Accuracy\n0.194181919098 seconds\n```", "```py\ndef plot_gallery(images, titles, n_row=3, n_col=4): \"\"\"Helper function to plot a gallery of portraits\"\"\" plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\nplt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\nfor i in range(n_row * n_col):\nplt.subplot(n_row, n_col, i + 1) plt.imshow(images[i], cmap=plt.cm.gray) plt.title(titles[i], size=12)\n# plot the gallery of the most significative eigenfaces eigenfaces = pca.components_.reshape((n_components, h, w))\neigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles)\nplt.show()\n```"]