- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictions Don’t Grow on Trees, or Do They?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal in this chapter is to see and apply concepts learned from previous
    chapters in order to construct and use modern learning algorithms to glean insights
    and make predictions on real datasets. While we explore the following algorithms,
    we should always remember that we are constantly keeping our metrics in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be looking at the following ML algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing naïve Bayes classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diving deep into **unsupervised** **learning** (**UL**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction and **principal component** **analysis** (**PCA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing naïve Bayes classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s get right into it! Let’s begin with **naïve Bayes** classification. This
    ML model relies heavily on results from previous chapters, specifically with Bayes’
    theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>H</mi><mo>|</mo><mi>D</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>D</mi><mo>|</mo><mi>H</mi></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mi>H</mi></mfenced></mrow><mrow><mi>P</mi><mfenced open="("
    close=")"><mi>D</mi></mfenced></mrow></mfrac></mrow></mrow></math>](img/164.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s look a little closer at the specific features of this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(H)* is the probability of the hypothesis before we observe the data, called
    the *prior probability*, or just *prior*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(H|D)* is what we want to compute: the probability of the hypothesis after
    we observe the data, called the *posterior*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(D|H)* is the probability of the data under the given hypothesis, called
    the *likelihood*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(D)* is the probability of the data under any hypothesis, called the *normalizing
    constant*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes classification is a classification model, and therefore a supervised
    model. Given this, what kind of data do we need – labeled or unlabeled data?
  prefs: []
  type: TYPE_NORMAL
- en: (Insert *Jeopardy* music here)
  prefs: []
  type: TYPE_NORMAL
- en: If you answered *labeled data*, then you’re well on your way to becoming a data
    scientist!
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a dataset with *n features*, *(x1, x2, …, xn)*, and a *class
    label*, *C*. For example, let’s take some data involving spam text classification.
    Our data would consist of rows of individual text samples and columns of both
    our features and our class labels. Our features would be words and phrases that
    are contained within the text samples, and our class labels are simply *spam*
    or *not spam*. In this scenario, I will replace the not-spam class with an easier-to-say
    word, ham. Let’s take a look at the following code snippet to better understand
    our spam and ham data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11**.1* is a sample of text data in a row-column format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – A sample of our spam versus not spam (ham) messages](img/B19488_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – A sample of our spam versus not spam (ham) messages
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do some preliminary statistics to see what we are dealing with. Let’s
    see the difference in the number of ham and spam messages at our disposal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a bar chart, as shown in *Figure 11**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – The distribution of ham versus spam](img/B19488_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – The distribution of ham versus spam
  prefs: []
  type: TYPE_NORMAL
- en: Because we are dealing with classification, it would help to itemize some of
    the metrics we will be using to evaluate our model.
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When evaluating classification models, different metrics are used compared
    to regression models. These metrics help to understand how well the model is performing,
    especially in terms of correctly predicting different classes. Let’s look at what
    they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This is the most intuitive performance measure, and it is simply
    a ratio of correctly predicted observations to the total observations. It’s suitable
    for binary and multiclass classification problems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Accuracy</mtext><mo>=</mo><mfrac><mrow><mtext>Number</mtext><mtext>of</mtext><mtext>correct</mtext><mtext>predictions</mtext></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/165.png)'
  prefs: []
  type: TYPE_IMG
- en: '2. **Precision (best for binary classification – with only two classes)**:
    Also known as positive predictive value, this metric helps to answer the question:
    “What proportion of positive identifications was actually correct?”'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Precision</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mrow></math>](img/166.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *TP* is the number of true positives (predicted positive and the prediction
    was correct), and *FP* is the number of false positives (predicted positive but
    the prediction was incorrect).
  prefs: []
  type: TYPE_NORMAL
- en: '3. **Recall (Sensitivity) (best for binary classification – with only two classes)**:
    This metric helps to answer the question: “What proportion of actual positives
    was identified correctly?”'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Recall</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mrow></math>](img/167.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *TP* (predicted positive and the prediction was correct) is the number
    of true positives, and *FN* is the number of false negatives (predicted negative
    but the prediction was incorrect).
  prefs: []
  type: TYPE_NORMAL
- en: '4. **F1 Score**: The *F1* Score is the weighted average of precision and recall.
    Therefore, this score takes both false positives and false negatives into account.
    It is a good way to show that a classifier has a good value for both precision
    and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mrow></mrow></math>](img/168.png)'
  prefs: []
  type: TYPE_IMG
- en: These metrics are crucial for understanding the behavior of classification models,
    especially in domains where the costs of false positives and false negatives are
    very different.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have way more *ham* messages than we do *spam*. Because this is a classification
    problem, it would be very useful to know our `ham`. Here’s how we do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So, if we blindly guessed *ham*, we would be correct about 87% of the time,
    but we can do better than that.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a set of classes, C, and features, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/169.png),
    then we can use Bayes’ theorem to predict the probability that a single row belongs
    to class C, using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>class</mtext><mi>C</mi><mo>|</mo><mrow><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mrow></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mrow><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mrow><mo>|</mo><mtext>class</mtext><mi>C</mi></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>class</mtext><mi>C</mi></mrow></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mo>{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>}</mo></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/170.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s look at this formula in a little more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(class C | {* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/171.png)*
    *})*: The posterior probability is the probability that the row belongs to class
    C given the features *{xi}*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P({* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/172.png)*
    *} | class C)*: This is the likelihood that we would observe these features given
    that the row was in class *C*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(class C)*: This is the prior probability. It is the probability that the
    data point belongs to class *C* before we see any data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P({* *![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/173.png)*
    *})*: This is our normalization constant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, imagine we have an email with three words: *send cash now*. We’ll
    use naïve Bayes to classify the email as either being spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>spam</mtext><mspace width="0.125em" /><mo>|</mo><mspace
    width="0.125em" /><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/174.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>ham</mtext><mspace width="0.125em" /><mo>|</mo><mspace
    width="0.125em" /><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>ham</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>ham</mtext></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/175.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are concerned with the difference of these two numbers. We can use the following
    criteria to classify any single text sample:'
  prefs: []
  type: TYPE_NORMAL
- en: If *P(spam | send cash now)* is larger than *P(ham | send cash now)*, then we
    will classify the text as spam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *P(ham | send cash now)* is larger than *P(spam | send cash now)*, then we
    will label the text ham
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because both equations have *P (send money now)* in the denominator, we can
    ignore them. So, now we are concerned with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced><mspace width="0.125em" /><mtext>VS</mtext><mspace
    width="0.125em" /><mi>P</mi><mfenced open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mspace
    width="0.125em" /><mo>|</mo><mspace width="0.125em" /><mtext>ham</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>ham</mtext></mfenced></mrow></mrow></math>](img/176.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s work out the numbers in this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: P(spam) = 0.134063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(ham) = 0.865937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(send cash now | spam) = ???
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(send cash now | ham) = ???
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final two likelihoods might seem like they would not be so difficult to
    calculate. All we have to do is count the number of spam messages that include
    the send money, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, phrase and divide that by the total number of spam messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Oh no! There are none! There are literally zero texts with the exact phrase
    *send cash now*. The hidden problem here is that this phrase is very specific,
    and we can’t assume that we will have enough data in the world to have seen this
    exact phrase many times before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we can make a naïve assumption in our Bayes’ theorem. If we assume
    that the features (words) are conditionally independent (meaning that no word
    affects the existence of another word), then we can rewrite the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>=</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>cash</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced></mrow></mrow></math>](img/177.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And here’s what it looks like done in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing out the conditional probabilities yields:'
  prefs: []
  type: TYPE_NORMAL
- en: P(send|spam) = 0.096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(cash|spam) = 0.091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(now|spam) = 0.280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this, we can calculate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>send</mtext><mtext>cash</mtext><mtext>now</mtext><mo>|</mo><mtext>spam</mtext></mrow></mfenced><mo>⋅</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>spam</mtext></mfenced><mo>=</mo><mfenced open="(" close=")"><mrow><mn>0.096</mn><mo>⋅</mo><mn>0.091</mn><mo>⋅</mo><mn>0.280</mn></mrow></mfenced><mo>⋅</mo><mn>0.134</mn><mo>=</mo><mn>0.00032</mn></mrow></mrow></math>](img/178.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeating the same procedure for ham gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: P(send|ham) = 0.03
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(cash|ham) = 0.003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(now|ham) = 0.109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fact that these numbers are both very low is not as important as the fact
    that the spam probability is much larger than the ham calculation. If we do the
    calculations, we get that the *send cash now* probability for spam is 38 times
    bigger than for spam! Doing this means that we can classify *send cash now* as
    spam! Simple, right?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use Python to implement a naïve Bayes classifier without having to do
    all of these calculations ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s revisit the count vectorizer in scikit-learn, which turns text
    into numerical data for us. Let’s assume that we will train on three documents
    (sentences), in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11**.3* demonstrates the feature vectors learned from our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The first five rows of our SMS dataset after breaking up each
    text into a count of unique words](img/B19488_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – The first five rows of our SMS dataset after breaking up each
    text into a count of unique words
  prefs: []
  type: TYPE_NORMAL
- en: Note that each row represents one of the three documents (sentences), each column
    represents one of the words present in the documents, and each cell contains the
    number of times each word appears in each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then use the count vectorizer to transform new incoming test documents
    to conform with our training set (the three sentences):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11**.4* is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Representation of the “please don’t call me” SMS in the same
    vocabulary as our training data](img/B19488_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Representation of the “please don’t call me” SMS in the same vocabulary
    as our training data
  prefs: []
  type: TYPE_NORMAL
- en: Note how, in our test sentence, we had a new word – namely, *don’t*. When we
    vectorized it, because we hadn’t seen that word previously in our training data,
    the vectorizer simply ignored it. This is important and incentivizes data scientists
    to obtain as much data as possible for their training sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s do this for our actual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that the format is in a sparse matrix, meaning the matrix is large and
    full of zeros. There is a special format to deal with objects such as this. Take
    a look at the number of columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 7,456 words. That’s a lot! This means that in our training set, there
    are 7,456 unique words to look at. We can now transform our test data to conform
    to our vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have the same exact number of columns because it is conforming
    to our test set to be exactly the same vocabulary as before. No more, no less.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s build a naïve Bayes model (similar to the linear regression process):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `nb` variable holds our fitted model. The training phase of the model
    involves computing the likelihood function, which is the conditional probability
    of each feature given each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The prediction phase of the model involves computing the posterior probability
    of each class given the observed features and choosing the class with the highest
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `sklearn`’s built-in accuracy and confusion matrix to look at how
    well our naïve Bayes models are performing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: First off, our accuracy is great! Compared to our null accuracy, which was 87%,
    99% is a fantastic improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Now to our confusion matrix. From before, we know that each row represents actual
    values while columns represent predicted values, so the top-left value, 1,203,
    represents our true negatives. But what is negative and positive? We gave the
    model the spam and ham strings as our classes, not positive and negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can then line up the indices so that 1,203 refers to true ham predictions
    and 174 refers to true spam predictions. There were also five false spam classifications,
    meaning that five messages were predicted as spam but were actually ham, as well
    as 11 false ham classifications. In summary, naïve Bayes classification uses Bayes’
    theorem in order to fit posterior probabilities of classes so that data points
    are correctly labeled as belonging to the proper class.
  prefs: []
  type: TYPE_NORMAL
- en: Every ML model has its own set of unique properties and advantages or disadvantages
    for use with different types of data. The naïve Bayes classifier, for example,
    is known for its speed and efficiency. It is particularly fast when fitting to
    training data and when making predictions on test data. This is due to its assumption
    of feature independence, which simplifies the calculations involved in probability
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: However, this same assumption can also be seen as a limitation. In reality,
    features often do exhibit some level of dependency, and the naïve Bayes classifier
    may oversimplify complex relationships in data. Moreover, it is based on the assumption
    that the form of the data distribution (often assumed to be Gaussian) holds true,
    which might not always be the case in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, naïve Bayes can perform exceptionally well with appropriate
    data and is particularly useful for text classification tasks, such as spam filtering
    and **sentiment** **analysis** (**SA**).
  prefs: []
  type: TYPE_NORMAL
- en: Another widely used ML technique is the decision tree. Decision trees are a
    **supervised learning** (**SL**) method used for classification and regression.
    They are intuitive and easy to interpret since they mimic human decision-making
    more closely than other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Decision trees** are supervised models that can either perform regression
    or classification. They are a flowchart-like structure in which each internal
    node represents a test on an attribute, each branch represents the outcome of
    the test, and each leaf node represents a class label (for classification) or
    a value (for regression). One of the primary advantages of decision trees is their
    simplicity; they do not require any complex mathematical formulations, making
    them easier to understand and visualize.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a decision tree is to split the data in a manner that maximizes
    the purity of the nodes resulting from those splits. In the context of a classification
    problem, “purity” refers to how homogeneous the nodes are with respect to the
    target variable. A perfectly pure node would contain instances of only a single
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees achieve this by using measures of impurity, such as the Gini
    index or entropy (more on that soon), to evaluate potential splits. A good split
    is one that most effectively separates the data into nodes with high purity, meaning
    that it increases the homogeneity of the nodes with respect to the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best attribute to split the data based on a specific criterion
    (such as Gini or entropy).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partitioning the dataset into subsets that contain instances with similar values
    for that attribute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeating this process recursively for each derived subset until the stopping
    criteria are met (which could be a maximum depth of the tree, a minimum number
    of instances in a node, or the achievement of a node with high purity).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This recursive partitioning makes decision trees a powerful and interpretable
    modeling technique for classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring purity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Gini index** is a measure of inequality among values of a frequency distribution
    (for example, levels of income). In the context of ML and decision trees, it measures
    the impurity of a node with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>](img/179.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *D* is the dataset, *J* is the number of classes, and *pi* is the probability
    of class *i* in the dataset *D*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy, on the other hand, is a measure from information theory that quantifies
    the amount of uncertainty or randomness in the data. It’s used in the construction
    of decision trees to represent the impurity of a dataset with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Entropy</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/180.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *pi* is the probability of class *i* within the dataset *D*.
  prefs: []
  type: TYPE_NORMAL
- en: Both the Gini index and entropy are used to choose where to split the data when
    building a decision tree. The choice between using the Gini index and entropy
    often depends on the specific dataset and the preferences of the modeler, as they
    can lead to slightly different trees. In practice, the difference in the trees
    generated by these two methods is often very small.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Titanic dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Titanic* dataset is truly a classic in the field of data science, often
    used to illustrate the fundamentals of ML. It details the tragic sinking of the
    RMS Titanic, one of the most infamous shipwrecks in history. This dataset serves
    as a rich source of demographic and travel information about the passengers, which
    can be utilized to model and predict survival outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Through the lens of this dataset, we can apply statistical analysis and predictive
    modeling to understand factors that may have influenced the chances of survival.
    For instance, consider a subset of only 25 passengers from the *Titanic* dataset.
    Out of 25, 10 of these individuals survived the disaster, while 15 did not. By
    examining attributes such as age, gender, class, and fare paid, we can begin to
    construct a predictive model that estimates the likelihood of survival for each
    passenger in similar circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: We first calculate the *Gini index* before doing anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, overall classes are `survived` and `died`, illustrated in
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>survived</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>died</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mn>25</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.48</mml:mn></mml:math>](img/181.png)'
  prefs: []
  type: TYPE_IMG
- en: This Gini index of 0.48 indicates the level of impurity in the dataset. The
    value suggests a moderate separation between the classes, with some degree of
    mixture between the *survived* and *died* categories within this group of passengers.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to make a split in the dataset based on a certain feature, we would
    calculate the Gini index for each resulting subset. The goal is to choose a split
    that minimizes the Gini index, thus increasing the purity of the subsets with
    respect to the target variable, which in this case is survival on the Titanic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider a potential split on gender. We first calculate the Gini
    index for each given gender, as seen in *Figure 11**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Calculating impurity of our dataset on gender](img/B19488_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Calculating impurity of our dataset on gender
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula calculates the Gini index for male and female, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>13</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn></mml:math>](img/182.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.32</mml:mn></mml:math>](img/183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have the Gini index for each gender, we then calculate the overall
    Gini index for the split on gender, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Gini</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>Gini</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.32</mml:mn><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.27</mml:mn></mml:math>](img/184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the *Gini coefficient* for splitting on *gender* is *0.27*. We then follow
    this procedure for three potential splits (shown in *Figure 11**.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: Gender (male or female)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of siblings on board (0 or 1+)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class (first and second versus third)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Calculating the resulting Gini coefficient for multiple splits
    on our dataset to decide which one to use for our decision tree](img/B19488_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Calculating the resulting Gini coefficient for multiple splits
    on our dataset to decide which one to use for our decision tree
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we would choose the gender to split on as it has the *lowest*
    *Gini index*!
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to some more code, we need to think about how to deal with categorical
    features that are not numerically encoded. ML algorithms require numerical inputs,
    and most datasets will have at least one feature that is not numerical.
  prefs: []
  type: TYPE_NORMAL
- en: Dummy variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dummy variables are used when we are hoping to convert a categorical feature
    into a quantitative one. Remember that we have two types of categorical features:
    nominal and ordinal. Ordinal features have natural order among them, while nominal
    data does not.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding qualitative (nominal) data using separate columns is called making
    dummy variables, and it works by turning each unique category of a nominal column
    into its own column that is either `true` or `false`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we had a column for someone’s college major and we wished to
    plug that information into linear or logistic regression, we couldn’t because
    they only take in numbers! So, for each row, we had new columns that represent
    the single nominal column. In this case, we have four unique majors: computer
    science, engineering, business, and literature. We end up with three new columns
    (we omit computer science as it is not necessary and can be inferred if all of
    the other three majors are 0). *Figure 11**.7* shows us an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Creating dummy variables for a single feature involves creating
    a new binary feature for each option except for one, which can be inferred by
    having all 0s in the rest of the features](img/B19488_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Creating dummy variables for a single feature involves creating
    a new binary feature for each option except for one, which can be inferred by
    having all 0s in the rest of the features
  prefs: []
  type: TYPE_NORMAL
- en: Note that the first row has a 0 in all of the columns, which means that this
    person did not major in engineering, did not major in business, and did not major
    in literature. The second person has a single 1 in the `Engineering` column as
    that is the major they studied.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to need to make some dummy variables using pandas as we use scikit-learn’s
    built-in decision tree function in order to build a decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11**.8* shows what our dataset looks like after our preceding code
    block. Note that we are going to use class, sex, age, and dummy variables for
    city embarked as our features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Our Titanic dataset after creating dummy variables for Embarked](img/B19488_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Our Titanic dataset after creating dummy variables for Embarked
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can fit our decision tree classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`max_depth` is a hyperparameter that limits the depth of our tree. It means
    that, for any data point, our tree is only able to ask up to three questions and
    create three splits. We can output our tree into a visual format, and we will
    obtain the result seen in *Figure 11**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – The decision tree produced with scikit-learn with the Gini
    coefficient calculated at each node](img/B19488_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – The decision tree produced with scikit-learn with the Gini coefficient
    calculated at each node
  prefs: []
  type: TYPE_NORMAL
- en: 'We can notice a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sex** is the first split, meaning that sex is the most important determining
    factor of whether or not a person survived the crash'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embarked_Q** was never used in any split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For either classification or regression trees, we can also do something very
    interesting with decision trees, which is that we can output a number that represents
    each feature’s importance in the prediction of our data points (shown in *Figure
    11**.10*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 11.10 – Features that contributed most to the change in the Gini coefficient
    displayed as percentages adding up to 1; it’s no coincidence that our highest
    value (Sex) is also our first split](img/B19488_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Features that contributed most to the change in the Gini coefficient
    displayed as percentages adding up to 1; it’s no coincidence that our highest
    value (Sex) is also our first split
  prefs: []
  type: TYPE_NORMAL
- en: The importance scores are an average Gini index difference for each variable,
    with higher values corresponding to higher importance to the prediction. We can
    use this information to select fewer features in the future. For example, both
    of the embarked variables are very low in comparison to the rest of the features,
    so we may be able to say that they are not important in our prediction of life
    or death.
  prefs: []
  type: TYPE_NORMAL
- en: As we transition from the structured realm of SL, where the outcomes are known
    and the model learns from labeled data, we venture into the domain of UL. Recall
    that UL algorithms uncover hidden patterns and intrinsic structures within data
    that isn’t explicitly labeled. In the upcoming section, we will explore how unsupervised
    techniques can discern underlying relationships in data and provide deeper insights
    without the guidance of a predefined outcome, and how they can complement the
    predictive models we’ve discussed so far.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deep into UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time to see some examples of UL, given that we’ve spent some time on *SL
    algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: When to use UL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many times when UL can be appropriate. Some very common examples
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no clear response variable. There is nothing that we are explicitly
    trying to predict or correlate to other variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To extract structure from data where no apparent structure or patterns exist
    (can be an SL problem).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an unsupervised concept called **feature extraction** is used. Feature
    extraction is the process of creating new features from existing ones. These new
    features can be even stronger than the original features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first tends to be the most common reason that data scientists choose to
    use UL. This case arises frequently when we are working with data and we are not
    explicitly trying to predict any of the columns, and we merely wish to find patterns
    of similar (and dissimilar) groups of points. The second option comes into play
    even if we are explicitly attempting to use a supervised model to predict a response
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, simple **exploratory data analysis** (**EDA**) might not produce
    any clear patterns in the data in the few dimensions that humans can imagine,
    whereas a machine might pick up on data points behaving similarly to each other
    in greater dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The third common reason to use UL is to extract new features from features that
    already exist. This process (lovingly called feature extraction) might produce
    features that can be used in a future supervised model or that can be used for
    presentation purposes (marketing or otherwise).
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**k-means clustering** is our first example of an **unsupervised ML** (**UML**)
    model. Remember – this means that we are not making predictions. We are trying
    instead to extract structure from seemingly unstructured data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering** is a family of UML models that attempt to group data points
    into clusters with centroids. The concept of **similarity** is central to the
    definition of a cluster, and therefore to cluster analysis. In general, greater
    similarity between points leads to better clustering. In most cases, we turn data
    into points in *n*-dimensional space and use the distance between these points
    as a form of similarity. The **centroid** of the cluster is then usually the average
    of each dimension (column) for each data point in each cluster. So, for example,
    the centroid of the red cluster is the result of taking the average value of each
    column of each red data point.'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of cluster analysis is to enhance our understanding of a dataset
    by dividing the data into groups. Clustering provides a layer of abstraction from
    individual data points. The goal is to extract and enhance the natural structure
    of the data. There are many kinds of classification procedures. For our class,
    we will be focusing on k-means clustering, which is one of the most popular clustering
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**k-means** is an iterative method that partitions a dataset into *k* clusters.
    It works in four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose *k* initial centroids (note that *k* is an input).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each point, assign the point to the nearest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recalculate the centroid positions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* and *3* until the stopping criteria are met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An illustrative example of clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine that we have data points in a two-dimensional space, as seen in *Figure
    11**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – A mock dataset to be clustered](img/B19488_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – A mock dataset to be clustered
  prefs: []
  type: TYPE_NORMAL
- en: 'Each dot is colored gray to assume no prior grouping before applying the k-means
    algorithm. The goal here is to eventually color in each dot and create groupings
    (clusters), as illustrated in *Figure 11**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Step 1: k-means clustering begins by placing random centroids](img/B19488_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12 – Step 1: k-means clustering begins by placing random centroids'
  prefs: []
  type: TYPE_NORMAL
- en: We have (randomly) chosen *three centroids* (*red*, *blue*, and *yellow*).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Most k-means algorithms place random initial centroids, but there exist other
    pre-computed methods to place initial centroids. For now, random is fine.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2* has been applied in *Figure 11**.13*. For each data point, we found
    the most similar centroid (closest):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Step 2: For each point, assign the point to the nearest centroid](img/B19488_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.13 – Step 2: For each point, assign the point to the nearest centroid'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then apply *step 3* in *Figure 11**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Step 3: Recalculate the centroid positions](img/B19488_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.14 – Step 3: Recalculate the centroid positions'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is *step 3* and the crux of k-means. Note that we have physically moved
    the centroids to be the actual center of each cluster. We have, for each color,
    computed the average point and made that point the new centroid. For example,
    suppose the three red data points had the following coordinates: *(1, 3)*, *(2,
    5)*, and *(3, 4)*. The *center (red cross)* would be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: That is, the *(2, 4)* point would be the coordinates of the preceding red cross.
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue with our algorithm by repeating *step 2*. Here is the first part
    where we find the closest center for each point. Note a big change – the point
    in the bottom left used to be a yellow point but has changed to be a red cluster
    point because the yellow cluster moved closer to its yellow constituents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Repeating step 2; note the data point on the lower left was
    yellow in the previous step and is now red](img/B19488_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Repeating step 2; note the data point on the lower left was yellow
    in the previous step and is now red
  prefs: []
  type: TYPE_NORMAL
- en: 'If we follow *step 3* again, we get the result shown in *Figure 11**.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Step 3 again](img/B19488_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Step 3 again
  prefs: []
  type: TYPE_NORMAL
- en: Here, we recalculate once more the centroids for each cluster (*step 3*). Note
    that the blue center did not move at all, while the yellow and red centers both
    moved.
  prefs: []
  type: TYPE_NORMAL
- en: Because we have reached a **stopping criterion** (clusters do not move if we
    repeat *steps 2* and *3*), we finalize our algorithm and we have our three clusters,
    which is the final result of the k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: An illustrative example – beer!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s run a cluster analysis on a new dataset outlining different beers with
    different characteristics. We know that there are many types of beer, but I wonder
    if we could possibly group beers into different categories based on different
    quantitative features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try! Let’s import a dataset of just a few types of beer and visualize
    a few rows in *Figure 11**.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 11.17 – The first five rows of our beer dataset](img/B19488_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – The first five rows of our beer dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset has 20 beers with 5 columns: `name`, `calories`, `sodium`, `alcohol`,
    and `cost`. In clustering (as with almost all ML models), we like quantitative
    features, so we will ignore the name of the beer in our clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will perform k-means clustering using `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our k-means algorithm has run the algorithm on our data points and come up
    with three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take a look at the center of each cluster by using `groupby` and `mean`
    statements (visualized in *Figure 11**.18*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 11.18 – Our found clusters for the beer dataset with k=3](img/B19488_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Our found clusters for the beer dataset with k=3
  prefs: []
  type: TYPE_NORMAL
- en: On inspection, we can see that *cluster 0* has, on average, a higher calorie,
    sodium, and alcohol content and costs more. These might be considered heavier
    beers. *Cluster 2* has on average a very low alcohol content and very few calories.
    These are probably light beers. *Cluster 1* is somewhere in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use Python to make a graph to see this in more detail, as seen in *Figure
    11**.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 11.19 – Our cluster analysis visualized using two dimensions of our
    dataset](img/B19488_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Our cluster analysis visualized using two dimensions of our dataset
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an optimal number for K and cluster validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A big part of k-means clustering is knowing the optimal number of clusters.
    If we knew this number ahead of time, then that might defeat the purpose of even
    using UL. So, we need a way to evaluate the output of our cluster analysis. The
    problem here is that, because we are not performing any kind of prediction, we
    cannot gauge how right the algorithm is at predictions. Metrics such as accuracy
    and RMSE go right out of the window. Luckily, we do have a pretty useful metric
    to help optimize our cluster analyses, called the Silhouette Coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette Coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Silhouette Coefficient** is a common metric for evaluating clustering
    performance in situations when the true cluster assignments are not known. The
    Silhouette Coefficient is a measure used to assess the quality of clusters created
    by a clustering algorithm. It quantifies how similar an object is to its own cluster
    (cohesion) compared to other clusters (separation). The Silhouette Coefficient
    for a single data point is calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>S</mi><mi>C</mi><mo>=</mo><mfrac><mrow><mi>b</mi><mo>−</mo><mi>a</mi></mrow><mrow><mi>max</mi><mfenced
    open="(" close=")"><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/185.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a* is the mean distance between a sample and all other points in the same
    class or cluster. It represents the cohesion of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the mean distance between a sample and all other points in the next
    nearest cluster. It represents the separation from the nearest cluster that the
    sample is not a part of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It ranges from *-1 (worst)* to *1 (best)*. A global score is calculated by
    taking the mean score for all observations. The Silhouette Coefficient is particularly
    useful for determining the effectiveness of a clustering algorithm because it
    takes into account both the compactness of the clusters and the separation between
    them. In general, a Silhouette Coefficient of 1 is preferred, while a score of
    -1 is not preferable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try calculating the coefficient for multiple values of `K` to find the
    best value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: So, it looks like our optimal number of beer clusters is 4! This means that
    our k-means algorithm has determined that there seem to be four distinct types
    of beer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – The Silhouette Coefficient for a varying number of clusters](img/B19488_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – The Silhouette Coefficient for a varying number of clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'k-means is a popular algorithm because of its computational efficiency and
    simple and intuitive nature. k-means, however, is highly scale-dependent and is
    not suitable for data with widely varying shapes and densities. There are ways
    to combat this issue by scaling data using `scikit-learn`’s `StandardScalar`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Easy!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at the third option in our reasons for using unsupervised
    methods: *feature extraction*.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction and PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common problem when working with data, particularly when it comes to ML, is
    having an overwhelming number of columns and not enough rows to handle such a
    quantity of columns.
  prefs: []
  type: TYPE_NORMAL
- en: A great example of this is when we were looking at the *send cash now* example
    in our naïve Bayes example earlier. Remember we had literally 0 instances of texts
    with that exact phrase? In that case, we turned to a naïve assumption that allowed
    us to extrapolate a probability for both of our categories.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we had this problem in the first place is because of something called
    the **curse of dimensionality** (**COD**). The COD basically says that as we introduce
    new feature columns, we need exponentially more rows (data points) to consider
    the increased number of possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example where we attempt to use a learning model that utilizes
    the distance between points on a corpus of text that has 4,086 pieces of text
    and that the whole thing has been count-vectorized using `scikit-learn`. Now,
    let’s do an experiment. I will first consider a single word as the only dimension
    of our text. Then, I will count how many pieces of text are within 1 unit of each
    other. For example, if 2 sentences both contain that word, they would be 0 units
    away and, similarly, if neither of them contains the word, they would be 0 units
    away from one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have **16,695,396** (**4086*4086**) distances to scan over.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, 16.2 million pairs of texts are within a single unit of distance. Now,
    let’s try again with the first two words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'By considering a single new column, we lost about 100,000 pairs of points that
    were within a single unit of distance. This is because we are adding space in
    between them for every dimension that we add. Let’s take this test a step further
    and calculate this number for the first 100 words and then plot the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s plot the number of points within 1 unit versus the number of dimensions
    we consider in *Figure 11**.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – The COD says that as we increase the number of feature columns
    in our dataset, data points become further away from each other due to the increase
    in high-dimensional space](img/B19488_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – The COD says that as we increase the number of feature columns
    in our dataset, data points become further away from each other due to the increase
    in high-dimensional space
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, the COD states that as we increase the number of feature columns,
    we need exponentially more data to maintain the same level of model performance.
    This is because, in high-dimensional spaces, even the nearest neighbors can be
    very far away from a given data point, making it difficult to create good predictions.
    High dimensionality also increases the risk of overfitting as the model may start
    to fit to noise in the data rather than the actual signal.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, with more dimensions, the volume of the space increases so rapidly
    that the available data becomes sparse. This sparsity is problematic for any method
    that requires statistical significance. In order to obtain a reliable result,
    the amount of data needed to support the analysis often grows exponentially with
    the dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: We can see clearly that the number of points within a single unit of one another
    goes down dramatically as we introduce more and more columns. And this is only
    the first 100 columns!
  prefs: []
  type: TYPE_NORMAL
- en: All of this space that we add in by considering new columns makes it harder
    for the finite amount of points we have to stay happily within range of each other.
    We would have to add more points in order to fill in this gap. And that, my friends,
    is why we should consider using dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The COD is solved by either adding more data points (which is not always possible)
    or implementing dimension reduction. **Dimension reduction** is simply the act
    of reducing the number of columns in our dataset and not the number of rows. There
    are two ways of implementing dimension reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: This is the act of creating a subset of our column features
    and only using the best features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: This is the act of mathematically transforming our
    feature set into a new extracted coordinate system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are familiar with feature selection as the process of saying the `Embarked_Q`
    column is not helping our decision tree. Let’s get rid of it and see how it performs.
    It is literally when we (or the machine) make the decision to ignore certain columns.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction is a bit trickier.
  prefs: []
  type: TYPE_NORMAL
- en: In *feature extraction*, we are using usually fairly complicated mathematical
    formulas in order to obtain new super columns that are usually better than any
    single original column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our primary model for doing so is called **PCA**. PCA will extract a set number
    of super columns in order to represent our original data with much fewer columns.
    Let’s take a concrete example. Previously, I mentioned some text with 4,086 rows
    and over 18,000 columns. That dataset is actually a set of *Yelp* online reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Our goal is to predict whether or not a person gave a 5- or 1-star review based
    on the words they used in the review. Let’s set a baseline with logistic regression
    and see how well we can predict this binary category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: So, by utilizing all of the words in our corpus, our model seems to have over
    a 91% accuracy. Not bad!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try only using the top 100 used words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note how our training and testing matrices have 100 columns. This is because
    I told our vectorizer to only look at the top 100 words. See also that our performance
    took a hit and is now down to 88% accuracy. This makes sense because we are ignoring
    over 4,700 words in our corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a different approach. Let’s import a PCA module and tell it
    to make us 100 new super columns and see how that performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Not only do our matrices still have 100 columns, but these columns are no longer
    words in our corpus. They are complex transformations of columns and are 100 new
    columns. Also, note that using 100 of these new columns gives us a better predictive
    performance than using the 100 top words!
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction is a great way to use mathematical formulas to extract brand-new
    columns that generally perform better than just selecting the best ones beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how do we visualize these new super columns? Well, I can think of no better
    way than to look at an example using image analysis. Specifically, let’s make
    facial recognition software. OK? OK. Let’s begin by importing some faces given
    to us by `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We have gathered 1,288 images of people’s faces, and each one has 1,850 features
    (pixels) that identify that person. Here’s the code we used – an example of one
    of our faces can be seen in *Figure 11**.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 11.22 – A face from our dataset: George W. Bush](img/B19488_11_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.22 – A face from our dataset: George W. Bush'
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! To get a glimpse at the type of dataset we are looking at, let’s look
    at a few overall metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: So, we have 1,288 images, 1,850 features, and 7 classes (people) to choose from.
    Our goal is to make a classifier that will assign the person’s face a name based
    on the 1,850 pixels given to us.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a baseline and see how a logistic regression (a classifier that is
    based on linear regression) performs on our data without doing anything to our
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I know we haven’t formally introduced logistic regressions before, but they
    are a very lightweight classifier that works off of very similar assumptions as
    linear regressions from the last chapter. All we need to know for now is that
    it performs classification!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: So, within 6.3 seconds, we were able to get 81% on our test set. Not too bad.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try this with our decomposed faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is collecting `75` extracted columns from our 1,850 unprocessed
    columns. These are our super faces. Now, let’s plug in our newly extracted columns
    into our logistic regression and compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Wow! Not only was this entire calculation about 30 times faster than the unprocessed
    images, but the predictive performance also got better! This shows us that PCA
    and feature extraction, in general, can help us all around when performing ML
    on complex datasets with many columns. By searching for these patterns in the
    dataset and extracting new feature columns, we can speed up and enhance our learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at one more interesting thing. I mentioned before that one of the
    purposes of this example was to examine and visualize our eigenfaces, as they
    are called: our super columns. I will not disappoint. Let’s write some code that
    will show us our super columns as they would look to us humans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Warning: the faces in *Figure 11**.23* are a bit creepy!'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.23 – Performing PCA on the pixels of our faces creates what is
    known as “eigenfaces” that represent features that our classifiers look for when
    trying to recognize faces](img/B19488_11_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.23 – Performing PCA on the pixels of our faces creates what is known
    as “eigenfaces” that represent features that our classifiers look for when trying
    to recognize faces
  prefs: []
  type: TYPE_NORMAL
- en: Wow! A haunting and yet beautiful representation of what the data believes to
    be the most important features of a face. As we move from the top left (first
    super column) to the bottom, it is actually somewhat easy to see what the image
    is trying to tell us. The first super column looks like a very general face structure
    with eyes and nose and a mouth. It is almost saying “I represent the basic qualities
    of a face that all faces must have.” Our second super column directly to its right
    seems to be telling us about shadows in the image. The next one might be telling
    us that skin tone plays a role in detecting who this is, which might be why the
    third face is much darker than the first two.
  prefs: []
  type: TYPE_NORMAL
- en: Using feature extraction UL methods such as PCA can give us a very deep look
    into our data and reveal to us what the data believes to be the most important
    features, not just what we believe them to be. Feature extraction is a great preprocessing
    tool that can speed up our future learning methods, make them more powerful, and
    give us more insight into how the data believes it should be viewed. To sum up
    this section, we will list the pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the advantages of using feature extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: Our models become much faster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our predictive performance can become better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can give us insight into the extracted features (eigenfaces)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And here are some of the disadvantages of using feature extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: We lose some of the interpretability of our features as they are new mathematically
    derived columns, not our old ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can lose predictive performance because we are losing information as we extract
    fewer columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to the summary next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our exploration into the world of ML has revealed a vast landscape that extends
    well beyond the foundational techniques of linear and logistic regression. We
    delved into decision trees, which provide intuitive insights into data through
    their hierarchical structure. Naïve Bayes classification offered us a probabilistic
    perspective, showing how to make predictions under the assumption of feature independence.
    We ventured into dimensionality reduction, encountering techniques such as feature
    extraction, which help overcome the COD and reduce computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering introduced us to the realm of UL, where we learned to find
    hidden patterns and groupings in data without pre-labeled outcomes. Across these
    methods, we’ve seen how ML can tackle a plethora of complex problems, from predicting
    categorical outcomes to uncovering latent structures in data.
  prefs: []
  type: TYPE_NORMAL
- en: Through practical examples, we’ve compared and contrasted SL, which relies on
    labeled data, with UL, which operates without explicit guidance on the output.
    This journey has equipped us with a deeper understanding of the various techniques
    and their appropriate applications within the broad and dynamic field of data
    science.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue to harness the power of these algorithms, we are reminded of
    the importance of selecting the right model for the right task—a principle that
    remains central to the practice of effective data science.
  prefs: []
  type: TYPE_NORMAL
