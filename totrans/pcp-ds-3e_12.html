<html><head></head><body>
<div id="_idContainer314" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-172"><a id="_idTextAnchor329" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">12</span></h1>
<h1 id="_idParaDest-173" class="calibre6"><a id="_idTextAnchor330" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">Introduction to Transfer Learning and Pre-Trained Models</span></h1>
<p class="calibre3"><a id="_idTextAnchor331" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.3.1">Just as one wouldn’t try to reinvent the wheel, in the world of data science and </span><strong class="bold"><span class="kobospan" id="kobo.4.1">machine learning</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">ML</span></strong><span class="kobospan" id="kobo.7.1">), it’s often more efficient to build upon existing knowledge. </span><span class="kobospan" id="kobo.7.2">This is where the concepts of </span><strong class="bold"><span class="kobospan" id="kobo.8.1">transfer learning</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">TL</span></strong><span class="kobospan" id="kobo.11.1">) and pre-trained models come into play, two incredibly important tools in a data </span><span><span class="kobospan" id="kobo.12.1">scientist’s repertoire.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.13.1">TL is almost like a shortcut in ML. </span><span class="kobospan" id="kobo.13.2">Instead of taking a model architecture that has never seen data before, such as a Logistic Regression model or a Random Forest model, imagine being able to take a model trained on one task and then repurposing it for a different, yet related task. </span><span class="kobospan" id="kobo.13.3">That’s TL in a nutshell – leveraging existing knowledge to learn new things more efficiently. </span><span class="kobospan" id="kobo.13.4">It’s a concept that echoes throughout many facets of life and is a key technique in </span><span><span class="kobospan" id="kobo.14.1">data science.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.15.1">Pre-trained models are off-the-shelf components, ready to be used right out of the box. </span><span class="kobospan" id="kobo.15.2">They’re ML models that have been trained on large datasets, capturing an immense amount of information about the task they were trained on. </span><span class="kobospan" id="kobo.15.3">When it comes to tackling new tasks, these pre-trained models provide a substantial head start. </span><span class="kobospan" id="kobo.15.4">The importance of TL and pre-trained models cannot be overstated in modern ML. </span><span class="kobospan" id="kobo.15.5">They are fundamental to many cutting-edge applications in data science, from </span><strong class="bold"><span class="kobospan" id="kobo.16.1">computer vision</span></strong><span class="kobospan" id="kobo.17.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.18.1">CV</span></strong><span class="kobospan" id="kobo.19.1">) tasks such as image recognition to </span><strong class="bold"><span class="kobospan" id="kobo.20.1">natural language processing</span></strong><span class="kobospan" id="kobo.21.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.22.1">NLP</span></strong><span class="kobospan" id="kobo.23.1">) tasks such as </span><strong class="bold"><span class="kobospan" id="kobo.24.1">sentiment analysis</span></strong><span class="kobospan" id="kobo.25.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.26.1">SA</span></strong><span class="kobospan" id="kobo.27.1">). </span><span class="kobospan" id="kobo.27.2">By leveraging these techniques, we can achieve impressive results even with limited data </span><span><span class="kobospan" id="kobo.28.1">or resources.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.29.1">In the following sections, we will investigate these concepts, exploring their nuances, applications, </span><span><span class="kobospan" id="kobo.30.1">and potential:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.31.1">Understanding </span><span><span class="kobospan" id="kobo.32.1">pre-trained models</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.33.1">Different types </span><span><span class="kobospan" id="kobo.34.1">of TL</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.35.1">TL with </span><strong class="bold"><span class="kobospan" id="kobo.36.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="kobospan" id="kobo.37.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.38.1">BERT</span></strong><span class="kobospan" id="kobo.39.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.40.1">Generative Pre-trained </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.41.1">Transformer</span></strong></span><span><span class="kobospan" id="kobo.42.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.43.1">GPT</span></strong></span><span><span class="kobospan" id="kobo.44.1">)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.45.1">We’ll also walk through practical examples that highlight their power in real-world scenarios. </span><span class="kobospan" id="kobo.45.2">By the end of this chapter, you’ll have a solid understanding of TL and pre-trained models and be equipped to harness their potential in your </span><span><span class="kobospan" id="kobo.46.1">own projects.</span></span></p>
<h1 id="_idParaDest-174" class="calibre6"><a id="_idTextAnchor332" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.47.1">Understanding pre-trained models</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.48.1">Pre-trained models are</span><a id="_idIndexMarker740" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.49.1"> like learning from the experience of others. </span><span class="kobospan" id="kobo.49.2">These models have been trained on extensive datasets, learning patterns, and features that make them adept at their tasks. </span><span class="kobospan" id="kobo.49.3">Think of it as if a model has been reading thousands of books on a subject, absorbing all that information. </span><span class="kobospan" id="kobo.49.4">When we use a pre-trained model, we’re leveraging all that </span><span><span class="kobospan" id="kobo.50.1">prior knowledge.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.51.1">In general, pre-training steps are not necessarily “useful” to a human, but it is crucial to a model to simply learn about a domain and about a medium. </span><span class="kobospan" id="kobo.51.2">Pre-training helps models learn how language works in general but not how to classify sentiments or detect </span><span><span class="kobospan" id="kobo.52.1">an object.</span></span><a id="_idTextAnchor333" class="pcalibre calibre4 pcalibre1"/></p>
<h2 id="_idParaDest-175" class="calibre7"><a id="_idTextAnchor334" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.53.1">Benefits of using pre-trained models</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.54.1">The benefits of </span><a id="_idIndexMarker741" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.55.1">using pre-trained models are numerous. </span><span class="kobospan" id="kobo.55.2">For starters, they save us a lot of time. </span><span class="kobospan" id="kobo.55.3">Training a model from scratch can be a time-consuming process, but using a pre-trained model gives us a head start. </span><span class="kobospan" id="kobo.55.4">Furthermore, these models often lead to better performance, especially when our dataset is relatively small. </span><span class="kobospan" id="kobo.55.5">The reason? </span><span class="kobospan" id="kobo.55.6">Pre-trained models have seen much more data than we usually have at our disposal, and they’ve learned a lot </span><span><span class="kobospan" id="kobo.56.1">from it.</span></span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.57.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.58.1">.1</span></em><span class="kobospan" id="kobo.59.1"> shows the result of a study </span><a id="_idIndexMarker742" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.60.1">done on </span><strong class="bold"><span class="kobospan" id="kobo.61.1">large language models</span></strong><span class="kobospan" id="kobo.62.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.63.1">LLMs</span></strong><span class="kobospan" id="kobo.64.1">) such as BERT where one of the goals was to show that pre-training was leading to some obvious patterns in how BERT was recognizing basic grammatical constructs. </span><span class="kobospan" id="kobo.64.2">The study visualized that models post pre-training were able to recognize what we would consider as obvious grammatical patterns, such as pronoun-antecedent relationships and direct </span><span><span class="kobospan" id="kobo.65.1">object/verb relationships:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer302">
<span class="kobospan" id="kobo.66.1"><img alt="Figure 12.1 – A study visualizing how BERT’s pre-training allowed it to pick up on common grammatical constructs without ever being told what they were" src="image/B19488_12_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.67.1">Figure 12.1 – A study visualizing how BERT’s pre-training allowed it to pick up on common grammatical constructs without ever being told what they were</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.68.1">BERT, of course, is</span><a id="_idIndexMarker743" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.69.1"> not the only model that undergoes pre-training, and this practice is not even limited to </span><span><span class="kobospan" id="kobo.70.1">text-based model</span><a id="_idTextAnchor335" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.71.1">s.</span></span></p>
<h2 id="_idParaDest-176" class="calibre7"><a id="_idTextAnchor336" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.72.1">Commonly used pre-trained models</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Pre-trained </span><a id="_idIndexMarker744" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.74.1">models come in all shapes and sizes, each tailored to different types of data and tasks. </span><span class="kobospan" id="kobo.74.2">Let’s talk about some of the most </span><span><span class="kobospan" id="kobo.75.1">popular on</span><a id="_idTextAnchor337" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.76.1">es.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.77.1">Image-based models</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.78.1">For tasks related</span><a id="_idIndexMarker745" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.79.1"> to images, models such as </span><a id="_idIndexMarker746" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.80.1">the Vision Transformer (more on this one later in this chapter), models </span><a id="_idIndexMarker747" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.81.1">from the </span><strong class="bold"><span class="kobospan" id="kobo.82.1">Visual Geometry Group</span></strong><span class="kobospan" id="kobo.83.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.84.1">VGG</span></strong><span class="kobospan" id="kobo.85.1">) (as seen in </span><span><em class="italic"><span class="kobospan" id="kobo.86.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.87.1">.2</span></em><span class="kobospan" id="kobo.88.1">), and ResNet are some common options to choose from. </span><span class="kobospan" id="kobo.88.2">Models from these families have been trained on tens of thousands of images, learning to recognize everything from shapes and textures to complex objects. </span><span class="kobospan" id="kobo.88.3">They’re incredibly versatile </span><a id="_idIndexMarker748" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.89.1">and can be</span><a id="_idIndexMarker749" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.90.1"> fine-tuned for a wide array of </span><span><span class="kobospan" id="kobo.91.1">image-based tasks:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer303">
<span class="kobospan" id="kobo.92.1"><img alt="Figure 12.2 – The VGG16 model (from the VGG) is a convolutional neural network (CNN) that can be pre-trained on ima﻿ge data" src="image/B19488_12_02.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.93.1">Figure 12.2 – The VGG16 model (from the VGG) is a convolutional neural network (CNN) that can be pre-trained on ima</span><a id="_idTextAnchor338" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.94.1">ge data</span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.95.1">Text-based models</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.96.1">When it comes to</span><a id="_idIndexMarker750" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.97.1"> text, models such as BERT (</span><span><em class="italic"><span class="kobospan" id="kobo.98.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.99.1">.3</span></em><span class="kobospan" id="kobo.100.1">) and </span><a id="_idIndexMarker751" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.101.1">GPT are among the most used language models. </span><span class="kobospan" id="kobo.101.2">They were first originally architected in 2018 (only 1 year after the primary Transformer architecture that both GPT and BERT are based on was even proposed or mentioned), and they’ve, as with their image counterparts, been trained on vast amounts of text data, learning the intricacies of human language. </span><span class="kobospan" id="kobo.101.3">Whether it’s understanding the sentiment behind a tweet or answering questions about a piece of text, these models are up to the task. </span><span class="kobospan" id="kobo.101.4">As we move forward, we’ll see how these pre-trained models can be combined with TL to tackle new tasks with </span><span><span class="kobospan" id="kobo.102.1">impressive efficiency:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer304">
<span class="kobospan" id="kobo.103.1"><img alt="Figure 12.3 – A figure from the original blog post from Google open sourcing BERT in 2018 calls out OpenAI’s GPT-1 model" src="image/B19488_12_03.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.104.1">Figure 12.3 – A figure from the original blog post from Google open sourcing BERT in 2018 calls out OpenAI’s GPT-1 model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.105.1">This came out a few months prior, highlighting BERT’s ability to process more relationships</span><a id="_idIndexMarker752" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.106.1"> between tokens with the relatively same </span><a id="_idIndexMarker753" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.107.1">number of parameters</span><a id="_idTextAnchor339" class="pcalibre calibre4 pcalibre1"/> <span><span class="kobospan" id="kobo.108.1">than GPT.</span></span></p>
<h2 id="_idParaDest-177" class="calibre7"><a id="_idTextAnchor340" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.109.1">Decoding BERT’s pre-training</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.110.1">One of the most </span><a id="_idIndexMarker754" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.111.1">impressive feats of TL can be observed in </span><a id="_idIndexMarker755" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.112.1">BERT, a pre-trained model that revolutionized the NLP landscape. </span><span class="kobospan" id="kobo.112.2">Two fundamental training tasks drive BERT’s robust understanding of language semantics and relationships: </span><strong class="bold"><span class="kobospan" id="kobo.113.1">masked language modeling</span></strong><span class="kobospan" id="kobo.114.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.115.1">MLM</span></strong><span class="kobospan" id="kobo.116.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.117.1">next sentence prediction</span></strong><span class="kobospan" id="kobo.118.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.119.1">NSP</span></strong><span class="kobospan" id="kobo.120.1">). </span><span class="kobospan" id="kobo.120.2">Let’s break them down and see how each one contributes to BERT’s language </span><span><span class="kobospan" id="kobo.121.1">processin</span><a id="_idTextAnchor341" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.122.1">g abilities.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.123.1">MLM</span></h3>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.124.1">MLM</span></strong><span class="kobospan" id="kobo.125.1"> (visualized in </span><span><em class="italic"><span class="kobospan" id="kobo.126.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.127.1">.4</span></em><span class="kobospan" id="kobo.128.1">) is a key</span><a id="_idIndexMarker756" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.129.1"> component of </span><a id="_idIndexMarker757" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.130.1">BERT’s pre-training process. </span><span class="kobospan" id="kobo.130.2">In essence, MLM works by randomly replacing approximately 15% of the words in the input data with a special (</span><em class="italic"><span class="kobospan" id="kobo.131.1">MASK</span></em><span class="kobospan" id="kobo.132.1">) token. </span><span class="kobospan" id="kobo.132.2">It’s then up to BERT to figure out which word was replaced, essentially filling in the blank. </span><span class="kobospan" id="kobo.132.3">Think of it as a sophisticated game of </span><em class="italic"><span class="kobospan" id="kobo.133.1">Mad Libs</span></em><span class="kobospan" id="kobo.134.1"> that BERT plays during its training. </span><span class="kobospan" id="kobo.134.2">If we were to take a sentence such as “Stop at the light,” MLM might replace “light” with (</span><em class="italic"><span class="kobospan" id="kobo.135.1">MASK</span></em><span class="kobospan" id="kobo.136.1">), prompting BERT to predict the </span><span><span class="kobospan" id="kobo.137.1">missing word:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer305">
<span class="kobospan" id="kobo.138.1"><img alt="Figure 12.4 – The MLM pre-training task has BERT filling in missing tokens from a sequence of tokens" src="image/B19488_12_04.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.139.1">Figure 12.4 – The MLM pre-training task has BERT filling in missing tokens from a sequence of tokens</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.140.1">This process of </span><a id="_idIndexMarker758" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.141.1">MLM helps BERT understand the context around each </span><a id="_idIndexMarker759" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.142.1">word and construct meaningful relationships between different parts</span><a id="_idTextAnchor342" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.143.1"> of </span><span><span class="kobospan" id="kobo.144.1">a sentence.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.145.1">NSP</span></h3>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.146.1">NSP</span></strong><span class="kobospan" id="kobo.147.1"> (visualized</span><a id="_idIndexMarker760" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.148.1"> in </span><span><em class="italic"><span class="kobospan" id="kobo.149.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.150.1">.5</span></em><span class="kobospan" id="kobo.151.1">) is the </span><a id="_idIndexMarker761" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.152.1">second crucial part of BERT’s pre-training regimen. </span><span class="kobospan" id="kobo.152.2">NSP is a binary classification problem, where BERT is tasked with determining whether a provided sentence </span><em class="italic"><span class="kobospan" id="kobo.153.1">B</span></em><span class="kobospan" id="kobo.154.1"> follows sentence </span><em class="italic"><span class="kobospan" id="kobo.155.1">A</span></em><span class="kobospan" id="kobo.156.1"> in the original text. </span><span class="kobospan" id="kobo.156.2">In the grand scheme of language understanding, this is like asking BERT to understand the logical flow of sentences in a piece of text. </span><span class="kobospan" id="kobo.156.3">This ability to predict whether sentence </span><em class="italic"><span class="kobospan" id="kobo.157.1">B</span></em><span class="kobospan" id="kobo.158.1"> logically follows sentence </span><em class="italic"><span class="kobospan" id="kobo.159.1">A</span></em><span class="kobospan" id="kobo.160.1"> allows BERT to understand more nuanced, higher-level linguistic structures and </span><span><span class="kobospan" id="kobo.161.1">narrative flows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer306">
<span class="kobospan" id="kobo.162.1"><img alt="Figure 12.5 – In the NSP pre-training task, BERT is looking at two thoughts and deciding if the second phrase would come directly after the first phrase" src="image/B19488_12_05.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.163.1">Figure 12.5 – In the NSP pre-training task, BERT is looking at two thoughts and deciding if the second phrase would come directly after the first phrase</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.164.1">Put another way, MLM helps BERT get a grasp on intricate connections between words and their contexts, while NSP equips BERT with an understanding of the relationships between sentences. </span><span class="kobospan" id="kobo.164.2">It’s this combination that makes BERT such a powerful tool for a range of NLP tasks. </span><span class="kobospan" id="kobo.164.3">Between NSP and MLM (</span><span><em class="italic"><span class="kobospan" id="kobo.165.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.166.1">.6</span></em><span class="kobospan" id="kobo.167.1">), BERT’s training is meant</span><a id="_idIndexMarker762" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.168.1"> to give it a sense of how tokens affect phrase </span><a id="_idIndexMarker763" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.169.1">meanings (MLM) and how phrases work together to form larger </span><span><span class="kobospan" id="kobo.170.1">thoughts (NSP):</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer307">
<span class="kobospan" id="kobo.171.1"><img alt="Figure 12.6 – BERT’s pre-training helps it to learn about la﻿nguage in general" src="image/B19488_12_06.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.172.1">Figure 12.6 – BERT’s pre-training helps it to learn about la</span><a id="_idTextAnchor343" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.173.1">nguage in general</span></p>
<h2 id="_idParaDest-178" class="calibre7"><a id="_idTextAnchor344" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.174.1">TL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.175.1">Now that we have a </span><a id="_idIndexMarker764" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.176.1">strong grasp of pre-trained models, let’s</span><a id="_idIndexMarker765" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.177.1"> shift our attention toward the other compelling facet of this equation: </span><strong class="bold"><span class="kobospan" id="kobo.178.1">TL</span></strong><span class="kobospan" id="kobo.179.1">. </span><span class="kobospan" id="kobo.179.2">In essence, TL is the application of knowledge gained from one problem domain (source) to a different but related probl</span><a id="_idTextAnchor345" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.180.1">em </span><span><span class="kobospan" id="kobo.181.1">domain (target).</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.182.1">The process of TL</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.183.1">TL is all about </span><a id="_idIndexMarker766" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.184.1">adaptability. </span><span class="kobospan" id="kobo.184.2">It takes a model that’s been trained on one task and adapts it to perform a different but related task. </span><span class="kobospan" id="kobo.184.3">They might not have solved that particular mystery before, but their skills and experience can be adapted to the task at hand. </span><span class="kobospan" id="kobo.184.4">TL is particularly useful when we have a small amount of data for our specific task or when our task is very similar to the one the original model was trained on. </span><span class="kobospan" id="kobo.184.5">In these situations, TL can save time and resources while boosting our </span><a id="_idTextAnchor346" class="pcalibre calibre4 pcalibre1"/><span><span class="kobospan" id="kobo.185.1">model’s performance.</span></span></p>
<h1 id="_idParaDest-179" class="calibre6"><a id="_idTextAnchor347" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.186.1">Different types of TL</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.187.1">Let’s take a moment</span><a id="_idIndexMarker767" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.188.1"> to get familiar with the diverse landscape of TL. </span><span class="kobospan" id="kobo.188.2">It’s not a single monolithic idea, but rather a collection of varied strategies that fall under one umbrella term. </span><span class="kobospan" id="kobo.188.3">There’s a type of TL for just about every scenario y</span><a id="_idTextAnchor348" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.189.1">ou might </span><span><span class="kobospan" id="kobo.190.1">come across.</span></span></p>
<h2 id="_idParaDest-180" class="calibre7"><a id="_idTextAnchor349" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.191.1">Inductive TL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.192.1">First up, we </span><a id="_idIndexMarker768" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.193.1">have </span><strong class="bold"><span class="kobospan" id="kobo.194.1">inductive TL</span></strong><span class="kobospan" id="kobo.195.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.196.1">ITL</span></strong><span class="kobospan" id="kobo.197.1">). </span><span class="kobospan" id="kobo.197.2">This is all </span><a id="_idIndexMarker769" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.198.1">about using what’s already been learned and applying it in new, but related, scenarios. </span><span class="kobospan" id="kobo.198.2">The key here is the generalization of learned features from one task—let’s call this the source task—and then fine-tuning them to perform well on another task—the </span><span><span class="kobospan" id="kobo.199.1">target task.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.200.1">Imagine a model that’s spent its virtual lifetime learning from a broad text corpus, getting to grips with the complexities of language, grammar, and context. </span><span class="kobospan" id="kobo.200.2">Now, we have a different task on our hands: SA on product reviews. </span><span class="kobospan" id="kobo.200.3">With ITL, our model can use what it’s already learned about language and fine-tune itself to become a pro at detecting sentiment. </span><span><em class="italic"><span class="kobospan" id="kobo.201.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.202.1">.7</span></em><span class="kobospan" id="kobo.203.1"> visualizes how we will approach ITL in a later section. </span><span class="kobospan" id="kobo.203.2">The main idea is to take a pre-trained model from a model repository such as </span><strong class="source-inline"><span class="kobospan" id="kobo.204.1">HuggingFace</span></strong><span class="kobospan" id="kobo.205.1"> and perform any potential model modifications for our task, throw some labeled </span><a id="_idIndexMarker770" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.206.1">data at it (like we would any</span><a id="_idIndexMarker771" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.207.1"> other ML mod</span><a id="_idTextAnchor350" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.208.1">el), and watch </span><span><span class="kobospan" id="kobo.209.1">it learn:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer308">
<span class="kobospan" id="kobo.210.1"><img alt="Figure 12.7 – An ITL process might involve training BERT on﻿ supervised labeled data" src="image/B19488_12_07.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.211.1">Figure 12.7 – An ITL process might involve training BERT on</span><a id="_idTextAnchor351" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.212.1"> supervised labeled data</span></p>
<h2 id="_idParaDest-181" class="calibre7"><a id="_idTextAnchor352" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.213.1">Transductive TL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.214.1">Our second type </span><a id="_idIndexMarker772" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.215.1">of TL is called </span><strong class="bold"><span class="kobospan" id="kobo.216.1">transductive TL</span></strong><span class="kobospan" id="kobo.217.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.218.1">TTL</span></strong><span class="kobospan" id="kobo.219.1">) and</span><a id="_idIndexMarker773" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.220.1"> is a bit more nebulous in its task. </span><span class="kobospan" id="kobo.220.2">Rather than being given a concrete second task to perform (such as classification), our model instead is asked to adapt to new data without losing its grounding in the original task. </span><span class="kobospan" id="kobo.220.3">It’s a good choice when we have a bunch of unlabeled data for our </span><span><span class="kobospan" id="kobo.221.1">target task.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.222.1">For example, if a model was trained on one image dataset to identify different objects and we have a new, unlabeled image dataset, we could ask the model to use its knowledge from the source task to label the new dataset, even without any</span><a id="_idTextAnchor353" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.223.1"> explicit </span><span><span class="kobospan" id="kobo.224.1">labels provided.</span></span></p>
<h2 id="_idParaDest-182" class="calibre7"><a id="_idTextAnchor354" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.225.1">Unsupervised TL – feature extraction</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.226.1">Pre-trained </span><a id="_idIndexMarker774" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.227.1">models aren’t just </span><a id="_idIndexMarker775" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.228.1">useful for their predictive abilities. </span><span class="kobospan" id="kobo.228.2">They’re also treasure troves of features, ripe for extraction. </span><span class="kobospan" id="kobo.228.3">Using </span><strong class="bold"><span class="kobospan" id="kobo.229.1">unsupervised TL</span></strong><span class="kobospan" id="kobo.230.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.231.1">UTL</span></strong><span class="kobospan" id="kobo.232.1">), our model, which was trained on a vast text corpus, can use its understanding of language to find patterns and help us divide the text into meaningful categories. </span><span class="kobospan" id="kobo.232.2">Feature extraction with pre-trained models involves using a pre-trained model to transform raw data into a more useful format—one that highlights important features </span><span><span class="kobospan" id="kobo.233.1">and patterns.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.234.1">These are the three main flavors of TL, each with its own approach and ideal use cases. </span><span class="kobospan" id="kobo.234.2">In the wide world of ML, it’s all about picking the right tool for the job, and TL definitely gives us a whole toolbox to choose from. </span><span class="kobospan" id="kobo.234.3">With this newfound understanding of TL, we’re well equipped to start putting it into practice. </span><span class="kobospan" id="kobo.234.4">In the next section, we’ll see how TL </span><a id="_idIndexMarker776" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.235.1">and pre-trained </span><a id="_idIndexMarker777" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.236.1">models can come together t</span><a id="_idTextAnchor355" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.237.1">o conquer new tasks </span><span><span class="kobospan" id="kobo.238.1">with ease.</span></span></p>
<h1 id="_idParaDest-183" class="calibre6"><a id="_idTextAnchor356" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.239.1">TL with BERT and GPT</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.240.1">Having grasped</span><a id="_idIndexMarker778" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.241.1"> the</span><a id="_idIndexMarker779" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.242.1"> fundamental</span><a id="_idIndexMarker780" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.243.1"> concepts</span><a id="_idIndexMarker781" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.244.1"> of pre-trained models and TL, it’s time to put theory into practice. </span><span class="kobospan" id="kobo.244.2">It’s one thing to know the ingredients; it’s another to know how to mix them into a delicious dish with them. </span><span class="kobospan" id="kobo.244.3">In this section, we will take some models that have already learned a lot from their pre-training and fine-tune them to perform a new, related task. </span><span class="kobospan" id="kobo.244.4">This process involves adjusting the model’s parameters to better suit the new task, much like fine-tuning a </span><span><span class="kobospan" id="kobo.245.1">musical instrument:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer309">
<span class="kobospan" id="kobo.246.1"><img alt="Figure 12.8 – ITL" src="image/B19488_12_08.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.247.1">Figure 12.8 – ITL</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.248.1">ITL takes a pre-trained model that was generally trained on a semi-supervised (or unsupervised) task</span><a id="_idIndexMarker782" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.249.1"> and then is given labe</span><a id="_idTextAnchor357" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.250.1">led </span><a id="_idIndexMarker783" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.251.1">data to</span><a id="_idIndexMarker784" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.252.1"> learn a </span><span><span class="kobospan" id="kobo.253.1">specific </span></span><span><a id="_idIndexMarker785" class="pcalibre calibre4 pcalibre1"/></span><span><span class="kobospan" id="kobo.254.1">task.</span></span></p>
<h2 id="_idParaDest-184" class="calibre7"><a id="_idTextAnchor358" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.255.1">Examples of TL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.256.1">Let’s take a look </span><a id="_idIndexMarker786" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.257.1">at some examples o</span><a id="_idTextAnchor359" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.258.1">f TL with specific </span><span><span class="kobospan" id="kobo.259.1">pre-trained models.</span></span></p>
<h2 id="_idParaDest-185" class="calibre7"><a id="_idTextAnchor360" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.260.1">Example – Fine-tuning a pre-trained model for text classification</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.261.1">Consider a simple </span><a id="_idIndexMarker787" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.262.1">text classification </span><a id="_idIndexMarker788" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.263.1">problem. </span><span class="kobospan" id="kobo.263.2">Suppose we need to analyze customer reviews and determine whether they’re positive or negative. </span><span class="kobospan" id="kobo.263.3">We have a dataset of reviews, but it’s not nearly large enough to </span><a id="_idIndexMarker789" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.264.1">train a </span><strong class="bold"><span class="kobospan" id="kobo.265.1">deep learning</span></strong><span class="kobospan" id="kobo.266.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.267.1">DL</span></strong><span class="kobospan" id="kobo.268.1">) model from scratch. </span><span class="kobospan" id="kobo.268.2">We will fine-tune BERT on a text classification task, allowing the model to adapt its existing knowledge to our </span><span><span class="kobospan" id="kobo.269.1">specific problem.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.270.1">We will have to move away from the popular scikit-learn library to another popular library called </span><strong class="source-inline"><span class="kobospan" id="kobo.271.1">transformers</span></strong><span class="kobospan" id="kobo.272.1">, which was created by HuggingFace (the pre-trained model repository I mentioned earlier) as </span><strong class="source-inline"><span class="kobospan" id="kobo.273.1">scikit-learn </span></strong><span class="kobospan" id="kobo.274.1">does not (yet) support </span><span><span class="kobospan" id="kobo.275.1">Transformer models.</span></span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.276.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.277.1">.9</span></em><span class="kobospan" id="kobo.278.1"> shows how we will have to take the original BERT model and make some minor modifications to it to perform text classification. </span><span class="kobospan" id="kobo.278.2">Luckily, the </span><strong class="source-inline"><span class="kobospan" id="kobo.279.1">transformers</span></strong><span class="kobospan" id="kobo.280.1"> package has a built-in class to do this for </span><a id="_idTextAnchor361" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.281.1">us </span><span><span class="kobospan" id="kobo.282.1">called </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.283.1">BertForSequenceClassification</span></strong></span><span><span class="kobospan" id="kobo.284.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer310">
<span class="kobospan" id="kobo.285.1"><img alt="Figure 12.9 – Simplest text classification case" src="image/B19488_12_09.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.286.1">Figure 12.9 – Simplest text classification case</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.287.1">In many TL cases, we need to architect additional layers. </span><span class="kobospan" id="kobo.287.2">In the simplest text classification case, we add a classification layer on top of a pre-trained BERT model so that it can perform the kind of classification </span><span><span class="kobospan" id="kobo.288.1">we want.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.289.1">The following code block shows an end-to-end code example of fine-tuning BERT on a text classification task. </span><span class="kobospan" id="kobo.289.2">Note that we are also using a package called </span><strong class="source-inline"><span class="kobospan" id="kobo.290.1">datasets</span></strong><span class="kobospan" id="kobo.291.1">, also made</span><a id="_idIndexMarker790" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.292.1"> by </span><strong class="source-inline"><span class="kobospan" id="kobo.293.1">HuggingFace</span></strong><span class="kobospan" id="kobo.294.1">, to load </span><a id="_idIndexMarker791" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.295.1">a sentiment classification task from IMDb reviews. </span><span class="kobospan" id="kobo.295.2">Let’s </span><a id="_idIndexMarker792" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.296.1">begin by loading up </span><span><span class="kobospan" id="kobo.297.1">the dataset:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.298.1">
# Import necessary libraries
from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
# Load the dataset
imdb_data = load_dataset('imdb', split='train[:1000]')  # Loading only 1000 samples for a toy example
# Define the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# Preprocess the data
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)
imdb_data = imdb_data.map(encode, batched=True)
# Format the dataset to PyTorch tensors
imdb_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.299.1">With our dataset </span><a id="_idIndexMarker793" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.300.1">loaded up, we can run some training </span><a id="_idIndexMarker794" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.301.1">code</span><a id="_idIndexMarker795" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.302.1"> to update our BERT model on our </span><span><span class="kobospan" id="kobo.303.1">labeled data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.304.1">
# Define the model
model = BertForSequenceClassification.from_pretrained(
  'bert-base-uncased', num_labels=2)
# Define the training arguments
training_args = TrainingArguments(
  output_dir='./results',
  num_train_epochs=1,
  per_device_train_batch_size=4
)
# Define the trainer
trainer = Trainer(model=model, args=training_args, train_dataset=imdb_data)
# Train the model
trainer.train()
# Save the model
model.save_pretrained('./my_bert_model')</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.305.1">Once we have</span><a id="_idIndexMarker796" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.306.1"> our</span><a id="_idIndexMarker797" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.307.1"> saved model, we can use the following code to run the model against </span><span><span class="kobospan" id="kobo.308.1">unseen data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.309.1">
from transformers import pipeline
# Define the sentiment analysis pipeline
nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
# Use the pipeline to predict the sentiment of a new review
review = "The movie was fantastic! </span><span class="kobospan1" id="kobo.309.2">I enjoyed every moment of it."
</span><span class="kobospan1" id="kobo.309.3">result = nlp(review)
# Print the result
print(f"label: {result[0]['label']}, with score: {round(result[0]['score'], 4)}")
# "The movie was fantastic</span><a id="_idTextAnchor362" class="pcalibre pcalibre1 calibre162"/><a id="_idTextAnchor363" class="pcalibre pcalibre1 calibre162"/><span class="kobospan1" id="kobo.310.1">! </span><span class="kobospan1" id="kobo.310.2">I enjoyed every moment of it."
</span><span class="kobospan1" id="kobo.310.3"># POSITIVE: 99%</span></pre>
<h3 class="calibre8"><span class="kobospan" id="kobo.311.1">Example – TL for image classification</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.312.1">We could take a </span><a id="_idIndexMarker798" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.313.1">pre-trained model such as ResNet or the Vision Transformer (shown in </span><span><em class="italic"><span class="kobospan" id="kobo.314.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.315.1">.10</span></em><span class="kobospan" id="kobo.316.1">), initially trained on a large-scale image dataset such as ImageNet. </span><span class="kobospan" id="kobo.316.2">This model has already learned to detect various features from images, from simple shapes to complex objects. </span><span class="kobospan" id="kobo.316.3">We can take advantage of this knowledge, fine-tuning </span><a id="_idTextAnchor364" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.317.1">the model on a custom image </span><span><span class="kobospan" id="kobo.318.1">classification task:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer311">
<span class="kobospan" id="kobo.319.1"><img alt="Figure 12.10 – The Vision Transformer" src="image/B19488_12_10.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.320.1">Figure 12.10 – The Vision Transformer</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.321.1">The Vision Transformer is like a BERT model for images. </span><span class="kobospan" id="kobo.321.2">It relies on many of the same principles, except instead of text tokens, it uses segments of images as “</span><span><span class="kobospan" id="kobo.322.1">tokens” instead.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.323.1">The following code block shows an end-to-end code example of fine-tuning the Vision Transformer on an image classification task. </span><span class="kobospan" id="kobo.323.2">The code should look very similar to the BERT code from the previous section because the aim of the </span><strong class="source-inline"><span class="kobospan" id="kobo.324.1">transformers</span></strong><span class="kobospan" id="kobo.325.1"> library is to standardize training and usage of modern pre-trained models so that no matter what task you are performing, they can offer a relatively unified training</span><a id="_idIndexMarker799" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.326.1"> and </span><span><span class="kobospan" id="kobo.327.1">inference experience.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.328.1">Let’s begin by loading up our data and taking a look at the kinds of images we have (seen in </span><span><em class="italic"><span class="kobospan" id="kobo.329.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.330.1">.11</span></em><span class="kobospan" id="kobo.331.1">). </span><span class="kobospan" id="kobo.331.2">Note that we are only going to use 1% of the dataset to show that you really don’t need that much data to get a lot out of </span><span><span class="kobospan" id="kobo.332.1">pre-trained models!</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.333.1">
# Import necessary libraries
from datasets import load_dataset
from transformers import ViTImageProcessor, ViTForImageClassification
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torch
from torchvision.transforms.functional import to_pil_image
# Load the CIFAR10 dataset using Hugging Face datasets
# Load only the first 1% of the train and test sets
train_dataset = load_dataset("cifar10", split="train[:1%]")
test_dataset = load_dataset("cifar10", split="test[:1%]")
# Define the feature extractor
feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
# Preprocess the data
def transform(examples):
    # print(examples)
    # Convert to list of PIL Images
    examples['pixel_values'] = feature_extractor(images=examples["img"], return_tensors="pt")["pixel_values"]
    return examples
# Apply the transformations
train_dataset = train_dataset.map(
transform, batched=True, batch_size=32
).with_format('pt')
test_dataset = test_dataset.map(
transform, batched=True, batch_size=32
).with_format('pt')</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.334.1">We can similarly</span><a id="_idIndexMarker800" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.335.1"> use the model using the </span><span><span class="kobospan" id="kobo.336.1">following code:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer312">
<span class="kobospan" id="kobo.337.1"><img alt="Figure 12.11 – A single example from CIFAR10 showing an airplane" src="image/B19488_12_11.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.338.1">Figure 12.11 – A single example from CIFAR10 showing an airplane</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.339.1">Now, we can</span><a id="_idIndexMarker801" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.340.1"> train our pre-trained </span><span><span class="kobospan" id="kobo.341.1">Vision Transformer:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.342.1">
# Define the model
model = ViTForImageClassification.from_pretrained(
'google/vit-base-patch16-224',
num_labels=10, ignore_mismatched_sizes=True
)
LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
model.config.id2label = LABELS
# Define a function for computing metrics
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {"accuracy": accuracy_score(labels, preds)}
# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5,
    per_device_train_batch_size=4,
    load_best_model_at_end=True,
    # Save and evaluate at the end of each epoch
    evaluation_strategy='epoch',
    save_strategy='epoch'
)
# Define the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.343.1">Our final model </span><a id="_idIndexMarker802" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.344.1">has about 95% accuracy on 1% of the test set. </span><span class="kobospan" id="kobo.344.2">We can now use our new classifier on unseen images, as in this next </span><span><span class="kobospan" id="kobo.345.1">code block:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.346.1">
from PIL import Image
from transformers import pipeline
# Define an image classification pipeline
classification_pipeline = pipeline(
'image-classification',
model=model,
feature_extractor=feature_extractor
)
# Load an image
image = Image.open('stock_image_plane.jpg')
# Use the pipeline to classify the image
result = classification_pipeline(image)</span></pre>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.347.1">Figure 12</span></em></span><em class="italic"><span class="kobospan" id="kobo.348.1">.12</span></em><span class="kobospan" id="kobo.349.1"> shows the result of this single classification, and it looks like it did </span><span><span class="kobospan" id="kobo.350.1">pretty well:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer313">
<span class="kobospan" id="kobo.351.1"><img alt="Figure 12.12 – Our classifier predicting a stock image of a plane correctly" src="image/B19488_12_12.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.352.1">Figure 12.12 – Our classifier predicting a stock image of a plane correctly</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.353.1">With minimal</span><a id="_idIndexMarker803" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.354.1"> labeled data, we can leverage TL to turn mode</span><a id="_idTextAnchor365" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.355.1">ls off the shelf into powerhouse </span><span><span class="kobospan" id="kobo.356.1">predictive models.</span></span></p>
<h1 id="_idParaDest-186" class="calibre6"><a id="_idTextAnchor366" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.357.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.358.1">Our exploration of pre-trained models gave us insight into how these models, trained on extensive data and time, provide a solid foundation for us to build upon. </span><span class="kobospan" id="kobo.358.2">They help us overcome constraints related to computational resources and data availability. </span><span class="kobospan" id="kobo.358.3">Notably, we familiarized ourselves with image-based models such as VGG16 and ResNet, and text-based models such as BERT and GPT, adding them to </span><span><span class="kobospan" id="kobo.359.1">our repertoire.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.360.1">Our voyage continued into the domain of TL, where we learned its fundamentals, recognized its versatile applications, and acknowledged its different forms—inductive, transductive, and unsupervised. </span><span class="kobospan" id="kobo.360.2">Each type, with its unique characteristics, adds a different dimension to our ML toolbox. </span><span class="kobospan" id="kobo.360.3">Through practical examples, we saw these concepts in action, applying a BERT model for text classification and a Vision Transformer for </span><span><span class="kobospan" id="kobo.361.1">image classification.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.362.1">But, as we’ve come to appreciate, TL and pre-trained models, while powerful, are not the solution to all data science problems. </span><span class="kobospan" id="kobo.362.2">They shine in the right circumstances, and it’s our role as data scientists to discern when and how to deploy these powerful </span><span><span class="kobospan" id="kobo.363.1">methods effectively.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.364.1">As this chapter concludes, the understanding we’ve gleaned of TL and pre-trained models not only equips us to handle real-world ML tasks but also paves the way for tackling more advanced concepts and techniques. </span><span class="kobospan" id="kobo.364.2">By introducing more steps into our ML process (such as pre-training), we are opening ourselves up to more potential errors. </span><span class="kobospan" id="kobo.364.3">Our next chapter will also begin to investigate a hidden side of TL: transferring bias and </span><span><span class="kobospan" id="kobo.365.1">tackling drift.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.366.1">Moving forward, consider diving deeper into other pre-trained models, investigating other variants of TL, or even attempting to create your own pre-trained models. </span><span class="kobospan" id="kobo.366.2">Whichever path you opt for, remember—the world of ML is vast and always evolving. </span><span class="kobospan" id="kobo.366.3">Stay curious and </span><span><span class="kobospan" id="kobo.367.1">keep learning!</span></span></p>
</div>
</body></html>