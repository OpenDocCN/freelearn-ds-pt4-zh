- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Transfer Learning and Pre-Trained Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as one wouldn’t try to reinvent the wheel, in the world of data science
    and **machine learning** (**ML**), it’s often more efficient to build upon existing
    knowledge. This is where the concepts of **transfer learning** (**TL**) and pre-trained
    models come into play, two incredibly important tools in a data scientist’s repertoire.
  prefs: []
  type: TYPE_NORMAL
- en: TL is almost like a shortcut in ML. Instead of taking a model architecture that
    has never seen data before, such as a Logistic Regression model or a Random Forest
    model, imagine being able to take a model trained on one task and then repurposing
    it for a different, yet related task. That’s TL in a nutshell – leveraging existing
    knowledge to learn new things more efficiently. It’s a concept that echoes throughout
    many facets of life and is a key technique in data science.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained models are off-the-shelf components, ready to be used right out
    of the box. They’re ML models that have been trained on large datasets, capturing
    an immense amount of information about the task they were trained on. When it
    comes to tackling new tasks, these pre-trained models provide a substantial head
    start. The importance of TL and pre-trained models cannot be overstated in modern
    ML. They are fundamental to many cutting-edge applications in data science, from
    **computer vision** (**CV**) tasks such as image recognition to **natural language
    processing** (**NLP**) tasks such as **sentiment analysis** (**SA**). By leveraging
    these techniques, we can achieve impressive results even with limited data or
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we will investigate these concepts, exploring their
    nuances, applications, and potential:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding pre-trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of TL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TL with **Bidirectional Encoder Representations from Transformers** (**BERT**)
    and **Generative Pre-trained** **Transformer** (**GPT**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll also walk through practical examples that highlight their power in real-world
    scenarios. By the end of this chapter, you’ll have a solid understanding of TL
    and pre-trained models and be equipped to harness their potential in your own
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding pre-trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pre-trained models are like learning from the experience of others. These models
    have been trained on extensive datasets, learning patterns, and features that
    make them adept at their tasks. Think of it as if a model has been reading thousands
    of books on a subject, absorbing all that information. When we use a pre-trained
    model, we’re leveraging all that prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: In general, pre-training steps are not necessarily “useful” to a human, but
    it is crucial to a model to simply learn about a domain and about a medium. Pre-training
    helps models learn how language works in general but not how to classify sentiments
    or detect an object.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of using pre-trained models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The benefits of using pre-trained models are numerous. For starters, they save
    us a lot of time. Training a model from scratch can be a time-consuming process,
    but using a pre-trained model gives us a head start. Furthermore, these models
    often lead to better performance, especially when our dataset is relatively small.
    The reason? Pre-trained models have seen much more data than we usually have at
    our disposal, and they’ve learned a lot from it.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12**.1* shows the result of a study done on **large language models**
    (**LLMs**) such as BERT where one of the goals was to show that pre-training was
    leading to some obvious patterns in how BERT was recognizing basic grammatical
    constructs. The study visualized that models post pre-training were able to recognize
    what we would consider as obvious grammatical patterns, such as pronoun-antecedent
    relationships and direct object/verb relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – A study visualizing how BERT’s pre-training allowed it to pick
    up on common grammatical constructs without ever being told what they were](img/B19488_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – A study visualizing how BERT’s pre-training allowed it to pick
    up on common grammatical constructs without ever being told what they were
  prefs: []
  type: TYPE_NORMAL
- en: BERT, of course, is not the only model that undergoes pre-training, and this
    practice is not even limited to text-based models.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly used pre-trained models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-trained models come in all shapes and sizes, each tailored to different
    types of data and tasks. Let’s talk about some of the most popular ones.
  prefs: []
  type: TYPE_NORMAL
- en: Image-based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For tasks related to images, models such as the Vision Transformer (more on
    this one later in this chapter), models from the **Visual Geometry Group** (**VGG**)
    (as seen in *Figure 12**.2*), and ResNet are some common options to choose from.
    Models from these families have been trained on tens of thousands of images, learning
    to recognize everything from shapes and textures to complex objects. They’re incredibly
    versatile and can be fine-tuned for a wide array of image-based tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 12.2 – The VGG16 model (from the VGG) is a convolutional neural network\
    \ (CNN) that can be pre-trained on ima\uFEFFge data](img/B19488_12_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – The VGG16 model (from the VGG) is a convolutional neural network
    (CNN) that can be pre-trained on image data
  prefs: []
  type: TYPE_NORMAL
- en: Text-based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When it comes to text, models such as BERT (*Figure 12**.3*) and GPT are among
    the most used language models. They were first originally architected in 2018
    (only 1 year after the primary Transformer architecture that both GPT and BERT
    are based on was even proposed or mentioned), and they’ve, as with their image
    counterparts, been trained on vast amounts of text data, learning the intricacies
    of human language. Whether it’s understanding the sentiment behind a tweet or
    answering questions about a piece of text, these models are up to the task. As
    we move forward, we’ll see how these pre-trained models can be combined with TL
    to tackle new tasks with impressive efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – A figure from the original blog post from Google open sourcing
    BERT in 2018 calls out OpenAI’s GPT-1 model](img/B19488_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – A figure from the original blog post from Google open sourcing
    BERT in 2018 calls out OpenAI’s GPT-1 model
  prefs: []
  type: TYPE_NORMAL
- en: This came out a few months prior, highlighting BERT’s ability to process more
    relationships between tokens with the relatively same number of parameters than
    GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding BERT’s pre-training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most impressive feats of TL can be observed in BERT, a pre-trained
    model that revolutionized the NLP landscape. Two fundamental training tasks drive
    BERT’s robust understanding of language semantics and relationships: **masked
    language modeling** (**MLM**) and **next sentence prediction** (**NSP**). Let’s
    break them down and see how each one contributes to BERT’s language processing
    abilities.'
  prefs: []
  type: TYPE_NORMAL
- en: MLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**MLM** (visualized in *Figure 12**.4*) is a key component of BERT’s pre-training
    process. In essence, MLM works by randomly replacing approximately 15% of the
    words in the input data with a special (*MASK*) token. It’s then up to BERT to
    figure out which word was replaced, essentially filling in the blank. Think of
    it as a sophisticated game of *Mad Libs* that BERT plays during its training.
    If we were to take a sentence such as “Stop at the light,” MLM might replace “light”
    with (*MASK*), prompting BERT to predict the missing word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – The MLM pre-training task has BERT filling in missing tokens
    from a sequence of tokens](img/B19488_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – The MLM pre-training task has BERT filling in missing tokens from
    a sequence of tokens
  prefs: []
  type: TYPE_NORMAL
- en: This process of MLM helps BERT understand the context around each word and construct
    meaningful relationships between different parts of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: NSP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**NSP** (visualized in *Figure 12**.5*) is the second crucial part of BERT’s
    pre-training regimen. NSP is a binary classification problem, where BERT is tasked
    with determining whether a provided sentence *B* follows sentence *A* in the original
    text. In the grand scheme of language understanding, this is like asking BERT
    to understand the logical flow of sentences in a piece of text. This ability to
    predict whether sentence *B* logically follows sentence *A* allows BERT to understand
    more nuanced, higher-level linguistic structures and narrative flows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – In the NSP pre-training task, BERT is looking at two thoughts
    and deciding if the second phrase would come directly after the first phrase](img/B19488_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – In the NSP pre-training task, BERT is looking at two thoughts
    and deciding if the second phrase would come directly after the first phrase
  prefs: []
  type: TYPE_NORMAL
- en: 'Put another way, MLM helps BERT get a grasp on intricate connections between
    words and their contexts, while NSP equips BERT with an understanding of the relationships
    between sentences. It’s this combination that makes BERT such a powerful tool
    for a range of NLP tasks. Between NSP and MLM (*Figure 12**.6*), BERT’s training
    is meant to give it a sense of how tokens affect phrase meanings (MLM) and how
    phrases work together to form larger thoughts (NSP):'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 12.6 – BERT’s pre-training helps it to learn about la\uFEFFnguage\
    \ in general](img/B19488_12_06.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – BERT’s pre-training helps it to learn about language in general
  prefs: []
  type: TYPE_NORMAL
- en: TL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a strong grasp of pre-trained models, let’s shift our attention
    toward the other compelling facet of this equation: **TL**. In essence, TL is
    the application of knowledge gained from one problem domain (source) to a different
    but related problem domain (target).'
  prefs: []
  type: TYPE_NORMAL
- en: The process of TL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TL is all about adaptability. It takes a model that’s been trained on one task
    and adapts it to perform a different but related task. They might not have solved
    that particular mystery before, but their skills and experience can be adapted
    to the task at hand. TL is particularly useful when we have a small amount of
    data for our specific task or when our task is very similar to the one the original
    model was trained on. In these situations, TL can save time and resources while
    boosting our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of TL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a moment to get familiar with the diverse landscape of TL. It’s not
    a single monolithic idea, but rather a collection of varied strategies that fall
    under one umbrella term. There’s a type of TL for just about every scenario you
    might come across.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive TL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First up, we have **inductive TL** (**ITL**). This is all about using what’s
    already been learned and applying it in new, but related, scenarios. The key here
    is the generalization of learned features from one task—let’s call this the source
    task—and then fine-tuning them to perform well on another task—the target task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a model that’s spent its virtual lifetime learning from a broad text
    corpus, getting to grips with the complexities of language, grammar, and context.
    Now, we have a different task on our hands: SA on product reviews. With ITL, our
    model can use what it’s already learned about language and fine-tune itself to
    become a pro at detecting sentiment. *Figure 12**.7* visualizes how we will approach
    ITL in a later section. The main idea is to take a pre-trained model from a model
    repository such as `HuggingFace` and perform any potential model modifications
    for our task, throw some labeled data at it (like we would any other ML model),
    and watch it learn:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 12.7 – An ITL process might involve training BERT on\uFEFF supervised\
    \ labeled data](img/B19488_12_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – An ITL process might involve training BERT on supervised labeled
    data
  prefs: []
  type: TYPE_NORMAL
- en: Transductive TL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our second type of TL is called **transductive TL** (**TTL**) and is a bit more
    nebulous in its task. Rather than being given a concrete second task to perform
    (such as classification), our model instead is asked to adapt to new data without
    losing its grounding in the original task. It’s a good choice when we have a bunch
    of unlabeled data for our target task.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a model was trained on one image dataset to identify different
    objects and we have a new, unlabeled image dataset, we could ask the model to
    use its knowledge from the source task to label the new dataset, even without
    any explicit labels provided.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised TL – feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-trained models aren’t just useful for their predictive abilities. They’re
    also treasure troves of features, ripe for extraction. Using **unsupervised TL**
    (**UTL**), our model, which was trained on a vast text corpus, can use its understanding
    of language to find patterns and help us divide the text into meaningful categories.
    Feature extraction with pre-trained models involves using a pre-trained model
    to transform raw data into a more useful format—one that highlights important
    features and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: These are the three main flavors of TL, each with its own approach and ideal
    use cases. In the wide world of ML, it’s all about picking the right tool for
    the job, and TL definitely gives us a whole toolbox to choose from. With this
    newfound understanding of TL, we’re well equipped to start putting it into practice.
    In the next section, we’ll see how TL and pre-trained models can come together
    to conquer new tasks with ease.
  prefs: []
  type: TYPE_NORMAL
- en: TL with BERT and GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having grasped the fundamental concepts of pre-trained models and TL, it’s
    time to put theory into practice. It’s one thing to know the ingredients; it’s
    another to know how to mix them into a delicious dish with them. In this section,
    we will take some models that have already learned a lot from their pre-training
    and fine-tune them to perform a new, related task. This process involves adjusting
    the model’s parameters to better suit the new task, much like fine-tuning a musical
    instrument:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – ITL](img/B19488_12_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – ITL
  prefs: []
  type: TYPE_NORMAL
- en: ITL takes a pre-trained model that was generally trained on a semi-supervised
    (or unsupervised) task and then is given labeled data to learn a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of TL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at some examples of TL with specific pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Fine-tuning a pre-trained model for text classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a simple text classification problem. Suppose we need to analyze customer
    reviews and determine whether they’re positive or negative. We have a dataset
    of reviews, but it’s not nearly large enough to train a **deep learning** (**DL**)
    model from scratch. We will fine-tune BERT on a text classification task, allowing
    the model to adapt its existing knowledge to our specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: We will have to move away from the popular scikit-learn library to another popular
    library called `transformers`, which was created by HuggingFace (the pre-trained
    model repository I mentioned earlier) as `scikit-learn` does not (yet) support
    Transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12**.9* shows how we will have to take the original BERT model and
    make some minor modifications to it to perform text classification. Luckily, the
    `transformers` package has a built-in class to do this for us called `BertForSequenceClassification`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Simplest text classification case](img/B19488_12_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Simplest text classification case
  prefs: []
  type: TYPE_NORMAL
- en: In many TL cases, we need to architect additional layers. In the simplest text
    classification case, we add a classification layer on top of a pre-trained BERT
    model so that it can perform the kind of classification we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows an end-to-end code example of fine-tuning BERT
    on a text classification task. Note that we are also using a package called `datasets`,
    also made by `HuggingFace`, to load a sentiment classification task from IMDb
    reviews. Let’s begin by loading up the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With our dataset loaded up, we can run some training code to update our BERT
    model on our labeled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our saved model, we can use the following code to run the model
    against unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Example – TL for image classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We could take a pre-trained model such as ResNet or the Vision Transformer
    (shown in *Figure 12**.10*), initially trained on a large-scale image dataset
    such as ImageNet. This model has already learned to detect various features from
    images, from simple shapes to complex objects. We can take advantage of this knowledge,
    fine-tuning the model on a custom image classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – The Vision Transformer](img/B19488_12_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – The Vision Transformer
  prefs: []
  type: TYPE_NORMAL
- en: The Vision Transformer is like a BERT model for images. It relies on many of
    the same principles, except instead of text tokens, it uses segments of images
    as “tokens” instead.
  prefs: []
  type: TYPE_NORMAL
- en: The following code block shows an end-to-end code example of fine-tuning the
    Vision Transformer on an image classification task. The code should look very
    similar to the BERT code from the previous section because the aim of the `transformers`
    library is to standardize training and usage of modern pre-trained models so that
    no matter what task you are performing, they can offer a relatively unified training
    and inference experience.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by loading up our data and taking a look at the kinds of images
    we have (seen in *Figure 12**.11*). Note that we are only going to use 1% of the
    dataset to show that you really don’t need that much data to get a lot out of
    pre-trained models!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can similarly use the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – A single example from CIFAR10 showing an airplane](img/B19488_12_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – A single example from CIFAR10 showing an airplane
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can train our pre-trained Vision Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final model has about 95% accuracy on 1% of the test set. We can now use
    our new classifier on unseen images, as in this next code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 12**.12* shows the result of this single classification, and it looks
    like it did pretty well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Our classifier predicting a stock image of a plane correctly](img/B19488_12_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – Our classifier predicting a stock image of a plane correctly
  prefs: []
  type: TYPE_NORMAL
- en: With minimal labeled data, we can leverage TL to turn models off the shelf into
    powerhouse predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our exploration of pre-trained models gave us insight into how these models,
    trained on extensive data and time, provide a solid foundation for us to build
    upon. They help us overcome constraints related to computational resources and
    data availability. Notably, we familiarized ourselves with image-based models
    such as VGG16 and ResNet, and text-based models such as BERT and GPT, adding them
    to our repertoire.
  prefs: []
  type: TYPE_NORMAL
- en: Our voyage continued into the domain of TL, where we learned its fundamentals,
    recognized its versatile applications, and acknowledged its different forms—inductive,
    transductive, and unsupervised. Each type, with its unique characteristics, adds
    a different dimension to our ML toolbox. Through practical examples, we saw these
    concepts in action, applying a BERT model for text classification and a Vision
    Transformer for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: But, as we’ve come to appreciate, TL and pre-trained models, while powerful,
    are not the solution to all data science problems. They shine in the right circumstances,
    and it’s our role as data scientists to discern when and how to deploy these powerful
    methods effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'As this chapter concludes, the understanding we’ve gleaned of TL and pre-trained
    models not only equips us to handle real-world ML tasks but also paves the way
    for tackling more advanced concepts and techniques. By introducing more steps
    into our ML process (such as pre-training), we are opening ourselves up to more
    potential errors. Our next chapter will also begin to investigate a hidden side
    of TL: transferring bias and tackling drift.'
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, consider diving deeper into other pre-trained models, investigating
    other variants of TL, or even attempting to create your own pre-trained models.
    Whichever path you opt for, remember—the world of ML is vast and always evolving.
    Stay curious and keep learning!
  prefs: []
  type: TYPE_NORMAL
