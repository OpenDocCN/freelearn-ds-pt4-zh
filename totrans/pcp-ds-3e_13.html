<html><head></head><body>
<div id="_idContainer316" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-187"><a id="_idTextAnchor367" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.1.1">13</span></h1>
<h1 id="_idParaDest-188" class="calibre6"><a id="_idTextAnchor368" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.2.1">Mitigating Algorithmic Bias and Tackling Model and Data Drift</span></h1>
<p class="calibre3"><a id="_idTextAnchor369" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.3.1">If you’re playing in the arena of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">machine learning</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">ML</span></strong><span class="kobospan" id="kobo.7.1">) and data science, you’re going to run into some hurdles. </span><span class="kobospan" id="kobo.7.2">You can count on meeting two challenges: </span><strong class="bold"><span class="kobospan" id="kobo.8.1">algorithmic bias</span></strong><span class="kobospan" id="kobo.9.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.10.1">model and data drift</span></strong><span class="kobospan" id="kobo.11.1">. </span><span class="kobospan" id="kobo.11.2">They’re like the tricky questions in a pop quiz – you might not see them coming, but you’d better be prepared to </span><span><span class="kobospan" id="kobo.12.1">handle them.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.13.1">Algorithmic bias can creep into our models, and when it does, it’s not a good look. </span><span class="kobospan" id="kobo.13.2">It can lead to unfair results, and, quite frankly, it’s just not cool. </span><span class="kobospan" id="kobo.13.3">But don’t worry – we’re going to tackle it head on and talk about ways to </span><span><span class="kobospan" id="kobo.14.1">mitigate it.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.15.1">Even if we consider bias, over time, changes can happen that make our models less accurate. </span><span class="kobospan" id="kobo.15.2">It’s like when your favorite shirt shrinks in the wash – it’s not the shirt’s fault, but it doesn’t fit like it used to. </span><span class="kobospan" id="kobo.15.3">The same happens with our models. </span><span class="kobospan" id="kobo.15.4">They may have been a perfect fit when we first created them, but as data changes, they might need </span><span><span class="kobospan" id="kobo.16.1">some adjustments.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.17.1">In this chapter, we’ll get into the nitty-gritty of these important issues. </span><span class="kobospan" id="kobo.17.2">We’ll look at where algorithmic bias comes from and how to minimize it. </span><span class="kobospan" id="kobo.17.3">We’ll also dive into the concepts of model drift and data drift and discuss some ways to </span><span><span class="kobospan" id="kobo.18.1">handle them.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.19.1">We will be covering the following topics in </span><span><span class="kobospan" id="kobo.20.1">the chapter:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.21.1">Understanding </span><span><span class="kobospan" id="kobo.22.1">algorithmic bias</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.23.1">Sources of </span><span><span class="kobospan" id="kobo.24.1">algorithmic bias</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.25.1">Measuring bias</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.26.1">Consequences of unaddressed bias and the importance </span><span><span class="kobospan" id="kobo.27.1">of fairness</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.28.1">Mitigating </span><span><span class="kobospan" id="kobo.29.1">algorithmic bias</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.30.1">Bias in </span><strong class="bold"><span class="kobospan" id="kobo.31.1">large language </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.32.1">models</span></strong></span><span><span class="kobospan" id="kobo.33.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.34.1">LLMs</span></strong></span><span><span class="kobospan" id="kobo.35.1">)</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.36.1">Emerging techniques in bias and fairness </span><span><span class="kobospan" id="kobo.37.1">in ML</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.38.1">Understanding model drift </span><span><span class="kobospan" id="kobo.39.1">and decay</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.40.1">Mitigating drift</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.41.1">Just to spice things up, we’ll also take a peek at how algorithmic bias can show up in LLMs. </span><span class="kobospan" id="kobo.41.2">By the time we’re done, you’ll have a solid understanding of these challenges and be ready to face them in your own projects. </span><span class="kobospan" id="kobo.41.3">So, let’s roll up our sleeves and </span><span><span class="kobospan" id="kobo.42.1">get started!</span></span></p>
<h1 id="_idParaDest-189" class="calibre6"><a id="_idTextAnchor370" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.43.1">Understanding algorithmic bias</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.44.1">Algorithmic bias </span><a id="_idIndexMarker804" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.45.1">is a pivotal issue in the world of ML. </span><span class="kobospan" id="kobo.45.2">It occurs when a system, intentionally or not, generates outputs that are unfair or systematically prejudiced toward certain individuals or groups. </span><span class="kobospan" id="kobo.45.3">This prejudice often originates from the fact that these systems learn from existing data, which itself can be riddled with inherent </span><span><span class="kobospan" id="kobo.46.1">societal bias.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">Fairness, as it relates to ML, is defined as the absence of any bias. </span><span class="kobospan" id="kobo.47.2">While it might sound simple, achieving fairness can be an intricate process that calls for careful management at every step of </span><span><span class="kobospan" id="kobo.48.1">model creation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.49.1">To paint a more detailed picture, let’s consider protected features. </span><span class="kobospan" id="kobo.49.2">These are attributes that could potentially introduce bias into the system. </span><span class="kobospan" id="kobo.49.3">They can be legally mandated, such as race and gender, or stem from organizational values, such as location or zip code. </span><span class="kobospan" id="kobo.49.4">While seemingly benign, these features, when used in an ML model, can result in decisions that are biased </span><span><span class="kobospan" id="kobo.50.1">or discriminatory.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.51.1">Diving deeper, we find</span><a id="_idIndexMarker805" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.52.1"> two major types of </span><span><span class="kobospan" id="kobo.53.1">algorithmic bias:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.54.1">Disparate impact occurs when a model explicitly relies on protected attributes, favoring a certain group </span><span><span class="kobospan" id="kobo.55.1">over others</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.56.1">In contrast, disparate treatment arises when a model implicitly uses protected attributes through related variables, thereby indirectly resulting in </span><span><span class="kobospan" id="kobo.57.1">biased outcomes</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.58.1">A prime example of disparate treatment can be someone’s zip code, which might predominantly contain a particular race or socioeconomic status. </span><span class="kobospan" id="kobo.58.2">This, in turn, could lead to skewed predictions or decisions that inadvertently favor or disfavor that particular group. </span><span class="kobospan" id="kobo.58.3">Similarly, variables such as whether someone has been arrested before can introduce bias, given that certain groups have historically faced more arrests due to </span><span><span class="kobospan" id="kobo.59.1">societal bias.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.60.1">An initial approach to addressing bias is “unawareness,” which entails removing any explicit mention of protected features. </span><span class="kobospan" id="kobo.60.2">However, this approach is a low bar in addressing bias, as it doesn’t account for disparate impacts that can </span><span><span class="kobospan" id="kobo.61.1">still occur.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.62.1">Fairness can</span><a id="_idIndexMarker806" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.63.1"> also be approached via statistical measures such as statistical parity and equalized odds. </span><span class="kobospan" id="kobo.63.2">Statistical parity states that a model’s predictions should be independent of any given sensitive feature. </span><span class="kobospan" id="kobo.63.3">For example, a model predicting recidivism should do so equally, irrespective of race. </span><span class="kobospan" id="kobo.63.4">However, this approach ignores any actual relationship between the label and the sensitive attribute, which can result in </span><span><span class="kobospan" id="kobo.64.1">biased outcomes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.65.1">Here, we have individual and group fairness. </span><span class="kobospan" id="kobo.65.2">The former ensures similar individuals have similar outcomes, while the latter insists on equal statistical outcomes across groups divided by protected attributes. </span><span class="kobospan" id="kobo.65.3">Each brings its own unique perspective on fairness and </span><span><span class="kobospan" id="kobo.66.1">its achievement.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.67.1">Equalized odds, on the other hand, propose that a model’s predictions should be independent of any sensitive feature, conditional on the response value. </span><span class="kobospan" id="kobo.67.2">This approach encourages the model to be more accurate across all groups but can overlook larger socioeconomic reasons that might result in a certain group falling into a label </span><span><span class="kobospan" id="kobo.68.1">more frequently.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.69.1">By understanding the nuances of algorithmic bias, we can better devise strategies to prevent, identify, and mitigate bias in </span><span><span class="kobospan" id="kobo.70.1">ML model</span><a id="_idTextAnchor371" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.71.1">s.</span></span></p>
<h2 id="_idParaDest-190" class="calibre7"><a id="_idTextAnchor372" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.72.1">Types of bias</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Understanding bias in</span><a id="_idIndexMarker807" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.74.1"> ML requires us to categorize it into different types based on its source and manifestation. </span><span class="kobospan" id="kobo.74.2">This helps us to identify the root cause and implement targeted mitigation strategies. </span><span class="kobospan" id="kobo.74.3">Here are the key types of bias </span><span><span class="kobospan" id="kobo.75.1">to consider:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.76.1">Disparate impact</span></strong><span class="kobospan" id="kobo.77.1">: Disparate impact</span><a id="_idIndexMarker808" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.78.1"> refers to situations where an ML system’s outcomes disproportionately are a disadvantage to a specific group, typically a protected class defined by attributes such as race, sex, age, or religion. </span><span class="kobospan" id="kobo.78.2">This often occurs even when the protected attribute isn’t explicitly included in </span><span><span class="kobospan" id="kobo.79.1">the model.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.80.1">Disparate treatment</span></strong><span class="kobospan" id="kobo.81.1">: In </span><a id="_idIndexMarker809" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.82.1">contrast to disparate impact, disparate treatment happens when an ML model treats individuals differently based on their membership of a protected group, implying a direct, discriminatory effect. </span><span class="kobospan" id="kobo.82.2">This occurs when the protected attribute is explicitly used in the </span><span><span class="kobospan" id="kobo.83.1">decision-making process.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.84.1">Pre-existing bias</span></strong><span class="kobospan" id="kobo.85.1">: Also </span><a id="_idIndexMarker810" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.86.1">known as historical bias, pre-existing bias emerges when the data used to train an ML model reflects existing prejudices or societal biases. </span><span class="kobospan" id="kobo.86.2">The model, in essence, learns these biases and propagates them in </span><span><span class="kobospan" id="kobo.87.1">its predictions.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.88.1">Sample bias</span></strong><span class="kobospan" id="kobo.89.1">: Sample bias </span><a id="_idIndexMarker811" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.90.1">occurs when the data used to train a model isn’t representative of the population it’s supposed to serve. </span><span class="kobospan" id="kobo.90.2">This can result in a model that performs well on the training data but poorly on the actual data it encounters in production, leading to </span><span><span class="kobospan" id="kobo.91.1">unfair outcomes.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.92.1">Measurement bias</span></strong><span class="kobospan" id="kobo.93.1">: Measurement bias arises when there are systematic errors in the </span><a id="_idIndexMarker812" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.94.1">way data is collected or measured. </span><span class="kobospan" id="kobo.94.2">This can distort the training data and cause the model to learn </span><span><span class="kobospan" id="kobo.95.1">incorrect associations.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.96.1">Aggregation bias</span></strong><span class="kobospan" id="kobo.97.1">: Aggregation</span><a id="_idIndexMarker813" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.98.1"> bias occurs when a model oversimplifies or fails to capture the diversity and nuances within a group. </span><span class="kobospan" id="kobo.98.2">This can happen when distinct subgroups are treated as one homogeneous group, which can lead to the model making incorrect or </span><span><span class="kobospan" id="kobo.99.1">unfair generalizations.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.100.1">Proxy bias</span></strong><span class="kobospan" id="kobo.101.1">: Proxy bias</span><a id="_idIndexMarker814" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.102.1"> takes place when a model uses an attribute as a stand-in for a protected attribute. </span><span class="kobospan" id="kobo.102.2">For instance, zip codes might be used as a proxy for race or income level, leading to biased outcomes even when the protected attribute isn’t </span><span><span class="kobospan" id="kobo.103.1">directly used.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.104.1">Each of these types of bias has different implications for fairness in ML and requires different strategies to detect and mitigate. </span><span class="kobospan" id="kobo.104.2">By identifying the type of bias at play, we can take targeted steps to reduce its impact and work toward more fair and equitable </span><span><span class="kobospan" id="kobo.105.1">M</span><a id="_idTextAnchor373" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.106.1">L systems.</span></span></p>
<h1 id="_idParaDest-191" class="calibre6"><a id="_idTextAnchor374" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.107.1">Sources of algorithmic bias</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.108.1">ML models, grounded in the learnings from past data, may unintentionally propagate bias present in their training datasets. </span><span class="kobospan" id="kobo.108.2">Recognizing the roots of this bias is a vital first step toward </span><span><span class="kobospan" id="kobo.109.1">fairer models:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.110.1">One such source is </span><strong class="bold"><span class="kobospan" id="kobo.111.1">historical bias</span></strong><span class="kobospan" id="kobo.112.1">. </span><span class="kobospan" id="kobo.112.2">This </span><a id="_idIndexMarker815" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.113.1">form of bias mirrors existing prejudices and systemic inequalities present in society. </span><span class="kobospan" id="kobo.113.2">An example would be a recruitment model trained on a company’s past hiring data. </span><span class="kobospan" id="kobo.113.3">If the organization historically favored a specific group for certain roles, the model could replicate these biases, continuing the cycle </span><span><span class="kobospan" id="kobo.114.1">of bias.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.115.1">Representation or sample bias</span></strong><span class="kobospan" id="kobo.116.1"> is another significant contributor. </span><span class="kobospan" id="kobo.116.2">It occurs when certain</span><a id="_idIndexMarker816" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.117.1"> groups are over- or underrepresented in the training data. </span><span class="kobospan" id="kobo.117.2">For instance, training a facial recognition model predominantly on images of light-skinned individuals may cause the model to perform poorly when identifying faces with darker skin tones, favoring one group over </span><span><span class="kobospan" id="kobo.118.1">the other.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.119.1">Proxy bias</span></strong><span class="kobospan" id="kobo.120.1"> is </span><a id="_idIndexMarker817" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.121.1">when models use data from unrelated domains as input, leading to biased outcomes. </span><span class="kobospan" id="kobo.121.2">An example would be using arrest records to predict crime rates, which may introduce bias, as the arrest data could be influenced by systemic prejudice in </span><span><span class="kobospan" id="kobo.122.1">law enforcement.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.123.1">Aggregation bias</span></strong><span class="kobospan" id="kobo.124.1"> refers to the inappropriate grouping of data, simplifying the task at</span><a id="_idIndexMarker818" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.125.1"> the cost of accuracy. </span><span class="kobospan" id="kobo.125.2">An instance could be using average hemoglobin levels to predict diabetes, even though these levels vary among different ethnicities due to more </span><span><span class="kobospan" id="kobo.126.1">complex factors.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.127.1">Understanding these sources of algorithmic bias is the foundation upon which we can build strategies to prevent and mitigate bias in our ML models. </span><span class="kobospan" id="kobo.127.2">Thus, we contribute to the development of more equitable, fair, and inclusi</span><a id="_idTextAnchor375" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.128.1">ve </span><span><span class="kobospan" id="kobo.129.1">AI systems.</span></span></p>
<h1 id="_idParaDest-192" class="calibre6"><a id="_idTextAnchor376" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.130.1">Measuring bias</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.131.1">To </span><a id="_idIndexMarker819" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.132.1">successfully combat bias, we must first measure its existence and understand its impact on our ML models. </span><span class="kobospan" id="kobo.132.2">Several statistical methods and techniques have been developed for this purpose, each offering a different perspective on bias and fairness. </span><span class="kobospan" id="kobo.132.3">Here are a few </span><span><span class="kobospan" id="kobo.133.1">essential methods:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.134.1">Confusion matrix</span></strong><span class="kobospan" id="kobo.135.1">: A</span><a id="_idIndexMarker820" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.136.1"> fundamental tool for evaluating the performance of an ML model, the confusion matrix can also reveal bias. </span><span class="kobospan" id="kobo.136.2">It allows us to measure false positive and false negative rates, which can help us identify situations where the model performs differently for </span><span><span class="kobospan" id="kobo.137.1">different groups.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.138.1">Disparate impact analysis</span></strong><span class="kobospan" id="kobo.139.1">: This </span><a id="_idIndexMarker821" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.140.1">technique measures the ratio of favorable outcomes for a protected group compared to a non-protected group. </span><span class="kobospan" id="kobo.140.2">If the ratio is significantly below one, it implies a disparate impact on the protected group, signaling </span><span><span class="kobospan" id="kobo.141.1">potential bias.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.142.1">Equality of odds</span></strong><span class="kobospan" id="kobo.143.1">: This</span><a id="_idIndexMarker822" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.144.1"> method requires that a model’s error rates be equal across different groups. </span><span class="kobospan" id="kobo.144.2">In other words, if a model makes a mistake, the chances of that happening should be the same, regardless of the individual’s </span><span><span class="kobospan" id="kobo.145.1">group membership.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.146.1">Equality of opportunity</span></strong><span class="kobospan" id="kobo.147.1">: This is a variant of the equality of odds, which requires</span><a id="_idIndexMarker823" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.148.1"> only the true positive rates to be equal across groups. </span><span class="kobospan" id="kobo.148.2">This means that all individuals who should have received a positive outcome (according to the ground truth) have an equal chance of this happening, irrespective of </span><span><span class="kobospan" id="kobo.149.1">their group.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.150.1">Counterfactual analysis</span></strong><span class="kobospan" id="kobo.151.1">: This </span><a id="_idIndexMarker824" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.152.1">advanced technique involves imagining a scenario where an individual’s group membership is changed (the counterfactual scenario) and seeing </span><a id="_idIndexMarker825" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.153.1">whether the model’s decision changes. </span><span class="kobospan" id="kobo.153.2">If it does, this could be a sign </span><span><span class="kobospan" id="kobo.154.1">of bias.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.155.1">Fairness through awareness</span></strong><span class="kobospan" id="kobo.156.1">: This method acknowledges that individuals are different </span><a id="_idIndexMarker826" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.157.1">and that these differences should be factored into decision-making processes. </span><span class="kobospan" id="kobo.157.2">It demands that similar individuals, irrespective of their group, should be </span><span><span class="kobospan" id="kobo.158.1">treated similarly.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.159.1">These methods </span><a id="_idIndexMarker827" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.160.1">offer diverse perspectives on measuring bias and achieving fairness. </span><span class="kobospan" id="kobo.160.2">However, it’s important to note that fairness is a multifaceted concept, and what is considered fair can vary depending on the context. </span><span class="kobospan" id="kobo.160.3">Hence, it’s essential to consider these measures as tools that help us navigate toward a more equitable use of ML, rather than seeing them as defini</span><a id="_idTextAnchor377" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.161.1">tive solutions </span><span><span class="kobospan" id="kobo.162.1">to bias.</span></span></p>
<h1 id="_idParaDest-193" class="calibre6"><a id="_idTextAnchor378" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.163.1">Consequences of unaddressed bias and the importance of fairness</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.164.1">Ever</span><a id="_idIndexMarker828" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.165.1"> been at the receiving end of a raw deal? </span><span class="kobospan" id="kobo.165.2">Remember how that felt? </span><span class="kobospan" id="kobo.165.3">Now, imagine that happening systematically, over and over again, thanks to an ML model. </span><span class="kobospan" id="kobo.165.4">Not a pretty picture, right? </span><span class="kobospan" id="kobo.165.5">That’s exactly what happens when bias goes unaddressed in </span><span><span class="kobospan" id="kobo.166.1">AI systems.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.167.1">Consider a recruitment algorithm that has been trained on a skewed dataset. </span><span class="kobospan" id="kobo.167.2">It might consistently screen out potential candidates from minority groups, leading to unfair hiring practices. </span><span class="kobospan" id="kobo.167.3">Or, imagine a credit scoring algorithm that’s a little too fond of a particular zip code, making it harder for residents of other areas to get loans. </span><span><span class="kobospan" id="kobo.168.1">Unfair, right?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.169.1">These real-world implications of bias can severely erode trust in AI/ML systems. </span><span class="kobospan" id="kobo.169.2">If users feel that a system is consistently discriminating against them, they might lose faith in its decisions. </span><span class="kobospan" id="kobo.169.3">And let’s be honest – no one wants to use a tool that they believe is biased </span><span><span class="kobospan" id="kobo.170.1">against them.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.171.1">And it’s not just about trust. </span><span class="kobospan" id="kobo.171.2">There are serious ethical concerns here. </span><span class="kobospan" id="kobo.171.3">Unaddressed bias can have a disproportionately negative impact on marginalized communities, widening societal gaps rather than bridging them. </span><span class="kobospan" id="kobo.171.4">It’s akin to putting the ladder out of reach for those who might need it </span><span><span class="kobospan" id="kobo.172.1">the most.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.173.1">This brings us to the</span><a id="_idIndexMarker829" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.174.1"> importance of fairness. </span><span class="kobospan" id="kobo.174.2">Ensuring fairness in ML isn’t just a nice-to-have. </span><span class="kobospan" id="kobo.174.3">It’s a must-have. </span><span class="kobospan" id="kobo.174.4">A fair algorithm is not only more likely to gain the trust of its users but it also plays a crucial role in achieving </span><span><span class="kobospan" id="kobo.175.1">ethical outcomes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.176.1">Think about it. </span><span class="kobospan" id="kobo.176.2">Fair algorithms have the potential to level the playing field, to ensure that everyone gets a fair shot, irrespective of their background or identity. </span><span class="kobospan" id="kobo.176.3">They can help build a more equitable society, one decision at a time. </span><span class="kobospan" id="kobo.176.4">After all, isn’t that what technology should aim for? </span><span class="kobospan" id="kobo.176.5">To make our world not just more efficient but also </span><span><span class="kobospan" id="kobo.177.1">more equitable?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.178.1">And that’s why fairness in ML is so darn important. </span><span class="kobospan" id="kobo.178.2">It’s not just about the tech; it’s about the people it serves. </span><span class="kobospan" id="kobo.178.3">So, let’s take a look at some strategies for mitigating bias in th</span><a id="_idTextAnchor379" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.179.1">e next section, </span><span><span class="kobospan" id="kobo.180.1">shall we?</span></span></p>
<h1 id="_idParaDest-194" class="calibre6"><a id="_idTextAnchor380" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.181.1">Mitigating algorithmic bias</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.182.1">Even after </span><a id="_idIndexMarker830" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.183.1">understanding and measuring bias in ML, the job is only half done. </span><span class="kobospan" id="kobo.183.2">The next logical step is to implement strategies for mitigating bias. </span><span class="kobospan" id="kobo.183.3">Various techniques exist, each with its strengths and weaknesses, and a combination of these strategies can often yield the best results. </span><span class="kobospan" id="kobo.183.4">Here are some of the most </span><span><span class="kobospan" id="kobo.184.1">effective methods:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.185.1">Preprocessing techniques</span></strong><span class="kobospan" id="kobo.186.1">: These</span><a id="_idIndexMarker831" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.187.1"> techniques involve modifying the data before inputting it into the ML model. </span><span class="kobospan" id="kobo.187.2">They could include techniques such as resampling to correct imbalances in the data, or reweighing instances in the data to </span><span><span class="kobospan" id="kobo.188.1">reduce bias.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.189.1">In-processing techniques</span></strong><span class="kobospan" id="kobo.190.1">: These</span><a id="_idIndexMarker832" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.191.1"> are techniques that modify the model itself during training to reduce bias. </span><span class="kobospan" id="kobo.191.2">They could involve regularization techniques, cost-sensitive learning, or other forms of algorithmic tweaks to </span><span><span class="kobospan" id="kobo.192.1">minimize bias.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.193.1">Postprocessing techniques</span></strong><span class="kobospan" id="kobo.194.1">: These</span><a id="_idIndexMarker833" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.195.1"> techniques are applied after the model has been trained. </span><span class="kobospan" id="kobo.195.2">They can include modifying the outputs based on the sensitivity of predictions or adjusting the model’s thresholds</span><a id="_idIndexMarker834" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.196.1"> to ensure </span><span><span class="kobospan" id="kobo.197.1">fair outcomes.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.198.1">Fairness through unawareness</span></strong><span class="kobospan" id="kobo.199.1">: This method proposes that removing sensitive </span><a id="_idIndexMarker835" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.200.1">attributes (such as race or gender) from the dataset can lead to a fair model. </span><span class="kobospan" id="kobo.200.2">However, this method can often be overly simplistic and ignore deeper, systemic biases present in </span><span><span class="kobospan" id="kobo.201.1">the data.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.202.1">Fairness through awareness</span></strong><span class="kobospan" id="kobo.203.1">: In contrast to the previous method, this one suggests</span><a id="_idIndexMarker836" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.204.1"> incorporating sensitive attributes directly into the model in a controlled way to </span><span><span class="kobospan" id="kobo.205.1">counteract bias.</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.206.1">Adversarial debiasing</span></strong><span class="kobospan" id="kobo.207.1">: This </span><a id="_idIndexMarker837" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.208.1">novel approach treats bias as a kind of noise that an adversarial network tries </span><span><span class="kobospan" id="kobo.209.1">to remove.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.210.1">Implementing these methods will be dependent on the nature of the data, the model, and the context in which they are applied. </span><span class="kobospan" id="kobo.210.2">Bias mitigation is not a one-size-fits-all solution, and careful consideration must be given to each specific case. </span><span class="kobospan" id="kobo.210.3">Nevertheless, the aforementioned techniques can go a long way toward promoting fairness and r</span><a id="_idTextAnchor381" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.211.1">educing harmful bias in </span><span><span class="kobospan" id="kobo.212.1">ML models.</span></span></p>
<h2 id="_idParaDest-195" class="calibre7"><a id="_idTextAnchor382" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.213.1">Mitigation during data preprocessing</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.214.1">We’ve</span><a id="_idIndexMarker838" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.215.1"> all heard the saying, “Garbage in, garbage out,” right? </span><span class="kobospan" id="kobo.215.2">Well, it’s no different with ML. </span><span class="kobospan" id="kobo.215.3">What we feed our model matters, and if we feed it a biased diet, well... </span><span class="kobospan" id="kobo.215.4">you can guess what </span><span><span class="kobospan" id="kobo.216.1">comes out.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.217.1">Our first line of defense against bias is during data preprocessing. </span><span class="kobospan" id="kobo.217.2">Here, we have to put on our detective hats and start investigating potential biases that might lurk in our data. </span><span class="kobospan" id="kobo.217.3">Say, for example, we’re dealing with a healthcare algorithm. </span><span class="kobospan" id="kobo.217.4">If our data sample over-represents a particular demographic, we risk skewing the algorithm toward that group, like a toddler who only wants to </span><span><span class="kobospan" id="kobo.218.1">eat fries!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.219.1">Once we’ve identified these biases, it’s time for some spring cleaning. </span><span class="kobospan" id="kobo.219.2">Techniques such as oversampling, undersampling, or using the </span><strong class="bold"><span class="kobospan" id="kobo.220.1">synthetic minority oversampling technique </span></strong><span class="kobospan" id="kobo.221.1">(</span><strong class="bold"><span class="kobospan" id="kobo.222.1">SMOTE</span></strong><span class="kobospan" id="kobo.223.1">)</span><a id="_idIndexMarker839" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.224.1"> can help us achieve a more balanced training set for our model. </span><span class="kobospan" id="kobo.224.2">We go through a fuller example of preprocessing with bias mitigation </span><a id="_idTextAnchor383" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.225.1">in mind in our case </span><span><span class="kobospan" id="kobo.226.1">studies chapter.</span></span></p>
<h2 id="_idParaDest-196" class="calibre7"><a id="_idTextAnchor384" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.227.1">Mitigation during model in-processing</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.228.1">So, we’ve </span><a id="_idIndexMarker840" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.229.1">done our best to clean up our data, but what about when we’re training our model? </span><span class="kobospan" id="kobo.229.2">Can we do something there </span><span><span class="kobospan" id="kobo.230.1">too? </span><span class="kobospan" id="kobo.230.2">Absolutely!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.231.1">The model in-processing stage allows us to bake fairness right into the training process. </span><span class="kobospan" id="kobo.231.2">It’s a bit like adding spices to a dish as it cooks, ensuring the flavors permeate the </span><span><span class="kobospan" id="kobo.232.1">entire dish.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.233.1">We can use algorithmic fairness constraints during the training to make sure our model plays fair. </span><span class="kobospan" id="kobo.233.2">Take a loan approval algorithm, for instance. </span><span class="kobospan" id="kobo.233.3">We could introduce a fairness constraint to ensure that approval rates are similar across different demographic groups, much like making sure everyone at the table gets an equal slice </span><span><span class="kobospan" id="kobo.234.1">of pizza.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.235.1">Or, we could use fairness regularization, where we introduce fairness as a sort of spicy ingredient into the loss function. </span><span class="kobospan" id="kobo.235.2">This can help us strike a balance between accuracy and fairness, preventing our model from favoring the majority group in a dataset, much like avoiding that one spicy dish that only a few guests at the </span><span><span class="kobospan" id="kobo.236.1">party enjoy.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.237.1">Finally, we can use adversarial debiasing, where we train an adversarial network to learn fair representations. </span><span class="kobospan" id="kobo.237.2">It’s like having a little kitchen helper who’s making sure we don’t overuse a particular ingredient (such as our sensitive</span><a id="_idTextAnchor385" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.238.1"> attribute) while cooking </span><span><span class="kobospan" id="kobo.239.1">our model.</span></span></p>
<h2 id="_idParaDest-197" class="calibre7"><a id="_idTextAnchor386" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.240.1">Mitigation during model postprocessing</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.241.1">Alright – so we’ve</span><a id="_idIndexMarker841" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.242.1"> prepped our data and been careful during cooking, but what about after the meal is cooked? </span><span class="kobospan" id="kobo.242.2">Can we do anything then? </span><span class="kobospan" id="kobo.242.3">Of course </span><span><span class="kobospan" id="kobo.243.1">we can!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">Just like we might adjust the seasoning of a dish after tasting, we can calibrate our models after they’re trained. </span><span class="kobospan" id="kobo.244.2">This ensures our model’s prediction probabilities are equally flavorful across different </span><span><span class="kobospan" id="kobo.245.1">demographic groups.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.246.1">And if we find our model consistently scoring a minority group lower, we can adjust the decision threshold for that group. </span><span class="kobospan" id="kobo.246.2">It’s like lowering the bar for a high jump when you realize it’s unfairly high for </span><span><span class="kobospan" id="kobo.247.1">some participants.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.248.1">Also, we can use fairness-aware ensemble methods. </span><span class="kobospan" id="kobo.248.2">These are like a group of chefs, each focusing on a different part of a meal, thus ensuring the entire dining</span><a id="_idTextAnchor387" class="pcalibre calibre4 pcalibre1"/> <a id="_idTextAnchor388" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.249.1">experience is well balanced </span><span><span class="kobospan" id="kobo.250.1">and fair.</span></span></p>
<h1 id="_idParaDest-198" class="calibre6"><a id="_idTextAnchor389" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.251.1">Bias in LLMs</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.252.1">In the world </span><a id="_idIndexMarker842" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.253.1">of AI, we’ve seen a boom in the deployment of LLMs, and hey – why not? </span><span class="kobospan" id="kobo.253.2">These behemoths, such as GPT-3 or BERT, are capable of some jaw-dropping tasks, from writing emails that make sense to creating near-human-like text. </span><span class="kobospan" id="kobo.253.3">Impressive, isn’t it? </span><span class="kobospan" id="kobo.253.4">But let’s take a step back and think. </span><span class="kobospan" id="kobo.253.5">Just like every coin has two sides, there’s a not-so-glamorous side to these models – </span><span><span class="kobospan" id="kobo.254.1">bias.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.255.1">Yes – you heard it right. </span><span class="kobospan" id="kobo.255.2">These models are not immune to biases. </span><span class="kobospan" id="kobo.255.3">The ugly truth is that these models learn everything from the data they’re trained on. </span><span class="kobospan" id="kobo.255.4">And if that data has biases (which, unfortunately, is often the case), the model’s output can also be biased. </span><span class="kobospan" id="kobo.255.5">Think of it this way: if the model were trained on texts that are predominantly sexist or racist, it might end up generating content that reflects these biases. </span><span class="kobospan" id="kobo.255.6">Not a pleasant thought, </span><span><span class="kobospan" id="kobo.256.1">is it?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.257.1">And that’s not just a hypothetical scenario. </span><span class="kobospan" id="kobo.257.2">There have been instances where AI applications based on these models ended up causing serious trouble. </span><span class="kobospan" id="kobo.257.3">Remember when AI systems were caught making unfair decisions or when chatbots ended up spewing hate speech? </span><span class="kobospan" id="kobo.257.4">That’s the direct result of biases in the training data trickling down to the </span><span><span class="kobospan" id="kobo.258.1">AI application.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.259.1">Take a look at a few recent studies, and you’ll find them dotted with examples of biases in LLMs. </span><span class="kobospan" id="kobo.259.2">It’s like finding a needle in a haystack, but the needle is magnetized. </span><span class="kobospan" id="kobo.259.3">Bias, dear friends, is more prevalent in these models than we’d like </span><span><span class="kobospan" id="kobo.260.1">to admit.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.261.1">In a nutshell, bias in LLMs is real, and it’s high time we started addressing it seriously. </span><span class="kobospan" id="kobo.261.2">Stay tuned as we unpack more on </span><a id="_idTextAnchor390" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.262.1">this. </span><span class="kobospan" id="kobo.262.2">Let’s take a look at </span><span><span class="kobospan" id="kobo.263.1">an example.</span></span></p>
<h2 id="_idParaDest-199" class="calibre7"><a id="_idTextAnchor391" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.264.1">Uncovering bias in GPT-2</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.265.1">LLMs such </span><a id="_idIndexMarker843" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.266.1">as GPT-2 have </span><a id="_idIndexMarker844" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.267.1">revolutionized </span><strong class="bold"><span class="kobospan" id="kobo.268.1">natural language processing</span></strong><span class="kobospan" id="kobo.269.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.270.1">NLP</span></strong><span class="kobospan" id="kobo.271.1">) by producing high-quality, human-like text. </span><span class="kobospan" id="kobo.271.2">While these models can be valuable tools, they </span><a id="_idIndexMarker845" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.272.1">are not without their challenges. </span><span class="kobospan" id="kobo.272.2">One significant concern is the potential for bias in their outputs, which can result from biases in the training data. </span><span class="kobospan" id="kobo.272.3">To understand this concept in practice, let’s consider an example. </span><span class="kobospan" id="kobo.272.4">We’ll use the GPT-2 model from Hugging Face’s </span><strong class="source-inline"><span class="kobospan" id="kobo.273.1">transformers</span></strong><span class="kobospan" id="kobo.274.1"> library, trained on data from various internet sources, including </span><em class="italic"><span class="kobospan" id="kobo.275.1">Reddit</span></em><span class="kobospan" id="kobo.276.1">, a platform known for its diverse array of opinions and discussions but also for misinformation and potential biases. </span><span class="kobospan" id="kobo.276.2">This experiment aims to showcase the potential biases that LLMs can exhibit when generating text based on </span><span><span class="kobospan" id="kobo.277.1">specific prompts.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.278.1">Let’s set </span><a id="_idIndexMarker846" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.279.1">up some code for an experiment. </span><span class="kobospan" id="kobo.279.2">I want to ask GPT-2 a few questions where I expect a list as an answer, and I want to see what it says and how often it will say it. </span><span class="kobospan" id="kobo.279.3">This code block creates a function to ask a question and counts the elements of a comma-separated list in </span><span><span class="kobospan" id="kobo.280.1">the response:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.281.1">
from tqdm import tqdm
import pandas as pd
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2-large', tokenizer='gpt2-large')
set_seed(0)</span></pre>
<p class="calibre3"><span class="kobospan" id="kobo.282.1">Let’s look at this in </span><span><span class="kobospan" id="kobo.283.1">more detail:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.284.1">The pipeline is a high-level, easy-to-use API for doing inference with transformer models. </span><span class="kobospan" id="kobo.284.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.285.1">set_seed</span></strong><span class="kobospan" id="kobo.286.1"> function sets the seed for generating </span><span><span class="kobospan" id="kobo.287.1">random numbers.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.288.1">Next, we create an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.289.1">text-generation</span></strong><span class="kobospan" id="kobo.290.1"> pipeline. </span><span class="kobospan" id="kobo.290.2">We set up a pipeline for text generation, specifying the GPT-2 model and tokenizer. </span><span class="kobospan" id="kobo.290.3">The GPT-2 model is used because it’s been trained on a large corpus of text and is able to generate </span><span><span class="kobospan" id="kobo.291.1">human-like text.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.292.1">We then set the seed for the random number generator. </span><span class="kobospan" id="kobo.292.2">The seed is set to ensure the reproducibility of the results. </span><span class="kobospan" id="kobo.292.3">When the seed is set to a specific number, the generated sequences will be the same every time this script </span><span><span class="kobospan" id="kobo.293.1">is run.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.294.1">Finally, we use the generator to create text. </span><span class="kobospan" id="kobo.294.2">The generator receives a prompt and spits back </span><span><span class="kobospan" id="kobo.295.1">a response:</span></span><ol class="calibre190"><li class="calibre14"><span class="kobospan" id="kobo.296.1">It generates multiple different continuations of the prompt per call (controlled by the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.297.1">num_return_sequences</span></strong></span><span><span class="kobospan" id="kobo.298.1"> parameter).</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.299.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.300.1">max_length</span></strong><span class="kobospan" id="kobo.301.1"> parameter restricts the total length of the generated text to </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.302.1">10</span></strong></span><span><span class="kobospan" id="kobo.303.1"> tokens.</span></span></li><li class="calibre14"><span class="kobospan" id="kobo.304.1">The temperature is set to </span><strong class="source-inline1"><span class="kobospan" id="kobo.305.1">1.0</span></strong><span class="kobospan" id="kobo.306.1"> and affects the randomness of the output (higher values make the output more random, and lower values make it </span><span><span class="kobospan" id="kobo.307.1">more deterministic).</span></span></li></ol><p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.308.1">Figure 13</span></em></span><em class="italic"><span class="kobospan" id="kobo.309.1">.1</span></em><span class="kobospan" id="kobo.310.1"> shows some top results for a </span><span><span class="kobospan" id="kobo.311.1">few prompts:</span></span></p></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer315">
<span class="kobospan" id="kobo.312.1"><img alt="Figure 13.1 – Differences in asking GPT-2 what kinds of jobs men and women hold" src="image/B19488_13_01.jpg" class="calibre5"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.313.1">Figure 13.1 – Differences in asking GPT-2 what kinds of jobs men and women hold</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.314.1">These outputs </span><a id="_idIndexMarker847" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.315.1">clearly show that there may be biases in the language model’s output. </span><span class="kobospan" id="kobo.315.2">Some of the generated sentences could be perceived as negative or stereotyping, demonstrating that there could be potential for bias in LLMs. </span><span class="kobospan" id="kobo.315.3">Therefore, it is crucial to manage and be aware of these biases, especially when using the model’s output for any </span><span><span class="kobospan" id="kobo.316.1">sensitive tasks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.317.1">Language models such as GPT-2 are trained on large amounts of text data. </span><span class="kobospan" id="kobo.317.2">They learn to generate text by predicting the next word in a sentence, given the context of the preceding words. </span><span class="kobospan" id="kobo.317.3">This learning process doesn’t include explicit information about the facts or morality of the statements; it just learns patterns from the data it’s </span><span><span class="kobospan" id="kobo.318.1">trained on.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.319.1">Biases in these models arise due to the nature of the data they are trained on. </span><span class="kobospan" id="kobo.319.2">In the case of GPT-2, a significant portion of its training data comes from websites such as Reddit. </span><span class="kobospan" id="kobo.319.3">While Reddit can be a rich source of diverse views and discussions, it is also a platform that contains a wide range of content, including misinformation, stereotypes, and </span><span><span class="kobospan" id="kobo.320.1">discriminatory language.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.321.1">So, when the model is trained on such data, it can potentially learn and replicate these biases. </span><span class="kobospan" id="kobo.321.2">For example, in the code we provided, some generated sentences could be seen as promoting stereotypes or misinformation. </span><span class="kobospan" id="kobo.321.3">This demonstrates that the model has possibly learned these patterns from the biases present in its </span><span><span class="kobospan" id="kobo.322.1">training data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.323.1">This has serious </span><a id="_idIndexMarker848" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.324.1">implications. </span><span class="kobospan" id="kobo.324.2">For example, if LLMs are used in applications that involve making decisions that impact people’s lives, such as job recruitment or loan approval, any bias in the model’s predictions could lead to </span><span><span class="kobospan" id="kobo.325.1">unfair outcomes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.326.1">Therefore, addressing these biases is a significant challenge in the deployment of LLMs. </span><span class="kobospan" id="kobo.326.2">Possible solutions could involve careful curation of training data, bias mitigation techniques during the training process, or the postprocessing of model outputs. </span><span class="kobospan" id="kobo.326.3">Additionally, understanding and communicating the potential biases of a model to its users is also a</span><a id="_idTextAnchor392" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.327.1"> crucial part of responsible </span><span><span class="kobospan" id="kobo.328.1">AI deployment.</span></span></p>
<h1 id="_idParaDest-200" class="calibre6"><a id="_idTextAnchor393" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.329.1">Emerging techniques in bias and fairness in ML</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.330.1">When it </span><a id="_idIndexMarker849" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.331.1">comes to the world of tech, one thing is certain – it never stands still. </span><span class="kobospan" id="kobo.331.2">And ML is no exception. </span><span class="kobospan" id="kobo.331.3">The quest for fairness and the need to tackle bias has given rise to some innovative and game-changing techniques. </span><span class="kobospan" id="kobo.331.4">So, put on your techie hats, and let’s dive into some of these </span><span><span class="kobospan" id="kobo.332.1">groundbreaking developments.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.333.1">First off, let’s talk </span><a id="_idIndexMarker850" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.334.1">about interpretability. </span><span class="kobospan" id="kobo.334.2">In an age where complex ML models are becoming the norm, interpretable models are a breath of fresh air. </span><span class="kobospan" id="kobo.334.3">They’re transparent and easier to understand, and they allow us to gain insights into their decision-making process. </span><span class="kobospan" id="kobo.334.4">Techniques </span><a id="_idIndexMarker851" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.335.1">such as </span><strong class="bold"><span class="kobospan" id="kobo.336.1">Local Interpretable Model-Agnostic Explanations</span></strong><span class="kobospan" id="kobo.337.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.338.1">LIME</span></strong><span class="kobospan" id="kobo.339.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.340.1">SHapley Additive exPlanations</span></strong><span class="kobospan" id="kobo.341.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.342.1">SHAP</span></strong><span class="kobospan" id="kobo.343.1">) are </span><a id="_idIndexMarker852" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.344.1">leading the charge in this space. </span><span class="kobospan" id="kobo.344.2">They not only shed light on the “how” and “why” of a model’s decision but also help in identifying any biases lurking in the shadows. </span><span class="kobospan" id="kobo.344.3">We will talk more about LIME in the next chapter with some </span><span><span class="kobospan" id="kobo.345.1">code examples!</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.346.1">Next up is the growth of counterfactual explanations. </span><span class="kobospan" id="kobo.346.2">This is all about understanding what could change an algorithm’s decision about a particular individual. </span><span class="kobospan" id="kobo.346.3">For instance, what changes would flip a loan rejection into an approval? </span><span class="kobospan" id="kobo.346.4">These explanations can help spot potential areas of bias and can also make these complex systems more relatable to the people </span><span><span class="kobospan" id="kobo.347.1">they serve.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.348.1">In the realm of fairness metrics, the winds of change are blowing too. </span><span class="kobospan" id="kobo.348.2">The traditional focus on statistical parity is giving way to more nuanced measures such as group fairness, individual fairness, and counterfactual fairness. </span><span class="kobospan" id="kobo.348.3">These metrics aim to ensure that similar </span><a id="_idIndexMarker853" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.349.1">individuals are treated similarly and</span><a id="_idIndexMarker854" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.350.1"> also take into account the potential impact of </span><span><span class="kobospan" id="kobo.351.1">hypothetical scenarios.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.352.1">Lastly, there’s a growing interest in fairness-aware algorithms. </span><span class="kobospan" id="kobo.352.2">These are not your run-of-the-mill ML models. </span><span class="kobospan" id="kobo.352.3">They’re designed to actively mitigate bias. </span><span class="kobospan" id="kobo.352.4">Take </span><strong class="bold"><span class="kobospan" id="kobo.353.1">Learning Fair Representations</span></strong><span class="kobospan" id="kobo.354.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.355.1">LFR</span></strong><span class="kobospan" id="kobo.356.1">), for </span><a id="_idIndexMarker855" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.357.1">example. </span><span class="kobospan" id="kobo.357.2">It’s an approach that attempts to learn a transformation of the features that removes bias, ensuring that decisions made by the model </span><span><span class="kobospan" id="kobo.358.1">are fair.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.359.1">All these advancements are evidence of the field’s commitment to making AI/ML systems more fair and less biased. </span><span class="kobospan" id="kobo.359.2">But remember – technology is only as good as how we use it. </span><span class="kobospan" id="kobo.359.3">So, let’s continue to use these tools to build models that are not just smart but al</span><a id="_idTextAnchor394" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.360.1">so fair. </span><span class="kobospan" id="kobo.360.2">After all, isn’t that the real </span><span><span class="kobospan" id="kobo.361.1">goal here?</span></span></p>
<h1 id="_idParaDest-201" class="calibre6"><a id="_idTextAnchor395" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.362.1">Understanding model drift and decay</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.363.1">Just like a river that changes its course over time, models in ML can experience drift and decay. </span><span class="kobospan" id="kobo.363.2">Now, you might be wondering, what does this mean? </span><span class="kobospan" id="kobo.363.3">Let’s delve into it. </span><span class="kobospan" id="kobo.363.4">Model drift refers to when our ML model’s performance degrades over time due to changes in the underlying data it was trained on or due to changes in the problem </span><span><span class="kobospan" id="kobo.364.1">space itself.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.365.1">As we know, ML models are not set in stone. </span><span class="kobospan" id="kobo.365.2">They are designed to adapt and learn from new information. </span><span class="kobospan" id="kobo.365.3">However, when the dynamics of the input data or the patterns that were initially recognized start to shift, our models might fail to adapt swiftly enough. </span><span class="kobospan" id="kobo.365.4">This</span><a id="_idTextAnchor396" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.366.1"> is where we encounter the problem of </span><span><span class="kobospan" id="kobo.367.1">model drift.</span></span></p>
<h2 id="_idParaDest-202" class="calibre7"><a id="_idTextAnchor397" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.368.1">Model drift</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.369.1">Now, there are</span><a id="_idIndexMarker856" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.370.1"> several types of model drift we should be aware of. </span><span class="kobospan" id="kobo.370.2">Each tells a different tale of how our models </span><span><span class="kobospan" id="kobo.371.1">can falter:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.372.1">The first</span><a id="_idIndexMarker857" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.373.1"> type is </span><strong class="bold"><span class="kobospan" id="kobo.374.1">concept drift</span></strong><span class="kobospan" id="kobo.375.1">. </span><span class="kobospan" id="kobo.375.2">Think </span><a id="_idIndexMarker858" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.376.1">of a </span><strong class="bold"><span class="kobospan" id="kobo.377.1">sentiment analysis</span></strong><span class="kobospan" id="kobo.378.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.379.1">SA</span></strong><span class="kobospan" id="kobo.380.1">) algorithm. </span><span class="kobospan" id="kobo.380.2">Over</span><a id="_idIndexMarker859" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.381.1"> time, the way people use certain words or phrases can change. </span><span class="kobospan" id="kobo.381.2">Slang evolves, cultural contexts shift, and the algorithm may start to misinterpret sentiments because it’s not keeping up with these changes. </span><span class="kobospan" id="kobo.381.3">This is a classic case of </span><span><span class="kobospan" id="kobo.382.1">concept drift.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.383.1">Next, we</span><a id="_idIndexMarker860" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.384.1"> have </span><strong class="bold"><span class="kobospan" id="kobo.385.1">prediction drift</span></strong><span class="kobospan" id="kobo.386.1">. </span><span class="kobospan" id="kobo.386.2">Imagine you have a chatbot that’s handling customer</span><a id="_idIndexMarker861" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.387.1"> queries. </span><span class="kobospan" id="kobo.387.2">Due to an unforeseen event, such as a temporary outage, your chatbot suddenly receives an influx of similar queries. </span><span class="kobospan" id="kobo.387.3">Your model’s predictions start to skew toward this particular issue, causing </span><span><span class="kobospan" id="kobo.388.1">prediction drift.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.389.1">To summarize, model drift </span><a id="_idIndexMarker862" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.390.1">is a challenge that reminds us of the ever-changing nature of data and user behavior. </span><span class="kobospan" id="kobo.390.2">As we navigate these currents, understanding the types of drift can act as our compass, guiding us in maintaining the performance and accuracy of our models. </span><a id="_idTextAnchor398" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.391.1">Now, let’s delve into another type of drift – </span><span><span class="kobospan" id="kobo.392.1">data drift.</span></span></p>
<h2 id="_idParaDest-203" class="calibre7"><a id="_idTextAnchor399" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.393.1">Data drift</span></h2>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.394.1">Data drift</span></strong><span class="kobospan" id="kobo.395.1">, sometimes referred to as </span><strong class="bold"><span class="kobospan" id="kobo.396.1">feature drift</span></strong><span class="kobospan" id="kobo.397.1">, is</span><a id="_idIndexMarker863" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.398.1"> another phenomenon </span><a id="_idIndexMarker864" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.399.1">we need to keep our eye </span><a id="_idIndexMarker865" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.400.1">on. </span><span class="kobospan" id="kobo.400.2">Let’s imagine a scene. </span><span class="kobospan" id="kobo.400.3">Your ML model is a ship, and the data it’s trained on is the ocean. </span><span class="kobospan" id="kobo.400.4">Now, as we know, the ocean is not a static entity – currents shift, tides rise and fall, and new islands may even emerge. </span><span class="kobospan" id="kobo.400.5">Just as a skilled captain navigates these changes, our models need to adapt to the changing currents </span><span><span class="kobospan" id="kobo.401.1">of data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.402.1">But what exactly does data drift entail? </span><span class="kobospan" id="kobo.402.2">In essence, it’s a change in the model’s input data distribution. </span><span class="kobospan" id="kobo.402.3">For example, consider an e-commerce recommendation system. </span><span class="kobospan" id="kobo.402.4">Suppose a new product is introduced, and it quickly becomes a hit among consumers. </span><span class="kobospan" id="kobo.402.5">People start using a new term to refer to this product in their reviews and feedback. </span><span class="kobospan" id="kobo.402.6">If your model doesn’t adapt to include this new term in its understanding, it’s going to miss a significant aspect of current customer sentiment and preferences – classic </span><span><span class="kobospan" id="kobo.403.1">data drift.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.404.1">Data drift is a reminder that the world our models operate in is not static. </span><span class="kobospan" id="kobo.404.2">Trends emerge, customer behaviors evolve, and new information becomes relevant. </span><span class="kobospan" id="kobo.404.3">As such, it’s vital for our models to stay agile and responsive to </span><span><span class="kobospan" id="kobo.405.1">these changes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.406.1">Another kind of </span><a id="_idIndexMarker866" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.407.1">drift is </span><strong class="bold"><span class="kobospan" id="kobo.408.1">label drift</span></strong><span class="kobospan" id="kobo.409.1">, and this is when there’s a shift in the actual</span><a id="_idIndexMarker867" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.410.1"> label distribution. </span><span class="kobospan" id="kobo.410.2">Let’s consider a customer service bot again. </span><span class="kobospan" id="kobo.410.3">If there’s a change in customer behavior, such as from asking for returns to enquiring about the status of their returns, the distribution of labels in your data shifts, leading to </span><span><span class="kobospan" id="kobo.411.1">label drift.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.412.1">Now that we’ve demystified model and data drift, let’s delve into their various sources to better understand and mitigate them. </span><span class="kobospan" id="kobo.412.2">When we talk about drift in the context of ML, we typically distinguish between two main sources: model drift and data drift. </span><span class="kobospan" id="kobo.412.3">It’s a bit like considering the source of changes in the taste of a dish – is</span><a id="_idTextAnchor400" class="pcalibre calibre4 pcalibre1"/><a id="_idTextAnchor401" class="pcalibre calibre4 pcalibre1"/><a id="_idTextAnchor402" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.413.1"> it the ingredients that have changed or the </span><span><span class="kobospan" id="kobo.414.1">chef’s technique?</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.415.1">Sources of model drift</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.416.1">Model drift occurs</span><a id="_idIndexMarker868" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.417.1"> when the underlying assumptions of our model change. </span><span class="kobospan" id="kobo.417.2">This is akin to a chef changing their technique. </span><span class="kobospan" id="kobo.417.3">Maybe the oven’s temperature has been altered, or the baking time has been modified. </span><span class="kobospan" id="kobo.417.4">In the ML world, this could be due to changes in the environment where the model is deployed. </span><span class="kobospan" id="kobo.417.5">A good example is a traffic prediction model. </span><span class="kobospan" id="kobo.417.6">Let’s say the model was trained on data before a major roadway was constructed. </span><span class="kobospan" id="kobo.417.7">After the construction, traffic patterns change, leading to model drift as the underlying assumptions no </span><span><span class="kobospan" id="kobo.418.1">longer hold.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.419.1">Sources of data drift</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.420.1">Data drift, on the </span><a id="_idIndexMarker869" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.421.1">other hand, is driven by changes in the statistical properties of the model inputs over time. </span><span class="kobospan" id="kobo.421.2">This is like the ingredients in our dish changing. </span><span class="kobospan" id="kobo.421.3">For instance, if a seasonal fruit that’s typically part of our recipe is no longer available and we have to replace it, the taste of the dish might drift from its original flavor. </span><span class="kobospan" id="kobo.421.4">In the realm of ML, an example could be an SA model that fails to account for the emergence of new slang terms or emojis, leading to a drift in the data the model </span><span><span class="kobospan" id="kobo.422.1">is analyzing.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.423.1">Understanding the sources of drift is essential because it allows us to develop strategies for monitoring and mitigating these changes, ensuring that our models stay fresh and relevant in the ever-evolving real world. </span><span class="kobospan" id="kobo.423.2">In the next sections, we</span><a id="_idTextAnchor403" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.424.1">’ll explore some strategies for managing these sources </span><span><span class="kobospan" id="kobo.425.1">of drift.</span></span></p>
<h1 id="_idParaDest-204" class="calibre6"><a id="_idTextAnchor404" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.426.1">Mitigating drift</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.427.1">The world of ML is ever-evolving, making</span><a id="_idIndexMarker870" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.428.1"> it crucial for us to remain adaptable. </span><span class="kobospan" id="kobo.428.2">We’ve seen how the concept of drift is integral to understanding changes in our data or model over time. </span><span class="kobospan" id="kobo.428.3">But what can we do when faced with these shifting sands? </span><span class="kobospan" id="kobo.428.4">Are we merely left to witness the disintegration of our model’s performance? </span><span class="kobospan" id="kobo.428.5">Not quite. </span><span class="kobospan" id="kobo.428.6">This section presents actionable strategies for mitigating drift, each one holdin</span><a id="_idTextAnchor405" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.429.1">g its unique place in our toolbox for effective </span><span><span class="kobospan" id="kobo.430.1">drift management.</span></span></p>
<h2 id="_idParaDest-205" class="calibre7"><a id="_idTextAnchor406" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.431.1">Understanding the context</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.432.1">Before we </span><a id="_idIndexMarker871" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.433.1">delve into the technicalities of mitigating drift, let’s acknowledge the necessity of understanding the context in which our model operates. </span><span class="kobospan" id="kobo.433.2">Just as a ship captain needs to understand the sea and the weather conditions, we need to comprehend our data sources, user behavior, environmental changes, and all other nuances that form the backdrop against which our </span><span><span class="kobospan" id="kobo.434.1">model functions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.435.1">Consider an e-commerce recommendation system. </span><span class="kobospan" id="kobo.435.2">Understanding the context would mean being aware of seasonal trends, ongoing sales, or any recent global events that could influence customer behavior. </span><span class="kobospan" id="kobo.435.3">For instance, during a global sporting event, there might be a surge in sports-related purchases. </span><span class="kobospan" id="kobo.435.4">Being aware of these contextu</span><a id="_idTextAnchor407" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.436.1">al cues can help us preempt drift and prepare our models </span><span><span class="kobospan" id="kobo.437.1">to adapt.</span></span></p>
<h2 id="_idParaDest-206" class="calibre7"><a id="_idTextAnchor408" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.438.1">Continuous monitoring</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.439.1">Knowledge </span><a id="_idIndexMarker872" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.440.1">without action is futile. </span><span class="kobospan" id="kobo.440.2">Once we’re familiar with our context, the next step is to keep a vigilant eye on our model’s performance. </span><span class="kobospan" id="kobo.440.3">We need to continuously monitor the heartbeat of our models and data. </span><span class="kobospan" id="kobo.440.4">This could be achieved by tracking model performance metrics over time or using statistical tests to identify significant shifts in </span><span><span class="kobospan" id="kobo.441.1">data distributions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.442.1">Take the case of a credit scoring model. </span><span class="kobospan" id="kobo.442.2">If we notice a sudden surge in the number of credit defaults, it might indicate a drift that needs our attention. </span><span class="kobospan" id="kobo.442.3">Monitoring systems such as dashboards with real-time updates can prove to be valuable assets in catchin</span><a id="_idTextAnchor409" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.443.1">g these shifts before they snowball into more </span><span><span class="kobospan" id="kobo.444.1">significant problems.</span></span></p>
<h2 id="_idParaDest-207" class="calibre7"><a id="_idTextAnchor410" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.445.1">Regular model retraining</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.446.1">Stagnation is </span><a id="_idIndexMarker873" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.447.1">the enemy of progress. </span><span class="kobospan" id="kobo.447.2">As the world around us changes, our models need to keep up by learning from fresh data. </span><span class="kobospan" id="kobo.447.3">Regularly retraining the model can help it stay updated with recent trends and patterns. </span><span class="kobospan" id="kobo.447.4">How often should we retrain? </span><span class="kobospan" id="kobo.447.5">Well, it depends on the velocity of change in our data or context. </span><span class="kobospan" id="kobo.447.6">In some cases, retraining may be necessary every few months, while in others, it might be required every </span><span><span class="kobospan" id="kobo.448.1">few days.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.449.1">Consider a model predicting stock market trends. </span><span class="kobospan" id="kobo.449.2">Given the volatility of the markets, the model might benefit from daily or even hourly retraining. </span><span class="kobospan" id="kobo.449.3">Conversely, a model predicting housing prices might only need semi-an</span><a id="_idTextAnchor411" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.450.1">nual retraining due to the relative stability of the </span><span><span class="kobospan" id="kobo.451.1">housing market.</span></span></p>
<h2 id="_idParaDest-208" class="calibre7"><a id="_idTextAnchor412" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.452.1">Implementing feedback systems</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.453.1">Feedback isn’t</span><a id="_idIndexMarker874" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.454.1"> just for Amazon reviews or post-workshop surveys. </span><span class="kobospan" id="kobo.454.2">In the world of ML, feedback systems can act as our reality checks. </span><span class="kobospan" id="kobo.454.3">They can help us understand whether our model’s predictions align with the evolving real world. </span><span class="kobospan" id="kobo.454.4">Feedback can come from various sources, such as users flagging incorrect predictions or automated checks on </span><span><span class="kobospan" id="kobo.455.1">model outputs.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.456.1">Suppose we’ve deployed an ML model for SA in social media posts. </span><span class="kobospan" id="kobo.456.2">We could set up a feedback system where users can report when the model incorrectly labels their posts’ sentiment. </span><span class="kobospan" id="kobo.456.3">This information can help us</span><a id="_idTextAnchor413" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.457.1"> identify any drift in language use and update our </span><span><span class="kobospan" id="kobo.458.1">model accordingly.</span></span></p>
<h2 id="_idParaDest-209" class="calibre7"><a id="_idTextAnchor414" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.459.1">Model adaptation techniques</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.460.1">We’re now</span><a id="_idIndexMarker875" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.461.1"> stepping into the more advanced territory of drift mitigation. </span><span class="kobospan" id="kobo.461.2">Techniques such as online learning, where the model learns incrementally from a stream of data, or drift detection algorithms, which alert when the data distribution has significantly deviated, can automatically adapt the model to </span><span><span class="kobospan" id="kobo.462.1">mitigate drift.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.463.1">Despite their apparent appeal, these techniques come with their set of challenges, including computational cost and the need for continuous data streams. </span><span class="kobospan" id="kobo.463.2">They’re like the high-tech equipment in our toolkit – powerful, but requiring expert handling. </span><span class="kobospan" id="kobo.463.3">For example, an algorithm trading in the stock market might employ online learning to instantly react to market trends, demonstrating the power of model adaptation techniques when </span><span><span class="kobospan" id="kobo.464.1">appropriately utilized.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.465.1">In conclusion, mitigating</span><a id="_idIndexMarker876" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.466.1"> drift is not a one-size-fits-all solution. </span><span class="kobospan" id="kobo.466.2">It’s a layered approach where each strategy plays a critical role, much like the gears in a watch. </span><span class="kobospan" id="kobo.466.3">Understanding the context sets the stage, continuous monitoring keeps us alert, regular retraining ensures our model remains relevant, feedback systems provide a reality check, and model adaptation techniques allow for automatic adjustments. </span><span class="kobospan" id="kobo.466.4">The key lies in understanding which strategies to implement when, giving us th</span><a id="_idTextAnchor415" class="pcalibre calibre4 pcalibre1"/><a id="_idTextAnchor416" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.467.1">e power to ensure our models’ longevity despite the </span><span><span class="kobospan" id="kobo.468.1">ever-present drift.</span></span></p>
<h1 id="_idParaDest-210" class="calibre6"><a id="_idTextAnchor417" class="pcalibre calibre4 pcalibre1"/><span class="kobospan" id="kobo.469.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.470.1">In the ever-evolving domain of ML, confronting the dual challenges of algorithmic bias and model/data drift is not just about immediate fixes but also about establishing enduring practices. </span><span class="kobospan" id="kobo.470.2">The strategies delineated in this chapter are critical steps toward more equitable and adaptable ML models. </span><span class="kobospan" id="kobo.470.3">They are the very embodiment of vigilance and adaptability that ensure the integrity and applicability of AI in the face of data’s </span><span><span class="kobospan" id="kobo.471.1">dynamic nature.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.472.1">As we turn the page from confronting biases and drifts, we enter the expansive realm of AI governance. </span><span class="kobospan" id="kobo.472.2">Our next chapter will focus on structuring robust governance mechanisms that do not merely react to issues but proactively shape the development and deployment of AI systems. </span><span class="kobospan" id="kobo.472.3">The principles of governance – encompassing the stewardship of data, the responsibility of ML, and the strategic oversight of architecture – are not just tactical elements; they are the backbone of ethical, sustainable, and effective </span><span><span class="kobospan" id="kobo.473.1">AI deployment.</span></span></p>
</div>
</body></html>