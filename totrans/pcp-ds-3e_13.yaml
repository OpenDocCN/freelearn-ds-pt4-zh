- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mitigating Algorithmic Bias and Tackling Model and Data Drift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’re playing in the arena of **machine learning** (**ML**) and data science,
    you’re going to run into some hurdles. You can count on meeting two challenges:
    **algorithmic bias** and **model and data drift**. They’re like the tricky questions
    in a pop quiz – you might not see them coming, but you’d better be prepared to
    handle them.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic bias can creep into our models, and when it does, it’s not a good
    look. It can lead to unfair results, and, quite frankly, it’s just not cool. But
    don’t worry – we’re going to tackle it head on and talk about ways to mitigate
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we consider bias, over time, changes can happen that make our models
    less accurate. It’s like when your favorite shirt shrinks in the wash – it’s not
    the shirt’s fault, but it doesn’t fit like it used to. The same happens with our
    models. They may have been a perfect fit when we first created them, but as data
    changes, they might need some adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll get into the nitty-gritty of these important issues.
    We’ll look at where algorithmic bias comes from and how to minimize it. We’ll
    also dive into the concepts of model drift and data drift and discuss some ways
    to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding algorithmic bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of algorithmic bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequences of unaddressed bias and the importance of fairness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigating algorithmic bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias in **large language** **models** (**LLMs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emerging techniques in bias and fairness in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding model drift and decay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigating drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to spice things up, we’ll also take a peek at how algorithmic bias can
    show up in LLMs. By the time we’re done, you’ll have a solid understanding of
    these challenges and be ready to face them in your own projects. So, let’s roll
    up our sleeves and get started!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding algorithmic bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic bias is a pivotal issue in the world of ML. It occurs when a system,
    intentionally or not, generates outputs that are unfair or systematically prejudiced
    toward certain individuals or groups. This prejudice often originates from the
    fact that these systems learn from existing data, which itself can be riddled
    with inherent societal bias.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness, as it relates to ML, is defined as the absence of any bias. While
    it might sound simple, achieving fairness can be an intricate process that calls
    for careful management at every step of model creation.
  prefs: []
  type: TYPE_NORMAL
- en: To paint a more detailed picture, let’s consider protected features. These are
    attributes that could potentially introduce bias into the system. They can be
    legally mandated, such as race and gender, or stem from organizational values,
    such as location or zip code. While seemingly benign, these features, when used
    in an ML model, can result in decisions that are biased or discriminatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Diving deeper, we find two major types of algorithmic bias:'
  prefs: []
  type: TYPE_NORMAL
- en: Disparate impact occurs when a model explicitly relies on protected attributes,
    favoring a certain group over others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast, disparate treatment arises when a model implicitly uses protected
    attributes through related variables, thereby indirectly resulting in biased outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A prime example of disparate treatment can be someone’s zip code, which might
    predominantly contain a particular race or socioeconomic status. This, in turn,
    could lead to skewed predictions or decisions that inadvertently favor or disfavor
    that particular group. Similarly, variables such as whether someone has been arrested
    before can introduce bias, given that certain groups have historically faced more
    arrests due to societal bias.
  prefs: []
  type: TYPE_NORMAL
- en: An initial approach to addressing bias is “unawareness,” which entails removing
    any explicit mention of protected features. However, this approach is a low bar
    in addressing bias, as it doesn’t account for disparate impacts that can still
    occur.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness can also be approached via statistical measures such as statistical
    parity and equalized odds. Statistical parity states that a model’s predictions
    should be independent of any given sensitive feature. For example, a model predicting
    recidivism should do so equally, irrespective of race. However, this approach
    ignores any actual relationship between the label and the sensitive attribute,
    which can result in biased outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have individual and group fairness. The former ensures similar individuals
    have similar outcomes, while the latter insists on equal statistical outcomes
    across groups divided by protected attributes. Each brings its own unique perspective
    on fairness and its achievement.
  prefs: []
  type: TYPE_NORMAL
- en: Equalized odds, on the other hand, propose that a model’s predictions should
    be independent of any sensitive feature, conditional on the response value. This
    approach encourages the model to be more accurate across all groups but can overlook
    larger socioeconomic reasons that might result in a certain group falling into
    a label more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the nuances of algorithmic bias, we can better devise strategies
    to prevent, identify, and mitigate bias in ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Types of bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Understanding bias in ML requires us to categorize it into different types
    based on its source and manifestation. This helps us to identify the root cause
    and implement targeted mitigation strategies. Here are the key types of bias to
    consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disparate impact**: Disparate impact refers to situations where an ML system’s
    outcomes disproportionately are a disadvantage to a specific group, typically
    a protected class defined by attributes such as race, sex, age, or religion. This
    often occurs even when the protected attribute isn’t explicitly included in the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disparate treatment**: In contrast to disparate impact, disparate treatment
    happens when an ML model treats individuals differently based on their membership
    of a protected group, implying a direct, discriminatory effect. This occurs when
    the protected attribute is explicitly used in the decision-making process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-existing bias**: Also known as historical bias, pre-existing bias emerges
    when the data used to train an ML model reflects existing prejudices or societal
    biases. The model, in essence, learns these biases and propagates them in its
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample bias**: Sample bias occurs when the data used to train a model isn’t
    representative of the population it’s supposed to serve. This can result in a
    model that performs well on the training data but poorly on the actual data it
    encounters in production, leading to unfair outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement bias**: Measurement bias arises when there are systematic errors
    in the way data is collected or measured. This can distort the training data and
    cause the model to learn incorrect associations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation bias**: Aggregation bias occurs when a model oversimplifies or
    fails to capture the diversity and nuances within a group. This can happen when
    distinct subgroups are treated as one homogeneous group, which can lead to the
    model making incorrect or unfair generalizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proxy bias**: Proxy bias takes place when a model uses an attribute as a
    stand-in for a protected attribute. For instance, zip codes might be used as a
    proxy for race or income level, leading to biased outcomes even when the protected
    attribute isn’t directly used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these types of bias has different implications for fairness in ML and
    requires different strategies to detect and mitigate. By identifying the type
    of bias at play, we can take targeted steps to reduce its impact and work toward
    more fair and equitable ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of algorithmic bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML models, grounded in the learnings from past data, may unintentionally propagate
    bias present in their training datasets. Recognizing the roots of this bias is
    a vital first step toward fairer models:'
  prefs: []
  type: TYPE_NORMAL
- en: One such source is **historical bias**. This form of bias mirrors existing prejudices
    and systemic inequalities present in society. An example would be a recruitment
    model trained on a company’s past hiring data. If the organization historically
    favored a specific group for certain roles, the model could replicate these biases,
    continuing the cycle of bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Representation or sample bias** is another significant contributor. It occurs
    when certain groups are over- or underrepresented in the training data. For instance,
    training a facial recognition model predominantly on images of light-skinned individuals
    may cause the model to perform poorly when identifying faces with darker skin
    tones, favoring one group over the other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proxy bias** is when models use data from unrelated domains as input, leading
    to biased outcomes. An example would be using arrest records to predict crime
    rates, which may introduce bias, as the arrest data could be influenced by systemic
    prejudice in law enforcement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation bias** refers to the inappropriate grouping of data, simplifying
    the task at the cost of accuracy. An instance could be using average hemoglobin
    levels to predict diabetes, even though these levels vary among different ethnicities
    due to more complex factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these sources of algorithmic bias is the foundation upon which
    we can build strategies to prevent and mitigate bias in our ML models. Thus, we
    contribute to the development of more equitable, fair, and inclusive AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To successfully combat bias, we must first measure its existence and understand
    its impact on our ML models. Several statistical methods and techniques have been
    developed for this purpose, each offering a different perspective on bias and
    fairness. Here are a few essential methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: A fundamental tool for evaluating the performance of
    an ML model, the confusion matrix can also reveal bias. It allows us to measure
    false positive and false negative rates, which can help us identify situations
    where the model performs differently for different groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disparate impact analysis**: This technique measures the ratio of favorable
    outcomes for a protected group compared to a non-protected group. If the ratio
    is significantly below one, it implies a disparate impact on the protected group,
    signaling potential bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equality of odds**: This method requires that a model’s error rates be equal
    across different groups. In other words, if a model makes a mistake, the chances
    of that happening should be the same, regardless of the individual’s group membership.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equality of opportunity**: This is a variant of the equality of odds, which
    requires only the true positive rates to be equal across groups. This means that
    all individuals who should have received a positive outcome (according to the
    ground truth) have an equal chance of this happening, irrespective of their group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Counterfactual analysis**: This advanced technique involves imagining a scenario
    where an individual’s group membership is changed (the counterfactual scenario)
    and seeing whether the model’s decision changes. If it does, this could be a sign
    of bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness through awareness**: This method acknowledges that individuals are
    different and that these differences should be factored into decision-making processes.
    It demands that similar individuals, irrespective of their group, should be treated
    similarly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods offer diverse perspectives on measuring bias and achieving fairness.
    However, it’s important to note that fairness is a multifaceted concept, and what
    is considered fair can vary depending on the context. Hence, it’s essential to
    consider these measures as tools that help us navigate toward a more equitable
    use of ML, rather than seeing them as definitive solutions to bias.
  prefs: []
  type: TYPE_NORMAL
- en: Consequences of unaddressed bias and the importance of fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ever been at the receiving end of a raw deal? Remember how that felt? Now, imagine
    that happening systematically, over and over again, thanks to an ML model. Not
    a pretty picture, right? That’s exactly what happens when bias goes unaddressed
    in AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a recruitment algorithm that has been trained on a skewed dataset.
    It might consistently screen out potential candidates from minority groups, leading
    to unfair hiring practices. Or, imagine a credit scoring algorithm that’s a little
    too fond of a particular zip code, making it harder for residents of other areas
    to get loans. Unfair, right?
  prefs: []
  type: TYPE_NORMAL
- en: These real-world implications of bias can severely erode trust in AI/ML systems.
    If users feel that a system is consistently discriminating against them, they
    might lose faith in its decisions. And let’s be honest – no one wants to use a
    tool that they believe is biased against them.
  prefs: []
  type: TYPE_NORMAL
- en: And it’s not just about trust. There are serious ethical concerns here. Unaddressed
    bias can have a disproportionately negative impact on marginalized communities,
    widening societal gaps rather than bridging them. It’s akin to putting the ladder
    out of reach for those who might need it the most.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the importance of fairness. Ensuring fairness in ML isn’t
    just a nice-to-have. It’s a must-have. A fair algorithm is not only more likely
    to gain the trust of its users but it also plays a crucial role in achieving ethical
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Think about it. Fair algorithms have the potential to level the playing field,
    to ensure that everyone gets a fair shot, irrespective of their background or
    identity. They can help build a more equitable society, one decision at a time.
    After all, isn’t that what technology should aim for? To make our world not just
    more efficient but also more equitable?
  prefs: []
  type: TYPE_NORMAL
- en: And that’s why fairness in ML is so darn important. It’s not just about the
    tech; it’s about the people it serves. So, let’s take a look at some strategies
    for mitigating bias in the next section, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating algorithmic bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even after understanding and measuring bias in ML, the job is only half done.
    The next logical step is to implement strategies for mitigating bias. Various
    techniques exist, each with its strengths and weaknesses, and a combination of
    these strategies can often yield the best results. Here are some of the most effective
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing techniques**: These techniques involve modifying the data before
    inputting it into the ML model. They could include techniques such as resampling
    to correct imbalances in the data, or reweighing instances in the data to reduce
    bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-processing techniques**: These are techniques that modify the model itself
    during training to reduce bias. They could involve regularization techniques,
    cost-sensitive learning, or other forms of algorithmic tweaks to minimize bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Postprocessing techniques**: These techniques are applied after the model
    has been trained. They can include modifying the outputs based on the sensitivity
    of predictions or adjusting the model’s thresholds to ensure fair outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness through unawareness**: This method proposes that removing sensitive
    attributes (such as race or gender) from the dataset can lead to a fair model.
    However, this method can often be overly simplistic and ignore deeper, systemic
    biases present in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness through awareness**: In contrast to the previous method, this one
    suggests incorporating sensitive attributes directly into the model in a controlled
    way to counteract bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial debiasing**: This novel approach treats bias as a kind of noise
    that an adversarial network tries to remove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing these methods will be dependent on the nature of the data, the
    model, and the context in which they are applied. Bias mitigation is not a one-size-fits-all
    solution, and careful consideration must be given to each specific case. Nevertheless,
    the aforementioned techniques can go a long way toward promoting fairness and
    reducing harmful bias in ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation during data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve all heard the saying, “Garbage in, garbage out,” right? Well, it’s no
    different with ML. What we feed our model matters, and if we feed it a biased
    diet, well... you can guess what comes out.
  prefs: []
  type: TYPE_NORMAL
- en: Our first line of defense against bias is during data preprocessing. Here, we
    have to put on our detective hats and start investigating potential biases that
    might lurk in our data. Say, for example, we’re dealing with a healthcare algorithm.
    If our data sample over-represents a particular demographic, we risk skewing the
    algorithm toward that group, like a toddler who only wants to eat fries!
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve identified these biases, it’s time for some spring cleaning. Techniques
    such as oversampling, undersampling, or using the **synthetic minority oversampling
    technique** (**SMOTE**) can help us achieve a more balanced training set for our
    model. We go through a fuller example of preprocessing with bias mitigation in
    mind in our case studies chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation during model in-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we’ve done our best to clean up our data, but what about when we’re training
    our model? Can we do something there too? Absolutely!
  prefs: []
  type: TYPE_NORMAL
- en: The model in-processing stage allows us to bake fairness right into the training
    process. It’s a bit like adding spices to a dish as it cooks, ensuring the flavors
    permeate the entire dish.
  prefs: []
  type: TYPE_NORMAL
- en: We can use algorithmic fairness constraints during the training to make sure
    our model plays fair. Take a loan approval algorithm, for instance. We could introduce
    a fairness constraint to ensure that approval rates are similar across different
    demographic groups, much like making sure everyone at the table gets an equal
    slice of pizza.
  prefs: []
  type: TYPE_NORMAL
- en: Or, we could use fairness regularization, where we introduce fairness as a sort
    of spicy ingredient into the loss function. This can help us strike a balance
    between accuracy and fairness, preventing our model from favoring the majority
    group in a dataset, much like avoiding that one spicy dish that only a few guests
    at the party enjoy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can use adversarial debiasing, where we train an adversarial network
    to learn fair representations. It’s like having a little kitchen helper who’s
    making sure we don’t overuse a particular ingredient (such as our sensitive attribute)
    while cooking our model.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation during model postprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alright – so we’ve prepped our data and been careful during cooking, but what
    about after the meal is cooked? Can we do anything then? Of course we can!
  prefs: []
  type: TYPE_NORMAL
- en: Just like we might adjust the seasoning of a dish after tasting, we can calibrate
    our models after they’re trained. This ensures our model’s prediction probabilities
    are equally flavorful across different demographic groups.
  prefs: []
  type: TYPE_NORMAL
- en: And if we find our model consistently scoring a minority group lower, we can
    adjust the decision threshold for that group. It’s like lowering the bar for a
    high jump when you realize it’s unfairly high for some participants.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can use fairness-aware ensemble methods. These are like a group of
    chefs, each focusing on a different part of a meal, thus ensuring the entire dining
    experience is well balanced and fair.
  prefs: []
  type: TYPE_NORMAL
- en: Bias in LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of AI, we’ve seen a boom in the deployment of LLMs, and hey – why
    not? These behemoths, such as GPT-3 or BERT, are capable of some jaw-dropping
    tasks, from writing emails that make sense to creating near-human-like text. Impressive,
    isn’t it? But let’s take a step back and think. Just like every coin has two sides,
    there’s a not-so-glamorous side to these models – bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes – you heard it right. These models are not immune to biases. The ugly truth
    is that these models learn everything from the data they’re trained on. And if
    that data has biases (which, unfortunately, is often the case), the model’s output
    can also be biased. Think of it this way: if the model were trained on texts that
    are predominantly sexist or racist, it might end up generating content that reflects
    these biases. Not a pleasant thought, is it?'
  prefs: []
  type: TYPE_NORMAL
- en: And that’s not just a hypothetical scenario. There have been instances where
    AI applications based on these models ended up causing serious trouble. Remember
    when AI systems were caught making unfair decisions or when chatbots ended up
    spewing hate speech? That’s the direct result of biases in the training data trickling
    down to the AI application.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at a few recent studies, and you’ll find them dotted with examples
    of biases in LLMs. It’s like finding a needle in a haystack, but the needle is
    magnetized. Bias, dear friends, is more prevalent in these models than we’d like
    to admit.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, bias in LLMs is real, and it’s high time we started addressing
    it seriously. Stay tuned as we unpack more on this. Let’s take a look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: Uncovering bias in GPT-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs such as GPT-2 have revolutionized `transformers` library, trained on data
    from various internet sources, including *Reddit*, a platform known for its diverse
    array of opinions and discussions but also for misinformation and potential biases.
    This experiment aims to showcase the potential biases that LLMs can exhibit when
    generating text based on specific prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up some code for an experiment. I want to ask GPT-2 a few questions
    where I expect a list as an answer, and I want to see what it says and how often
    it will say it. This code block creates a function to ask a question and counts
    the elements of a comma-separated list in the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at this in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline is a high-level, easy-to-use API for doing inference with transformer
    models. The **set_seed** function sets the seed for generating random numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create an instance of the **text-generation** pipeline. We set up a
    pipeline for text generation, specifying the GPT-2 model and tokenizer. The GPT-2
    model is used because it’s been trained on a large corpus of text and is able
    to generate human-like text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then set the seed for the random number generator. The seed is set to ensure
    the reproducibility of the results. When the seed is set to a specific number,
    the generated sequences will be the same every time this script is run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we use the generator to create text. The generator receives a prompt
    and spits back a response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It generates multiple different continuations of the prompt per call (controlled
    by the **num_return_sequences** parameter).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The **max_length** parameter restricts the total length of the generated text
    to **10** tokens.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The temperature is set to **1.0** and affects the randomness of the output (higher
    values make the output more random, and lower values make it more deterministic).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 13**.1* shows some top results for a few prompts:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Differences in asking GPT-2 what kinds of jobs men and women
    hold](img/B19488_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Differences in asking GPT-2 what kinds of jobs men and women hold
  prefs: []
  type: TYPE_NORMAL
- en: These outputs clearly show that there may be biases in the language model’s
    output. Some of the generated sentences could be perceived as negative or stereotyping,
    demonstrating that there could be potential for bias in LLMs. Therefore, it is
    crucial to manage and be aware of these biases, especially when using the model’s
    output for any sensitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Language models such as GPT-2 are trained on large amounts of text data. They
    learn to generate text by predicting the next word in a sentence, given the context
    of the preceding words. This learning process doesn’t include explicit information
    about the facts or morality of the statements; it just learns patterns from the
    data it’s trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Biases in these models arise due to the nature of the data they are trained
    on. In the case of GPT-2, a significant portion of its training data comes from
    websites such as Reddit. While Reddit can be a rich source of diverse views and
    discussions, it is also a platform that contains a wide range of content, including
    misinformation, stereotypes, and discriminatory language.
  prefs: []
  type: TYPE_NORMAL
- en: So, when the model is trained on such data, it can potentially learn and replicate
    these biases. For example, in the code we provided, some generated sentences could
    be seen as promoting stereotypes or misinformation. This demonstrates that the
    model has possibly learned these patterns from the biases present in its training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: This has serious implications. For example, if LLMs are used in applications
    that involve making decisions that impact people’s lives, such as job recruitment
    or loan approval, any bias in the model’s predictions could lead to unfair outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, addressing these biases is a significant challenge in the deployment
    of LLMs. Possible solutions could involve careful curation of training data, bias
    mitigation techniques during the training process, or the postprocessing of model
    outputs. Additionally, understanding and communicating the potential biases of
    a model to its users is also a crucial part of responsible AI deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Emerging techniques in bias and fairness in ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to the world of tech, one thing is certain – it never stands still.
    And ML is no exception. The quest for fairness and the need to tackle bias has
    given rise to some innovative and game-changing techniques. So, put on your techie
    hats, and let’s dive into some of these groundbreaking developments.
  prefs: []
  type: TYPE_NORMAL
- en: First off, let’s talk about interpretability. In an age where complex ML models
    are becoming the norm, interpretable models are a breath of fresh air. They’re
    transparent and easier to understand, and they allow us to gain insights into
    their decision-making process. Techniques such as **Local Interpretable Model-Agnostic
    Explanations** (**LIME**) and **SHapley Additive exPlanations** (**SHAP**) are
    leading the charge in this space. They not only shed light on the “how” and “why”
    of a model’s decision but also help in identifying any biases lurking in the shadows.
    We will talk more about LIME in the next chapter with some code examples!
  prefs: []
  type: TYPE_NORMAL
- en: Next up is the growth of counterfactual explanations. This is all about understanding
    what could change an algorithm’s decision about a particular individual. For instance,
    what changes would flip a loan rejection into an approval? These explanations
    can help spot potential areas of bias and can also make these complex systems
    more relatable to the people they serve.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of fairness metrics, the winds of change are blowing too. The traditional
    focus on statistical parity is giving way to more nuanced measures such as group
    fairness, individual fairness, and counterfactual fairness. These metrics aim
    to ensure that similar individuals are treated similarly and also take into account
    the potential impact of hypothetical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, there’s a growing interest in fairness-aware algorithms. These are not
    your run-of-the-mill ML models. They’re designed to actively mitigate bias. Take
    **Learning Fair Representations** (**LFR**), for example. It’s an approach that
    attempts to learn a transformation of the features that removes bias, ensuring
    that decisions made by the model are fair.
  prefs: []
  type: TYPE_NORMAL
- en: All these advancements are evidence of the field’s commitment to making AI/ML
    systems more fair and less biased. But remember – technology is only as good as
    how we use it. So, let’s continue to use these tools to build models that are
    not just smart but also fair. After all, isn’t that the real goal here?
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model drift and decay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like a river that changes its course over time, models in ML can experience
    drift and decay. Now, you might be wondering, what does this mean? Let’s delve
    into it. Model drift refers to when our ML model’s performance degrades over time
    due to changes in the underlying data it was trained on or due to changes in the
    problem space itself.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, ML models are not set in stone. They are designed to adapt and learn
    from new information. However, when the dynamics of the input data or the patterns
    that were initially recognized start to shift, our models might fail to adapt
    swiftly enough. This is where we encounter the problem of model drift.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, there are several types of model drift we should be aware of. Each tells
    a different tale of how our models can falter:'
  prefs: []
  type: TYPE_NORMAL
- en: The first type is **concept drift**. Think of a **sentiment analysis** (**SA**)
    algorithm. Over time, the way people use certain words or phrases can change.
    Slang evolves, cultural contexts shift, and the algorithm may start to misinterpret
    sentiments because it’s not keeping up with these changes. This is a classic case
    of concept drift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have **prediction drift**. Imagine you have a chatbot that’s handling
    customer queries. Due to an unforeseen event, such as a temporary outage, your
    chatbot suddenly receives an influx of similar queries. Your model’s predictions
    start to skew toward this particular issue, causing prediction drift.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, model drift is a challenge that reminds us of the ever-changing
    nature of data and user behavior. As we navigate these currents, understanding
    the types of drift can act as our compass, guiding us in maintaining the performance
    and accuracy of our models. Now, let’s delve into another type of drift – data
    drift.
  prefs: []
  type: TYPE_NORMAL
- en: Data drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data drift**, sometimes referred to as **feature drift**, is another phenomenon
    we need to keep our eye on. Let’s imagine a scene. Your ML model is a ship, and
    the data it’s trained on is the ocean. Now, as we know, the ocean is not a static
    entity – currents shift, tides rise and fall, and new islands may even emerge.
    Just as a skilled captain navigates these changes, our models need to adapt to
    the changing currents of data.'
  prefs: []
  type: TYPE_NORMAL
- en: But what exactly does data drift entail? In essence, it’s a change in the model’s
    input data distribution. For example, consider an e-commerce recommendation system.
    Suppose a new product is introduced, and it quickly becomes a hit among consumers.
    People start using a new term to refer to this product in their reviews and feedback.
    If your model doesn’t adapt to include this new term in its understanding, it’s
    going to miss a significant aspect of current customer sentiment and preferences
    – classic data drift.
  prefs: []
  type: TYPE_NORMAL
- en: Data drift is a reminder that the world our models operate in is not static.
    Trends emerge, customer behaviors evolve, and new information becomes relevant.
    As such, it’s vital for our models to stay agile and responsive to these changes.
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of drift is **label drift**, and this is when there’s a shift in
    the actual label distribution. Let’s consider a customer service bot again. If
    there’s a change in customer behavior, such as from asking for returns to enquiring
    about the status of their returns, the distribution of labels in your data shifts,
    leading to label drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve demystified model and data drift, let’s delve into their various
    sources to better understand and mitigate them. When we talk about drift in the
    context of ML, we typically distinguish between two main sources: model drift
    and data drift. It’s a bit like considering the source of changes in the taste
    of a dish – is it the ingredients that have changed or the chef’s technique?'
  prefs: []
  type: TYPE_NORMAL
- en: Sources of model drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model drift occurs when the underlying assumptions of our model change. This
    is akin to a chef changing their technique. Maybe the oven’s temperature has been
    altered, or the baking time has been modified. In the ML world, this could be
    due to changes in the environment where the model is deployed. A good example
    is a traffic prediction model. Let’s say the model was trained on data before
    a major roadway was constructed. After the construction, traffic patterns change,
    leading to model drift as the underlying assumptions no longer hold.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of data drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data drift, on the other hand, is driven by changes in the statistical properties
    of the model inputs over time. This is like the ingredients in our dish changing.
    For instance, if a seasonal fruit that’s typically part of our recipe is no longer
    available and we have to replace it, the taste of the dish might drift from its
    original flavor. In the realm of ML, an example could be an SA model that fails
    to account for the emergence of new slang terms or emojis, leading to a drift
    in the data the model is analyzing.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the sources of drift is essential because it allows us to develop
    strategies for monitoring and mitigating these changes, ensuring that our models
    stay fresh and relevant in the ever-evolving real world. In the next sections,
    we’ll explore some strategies for managing these sources of drift.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating drift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world of ML is ever-evolving, making it crucial for us to remain adaptable.
    We’ve seen how the concept of drift is integral to understanding changes in our
    data or model over time. But what can we do when faced with these shifting sands?
    Are we merely left to witness the disintegration of our model’s performance? Not
    quite. This section presents actionable strategies for mitigating drift, each
    one holding its unique place in our toolbox for effective drift management.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we delve into the technicalities of mitigating drift, let’s acknowledge
    the necessity of understanding the context in which our model operates. Just as
    a ship captain needs to understand the sea and the weather conditions, we need
    to comprehend our data sources, user behavior, environmental changes, and all
    other nuances that form the backdrop against which our model functions.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an e-commerce recommendation system. Understanding the context would
    mean being aware of seasonal trends, ongoing sales, or any recent global events
    that could influence customer behavior. For instance, during a global sporting
    event, there might be a surge in sports-related purchases. Being aware of these
    contextual cues can help us preempt drift and prepare our models to adapt.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowledge without action is futile. Once we’re familiar with our context, the
    next step is to keep a vigilant eye on our model’s performance. We need to continuously
    monitor the heartbeat of our models and data. This could be achieved by tracking
    model performance metrics over time or using statistical tests to identify significant
    shifts in data distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Take the case of a credit scoring model. If we notice a sudden surge in the
    number of credit defaults, it might indicate a drift that needs our attention.
    Monitoring systems such as dashboards with real-time updates can prove to be valuable
    assets in catching these shifts before they snowball into more significant problems.
  prefs: []
  type: TYPE_NORMAL
- en: Regular model retraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stagnation is the enemy of progress. As the world around us changes, our models
    need to keep up by learning from fresh data. Regularly retraining the model can
    help it stay updated with recent trends and patterns. How often should we retrain?
    Well, it depends on the velocity of change in our data or context. In some cases,
    retraining may be necessary every few months, while in others, it might be required
    every few days.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a model predicting stock market trends. Given the volatility of the
    markets, the model might benefit from daily or even hourly retraining. Conversely,
    a model predicting housing prices might only need semi-annual retraining due to
    the relative stability of the housing market.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing feedback systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feedback isn’t just for Amazon reviews or post-workshop surveys. In the world
    of ML, feedback systems can act as our reality checks. They can help us understand
    whether our model’s predictions align with the evolving real world. Feedback can
    come from various sources, such as users flagging incorrect predictions or automated
    checks on model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’ve deployed an ML model for SA in social media posts. We could set
    up a feedback system where users can report when the model incorrectly labels
    their posts’ sentiment. This information can help us identify any drift in language
    use and update our model accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Model adaptation techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re now stepping into the more advanced territory of drift mitigation. Techniques
    such as online learning, where the model learns incrementally from a stream of
    data, or drift detection algorithms, which alert when the data distribution has
    significantly deviated, can automatically adapt the model to mitigate drift.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their apparent appeal, these techniques come with their set of challenges,
    including computational cost and the need for continuous data streams. They’re
    like the high-tech equipment in our toolkit – powerful, but requiring expert handling.
    For example, an algorithm trading in the stock market might employ online learning
    to instantly react to market trends, demonstrating the power of model adaptation
    techniques when appropriately utilized.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, mitigating drift is not a one-size-fits-all solution. It’s a
    layered approach where each strategy plays a critical role, much like the gears
    in a watch. Understanding the context sets the stage, continuous monitoring keeps
    us alert, regular retraining ensures our model remains relevant, feedback systems
    provide a reality check, and model adaptation techniques allow for automatic adjustments.
    The key lies in understanding which strategies to implement when, giving us the
    power to ensure our models’ longevity despite the ever-present drift.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the ever-evolving domain of ML, confronting the dual challenges of algorithmic
    bias and model/data drift is not just about immediate fixes but also about establishing
    enduring practices. The strategies delineated in this chapter are critical steps
    toward more equitable and adaptable ML models. They are the very embodiment of
    vigilance and adaptability that ensure the integrity and applicability of AI in
    the face of data’s dynamic nature.
  prefs: []
  type: TYPE_NORMAL
- en: As we turn the page from confronting biases and drifts, we enter the expansive
    realm of AI governance. Our next chapter will focus on structuring robust governance
    mechanisms that do not merely react to issues but proactively shape the development
    and deployment of AI systems. The principles of governance – encompassing the
    stewardship of data, the responsibility of ML, and the strategic oversight of
    architecture – are not just tactical elements; they are the backbone of ethical,
    sustainable, and effective AI deployment.
  prefs: []
  type: TYPE_NORMAL
