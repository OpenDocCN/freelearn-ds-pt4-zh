- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI Governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the era of technology that we currently reside in, disruptive innovations
    are largely fueled by data, analytics, and AI. These elements are forging new
    paths across multiple sectors, birthing fresh avenues for revenue, and redefining
    corporate management paradigms. Consider the projection by McKinsey & Co., which
    forecasts an addition of over $15 trillion in business value by 2030, solely attributed
    to analytics and AI. In recognizing this goldmine, global organizations are fervently
    channeling resources to establish a strong foothold in this data-driven domain.
    A 2022 survey by NewVantage Partners supports this notion and indicates that a
    staggering 97% of surveyed entities are amplifying their investments in data-centric
    and AI initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: However, a significant paradox emerges here. Despite the enormous capital influx,
    the majority of organizations grapple with extracting tangible benefits from their
    data endeavors. So, what seems to be the impediment? The core challenge often
    resides in not having a well-structured, actionable data governance blueprint
    encompassing diverse data applications from business intelligence to machine learning.
    Gartner’s analysis offers a more somber outlook. They speculate that, by 2025,
    about 80% of enterprises aiming to expand their digital footprint might falter
    primarily due to their outdated approach to data analytics governance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigating the realms of data governance is paramount, yet it presents a convoluted
    maze for many diving into the digital transformation journey. Data teams often
    find themselves wrestling with the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Harmonizing a tidal wave of data that originates from diverse sources and dismantling
    entrenched data silos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upholding impeccable data standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that reliable data is both accessible and easily locatable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategically gating data access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving transparency in data utilization and consumption patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitating secure data exchanges, both internally and externally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overseeing machine learning procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adhering to rigorous regulatory mandates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A shrewd data governance blueprint is the key to untangling these complexities,
    enabling organizations to truly harness the potential of their data assets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this digital age, where every operational facet is intertwined with technology,
    a holistic understanding of governance is indispensable. The narrative of governance
    within the digital transformation spectrum isn’t confined to data alone but spans
    across three pivotal dimensions: **data governance**, **machine learning** (**ML**)
    **governance**, and **architectural governance**. Each dimension carries its own
    weight, yet together, they provide a comprehensive framework and ensure that organizations
    exploit technological assets to their fullest potential:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data governance**: This focuses on the management and quality of data throughout
    its life cycle. It ensures that data is trustworthy, reliable, and usable. With
    the proliferation of data sources and the sheer volume of data being generated,
    it becomes paramount to have clear protocols for data acquisition, storage, access,
    and disposal. This form of governance lays the foundation, ensuring the integrity,
    security, and availability of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML governance**: This addresses the specific challenges associated with the
    deployment and operation of AI and ML models. Given the transformative power of
    ML in modern business processes, it’s vital to ensure these models are transparent
    and ethical and operate within predefined boundaries. ML governance deals with
    model development, deployment, monitoring, and continuous improvement, ensuring
    that they are both effective and ethically sound.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architectural governance**: This ensures that the IT landscape aligns well
    with the business objectives. It is one thing to have the right data or ML models,
    but ensuring that the underlying infrastructure, applications, and the interconnections
    between them are designed and operated optimally is another factor. This ensures
    scalability, resilience, and efficiency in the digital ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, why is it crucial to discuss all three together? Simply put, they are intrinsically
    linked. Data is the foundation upon which ML models are built, and the architecture
    is the environment in which both data and ML models reside. Ignoring one aspect
    can compromise the effectiveness of the others. For instance, without proper data
    governance, even the most sophisticated ML models might produce unreliable results.
    Similarly, without robust architectural governance, data might be stored inefficiently
    or insecurely, affecting both standard analytics and advanced ML operations.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these three pillars of governance create a cohesive, comprehensive
    strategy; addressing them in tandem ensures that the machinery of an organization
    operates smoothly, ethically, and effectively, turning the promise of digital
    transformation into a tangible reality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the topics we''ll cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the COMPAS dataset case study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text embeddings using pre-trained models and OpenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mastering data governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For countless organizations, data stands as a priceless treasure. Yet, it’s
    data governance that serves as the compass guiding one to extract the true worth
    of data. Envision data governance as a holistic amalgamation of principles, methodologies,
    and tools designed to oversee the entire life cycle of your data, ensuring it’s
    in sync with the broader business roadmap. A well-orchestrated data governance
    blueprint bestows data teams with unmatched data stewardship and transparency
    and audit trails of data interaction patterns throughout the enterprise. Rolling
    out a robust data governance regime not only safeguards data against unsanctioned
    access but also institutes compliance protocols as per regulatory benchmarks.
    By astutely playing their data governance cards, numerous entities have used this
    approach to gain a pivotal competitive edge, enhancing customer confidence, fortifying
    data and privacy norms, and shielding their invaluable data resources.
  prefs: []
  type: TYPE_NORMAL
- en: Current hurdles in data governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Crafting an impeccable data governance strategy in today’s era is akin to navigating
    a labyrinth, especially given the modern data collection and analysis dynamics.
    As firms amass vast reservoirs of data, which are predominantly unstructured,
    the lion’s share ends up in cloud-based data lakes such as AWS S3, Azure ADLS,
    or **Google Cloud Storage** (**GCS**). In order to put this into perspective,
    IDC projections suggest that by 2025, a staggering 80% of organizational data
    will be unstructured. Yet, it’s within this chaotic data landscape that AI’s goldmine
    lies. Selected segments of this unstructured trove are occasionally transported
    to a data warehouse, structured for business intelligence endeavors, and sometimes
    retraced. This cyclic movement births isolated data pockets, each governed by
    distinct protocols.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, within data lakes, the focus is predominantly on file and directory
    access rights. Conversely, in data warehouses, attention shifts to permissions
    at the granularity of tables, columns, and rows. An alteration in one landscape
    seldom reflects in the other. Governance, being executed at divergent echelons
    across these domains, lacks uniformity. Furthermore, the tools employed on top
    of these platforms vary drastically, stymieing seamless team collaboration. This
    translates to a governance approach that’s sporadic and error-prone, complicating
    permission allocation and audits, as well as data discovery or sharing.
  prefs: []
  type: TYPE_NORMAL
- en: However, data isn’t merely confined to files or tables. In today’s age, we also
    contend with evolving data forms such as dashboards, ML models, and notebooks.
    Each comes bundled with unique permission paradigms, adding layers of complexity
    to uniform access right management. This challenge is magnified when data assets
    sprawl across diverse cloud platforms, each with its unique access governance
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: What is the crux? The more multifaceted your data architectural landscape becomes,
    the more daunting and resource-intensive it is to master data governance. Let’s
    dive into a few specific aspects of data stewardship to help ground our understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data management: crafting the bedrock'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of a compelling data governance strategy lies proficient data management.
    This sphere encompasses the meticulous aggregation, fusion, orchestration, and
    retention of trustworthy datasets, acting as a catalyst for businesses to harness
    maximum value. With the contemporary business landscape rapidly evolving, an organization’s
    merit hinges significantly on its prowess in extracting insights from the wealth
    of data it stewards. Moreover, data management plays a pivotal role in furnishing
    organizations with insights on data interaction frequency, alongside offering
    a suite of tools tailored for holistic data life cycle oversight.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the bastion of analytics-driven data management has been the data
    warehouse, typified by tabular data that is managed via structures such as tables
    and views comprising rows and columns. Conversely, data lakes stand as reservoirs
    for an eclectic mix of structured or unstructured data tailored to data science
    or ML pursuits. From raw text files and Apache Parquet formats to multimedia content,
    such as images or videos, data lakes manage this myriad of datasets at the granular
    file echelon.
  prefs: []
  type: TYPE_NORMAL
- en: Enter the realm of the data lakehouse—a new paradigm where organizations can
    stockpile data singularly, making it accessible for a spectrum of analytical applications.
    This innovative approach curtails data redundancy and pares down the data management
    ambit for organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion – the gateway to information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before data finds its permanent abode for subsequent utilization, it undergoes
    the critical phase of collection. While data sources are myriad, the principal
    conduits include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based storage systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message relay channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional relational databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APIs of **software as a service** (**SaaS**) platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lately, a significant chunk of data has been sourced from files relayed to
    object storage facilities that are integral to public cloud service providers.
    These files, ranging from a handful to millions (daily), encapsulate a diverse
    array of formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured entities such as PDFs, audio files, or videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-structured formats such as JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured types, including Parquet and Avro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For those venturing into the realm of data streaming, distributed message queues,
    such as Apache Kafka, emerge as the go-to platform. This seamless integration
    paves the way for ultra-fast message processing in a linear sequence. By complementing
    open source queue systems, each marquee cloud provider boasts its native message
    relay service.
  prefs: []
  type: TYPE_NORMAL
- en: Data integration – from collection to delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data, in its raw form, originates from a myriad of sources, demanding integration
    to unlock its full potential. The integration might unfurl in batch sequences
    or be streamed in real time, aiming for prompt insight derivation. But integration
    isn’t a standalone task—it demands orchestrated actions to curate, ingest, amalgamate,
    reshape, and finally, disseminate the refined data.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses and entity resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A weapon in the arsenal of data warehouses is the capacity for **entity resolution**
    (**ER**). At its core, ER involves deciphering real-world entities from data representations.
    These entities might be individuals, products, or locations. This technique is
    pivotal in master data management and data mart dimension alignment. For instance,
    discerning that “Bill Wallace” and “William Wallace” from disparate systems are
    one and the same requires intricate algorithm applications using expansive data
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The quest for data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to exploit the richness of data, trust is paramount. Data of questionable
    quality can spur analytical inaccuracies and subpar decision-making, and inflate
    operational costs. According to Gartner, such data anomalies can bleed organizations
    of a staggering $12.9 million annually. A robust data governance blueprint, therefore,
    mandates an unwavering spotlight on data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation and cataloging – the unsung heroes of governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metadata isn’t just a side aspect of ML governance; it’s central. ML catalogs
    offer insights, guiding users on available resources and their potential applications.
    This shared knowledge repository accelerates model deployment, promoting best
    practices across the board.
  prefs: []
  type: TYPE_NORMAL
- en: 'When assessing data quality, several facets come under scrutiny:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the data impeccable and exhaustive?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is its origin?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How current is the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the data flout any quality standards?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you can auto-capture data lineage, this can facilitate a swift grasp of data
    ownership and genesis. This lineage isn’t just a backward trace; it projects forward,
    showcasing the entities that consume this data—be it other tables, dashboards,
    or notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, comprehending a dataset’s lineage isn’t sufficient. Grasping data
    integrity within that dataset is equally vital. Running real-time quality checks
    and aggregating these checks for easy access and monitoring is pivotal for ensuring
    pristine data quality for subsequent analytical tasks. They preempt the influx
    of flawed data, validate data quality, and instigate policies to counter anomalies.
    Monitoring the trajectory of data quality can be quite cumbersome but will offer
    keen insights into data evolution and any areas warranting intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the path of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern organizations grapple with an avalanche of data from diverse origins.
    Grasping where this data originates, as well as its consumption patterns, is crucial
    for assuring its quality and reliability. This is where data lineage emerges as
    a potent instrument, enabling a clearer overview of data within organizations.
    Essentially, data lineage maps the journey of data, from its inception to its
    end use. This mapping involves capturing exhaustive metadata and pertinent events
    throughout the data’s life cycle. It encompasses details such as data origin,
    the datasets employed in its creation, its creators, any associated transformations,
    timestamps, and much more. By adopting data lineage solutions, teams can visualize
    the entire spectrum and flow of data transformations across their infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: As the wave of data-driven decision-making swells, embedding data lineage becomes
    a cornerstone of robust data governance.
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory compliance and audit preparedness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerous regulatory frameworks, such as GDPR, CCPA, HIPAA, BCBS 239, and SOX,
    mandate organizations to maintain transparency over their data streams. They must
    certify that the data or reports shared stem from verifiable, trustworthy sources.
    This necessitates the tracing of tables and datasets utilized in reports, emphasizing
    data traceability within an organization’s data architecture. Data lineage eases
    compliance burdens by automating the creation of data flow trails for audit purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Change management and impact analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data isn’t static; it evolves. Comprehending the cascading effects of data modifications
    on subsequent users is pivotal. Through data lineage, teams can discern all downstream
    entities affected by data alterations. This visibility extends to applications,
    dashboards, ML models, datasets, and more. These visualization and analysis tools
    aid in gauging any potential fallout, enabling timely stakeholder notifications.
    Furthermore, it facilitates IT units in their transparent sharing of data migration
    updates, ensuring unhindered business operations.
  prefs: []
  type: TYPE_NORMAL
- en: Upholding data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring the quality of data is paramount in any data-centric initiative. The
    integrity, accuracy, and reliability of data directly influence the insights derived,
    models trained, and decisions made. Data lineage empowers data users, such as
    data engineers, scientists, and analysts, by providing them with a comprehensive
    understanding of the data’s origins and transformations. This contextual awareness
    results in improved analytical outcomes, as users can confidently trace data back
    to its source and understand any modifications made to it.
  prefs: []
  type: TYPE_NORMAL
- en: Example and scenario
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Synthetic labeling is an innovative technique wherein ML models, especially
    large language models, are employed to assist in labeling data. This method is
    particularly beneficial for vast datasets where manual labeling would be time-consuming
    and impractical.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: Imagine a dataset containing millions of customer feedback comments.
    Instead of having human annotators read through each comment and label it as “positive,”
    “negative,” or “neutral,” a trained language model can be used to process and
    label these comments quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: However, this method is not without challenges. Synthetic labeling can introduce
    biases or errors, particularly if the model itself was trained on skewed or imperfect
    data. If unchecked, these biases can propagate through the dataset, leading to
    misleading insights or flawed ML models.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the role of data custodians becomes pivotal. They must actively
    monitor and validate synthetic labels to ensure they maintain a high standard
    of accuracy. It’s essential to combine the efficiency of synthetic labeling with
    human oversight, ensuring that the generated labels are cross-checked, verified,
    and refined when necessary. By striking this balance, organizations can harness
    the benefits of rapid labeling while upholding stringent data quality standards.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting and analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of stringent checks, data processes can falter. Here, data lineage
    shines by assisting teams in pinpointing the origin of errors in their systems,
    whether in data pipelines, applications, or models. Such pinpoint accuracy drastically
    trims down the debugging duration, translating to immense time and effort savings.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the systematic mapping of the journey of data, or data lineage,
    is not just a luxury; it’s a necessity in today’s data-rich environments, enabling
    clearer decision-making, enhanced compliance, and more efficient operations.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the point of gathering this data is often to train an ML model, so
    let’s turn our attention toward how to think about ML governance.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating the intricacy and the anatomy of ML governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML doesn’t operate solely by using algorithms and the data they ingest. Instead,
    its essence lies in constructing models responsibly, a task underpinned by governance.
    Just as governance has been the bedrock of the realm of data, it’s equally crucial
    for ML, especially in aspects such as accountability, standardization, compliance,
    quality, and clarity. Let’s discuss this topic in greater detail in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: ML governance pillars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlocking ML’s potential is rooted in ensuring that models meet the following
    criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Aligns with relevant regulatory and ethical benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exhibits consistent outcomes and performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illuminates their development and implications in a transparent way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can undergo regular quality assessments and updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adheres to standard documentation and cataloging protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While adherence to industry-specific regulations sets the baseline, the navigation
    of the broader spectrum of ethical concerns often requires a nuanced approach.
    The essence of governance extends beyond mere legalities, delving into the realm
    of what is morally right. Here, proactive assessments and a robust evaluation
    process come into play, ensuring models aren’t just compliant but are also ethically
    sound.
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of ML, understanding how a model arrives at its decisions is crucial.
    This is not just for academic interest but has significant real-world implications,
    especially when decisions impact human lives, such as in healthcare or criminal
    justice.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a healthcare system where ML models predict the likelihood of patients
    developing certain diseases based on their medical records. A model might predict
    that a patient has a high risk of developing diabetes. But why did the model make
    this prediction? Is it due to the patient’s age, genetic history, dietary habits,
    or some other factor?
  prefs: []
  type: TYPE_NORMAL
- en: '**Local interpretable model-agnostic explanations** (**LIME**) is a tool that
    was developed to shed light on this black box of ML predictions. It works by perturbing
    the input data slightly (adding some noise) and observing how these changes affect
    the model’s predictions. By doing this many times, LIME builds up a picture of
    which input variables are most influential regarding a given prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: However, as with any tool, LIME is not infallible. It provides approximations
    of model behavior, not exact explanations. Furthermore, its effectiveness can
    vary depending on the model and data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: This highlights the necessity of ML governance. Ensuring that tools such as
    LIME are used correctly, understanding their limitations, and supplementing them
    with other methods when necessary are pivotal. It’s not enough for a model to
    be accurate; it must also be transparent and its decisions explainable, especially
    in high-stakes situations. ML governance policies can set standards for interpretability
    and guide the proper use and interpretation of tools such as LIME.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code segment demonstrates how to leverage the LIME
    tool by using a sentiment analysis model sourced from the Model Hub of Hugging
    Face. While this script provides an interpretability layer to the model by identifying
    influential words/features in the input, it’s imperative to understand that such
    interpretations provide approximations. The highlighted words can give insights
    into the model’s decisions, but they might not capture the entirety of the model’s
    complex reasoning. Hence, while tools such as LIME are valuable, they should be
    employed judiciously within a broader framework of ML governance to ensure that
    the insights they offer are actionable and reliable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then see the visual interpretation (*Figure 14**.1*) of which tokens
    impact the final decision the most:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, `num_features` determines how many features the explainer should
    use to describe the prediction. For instance, if it is set to `4`, LIME will provide
    explanations using up to four features that influence the prediction the most.
    It helps to simplify the explanation by focusing only on the top *n* features
    that have the most influence on the prediction rather than considering all features.
    `top_labels` determines how many of the most probable labels you’d like explanations
    for, and this is usually for multi-class classification. For instance, if you
    set it to `1`, LIME will only explain the most probable label. If set to `2`,
    it will explain the two most probable labels, and so on. *Figure 14**.1* shows
    what the output would be for this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Here, the tokens that most affect the decision are “love,”
    “helpful,” and, surprisingly, “new” and “feature”](img/B19488_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Here, the tokens that most affect the decision are “love,” “helpful,”
    and, surprisingly, “new” and “feature”
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, in our example, the tokens **new** and **feature** contribute
    greatly to the prediction of positive. In terms of governance, this is a great
    example of *illuminating ML’s development and implications transparently.* Let’s
    test this out by writing a statement that we believe should be negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following results in *Figure 14**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Our sentiment classifier correctly classifies our negative
    statement](img/B19488_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Our sentiment classifier correctly classifies our negative statement
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try one more example where we’d expect a neutral statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 14**.3* shows the resulting graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Our neutral statement “I am using the new feature” comes out
    positive again, thanks to the phrase “new feature”](img/B19488_14_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – Our neutral statement “I am using the new feature” comes out positive
    again, thanks to the phrase “new feature”
  prefs: []
  type: TYPE_NORMAL
- en: 'Fascinating! Our seemingly neutral statement *“I am using the new feature”*
    comes out mostly positive, and the tokens **new** and **feature** are the reasons
    why. To show this even further, let’s write one more neutral statement without
    saying *“**new feature”*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the results in *Figure 14**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Without “new feature,” the classifier performs as expected](img/B19488_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – Without “new feature,” the classifier performs as expected
  prefs: []
  type: TYPE_NORMAL
- en: From a governance standpoint, this is an opportunity for classification improvement,
    and with the help of LIME, we have identified a clear token pattern that leads
    to inaccuracies. Of course, finding these kinds of patterns one by one can become
    a tedious process, but sometimes, this is the nature of the iterative development
    of ML. We can deploy these tools in the cloud to run automatically, but, at the
    end of the day, the solutions here are to have better quality training data and
    update our model.
  prefs: []
  type: TYPE_NORMAL
- en: The many facets of ML development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve already touched on in our ML chapters, there are several dimensions
    to consider when thinking about ML at any level. Let’s recap some of the aspects
    of ML development as they relate to data stewardship:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mastering feature engineering**: Features form the bedrock of ML models.
    Hence, preserving and making them accessible is crucial. Using a feature store
    not only aids in model development but also provides lineage tracing, ensuring
    clarity in data transformation and eliminating potential discrepancies between
    training and real-time applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perfecting data handling**: Often, the cornerstone data for model training
    gets misplaced, complicating the recreation of model parameters. However, with
    tools such as Managed MLflow, datasets are meticulously logged, ensuring a seamless
    ML model development cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refining model training**: The journey from ideation to production in ML
    is seldom straightforward. Model selection involves rigorous evaluations, methodological
    considerations, and constant fine-tuning. Using platforms such as MLflow, each
    iteration (and the associated metrics) are captured, ensuring a transparent model
    training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond training – model deployment and monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensuring model accuracy doesn’t end with deployment; it demands continuous
    oversight, especially as models adapt to real-world scenarios. Monitoring encompasses
    several aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Concept drift**: Real-world variables, such as market shifts or evolving
    business strategies, can drastically affect model outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data adjustments**: While deliberate data changes might be easy to track,
    inadvertent shifts in data collection or representation can introduce model inconsistencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias**: Beyond statistical imbalance, bias can manifest as the unequal treatment
    of distinct groups, necessitating rigorous checks against potential disparities.
    We have already seen how techniques such as synthetic labeling can lead to biases
    bubbling up to the surface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For successful ML governance, establishing performance thresholds, monitoring
    frequencies, and using problem-alert procedures is pivotal. Many companies offer
    an ecosystem of tools, from automated dashboards and lineage tracking to data
    quality checks, ensuring models remain accurate, unbiased, and compliant.
  prefs: []
  type: TYPE_NORMAL
- en: A guide to architectural governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Architectural governance is the cornerstone of ensuring the seamless integration
    of IT infrastructure, and it supports core business processes. Its principal objectives
    encompass the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cataloging current architectural layouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing guidelines, principles, and benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aligning business and IT visions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafting a target infrastructure blueprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the value proposition of the target framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlighting disparities between the current and desired architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafting a comprehensive architectural roadmap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The five pillars of architectural governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With these five pillars in mind, we can also turn to some principles on how
    to modernize your governance architecture and optimize it's effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: Ensuring harmonious and integrated workflows without hitches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: This is paramount for protecting sensitive data and upholding
    regulatory compliance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: A forward-looking approach, taking into account the growing
    data needs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization**: Embracing universally accepted standards for flexibility
    and interoperability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reuse**: Promoting efficiency by creating and reutilizing components across
    the architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformative architectural principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to maximize the benefits of governance, certain foundational principles
    must be integrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Delivering trusted data as products**: Data isn’t merely a by-product; it
    is a resource, necessitating a product-centric mindset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritizing self-service accessibility**: This involves breaking down bureaucratic
    data barriers to allow more agile decision-making processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Democratizing data value creation**: Decentralizing data access ensures a
    more informed, data-driven organizational strategy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eliminating data silos**: A unified data strategy streamlines processes and
    prevents redundancy and discrepancies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zooming in on architectural dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take another look at our five pillars of governance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: Integrating diverse workloads, use cases, and platform components
    for a unified, coherent experience'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Championing data protection through robust access controls, especially
    for sensitive data types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: As a cloud-native, **platform as a service** (**PaaS**) solution,
    it effortlessly scales, adapting to varying computational demands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization**: By anchoring onto open source formats, such as Parquet,
    this prevents vendor lock-in and promotes interoperability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reuse**: The platform’s integrative approach with popular CI/CD tools ensures
    that any created components can be reused, ensuring efficiency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our journey through the realms of data, ML, and architectural governance,
    we’ve underscored the paramount importance of a cohesive strategy for organizations
    to navigate the digital age effectively. The era we currently inhabit is characterized
    by its rapid technological advancements, primarily propelled by the unparalleled
    power of data and analytics. In order to harness this power, organizations must
    have a clear blueprint—a blueprint that’s defined, structured, and actionable.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of this governance lies the principle of consistency. A uniform
    approach ensures that data from varied sources can be integrated seamlessly, enhancing
    the overall decision-making process. Equally crucial is the principle of security.
    With increasing threats in the digital domain, securing data assets is no longer
    optional but is mandatory for any forward-thinking organization. Furthermore,
    the dimension of scalability becomes essential, especially when we consider the
    exponential growth of data. Organizations need to be prepared not just for their
    present data requirements but also for future demands that might arise.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, these foundational elements are just the beginning. For data governance
    to be truly transformative, it must democratize access to data, ensuring that
    insights are not restricted to a select few but are available across the organization.
    This broad access, however, must not come at the cost of creating data silos,
    which can stifle innovation and hinder cross-functional collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude, it’s evident that the journey to optimal data governance is
    multi faceted. But with the right principles in place, organizations are better
    poised to unlock the vast potential that their data assets hold, heralding a new
    age of informed decision-making and strategic innovation.
  prefs: []
  type: TYPE_NORMAL
