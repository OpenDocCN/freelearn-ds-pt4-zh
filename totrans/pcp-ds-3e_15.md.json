["```py\n import pandas as pd\n import numpy as np\n compas_data = pd.read_csv('../data/compas-scores-two-years.csv')\n compas_data.head()\n```", "```py\ncompas_data.groupby('race')['decile_score'].value_counts(\n     normalize=True\n ).unstack().plot(\n     kind='bar', figsize=(20, 7),\n     title='Decile Score Histogram by Race', ylabel='% with Decile Score'\n )\n```", "```py\ncompas_df.groupby('race')['two_year_recid'].describe()\n```", "```py\n# Modify the race category for educational purposes and to address imbalance in the dataset\ncompas_df.loc[compas_df['race'].isin(['Native American', 'Asian']), 'race'] = 'Other'  # Adjust \"Asian\" and \"Native American\" categories to \"Other\"\ncompas_df.groupby('race')['two_year_recid'].value_counts(\n    normalize=True\n).unstack().plot(\n    kind='bar', figsize=(10, 5), title='Recidivism Rates Classified by Race'\n)  # Visualize Recidivism Rates across the refined racial groups\n```", "```py\ncompas_df['c_charge_degree'].value_counts(normalize=True).plot(\n    kind='bar', title='% of Charge Degree', ylabel='%', xlabel='Charge Degree'\n)\n```", "```py\n# feature construction, add up our three juv columns and remove the original features\ncompas_df['juv_count'] = compas_df[[\"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\"]].sum(axis=1)\ncompas_df[['juv_fel_count', 'juv_misd_count', 'juv_other_count', 'juv_count']].describe()\n```", "```py\n   dummies = pd.get_dummies(compas_df[['sex', 'race', 'c_charge_degree']], drop_first=True)\n   compas_df = pd.concat([compas_df, dummies], axis=1)\n```", "```py\n# Right skew on Age\n compas_df['age'].plot(\n     title='Histogram of Age', kind='hist', xlabel='Age', figsize=(10, 5)\n )\n # Right skew on Priors as well\n compas_df['priors_count'].plot(\n     title='Histogram of Priors Count', kind='hist', xlabel='Priors', figsize=(10, 5)\n )\n```", "```py\nWe can use a scikit-learn pipeline to run a standard scaler like so:\nnumerical_features = [\"age\", \"priors_count\"]\nnumerical_transformer = Pipeline(steps=[\n    ('scale', StandardScaler())\n])\n```", "```py\nimport os\nimport openai\nimport numpy as np\nfrom urllib.request import urlopen\nfrom openai.embeddings_utils import get_embedding\nfrom sentence_transformers import util\n```", "```py\nopenai.api_key = os.environ['OPENAI_API_KEY']\nENGINE = 'text-embedding-ada-002'\n```", "```py\ntext = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()\ndocuments = list(filter(lambda x: len(x) > 100, text.split('\\r\\n\\r\\n')))\nprint(f'There are {len(documents)} documents/paragraphs')\n```", "```py\nquestion_embedding = np.array(get_embedding(QUESTION))\nembeddings=[get_embedding(document) for document in documents]\nembeddings = np.array(embeddings)\n```", "```py\nQUESTION = 'How many horns does a flea have?'\nquestion_embedding = np.array(get_embedding(QUESTION, engine=ENGINE))\nhits = util.semantic_search(question_embedding, embeddings, top_k=1)[0]\nprint(f'Question: {QUESTION}\\n')\nfor i, hit in enumerate(hits):\n    print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n    print('\\n')\n```"]