<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer014">
			<h1 id="_idParaDest-15" class="chapter-number"><a id="_idTextAnchor014"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>The ML Process and Its Challenges</h1>
			<p>Welcome to the world of simplifying your <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) life cycle with the <span class="No-Break">Databricks platform.</span></p>
			<p>As a senior specialist <a id="_idIndexMarker000"/>solutions architect at Databricks specializing in ML, over the years, I have had the opportunity to collaborate with enterprises to architect ML-capable platforms to solve their unique business use cases using the Databricks platform. Now, that experience will be at your service to learn from. The knowledge you will gain from this book will open new career opportunities for you and change how you approach architecting ML pipelines for your organization’s ML <span class="No-Break">use cases.</span></p>
			<p>This book does assume that you have a reasonable understanding of the Python language as the accompanying code samples will be in Python. This book is not about teaching you ML techniques from scratch; it is assumed that you are an experienced data science practitioner who wants to learn how to take your ML use cases from development to production and all the steps in the middle using the <span class="No-Break">Databricks platform.</span></p>
			<p>For this book, some Python and pandas know-how is required. Being familiar with Apache Spark is a plus, and having a solid grasp of ML and data science <span class="No-Break">is necessary.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This book focuses on the features that are currently generally available. The code examples provided utilize Databricks notebooks. While Databricks is actively developing features to support <a id="_idIndexMarker001"/>workflows using external <strong class="bold">integrated development environments</strong> (<strong class="bold">IDEs</strong>), these specific features are not covered in this book. Also, going through this book will give you a solid foundation to quickly pick up new features as they <span class="No-Break">become GA.</span></p>
			<p>In this chapter, we will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Understanding the typical <span class="No-Break">ML process</span></li>
				<li>Discovering the personas involved with the machine learning process <span class="No-Break">in organizations</span></li>
				<li>Challenges with productionizing machine learning use cases <span class="No-Break">in organizations</span></li>
				<li>Understanding the requirements of an enterprise machine <span class="No-Break">learning platform</span></li>
				<li>Exploring Databricks and the <span class="No-Break">Lakehouse architecture</span></li>
			</ul>
			<p>By the end of this chapter, you should have a fundamental understanding of what a typical ML development life cycle looks like in an enterprise and the different personas involved in it. You will also know why most ML projects fail to deliver business value and how the Databricks Lakehouse Platform provides <span class="No-Break">a solution.</span></p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Understanding the typical machine learning process</h1>
			<p>The following <a id="_idIndexMarker002"/>diagram summarizes the ML process in <span class="No-Break">an organization:</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B17875_01_001.jpg" alt="Figure 1.1 – The data science development life cycle consists of three main stages – data preparation, modeling, and deployment" width="1650" height="685"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – The data science development life cycle consists of three main stages – data preparation, modeling, and deployment</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><span class="No-Break">Source: </span><a href="https://azure.microsoft.com/mediahandler/files/resourcefiles/standardizing-the-machine-learning-lifecycle/Standardizing%20ML%20eBook.pdf"><span class="No-Break">https://azure.microsoft.com/mediahandler/files/resourcefiles/standardizing-the-machine-learning-lifecycle/Standardizing%20ML%20eBook.pdf</span></a><span class="No-Break">.</span></p>
			<p>It is an iterative <a id="_idIndexMarker003"/>process. The raw structured and unstructured data first lands into a data lake from different sources. A data lake utilizes the scalable <a id="_idIndexMarker004"/>and cheap storage provided by cloud <a id="_idIndexMarker005"/>storage such as <strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">S3</strong>) or <strong class="bold">Azure Data Lake Storage</strong> (<strong class="bold">ADLS</strong>), depending on which cloud provider an organization uses. Due to regulations, many organizations have a multi-cloud strategy, making it essential to choose cloud-agnostic technologies and frameworks to simplify infrastructure management and reduce <span class="No-Break">operational overhead.</span></p>
			<p>Databricks defined a <a id="_idIndexMarker006"/>design pattern called the medallion architecture to organize data in a data lake. Before moving forward, let’s briefly understand what the medallion <span class="No-Break">architecture is:</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B17875_01_002.jpg" alt="Figure 1.2 – Databricks medallion architecture" width="944" height="320"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Databricks medallion architecture</p>
			<p>The medallion <a id="_idIndexMarker007"/>architecture is a data design pattern that’s used in a Lakehouse to organize data logically. It involves structuring data into layers (Bronze, Silver, and Gold) to progressively improve its quality and structure. The medallion architecture is also referred to as a “<span class="No-Break">multi-hop” architecture.</span></p>
			<p>The Lakehouse architecture, which combines the best features of data lakes and data warehouses, offers <a id="_idIndexMarker008"/>several benefits, including a simple data model, ease of implementation, incremental <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>), and the ability to recreate tables from raw data at any time. It also provides features such as ACID transactions and time travel for data versioning and historical analysis. We will expand more on the lakehouse in the <em class="italic">Exploring the Databricks Lakehouse </em><span class="No-Break"><em class="italic">architecture</em></span><span class="No-Break"> section.</span></p>
			<p>In the medallion architecture, the Bronze layer holds raw data sourced from external systems, preserving <a id="_idIndexMarker009"/>its original structure along with additional metadata. The focus here is on quick <strong class="bold">change data capture</strong> (<strong class="bold">CDC</strong>) and maintaining a historical archive. The Silver layer, on the other hand, houses cleansed, conformed, and “just enough” transformed data. It provides an enterprise-wide view of key business entities and serves as a source for self-service analytics, ad hoc reporting, and <span class="No-Break">advanced analytics.</span></p>
			<p>The Gold layer is where curated business-level tables reside that have been organized for consumption and reporting purposes. This layer utilizes denormalized, read-optimized data models with fewer joins. Complex transformations and data quality rules are applied here, facilitating the final presentation layer for various projects, such as customer analytics, product quality analytics, inventory analytics, and more. Traditional data <a id="_idIndexMarker010"/>marts and <strong class="bold">enterprise data warehouses</strong> (<strong class="bold">EDWs</strong>) can also be integrated into the lakehouse to enable comprehensive “pan-EDW” advanced analytics <span class="No-Break">and ML.</span></p>
			<p>The medallion <a id="_idIndexMarker011"/>architecture aligns well with the concept of a data mesh, where Bronze and Silver tables can be joined in a “one-to-many” fashion to generate multiple downstream tables, enhancing data scalability <span class="No-Break">and autonomy.</span></p>
			<p>Apache Spark has taken over Hadoop as the <em class="italic">de facto</em> standard for processing data at scale in the last six years due to advancements in performance and large-scale developer community adoption and support. There are many excellent books on Apache Spark written by the creators of Apache Spark themselves; these have been listed in the <em class="italic">Further reading</em> section. They can give more insights into the other benefits of <span class="No-Break">Apache Spark.</span></p>
			<p>Once the clean data lands in the Gold standard tables, features are generated by combining gold datasets, which act as input for ML <span class="No-Break">model training.</span></p>
			<p>During the model <a id="_idIndexMarker012"/>development and training phase, various sets of <strong class="bold">hyperparameters</strong> and ML algorithms are tested to identify the optimal combination of the model and corresponding hyperparameters. This process relies on predetermined evaluation metrics such as accuracy, R2 score, and <span class="No-Break">F1 score.</span></p>
			<p>In the context of ML, hyperparameters are parameters that govern the learning process of a model. They are not learned from the data itself but are set before training. Examples of hyperparameters include the learning rate, regularization strength, number of hidden layers in a neural network, or the choice of a kernel function in a support vector machine. Adjusting these hyperparameters can significantly impact the performance and behavior of <span class="No-Break">the model.</span></p>
			<p>On the other hand, training an ML model involves deriving values for other <strong class="bold">model parameters</strong>, such as node <a id="_idIndexMarker013"/>weights or model coefficients. These parameters are learned during the training process using the training data to minimize a chosen loss or error function. They are specific to the model being trained and are determined iteratively through optimization techniques such as gradient descent or <span class="No-Break">closed-form solutions.</span></p>
			<p>Expanding beyond node weights, model parameters can also include coefficients in regression models, intercept terms, feature importance scores in decision trees, or filter weights in convolutional neural networks. These parameters are directly learned from the data during the <a id="_idIndexMarker014"/>training process and contribute to the model’s ability to <span class="No-Break">make predictions.</span></p>
			<p class="callout-heading">Parameters</p>
			<p class="callout">You can <a id="_idIndexMarker015"/>learn more about parameters <span class="No-Break">at </span><a href="https://en.wikipedia.org/wiki/Parameter"><span class="No-Break">https://en.wikipedia.org/wiki/Parameter</span></a><span class="No-Break">.</span></p>
			<p>The finalized <a id="_idIndexMarker016"/>model is deployed either for batch, streaming, or real-time inference as a <strong class="bold">Representational State Transfer</strong> (<strong class="bold">REST</strong>) endpoint using containers. In this phase, we set up monitoring for drift and governance around the deployed models to manage the model life cycle and enforce access control around usage. Let’s take a look at the different personas involved in taking an ML use case from development <span class="No-Break">to production.</span></p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Discovering the roles associated with machine learning projects in organizations</h1>
			<p>Typically, three <a id="_idIndexMarker017"/>different types of persona are involved in developing an ML solution in <span class="No-Break">an organization:</span></p>
			<ul>
				<li><strong class="bold">Data engineers</strong>: The data <a id="_idIndexMarker018"/>engineers create data pipelines that take in structured, semi-structured, and unstructured data from source systems and ingest them in a data lake. Once the raw data lands in the data lake, the data engineers are also responsible for securely storing the data, ensuring that the data is reliable, clean, and easy to discover and utilize by the users in <span class="No-Break">the organization.</span></li>
				<li><strong class="bold">Data scientists</strong>: Data <a id="_idIndexMarker019"/>scientists collaborate with <strong class="bold">subject matter experts</strong> (<strong class="bold">SMEs</strong>) to understand and address business problems, ensuring <a id="_idIndexMarker020"/>a solid business justification for projects. They utilize clean data from data lakes and perform feature <a id="_idIndexMarker021"/>engineering, selecting and transforming relevant features. By developing and training multiple ML models with different sets of hyperparameters, data scientists can evaluate them on test sets to identify the best-performing model. Throughout this process, collaboration with SMEs validates the models against business requirements, ensuring <a id="_idIndexMarker022"/>their alignment with objectives and <strong class="bold">key performance indicators</strong> (<strong class="bold">KPIs</strong>). This iterative approach helps data scientists select a model that effectively solves the problem and meets the <span class="No-Break">specified KPIs.</span></li>
				<li><strong class="bold">Machine learning engineers</strong>: The ML engineering teams deploy the ML models <a id="_idIndexMarker023"/>created by data scientists into production environments. It is crucial to establish procedures, governance, and access control early on, including defining data scientist access to specific environments and data. ML engineers also implement monitoring systems to track model performance and data drift. They enforce governance practices, track model lineage, and ensure access control for data security and compliance throughout the ML <span class="No-Break">life cycle.</span></li>
			</ul>
			<p>A typical ML project life cycle consists of data engineering, then data science, and lastly, production deployment by the ML engineering team. This is an <span class="No-Break">iterative process.</span></p>
			<p>Now, let’s take a look at the various challenges involved in productionizing <span class="No-Break">ML models.</span></p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Challenges with productionizing machine learning use cases in organizations</h1>
			<p>At this point, we understand what a typical ML project life cycle looks like in an organization <a id="_idIndexMarker024"/>and the different personas involved in the ML process. It looks very intuitive, though we still see many enterprises struggling to deliver business value from their data <span class="No-Break">science projects.</span></p>
			<p>In 2017, Gartner analyst Nick Heudecker admitted that 85% of data science projects fail. A report published by <strong class="bold">Dimensional Research</strong> (<a href="https://dimensionalresearch.com/">https://dimensionalresearch.com/</a>) also uncovered that only 4% of companies have been successful in deploying ML use cases to production. A recent study done by Rackspace Global Technologies in 2021 uncovered that only 20% of the 1,870 organizations in various industries have mature AI and <span class="No-Break">ML practices.</span></p>
			<p class="callout-heading">Sources</p>
			<p class="callout">See the <em class="italic">Further reading</em> section for more details on <span class="No-Break">these statistics.</span></p>
			<p>Most enterprises face some common technical challenges in successfully delivering business value from data <span class="No-Break">science projects:</span></p>
			<ul>
				<li><strong class="bold">Unintended data silos and messy data</strong>: Data silos can be considered as groups of data in an organization that are governed and accessible only by specific users or groups within the organization. Some valid reasons to have data silos <a id="_idIndexMarker025"/>include compliance with <a id="_idIndexMarker026"/>particular regulations around privacy laws such as <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) in Europe or the <strong class="bold">California Privacy Rights Act</strong> (<strong class="bold">CCPA</strong>). These conditions are usually an exception to the norm. Gartner stated that almost 87% of organizations have low analytics and business intelligence maturity, meaning that data is not being <span class="No-Break">fully utilized.</span><p class="list-inset">Data silos generally arise as different departments within organizations. They have different technology stacks to manage and process <span class="No-Break">the data.</span></p><p class="list-inset">The following figure highlights <span class="No-Break">this challenge:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/Figure_01.3_B17875.jpg" alt="Figure 1.3 – The tools used by the different teams in an organization and the different silos" width="1614" height="760"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – The tools used by the different teams in an organization and the different silos</p>
			<p class="list-inset">The different <a id="_idIndexMarker027"/>personas work with different sets of tools and have different work environments. Data analysts, data engineers, data scientists, and ML engineers utilize different tools and development environments due to their distinct roles and objectives. Data analysts rely on SQL, spreadsheets, and visualization tools for insights and reporting. Data engineers work with programming languages and platforms such as Apache Spark to build and manage data infrastructure. Data scientists use statistical programming languages, ML frameworks, and data visualization libraries to develop predictive models. ML engineers combine ML expertise with software engineering skills to deploy models into production systems. These divergent toolsets can pose challenges in terms of data consistency, tool compatibility, and collaboration. Standardized processes and knowledge sharing can help mitigate these challenges and foster effective teamwork. Traditionally, there is little to no collaboration between these teams. As a result, a data science use case with a validated business value may not be developed at the required pace, negatively impacting the growth and effective management of <span class="No-Break">the business.</span></p>
			<p class="list-inset">When the concept <a id="_idIndexMarker028"/>of data lakes came up in the past decade, they promised a scalable and cheap solution to support structured and unstructured data. The goal was to enable organization-wide effective usage and collaboration of data. In reality, most data lakes ended up becoming data swamps, with little to no governance regarding the quality <span class="No-Break">of data.</span></p>
			<p class="list-inset">This inherently made ML very difficult since an ML model is only as good as the data it’s <span class="No-Break">trained on.</span></p>
			<ul>
				<li><strong class="bold">Building and managing an effective ML production environment is challenging</strong>: The ML teams at Google have done a lot of research on the technical challenges around setting up an ML development environment. A research paper published in NeurIPS on hidden technical debt in ML systems engineering from Google (<a href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a>) documented that writing ML code is just a tiny piece of the whole ML development life cycle. To develop an effective ML development practice in an organization, many tools, configurations, and monitoring aspects need to be integrated into the overall architecture. One of the critical components is monitoring drift in model performance and providing feedback <span class="No-Break">and retraining:</span></li>
			</ul>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/Figure_01.4_B17875.jpg" alt="Figure 1.4 – Hidden Technical Debt in Machine Learning Systems, NeurIPS 2015" width="1650" height="601"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Hidden Technical Debt in Machine Learning Systems, NeurIPS 2015</p>
			<p>Let’s understand <a id="_idIndexMarker029"/>the requirements of an enterprise-grade ML platform a <span class="No-Break">bit more.</span></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Understanding the requirements of an enterprise-grade machine learning platform</h1>
			<p>In the fast-paced world of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) and ML, an enterprise-grade ML platform takes <a id="_idIndexMarker030"/>center stage as a critical component. It is a comprehensive software platform that offers the infrastructure, tools, and processes required to <a id="_idIndexMarker031"/>construct, deploy, and manage ML models at a grand scale. However, a truly robust ML platform goes beyond these capabilities, extending to every stage of the ML life cycle, from data preparation, model training, and deployment to constant monitoring <span class="No-Break">and improvements.</span></p>
			<p>When we speak of an enterprise-grade ML platform, several key attributes determine its effectiveness, each of which is considered a cornerstone of such platforms. Let’s delve deeper into each of these critical requirements and understand their significance in an <span class="No-Break">enterprise setting.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Scalability – the growth catalyst</h2>
			<p>Scalability is an essential attribute, enabling the platform to adapt to the expanding needs of <a id="_idIndexMarker032"/>a burgeoning organization. In the context of ML, this encompasses the capacity to handle voluminous datasets, manage multiple models simultaneously, and accommodate a growing number of concurrent users. As the organization’s data grows exponentially, the platform must have the capability to expand and efficiently process the increasing data without <span class="No-Break">compromising performance.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Performance – ensuring efficiency and speed</h2>
			<p>In a real-world enterprise setting, the ML platform’s performance directly influences business <a id="_idIndexMarker033"/>operations. It should possess the capability to deliver high performance both in the training and inference stages. These stages are critical to ensure that models can be efficiently trained with minimum resources, and then deployed into production environments, ready to make timely and accurate predictions. A high-performance platform translates to faster decisions, and in today’s fast-paced business world, every <span class="No-Break">second counts.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Security – safeguarding data and models</h2>
			<p>In an era where data breaches are common, an ML platform’s security becomes a paramount <a id="_idIndexMarker034"/>concern. A robust ML platform should prioritize security and comply with industry regulations. This involves an assortment of features such as stringent data encryption techniques, access control mechanisms to prevent unauthorized access, and auditing capabilities to track activities in the system, all of which contribute to securely handling sensitive data and <span class="No-Break">ML models.</span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Governance – steering the machine learning life cycle</h2>
			<p>Governance <a id="_idIndexMarker035"/>is an often overlooked yet vital attribute of an enterprise-grade ML platform. Effective governance tools can facilitate the management of the entire life cycle of ML models. They can control versioning, maintain lineage tracking to understand the evolution of models, and audit for regulatory compliance and transparency. As the complexity of ML projects increases, governance <a id="_idIndexMarker036"/>tools ensure smooth sailing by managing the models and maintaining a clean and <span class="No-Break">understandable system.</span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Reproducibility – ensuring trust and consistency</h2>
			<p>Reproducibility serves as a foundation for trust in any ML model. The ML platform should <a id="_idIndexMarker037"/>ensure the reproducibility of the results from ML experiments, thereby establishing credibility and confidence in the models. This means that given the same data and the same conditions, the model should produce the same outputs consistently. Reproducibility directly impacts the decision-making process, ensuring the decisions are consistent and reliable, and the models can <span class="No-Break">be trusted.</span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Ease of use – balancing complexity and usability</h2>
			<p>Last, but by no means least, is the ease of use of the ML platform. Despite the inherent complexity <a id="_idIndexMarker038"/>of ML processes, the platform should be intuitive and user-friendly for a wide range of users, from data scientists to ML engineers. This extends to features such as a streamlined user interface, a well-documented API, and a user-centric design, making it easier for users to develop, deploy, and manage models. An easy-to-use platform reduces the barriers to entry, increases adoption, and empowers users to focus more on the ML tasks at hand rather than struggling with <span class="No-Break">the platform.</span></p>
			<p>In essence, an enterprise MLOps platform needs capabilities for model development, deployment, scalability, collaboration, monitoring, and automation. Databricks fits in by offering a unified environment for ML practitioners to develop and train models, deploy them at scale, and monitor their performance. It supports collaboration, integrates with popular deployment technologies, and provides automation and <span class="No-Break">CI/CD capabilities.</span></p>
			<p>Now, let’s delve deeper into the capabilities of the Databricks Lakehouse architecture and its unified AI/analytics platform, which establish it as an exceptional ML platform for <span class="No-Break">enterprise readiness.</span></p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Exploring Databricks and the Lakehouse architecture</h1>
			<p>Databricks is a <a id="_idIndexMarker039"/>renowned cloud-native and enterprise-ready data analytics platform that integrates data engineering, data science, and ML to enable organizations to develop and deploy ML models <span class="No-Break">at scale.</span></p>
			<p>Cloud-native <a id="_idIndexMarker040"/>refers to an approach where software applications are designed, developed, and deployed specifically for cloud environments. It involves utilizing technologies such as containers, microservices, and orchestration platforms to achieve scalability, resilience, and agility. By leveraging the cloud’s capabilities, Databricks can scale dynamically, recover from failures, and adapt quickly to changing demands, enabling organizations to maximize the benefits of <span class="No-Break">cloud computing.</span></p>
			<p>Databricks achieves the six cornerstones of an enterprise-grade ML platform. Let’s take a <span class="No-Break">closer look.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Scalability – the growth catalyst</h2>
			<p>Databricks provides fully managed Apache Spark (an open source distributed computing system known <a id="_idIndexMarker041"/>for its ability to handle large volumes of data and perform computations in a distributed <span class="No-Break">manner) clusters.</span></p>
			<p>Apache Spark consists <a id="_idIndexMarker042"/>of several components, including nodes and a driver program. <strong class="bold">Nodes</strong> refer to <a id="_idIndexMarker043"/>the individual machines or servers within the Spark cluster <a id="_idIndexMarker044"/>that contribute computational resources. The <strong class="bold">driver</strong> program is responsible for running the user’s application code and coordinating the overall execution of the Spark job. It communicates with the <strong class="bold">cluster manager</strong> to allocate <a id="_idIndexMarker045"/>resources and manages the <strong class="bold">SparkContext</strong>, which serves <a id="_idIndexMarker046"/>as the entry point to the Spark cluster. <strong class="bold">RDDs</strong> are the <a id="_idIndexMarker047"/>core data structure, enabling parallel processing, and Spark <a id="_idIndexMarker048"/>uses a <strong class="bold">directed acyclic graph</strong> (<strong class="bold">DAG</strong>) to optimize computations. <strong class="bold">Transformations</strong> and <strong class="bold">actions</strong> are performed on RDDs, while cluster managers handle resource allocation. Additionally, caching and shuffling <span class="No-Break">enhance performance.</span></p>
			<p>The <strong class="bold">DataFrames</strong> API in <a id="_idIndexMarker049"/>Spark is a distributed collection of data that’s organized into named columns. It provides a higher-level abstraction compared to working directly with RDDs in Spark, making it easier to manipulate and analyze structured data. It supports a SQL-like syntax and provides a wide range of functions for data manipulation <span class="No-Break">and transformation.</span></p>
			<p>Spark provides APIs in various languages, including Scala, Java, Python, and R, allowing users to leverage their existing skills and choose the language they are most <span class="No-Break">comfortable with.</span></p>
			<p>Apache Spark processes large datasets across multiple nodes, making it highly scalable. It supports both <a id="_idIndexMarker050"/>streaming and batch processing. This means that you can use Spark to process real-time data streams as well as large-scale batch jobs. Spark Structured Streaming, a component of Spark, allows you to process live data streams in a scalable and fault-tolerant manner. It provides high-level abstractions that make it easy to write streaming applications using familiar batch <span class="No-Break">processing concepts.</span></p>
			<p>Furthermore, Databricks allows for dynamic scaling and autoscaling of clusters, which adjusts resources based on the workload, ensuring the efficient use of resources while accommodating growing <span class="No-Break">organizational needs.</span></p>
			<p>While this book doesn’t delve into Apache Spark in detail, we have curated a <em class="italic">Further reading</em> section with excellent recommendations that will help you explore Apache Spark <span class="No-Break">more comprehensively.</span></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Performance – ensuring efficiency and speed</h2>
			<p>Databricks Runtime is optimized for the cloud and includes enhancements over open source Apache <a id="_idIndexMarker051"/>Spark that significantly increase performance. The Databricks Delta engine provides fast query execution for big data and AI workflows while reducing the time and resources needed for data preparation and iterative model training. Its optimized runtime improves both model training and inference speeds, resulting in more <span class="No-Break">efficient operations.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Security – safeguarding data and models</h2>
			<p>Databricks <a id="_idIndexMarker052"/>ensures a high level of security through various means. It offers <a id="_idIndexMarker053"/>data encryption at rest and in transit, uses <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) to provide fine-grained user permissions, and integrates <a id="_idIndexMarker054"/>with identity providers for <strong class="bold">single </strong><span class="No-Break"><strong class="bold">sign-on</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SSO</strong></span><span class="No-Break">).</span></p>
			<p>Databricks also has a feature called Unity Catalog. Unity Catalog is a centralized metadata store for Databricks workspaces that offers data governance capabilities such as access control, auditing, lineage, and data discovery. Its key features include centralized governance, a universal security model, automated lineage tracking, and easy data discovery. Its benefits include improved governance, reduced operational overhead, and increased <a id="_idIndexMarker055"/>data agility. Unity Catalog is a powerful tool for enhancing data governance in Databricks. Unity Catalog is a complex topic that will not be covered extensively in this book. However, you can find more information on it in the <em class="italic">Further reading</em> section, where a link has <span class="No-Break">been provided.</span></p>
			<p>The Databricks platform is compliant with several industry regulations, including GDPR, CCPA, HIPAA, SOC 2 Type II, and ISO/IEC 27017. For a complete list of certifications, check <span class="No-Break">out </span><a href="https://www.databricks.com/trust/compliance"><span class="No-Break">https://www.databricks.com/trust/compliance</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Governance – steering the machine learning life cycle</h2>
			<p>Databricks provides MLflow, an open source platform for managing the ML life cycle, including <a id="_idIndexMarker056"/>experimentation, reproducibility, and deployment. It supports model versioning and model registry for tracking model versions and their stages in the life cycle (staging, production, and others). Additionally, the platform provides audit logs for tracking user activity, helping meet regulatory requirements and promoting transparency. Databricks has its own hosted feature store as well, which we will cover in more detail in <span class="No-Break">later chapters.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Reproducibility – ensuring trust and consistency</h2>
			<p>With MLflow, Databricks ensures the reproducibility of ML models. MLflow allows users to log <a id="_idIndexMarker057"/>parameters, metrics, and artifacts for each run of an experiment, providing a record of what was done and allowing for exact replication of the results. It also supports packaging code into reproducible runs and sharing it with others, further ensuring the repeatability <span class="No-Break">of experiments.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Ease of use – balancing complexity and usability</h2>
			<p>Databricks <a id="_idIndexMarker058"/>provides a collaborative workspace that enables data scientists and engineers to work together seamlessly. It offers interactive notebooks with support for multiple languages (Python, R, SQL, and Scala) in a single notebook, allowing users to use their preferred language. The platform’s intuitive interface, coupled with extensive documentation and a robust API, makes it user-friendly, enabling users to focus more on ML tasks rather than the complexities of platform management. In addition to its collaborative and analytical capabilities, Databricks integrates with various data sources, storage systems, and cloud platforms, making it flexible and adaptable to different data ecosystems. It supports seamless integration with <a id="_idIndexMarker059"/>popular data lakes, databases, and cloud storage services, enabling users to easily access and process data from multiple sources. Although this book specifically focuses on the ML and MLOps capabilities of Databricks, it makes sense to understand what the Databricks Lakehouse architecture is and how it simplifies scaling and managing ML project life cycles <span class="No-Break">for organizations.</span></p>
			<p><strong class="bold">Lakehouse</strong>, as a term, is a <a id="_idIndexMarker060"/>combination <a id="_idIndexMarker061"/>of two terms: <strong class="bold">data lakes</strong> and <strong class="bold">data warehouses</strong>. Data <a id="_idIndexMarker062"/>warehouses are great at handling structured data and SQL queries. They are extensively used for powering <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) applications but have limited support for ML. They store <a id="_idIndexMarker063"/>data in proprietary formats and can only be accessed using <span class="No-Break">SQL queries.</span></p>
			<p>Data lakes, on the other hand, do a great job supporting ML use cases. A data lake allows organizations to store a large amount of their structured and unstructured data in a central scalable store. They are easy to scale and support open formats. However, data lakes have a significant drawback when it comes to running BI workloads. Their performance is not comparable to data warehouses. The lack of schema governance enforcement turned most data lakes in organizations <span class="No-Break">into swamps.</span></p>
			<p>Typically, in modern enterprise architecture, there is a need for both. This is where Databricks defined the Lakehouse architecture. Databricks provides a unified analytics platform called the Databricks Lakehouse Platform. The Lakehouse Platform provides a persona-based single platform that caters to all the personas involved in data processing and gains insights. The personas include data engineers, BI analysts, data scientists, and MLOps. This can tremendously simplify the data processing and analytics architecture of <span class="No-Break">any organization.</span></p>
			<p>At the time <a id="_idIndexMarker064"/>of writing this book, the Lakehouse Platform <a id="_idIndexMarker065"/>is available <a id="_idIndexMarker066"/>on all three major clouds: <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), <strong class="bold">Microsoft Azure</strong>, and <strong class="bold">Google Compute </strong><span class="No-Break"><strong class="bold">Platform</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GCP</strong></span><span class="No-Break">).</span></p>
			<p>Lakehouse can be thought of as a technology that combines data warehouses’ performance and data governance aspects and makes them available at the scale of data lakes. Under <a id="_idIndexMarker067"/>the hood, Lakehouse uses an open protocol called <span class="No-Break"><strong class="bold">Delta</strong></span><span class="No-Break"> (</span><a href="https://delta.io/"><span class="No-Break">https://delta.io/</span></a><span class="No-Break">).</span></p>
			<p>The Delta format adds reliability, performance, and governance to the data in data lakes. Delta <a id="_idIndexMarker068"/>also provides <strong class="bold">Atomicity, Consistency, Isolation, and Durability</strong> (<strong class="bold">ACID</strong>) transactions, making sure that all data operations either fully succeed or fail. In addition to ACID transaction support, under the hood, Delta uses the Parquet format. Unlike the regular Parquet format, the Delta format keeps track of transaction logs, offering enhanced capabilities. It also <a id="_idIndexMarker069"/>supports granular access controls to your data, along with versioning and the ability to roll back to previous versions. Delta format tables scale effortlessly with data and are underpinned by Apache Spark while utilizing advanced indexing and caching to improve performance at scale. There are many more benefits that the Delta format provides that you can read about on the <span class="No-Break">official website.</span></p>
			<p>When we say <strong class="bold">Delta Lake</strong>, we mean <a id="_idIndexMarker070"/>a data lake that uses the Delta format to provide the previously described benefits to the <span class="No-Break">data lake.</span></p>
			<p>The Databricks Lakehouse architecture is built on the foundation of <span class="No-Break">Delta Lake:</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/Figure_01.5_B17875.jpg" alt="Figure 1.5 – Databricks Lakehouse Platform" width="1421" height="872"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Databricks Lakehouse Platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Source: Courtesy <span class="No-Break">of Databricks</span></p>
			<p>Next, let’s discuss <a id="_idIndexMarker071"/>how the Databricks Lakehouse architecture can <span class="No-Break">simplify ML.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Simplifying machine learning development with the Lakehouse architecture</h2>
			<p>As we <a id="_idIndexMarker072"/>saw in the previous section, the Databricks Lakehouse Platform provides a cloud-native enterprise-ready solution that simplifies the data processing needs of an organization. It provides a single platform that enables different teams across enterprises to collaborate and reduces time to market for <span class="No-Break">new projects.</span></p>
			<p>The Lakehouse Platform has many components specific to data scientists and ML practitioners; we will cover these in more detail later in this book. For instance, at the time of writing this book, the Lakehouse Platform released a drop-down button that allows users <a id="_idIndexMarker073"/>to switch between persona-based views. There are tabs to quickly access the fully integrated and managed feature store, model registry, and MLflow tracking server in the ML practitioner <span class="No-Break">persona view:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/Figure_01.6_B17875.jpg" alt="Figure 1.6 – Databricks Lakehouse Platform persona selection dropdown" width="539" height="170"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Databricks Lakehouse Platform persona selection dropdown</p>
			<p>With that, let’s summarize <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Summary</h1>
			<p>In this chapter, we learned about ML, including the ML process, the personas involved, and the challenges organizations face in productionizing ML models. Then, we learned about the Lakehouse architecture and how the Databricks Lakehouse Platform can potentially simplify MLOps for organizations. These topics give us a solid foundation to develop a more profound understanding of how different Databricks ML-specific tools fit in the ML <span class="No-Break">life cycle.</span></p>
			<p>For in-depth learning about the various features and staying up to date with announcements, the Databricks documentation is the ideal resource. You can access the documentation via the link provided in the <em class="italic">Further reading</em> section. Moreover, on the documentation page, you can easily switch to different cloud-specific documentation to explore platform-specific details <span class="No-Break">and functionalities.</span></p>
			<p>In the next chapter, we will dive deeper into the ML-specific features of the Databricks <span class="No-Break">Lakehouse Platform.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li>Wikipedia, <em class="italic">Hyperparameter (machine </em><span class="No-Break"><em class="italic">learning)</em></span><span class="No-Break"> (</span><a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)"><span class="No-Break">https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)</span></a><span class="No-Break">).</span></li>
				<li>Matt Asay, 2017, <em class="italic">85% of big data projects fail</em>, TechRepublic, <span class="No-Break">November (</span><a href="https://www.techrepublic.com/article/85-of-big-data-projects-fail-but-your-developers-can-help-yours-succeed/"><span class="No-Break">https://www.techrepublic.com/article/85-of-big-data-projects-fail-but-your-developers-can-help-yours-succeed/</span></a><span class="No-Break">).</span></li>
				<li>Rackspace Technologies, <em class="italic">New Global Rackspace Technology Study Uncovers Widespread Artificial Intelligence and Machine Learning Knowledge Gap</em>, January <span class="No-Break">2021 (</span><a href="https://www.rackspace.com/newsroom/new-global-rackspace-technology-study-uncovers-widespread-artificial-intelligence-and"><span class="No-Break">https://www.rackspace.com/newsroom/new-global-rackspace-technology-study-uncovers-widespread-artificial-intelligence-and</span></a><span class="No-Break">).</span></li>
				<li>Gartner, <em class="italic">Gartner Data Shows 87 Percent of Organizations Have Low BI and Analytics Maturity</em>, December <span class="No-Break">2018 (</span><a href="https://www.gartner.com/en/newsroom/press-releases/2018-12-06-gartner-data-shows-87-percent-of-organizations-have-low-bi-and-analytics-maturity"><span class="No-Break">https://www.gartner.com/en/newsroom/press-releases/2018-12-06-gartner-data-shows-87-percent-of-organizations-have-low-bi-and-analytics-maturity</span></a><span class="No-Break">).</span></li>
				<li><em class="italic">Learning Spark: Lightning-Fast Data Analytics</em>, by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia: This comprehensive guide covers the fundamentals of Spark, including RDDs, the DataFrame API, Spark Streaming, MLlib, and GraphX. With practical examples and use cases, it will help you become proficient in using Spark for <span class="No-Break">data analytics.</span></li>
				<li><em class="italic">Spark: The Definitive Guide</em>, by Bill Chambers and Matei Zaharia: This acclaimed book provides a deep dive into Spark’s core concepts and advanced features. It covers Spark’s architecture, data processing techniques, ML, graph processing, and deployment considerations. Suitable for beginners and experienced users, it offers a comprehensive understanding <span class="No-Break">of Spark.</span></li>
				<li><em class="italic">High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark</em>, by Holden Karau, Rachel Warren, and Matei Zaharia: This book explores strategies for optimizing Spark applications to achieve maximum performance and scalability. It offers insights into tuning Spark configurations, improving data locality, leveraging advanced features, and designing efficient <span class="No-Break">data pipelines.</span></li>
				<li><em class="italic">Spark in Action</em>, by Jean-Georges Perrin: This practical guide takes you through the entire Spark ecosystem, covering data ingestion, transformation, ML, real-time processing, and integration with other technologies. With hands-on examples and real-world use cases, it enables you to apply Spark to your <span class="No-Break">specific projects.</span></li>
				<li><em class="italic">Get Started using Unity </em><span class="No-Break"><em class="italic">Catalog</em></span><span class="No-Break"> (</span><a href="https://docs.databricks.com/data-governance/unity-catalog/get-started.html"><span class="No-Break">https://docs.databricks.com/data-governance/unity-catalog/get-started.html</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Databricks </em><span class="No-Break"><em class="italic">documentation</em></span><span class="No-Break"> (</span><a href="https://docs.databricks.com/introduction/index.html"><span class="No-Break">https://docs.databricks.com/introduction/index.html</span></a><span class="No-Break">).</span></li>
			</ul>
		</div>
	</div>
</div>
</body></html>