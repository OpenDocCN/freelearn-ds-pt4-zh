<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer031">
			<h1 id="_idParaDest-37" class="chapter-number"><a id="_idTextAnchor036"/>2</h1>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Overview of ML on Databricks</h1>
			<p>This chapter will give you a fundamental understanding of how to get started with ML on Databricks. The ML workspace is data scientist-friendly and allows rapid ML development by providing out-of-the-box support for popular ML libraries such as TensorFlow, PyTorch, and <span class="No-Break">many more.</span></p>
			<p>We will cover setting up a trial Databricks account and learn about the various ML-specific features available at ML practitioners’ fingertips in the Databricks workspace. You will learn how to start a cluster on Databricks and create a <span class="No-Break">new notebook.</span></p>
			<p>In this chapter, we will cover these <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Setting up a Databricks <span class="No-Break">trial account</span></li>
				<li>Introduction to the ML workspace <span class="No-Break">on Databricks</span></li>
				<li>Exploring <span class="No-Break">the workspace</span></li>
				<li><span class="No-Break">Exploring clusters</span></li>
				<li><span class="No-Break">Exploring notebooks</span></li>
				<li><span class="No-Break">Exploring data</span></li>
				<li><span class="No-Break">Exploring experiments</span></li>
				<li>Discovering the <span class="No-Break">feature store</span></li>
				<li>Discovering the <span class="No-Break">model registry</span></li>
				<li><span class="No-Break">Libraries</span></li>
			</ul>
			<p>These topics will cover the essential features to perform effective <strong class="bold">ML Operations</strong> (<strong class="bold">MLOps</strong>) on Databricks. Links to the Databricks official documentation will also be included at relevant places if you wish to learn about a particular feature in <span class="No-Break">more detail.</span></p>
			<p>Let’s look at how we can get access to a <span class="No-Break">Databricks workspace.</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Technical requirements</h1>
			<p>For this chapter, you’ll need access to the Databricks workspace with cluster creation privileges. By default, the owner of the workspace has permission to create clusters. We will cover clusters in more detail in the <em class="italic">Exploring clusters</em> sections. You can read more about the various cluster access control options <span class="No-Break">here: </span><a href="https://docs.databricks.com/security/access-control/cluster-acl.html"><span class="No-Break">https://docs.databricks.com/security/access-control/cluster-acl.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Setting up a Databricks trial account</h1>
			<p>At the time of writing, Databricks<a id="_idIndexMarker074"/> is available on<a id="_idIndexMarker075"/> all the major<a id="_idIndexMarker076"/> cloud platforms, namely <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), <strong class="bold">Microsoft Azure</strong>, and <strong class="bold">Amazon Web </strong><span class="No-Break"><strong class="bold">Services</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AWS</strong></span><span class="No-Break">).</span></p>
			<p>Databricks provides an easy way to either<a id="_idIndexMarker077"/> create an account within the community edition or start a 14-day trial with all the enterprise features available in <span class="No-Break">the workspace.</span></p>
			<p>To fully leverage the code examples provided in this book and explore the enterprise features we’ll cover, I highly recommend taking advantage of the 14-day trial option. This trial will grant you access to all the necessary functionalities, ensuring a seamless experience throughout your <span class="No-Break">learning journey.</span></p>
			<p>Please go through this link to sign up for trial <span class="No-Break">account: </span><a href="https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account"><span class="No-Break">https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account</span></a></p>
			<p>On filling out the introductory form, you will be redirected to a page that will provide you with options to start with trial deployments on either of the three clouds or create a Community <span class="No-Break">Edition account:</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/Figure_02.01_B17875.jpg" alt="Figure 2.1 – How to get a free Databricks trial account" width="399" height="670"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – How to get a free Databricks trial account</p>
			<p>Once your signup<a id="_idIndexMarker078"/> is successful, you will receive an email describing how to log in to the <span class="No-Break">Databricks workspace.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Most of the features we will cover in this chapter will be accessible with the 14-day <span class="No-Break">trial option.</span></p>
			<p>Once you log into the workspace, access the persona selector tab on the left navigation bar. We will change<a id="_idIndexMarker079"/> our persona to <span class="No-Break"><strong class="bold">Machine Learning</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/Figure_02.02_B17875.jpg" alt="Figure 2.2 – The persona-based workspace switcher" width="559" height="622"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – The persona-based workspace switcher</p>
			<p>Now, let’s take a look at the new Databricks ML <span class="No-Break">workspace features.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Exploring the workspace</h1>
			<p>The <strong class="bold">workspace</strong> is within a Databricks ML<a id="_idIndexMarker080"/> environment. Each user of the Databricks ML environment will have a workspace. Users can create notebooks and develop code in isolation or collaborate with other teammates through granular access controls. You will be working within the workspace or repos for most of your time in the Databricks environment. We will learn more about repos in the <span class="No-Break"><em class="italic">Repos</em></span><span class="No-Break"> section:</span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/Figure_02.03_B17875.jpg" alt="Figure 2.3 – The Workspace tab" width="685" height="630"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – The Workspace tab</p>
			<p>It’s important to note that the <strong class="bold">Workspace</strong> area is primarily intended for Databricks notebooks. While the workspace<a id="_idIndexMarker081"/> does support version control for notebooks using Git providers within Databricks, it’s worth highlighting that this version control capability within workspace notebooks is now considered less recommended compared to <span class="No-Break">using repos.</span></p>
			<p>Version control, in the context of software development, is a system that helps track changes made to files over time. It allows you to maintain a historical record of modifications, enabling collaboration, bug tracking, and reverting to previous versions if needed. In the case of Databricks, version control specifically refers to tracking changes made <span class="No-Break">to notebooks.</span></p>
			<p>To enhance best practices, Databricks is transitioning away from relying solely on the version control feature within the workspace. Instead, it emphasizes the use of repos, which offers improved support for both Databricks and non-Databricks-specific files. This strategic shift provides a more comprehensive and versatile approach to managing code and files within the <span class="No-Break">Databricks environment.</span></p>
			<p>By utilizing repos, you can effectively manage and track changes not only to notebooks but also to other file types. This includes code files, scripts, configuration files, and more. Repos leverage popular version control systems such as Git, enabling seamless collaboration, branch management, code review workflows, and integration with external tools<a id="_idIndexMarker082"/> and services. Let’s look at the <strong class="bold">Repos</strong> feature, which was recently added to <span class="No-Break">the workspace.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Repos</h2>
			<p>Repos is short for<a id="_idIndexMarker083"/> repository. This convenient<a id="_idIndexMarker084"/> feature allows you to version control your code in the Databricks environment. Using repos, you can store arbitrary files within a Git repository. At the time of writing, Databricks supports the following <span class="No-Break">Git providers:</span></p>
			<ul>
				<li><span class="No-Break">GitHub</span></li>
				<li><span class="No-Break">Bitbucket</span></li>
				<li><span class="No-Break">GitLab</span></li>
				<li><span class="No-Break">Azure DevOps</span></li>
			</ul>
			<p>Databricks repos provide a logging mechanism to track and record various user interactions with a Git repository. These interactions include actions such as committing code changes<a id="_idIndexMarker085"/> and submitting pull requests. The repo<a id="_idIndexMarker086"/> features are also available through the REST <strong class="bold">application programming interface</strong> (<span class="No-Break"><strong class="bold">API</strong></span><span class="No-Break">) (</span><a href="https://docs.databricks.com/dev-tools/api/latest/repos.html"><span class="No-Break">https://docs.databricks.com/dev-tools/api/latest/repos.html</span></a><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/Figure_02.04_B17875.jpg" alt="Figure 2.4 – The Repos tab" width="476" height="459"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – The Repos tab</p>
			<p>You can read more about<a id="_idIndexMarker087"/> how to set up repos for your environment at https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks. Repositories <a id="_idIndexMarker088"/>are essential for setting up your CI/CD processes in the Databricks<a id="_idIndexMarker089"/> environment. The <strong class="bold">Repos</strong> feature allows users to version their code and also <span class="No-Break">allows reproducibility.</span></p>
			<p><strong class="bold">Continuous integration/continuous deployment</strong> (<strong class="bold">CI/CD</strong>) is a software development approach<a id="_idIndexMarker090"/> that involves automating the processes of integrating code changes, testing them, and deploying them to production environments. In the last chapter of this book, we will discuss more about the deployment paradigms in Databricks and CI/CD as part of <span class="No-Break">your MLOps:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/Figure_02.05_B17875.jpg" alt="Figure 2.5 – The supported Git providers" width="691" height="475"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – The supported Git providers</p>
			<p>Now, let’s look<a id="_idIndexMarker091"/> at clusters, the central compute units for performing model training <a id="_idIndexMarker092"/>in the <span class="No-Break">Databricks environment.</span></p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Exploring clusters</h1>
			<p><strong class="bold">Clusters</strong> are the primary computing units<a id="_idIndexMarker093"/> that will do the heavy lifting when you’re training your ML models. The VMs associated with a cluster are provisioned in Databricks users’ cloud subscriptions; however, the Databricks UI provides an interface to control the cluster type and <span class="No-Break">its settings.</span></p>
			<p>Clusters are ephemeral compute resources. No data is stored <span class="No-Break">on clusters:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/Figure_02.06_B17875.jpg" alt="Figure 2.6 – The Clusters tab" width="667" height="409"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – The Clusters tab</p>
			<p>The <strong class="bold">Pools</strong> feature allows end users to create<a id="_idIndexMarker094"/> Databricks VM pools. One of the benefits of working in the cloud environment is that you can request new compute resources on demand. The end user pays by the second and returns the compute once the load on the cluster is low. This is great; however, requesting a VM from the cloud provider, ramping it up, and adding it to a cluster still takes some time. Using pools, you can pre-provision VMs to keep them in a standby state. If a cluster requests new nodes and has access to the pool, then if the pool has the required VMs available, within seconds, these nodes will be added to the cluster, helping reduce the cluster scale uptime. Once the cluster is done processing high load or is terminated, the machine borrowed from the pool is returned to the pool and can be used by the next cluster. More about pools<a id="_idIndexMarker095"/> can be found <span class="No-Break">here: </span><a href="https://docs.databricks.com/clusters/instance-pools/index.html"><span class="No-Break">https://docs.databricks.com/clusters/instance-pools/index.html</span></a><span class="No-Break">.</span></p>
			<p>Databricks <strong class="bold">jobs</strong> allow users to automate code<a id="_idIndexMarker096"/> execution on a particular schedule. It has a lot of other valuable configurations around how many times you can retry code execution in case of failure<a id="_idIndexMarker097"/> and can set up alerts in case of failure. You can read a lot more about jobs here: <a href="https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html">https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html</a>. This link is for a Databricks workspace deployed on AWS; however, you can click on the <strong class="bold">Change cloud</strong> tab to match <span class="No-Break">your deployment:</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/Figure_02.07_B17875.jpg" alt="Figure 2.7 – The dropdown for selecting documentation pertinent to your cloud" width="794" height="487"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – The dropdown for selecting documentation pertinent to your cloud</p>
			<p>For now, let’s focus on the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">Cluster</strong></span><span class="No-Break"> tab.</span></p>
			<p>If you are coming from an ML background<a id="_idIndexMarker098"/> where most of the work has been done on your laptop or a single isolated VM, then Databricks provides an easy way to get started by providing a single-node mode. In this case, you will get all the benefits of Databricks while working on a single-node cluster. The existing non-distributed code should run as is on this cluster. As an example, the following code will run only on the driver node as is in a <span class="No-Break">non-distributed manner:</span></p>
			<pre class="console">
from sklearn.datasets import load_iris  # Importing the Iris datasetfrom sklearn.model_selection import train_test_split  # Importing train_test_split function
from sklearn.linear_model import LogisticRegression  # Importing Logistic Regression model
from sklearn.metrics import accuracy_score  # Importing accuracy_score metric
# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize the logistic regression model
model = LogisticRegression()
# Train the model
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)
# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Typically, a cluster refers to a collection of machines processing data in a distributed fashion. In the case of a single-node cluster, a single <strong class="bold">VM</strong> runs all the processes on multiple VMs in a <span class="No-Break">regular cluster.</span></p>
			<p>It is straightforward to start up a cluster<a id="_idIndexMarker099"/> in the Databricks environment. All the code provided in this book can be run on a single-node cluster. To spin up a single-node cluster, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Give a name to <span class="No-Break">the cluster.</span></li>
				<li>Change the cluster mode to <span class="No-Break"><strong class="bold">Single Node</strong></span><span class="No-Break">.</span></li>
				<li>Set the latest ML runtime to <strong class="bold">Databricks </strong><span class="No-Break"><strong class="bold">Runtime Version</strong></span><span class="No-Break">.</span></li>
				<li>Click <span class="No-Break"><strong class="bold">Create cluster</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/Figure_02.08_B17875.jpg" alt="Figure 2.8 – The New Cluster screen" width="500" height="634"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – The New Cluster screen</p>
			<p>This will start the process of provisioning<a id="_idIndexMarker100"/> our cluster. There are some advanced settings, such as adding tags, using init scripts, and connecting through JDBC to this cluster, that you can <span class="No-Break">read about.</span></p>
			<p><strong class="bold">Databricks Runtime</strong> is a powerful platform that enhances<a id="_idIndexMarker101"/> big data analytics by improving the performance, security, and usability of Spark jobs. With features such as optimized I/O, enhanced security, and simplified operations, it offers a comprehensive<a id="_idIndexMarker102"/> solution. It comes in various flavors, including <strong class="bold">ML</strong> and <strong class="bold">Photon</strong>, catering to specific needs. Databricks Runtime<a id="_idIndexMarker103"/> is the ideal choice for running big data analytics workloads effectively. Databricks Runtime is powered by Delta Lake, which seamlessly integrates batch and streaming data to enable near-real-time analytics. Delta Lake’s capability to track data versions over time is crucial for reproducing ML model training and experimentation. This ensures data consistency and empowers reproducibility in your workflows. You can read more about Databricks Runtime <span class="No-Break">here: </span><a href="https://docs.databricks.com/runtime/index.html"><span class="No-Break">https://docs.databricks.com/runtime/index.html</span></a><span class="No-Break">.</span><a id="_idIndexMarker104"/></p>
			<p>You will be using the ML runtime as an ML practitioner on Databricks. Databricks Runtime ML is a pre-built ML infrastructure that’s integrated with the capabilities of the Databricks workspace. It provides popular ML libraries such as TensorFlow and PyTorch, distributed training libraries such as Horovod, and pre-configured GPU support. With faster cluster creation and compatibility with installed libraries, it simplifies scaling ML and deep learning tasks. Additionally, it offers data exploration, cluster management, code and environment management, automation support, and integrated MLflow for model development <span class="No-Break">and deployment.</span></p>
			<p>Databricks provides three different cluster access modes and their specific recommended use case patterns. All these cluster access modes can be used either in a multi-node (your cluster has a dedicated driver node and one or more executor nodes) or a single-node fashion (your cluster has a single node; both the driver program<a id="_idIndexMarker105"/> and executor programs run on a <span class="No-Break">single node).</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Single user</h2>
			<p>This mode<a id="_idIndexMarker106"/> is recommended for single users and data applications that can be developed using Python, Scala, SQL, and R. Clusters are set to terminate after 120 minutes of inactivity, and the standard cluster is the default cluster mode. End users can also use this cluster to execute a notebook through a Databricks job using a scheduled activity. It is best to segregate different data processing pipelines into separate standard clusters. Segregating data pipelines prevents the failure of one cluster from affecting another. As Databricks charges customers by the second, this approach is viable and widely used. A cluster with this access type supports <span class="No-Break">ML workloads.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Shared</h2>
			<p>This mode is ideal when multiple users<a id="_idIndexMarker107"/> try to use the same cluster. It can provide maximum resource utilization and has lower query latency requirements in multiuser scenarios. Data applications can be developed using Python and SQL but not R and Scala. These clusters provide user isolation and also support <span class="No-Break">ML workloads.</span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>No isolation shared</h2>
			<p>This type of cluster<a id="_idIndexMarker108"/> is intended only for admin users. We won’t cover too much about this type of access as this cluster type doesn’t support ML <span class="No-Break">use cases.</span></p>
			<p>You can read more about user isolation<a id="_idIndexMarker109"/> <span class="No-Break">here: </span><a href="https://docs.databricks.com/notebooks/notebook-isolation.html"><span class="No-Break">https://docs.databricks.com/notebooks/notebook-isolation.html</span></a><span class="No-Break">.</span></p>
			<p>Let’s take a look at single-node clusters as this is the type of cluster you will be using to run the code that’s been shared as part of <span class="No-Break">this book.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Single-node clusters</h2>
			<p>Single-node clusters<a id="_idIndexMarker110"/> do not have worker nodes, and all the Python code runs on the driver node. These clusters are configured to terminate after 120 minutes of inactivity by default<a id="_idIndexMarker111"/> and can be used to build and test small data pipelines and do lightweight <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) and ML development. Python, Scala, SQL, and R <span class="No-Break">are supported.</span></p>
			<p>If you want to use specific libraries not included in the runtime, we will explore the various options to install <a id="_idTextAnchor047"/>the required libraries in the <em class="italic">Library</em> section of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Exploring notebooks</h1>
			<p>If you are familiar with <strong class="bold">Jupyter</strong> and <strong class="bold">IPython notebooks</strong>, then Databricks notebooks<a id="_idIndexMarker112"/> will look very<a id="_idIndexMarker113"/> familiar. A Databricks<a id="_idIndexMarker114"/> notebook development environment consists of cells where end users can interactively write code in R, Python, Scala, <span class="No-Break">or SQL.</span></p>
			<p>Databricks notebooks also have additional functionalities such as integration with the Spark UI, powerful integrated visualizations, version control, and an MLflow model tracking server. We can also parameterize a notebook and pass parameters to it at <span class="No-Break">execution time.</span></p>
			<p>We will cover notebooks in more detail as the code examples presented to you in this book utilize the Databricks notebook environment. Additional details about<a id="_idIndexMarker115"/> notebooks can be found <span class="No-Break">at </span><a href="https://docs.databricks.com/notebooks/index.html"><span class="No-Break">https://docs.databricks.com/notebooks/index.html</span></a><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/Figure_02.09_B17875.jpg" alt="Figure 2.9 – Databricks notebooks" width="919" height="271"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Databricks notebooks</p>
			<p>Let’s take a look at the next feature<a id="_idIndexMarker116"/> on the <strong class="bold">Data</strong> tab, also<a id="_idIndexMarker117"/> called the <span class="No-Break">Databricks metastore.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/>Exploring data</h1>
			<p>By default, when<a id="_idIndexMarker118"/> a new workspace<a id="_idIndexMarker119"/> is deployed, it comes<a id="_idIndexMarker120"/> with a managed Hive <strong class="bold">metastore</strong>. A metastore<a id="_idIndexMarker121"/> allows you<a id="_idIndexMarker122"/> to register <a id="_idIndexMarker123"/>datasets<a id="_idIndexMarker124"/> in various formats such as <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>), <strong class="bold">Parquet</strong>, <strong class="bold">Delta format</strong>, <strong class="bold">text</strong>, or <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) as an external table (<a href="https://docs.databricks.com/data/metastores/index.html">https://docs.databricks.com/data/metastores/index.html</a>). We will not go too<a id="_idIndexMarker125"/> much into detail about the <span class="No-Break">metastore here:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/Figure_02.10_B17875.jpg" alt="Figure 2.10 – The Data tab" width="1534" height="963"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – The Data tab</p>
			<p>It’s all right if you are not familiar<a id="_idIndexMarker126"/> with the term metastore. In simple terms, it is similar to a relational database. In relational databases, there are databases and then table names and schemas. The end user can use SQL to interact with the data stored in databases and tables. Similarly, in Databricks, end users can decide to register datasets stored<a id="_idIndexMarker127"/> in cloud storage so that they’re available as tables. You can learn more <span class="No-Break">here: </span><a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html"><span class="No-Break">https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html</span></a><span class="No-Break">.</span></p>
			<p>The Hive metastore provides a means of implementing access control by utilizing table access control lists for local users within the workspace. However, to enhance data access governance and ensure unified control over various assets such as deployed models and AI assets, Databricks has introduced Unity Catalog as a best practice solution. This enables comprehensive management and governance across <span class="No-Break">multiple workspaces.</span></p>
			<p>Let’s understand Unity Catalog in a bit <span class="No-Break">more detail.</span></p>
			<p>Unity Catalog is a unified governance<a id="_idIndexMarker128"/> solution for data and AI assets on the lakehouse. It provides centralized access control, auditing, lineage, and data discovery capabilities across <span class="No-Break">Databricks workspaces:</span></p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/Figure_02.11_B17875.jpg" alt="Figure 2.11 – Unity Catalog’s relationship to workspaces" width="964" height="325"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Unity Catalog’s relationship to workspaces</p>
			<p>Here are some of the key features<a id="_idIndexMarker129"/> of <span class="No-Break">Unity Catalog:</span></p>
			<ul>
				<li><strong class="bold">Define once, secure everywhere</strong>: Unity Catalog administers data access policies across all workspaces and personas from a <span class="No-Break">single place</span></li>
				<li><strong class="bold">Standards-compliant security model</strong>: Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing <span class="No-Break">data lake</span></li>
				<li><strong class="bold">Built-in auditing and lineage</strong>: Unity Catalog captures user-level audit logs and lineage data, tracking how data assets are created and used across all languages <span class="No-Break">and personas</span></li>
				<li><strong class="bold">Data discovery</strong>: Unity Catalog provides a search interface to help data consumers find data and lets users tag and document <span class="No-Break">data assets</span></li>
				<li><strong class="bold">System tables (Public Preview)</strong>: Unity Catalog provides<a id="_idIndexMarker130"/> operational data, including audit logs, billable usage, <span class="No-Break">and lineage</span></li>
			</ul>
			<p>Let’s understand<a id="_idIndexMarker131"/> what the Unity Catalog<a id="_idIndexMarker132"/> object model <span class="No-Break">looks like:</span></p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/Figure_02.12_B17875.jpg" alt="Figure 2.12 – The Unity Catalog object model" width="1487" height="457"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – The Unity Catalog object model</p>
			<p>The Unity Catalog’s hierarchy of primary data objects flows from metastore <span class="No-Break">to table:</span></p>
			<ul>
				<li><strong class="bold">Metastore</strong>: The top-level container for metadata. Each metastore exposes a three-level namespace (<strong class="bold">catalog.schema.table</strong>) that organizes <span class="No-Break">your data.</span></li>
				<li><strong class="bold">Catalog</strong>: This is the first layer of the object hierarchy and is used to organize your <span class="No-Break">data assets.</span></li>
				<li><strong class="bold">Schema</strong>: Also known as databases, schemas <a id="_idIndexMarker133"/>contain tables <span class="No-Break">and views.</span></li>
				<li><strong class="bold">Table</strong>: The lowest level in the object hierarchy are tables <span class="No-Break">and views.</span></li>
			</ul>
			<p>As mentioned previously, doing a deep dive into Unity Catalog is a big topic in itself and outside the scope of this book. Unity Catalog offers centralized governance, auditing, and data discovery capabilities for data and AI assets across Databricks workspaces. It provides a secure model based on ANSI SQL, automatic capture of user-level audit logs and data lineage, and a hierarchical data organization system. It also supports a variety of data formats, advanced identity management, specified admin roles for data governance, and is compatible with Databricks Runtime 11.3 LTS <span class="No-Break">or above.</span></p>
			<p>For a more comprehensive<a id="_idIndexMarker134"/> understanding of Unity Catalog, go <span class="No-Break">to </span><a href="https://docs.databricks.com/data-governance/unity-catalog/index.html"><span class="No-Break">https://docs.databricks.com/data-governance/unity-catalog/index.html</span></a><span class="No-Break">.</span></p>
			<p>All the features we’ve covered so far are standard among all the Databricks <span class="No-Break">persona-specific features.</span></p>
			<p>The following three features, namely<a id="_idIndexMarker135"/> experiments, the feature store, and models, are critical for the <span class="No-Break">ML persona.</span></p>
			<p>Let’s take a look at them one <span class="No-Break">by one.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor050"/>Exploring experiments</h1>
			<p>As the name suggests, experiments<a id="_idIndexMarker136"/> are the central location where all the model training pertinent to business problems can be accessed. Users can define their name for the experiment or a default system-generated one and use it to train the different ML model training runs. Experiments in the Databricks UI come from integrating MLflow into the platform. We will dive deeper into MLflow in the coming chapters to understand more details; however, it’s important to get a sense of what MLflow is and some of the terminology that <span class="No-Break">is MLflow-specific.</span></p>
			<p>MLflow is an open source platform for managing the end-to-end ML life cycle. Here are the key components<a id="_idIndexMarker137"/> <span class="No-Break">of MLflow:</span></p>
			<ul>
				<li><strong class="bold">Tracking</strong>: This allows you to track experiments to record and compare parameters <span class="No-Break">and results.</span></li>
				<li><strong class="bold">Models</strong>: This component helps manage and deploy models from various ML libraries to a variety of model serving and <span class="No-Break">inference platforms.</span></li>
				<li><strong class="bold">Projects</strong>: This allows you to package ML code in a reusable, reproducible form so that you can share it with other data scientists or transfer it <span class="No-Break">to production.</span></li>
				<li><strong class="bold">Model Registry</strong>: This centralizes a model store for managing models’ full life cycle stage transitions: from staging to production, with capabilities for versioning and annotating. Databricks<a id="_idIndexMarker138"/> provides a managed version of the Model Registry in <span class="No-Break">Unity Catalog.</span></li>
				<li><strong class="bold">Model Serving</strong>: This allows you to host MLflow models as <span class="No-Break">REST endpoints.</span></li>
			</ul>
			<p>There are also certain<a id="_idIndexMarker139"/> terms specific <span class="No-Break">to MLflow:</span></p>
			<ul>
				<li><strong class="bold">Run</strong>: A run represents<a id="_idIndexMarker140"/> a specific instance of training an ML model. It comprises parameters, metrics, artifacts, and metadata associated with the <span class="No-Break">training process.</span></li>
				<li><strong class="bold">Experiment</strong>: An experiment serves as a container<a id="_idIndexMarker141"/> for organizing and tracking the results of ML experiments. It consists of multiple runs, allowing for easy comparison and analysis of <span class="No-Break">different approaches.</span></li>
				<li><strong class="bold">Parameter</strong>: A parameter refers<a id="_idIndexMarker142"/> to a configurable value that can be adjusted during the training of an ML model. These values influence the behavior and performance of <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Metric</strong>: A metric is a quantitative measure<a id="_idIndexMarker143"/> that’s used to evaluate the performance of an ML model. Metrics provide insights into how well the model is performing on specific tasks <span class="No-Break">or datasets.</span></li>
				<li><strong class="bold">Artifact</strong>: An artifact refers to any output<a id="_idIndexMarker144"/> generated during an ML experiment. This can include files, models, images, or other relevant data that captures the results or intermediate stages of <span class="No-Break">the experiment.</span></li>
				<li><strong class="bold">Project</strong>: A project encompasses<a id="_idIndexMarker145"/> the code, data, and configuration necessary to reproduce an ML experiment. It provides a structured and organized approach to managing all the components required for a specific <span class="No-Break">ML workflow.</span></li>
				<li><strong class="bold">Model</strong>: A model represents<a id="_idIndexMarker146"/> a trained ML model that can be utilized to make predictions or perform specific tasks based on the learned patterns and information from the <span class="No-Break">training data.</span></li>
				<li><strong class="bold">Model registry</strong>: A model registry<a id="_idIndexMarker147"/> serves as a centralized repository for storing and managing ML models. It provides versioning, tracking, and collaboration capabilities for different model versions and their <span class="No-Break">associated metadata.</span></li>
				<li><strong class="bold">Backend store</strong>: The backend store is responsible<a id="_idIndexMarker148"/> for storing MLflow entities such as runs, parameters, metrics, and tags. It provides the underlying storage infrastructure for managing <span class="No-Break">experiment data.</span></li>
				<li><strong class="bold">Artifact store</strong>: The artifact store is responsible for storing<a id="_idIndexMarker149"/> artifacts produced during ML experiments. This can include files, models, images, or any other relevant data that’s generated throughout the <span class="No-Break">experimentation process.</span></li>
				<li><strong class="bold">Flavor</strong>: A flavor represents<a id="_idIndexMarker150"/> a standardized way of packaging an ML model, allowing it to be easily consumed by specific tools or platforms. Flavors provide flexibility and interoperability when deploying and <span class="No-Break">serving models.</span></li>
				<li><strong class="bold">UI</strong>: The UI refers to the graphical<a id="_idIndexMarker151"/> interface provided by MLflow, allowing users to interact with and visualize experiment results, track runs, and manage models through an <span class="No-Break">intuitive interface.</span></li>
			</ul>
			<p>MLflow also employs additional<a id="_idIndexMarker152"/> terms, but the ones mentioned here are some of the most commonly<a id="_idIndexMarker153"/> used. For further details, please consult the MLflow <span class="No-Break">documentation: </span><a href="https://mlflow.org/docs/latest/index.html"><span class="No-Break">https://mlflow.org/docs/latest/index.html</span></a><span class="No-Break">.</span></p>
			<p>Databricks AutoML is fully integrated with MLflow, so all the model training and the artifacts that are generated are automatically logged in the <span class="No-Break">MLflow server:</span></p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/Figure_02.13_B17875.jpg" alt="Figure 2.13 – The Experiments tab" width="193" height="536"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – The Experiments tab</p>
			<p>End users can also utilize Databricks AutoML to start modeling a solution for their ML problems. Databricks <a id="_idIndexMarker154"/>has taken a different approach with its AutoML capability, called <strong class="bold">glass </strong><span class="No-Break"><strong class="bold">box AutoML</strong></span><span class="No-Break">.</span></p>
			<p>Databricks AutoML simplifies<a id="_idIndexMarker155"/> the workflow for ML practitioners by automatically generating comprehensive notebooks. These notebooks encompass all the necessary code for feature engineering and model training, covering various combinations of ML models and hyperparameters. This feature allows ML practitioners to thoroughly inspect the generated code and gain deeper insights into <span class="No-Break">the process.</span></p>
			<p>Databricks AutoML currently supports classification, regression, and forecasting models. For a list of algorithms<a id="_idIndexMarker156"/> that AutoML can use to create models, go <span class="No-Break">to </span><a href="https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms"><span class="No-Break">https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms</span></a><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/Figure_02.14_B17875.jpg" alt="Figure 2.14 – The default experiment is linked to Python notebooks by default" width="1106" height="543"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – The default experiment is linked to Python notebooks by default</p>
			<p>MLflow was developed in-house at Databricks to ease the end-to-end ML life cycle and MLOps. Since the launch of MLflow, it has been widely<a id="_idIndexMarker157"/> adopted and supported by the open <span class="No-Break">source community.</span></p>
			<p>Now, let’s look at the <span class="No-Break">feature store.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor051"/>Discovering the feature store</h1>
			<p>The <strong class="bold">feature store</strong> is a relatively new yet stable release<a id="_idIndexMarker158"/> in the latest Databricks ML workspace. Many organizations that have mature ML processes in place, such as Uber, Facebook, DoorDash, and many more, have internally implemented their <span class="No-Break">feature stores.</span></p>
			<p>ML life cycle management and workflows are complex. Forbes conducted a survey (<a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says">https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says</a>) with data scientists and uncovered that managing data is the most expensive and time-consuming operation in their <span class="No-Break">day-to-day work.</span></p>
			<p>Data scientists need to spend a lot of time finding the data, cleaning it, doing EDA, and then performing feature engineering to train their ML models. This is an iterative process. The effort that needs to be put in to make the process repeatable is an enormous challenge. This is where feature stores <span class="No-Break">come in.</span></p>
			<p>Databricks Feature Store<a id="_idIndexMarker159"/> is standardized on the open source Delta format, which allows data scientists to govern features similar to those used to govern access to models, notebooks, or jobs in the <span class="No-Break">Databricks environment.</span></p>
			<p>Databricks Feature Store is unique in a couple <span class="No-Break">of ways:</span></p>
			<ul>
				<li>It uses Delta Lake to store feature tables. This allows end users to read data from any of the supported languages<a id="_idIndexMarker160"/> and connectors outside of Databricks. More can be read <span class="No-Break">here: </span><a href="https://docs.delta.io/latest/delta-intro.html"><span class="No-Break">https://docs.delta.io/latest/delta-intro.html</span></a><span class="No-Break">.</span></li>
				<li>The integrated Feature Store UI within the Databricks ML workspace provides end-to-end traceability and lineage of how the features were generated and which downstream models use it in a single unified view. We will look at this in more detail in <a href="B17875_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></li>
			</ul>
			<p>Databricks Feature Store also integrates seamlessly with MLflow. This allows Databricks Feature Store to utilize all the great features of MLflow’s feature pipelines, as well as to generate features and write them out as feature tables in Delta format. The Feature Store has its own generic model packaging format that is compatible with the MLflow Models component, which lets your models know exactly which features were used to train the models. This integration makes it possible to simplify our <span class="No-Break">MLOps pipeline.</span></p>
			<p>A client can call the serving endpoint either in batch mode or online mode, and the model will automatically retrieve the latest features from the Feature Store and provide inference. We will see practical<a id="_idIndexMarker161"/> examples of this in the <span class="No-Break">coming chapters.</span></p>
			<p>You can also read more about the current state of Databricks Feature Store here:  <a href="https://docs.databricks.com/machine-learning/feature-store/index.html"><span class="No-Break">https://docs.databricks.com/machine-learning/feature-store/index.html</span></a></p>
			<p>Lastly, let’s discuss the <span class="No-Break">model registry.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Discovering the model registry</h1>
			<p><strong class="bold">Models</strong> is a fully managed<a id="_idIndexMarker162"/> and integrated MLflow model registry available to each deployed Databricks ML workspace. The registry has its own set of APIs and a UI to collaborate with data scientists across the organization and fully manage the MLflow model. Data scientists and ML engineers<a id="_idIndexMarker163"/> can develop models in any of the supported ML frameworks (<a href="https://mlflow.org/docs/latest/models.html#built-in-model-flavors">https://mlflow.org/docs/latest/models.html#built-in-model-flavors</a>) and package them in a generic MLfLow <span class="No-Break">model format:</span></p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/Figure_02.15_B17875.jpg" alt="Figure 2.15 – The Models tab" width="192" height="554"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – The Models tab</p>
			<p>The model registry provides<a id="_idIndexMarker164"/> features to manage the versioning, tagging, and state transitioning between different environments (moving models from staging to production <span class="No-Break">to archive):</span></p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/Figure_02.16_B17875.jpg" alt="Figure 2.16 – The Registered Models tab" width="1280" height="488"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – The Registered Models tab</p>
			<p>Before we move on, there is another important feature that we need to understand: the <strong class="bold">Libraries</strong> feature of Databricks. This feature<a id="_idIndexMarker165"/> allows users to utilize third-party or custom code available to Databricks notebooks and jobs running on <span class="No-Break">your cluster.</span></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>Libraries</h1>
			<p>Libraries are fundamental building blocks<a id="_idIndexMarker166"/> of any programming ecosystem. They are akin to toolboxes, comprising pre-compiled routines that offer enhanced functionality and assist in optimizing code efficiency. In Databricks, libraries are used to make third-party or custom code available to notebooks and jobs running on clusters. These libraries can be written in various languages, including Python, Java, Scala, <span class="No-Break">and R.</span></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Storing libraries</h2>
			<p>When it comes to storage, libraries uploaded<a id="_idIndexMarker167"/> using the library UI are stored in the <strong class="bold">Databricks File System</strong> (<strong class="bold">DBFS</strong>) root. However, all workspace users can modify data and files stored<a id="_idIndexMarker168"/> in the DBFS root. If a more secure storage option is desired, you can opt to store libraries in cloud object storage, use library package repositories, or upload libraries to <span class="No-Break">workspace files.</span></p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>Managing libraries</h2>
			<p>Library management<a id="_idIndexMarker169"/> in Databricks can be handled via three different interfaces: the workspace UI, the <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), or the Libraries API. Each option caters to different workflows<a id="_idIndexMarker170"/> and user preferences, and the choice often depends on individual use cases or <span class="No-Break">project requirements.</span></p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>Databricks Runtime and libraries</h2>
			<p>Databricks Runtime<a id="_idIndexMarker171"/> comes equipped with many common libraries. To find out which libraries<a id="_idIndexMarker172"/> are included in your runtime, you can refer to the <strong class="bold">System Environment</strong> subsection of the Databricks Runtime release notes to check your specific <span class="No-Break">runtime version.</span></p>
			<p>Note that Python <strong class="source-inline">atexit</strong> functions aren’t invoked by Databricks when your notebook or job finishes processing. If you’re utilizing a Python library that registers <strong class="source-inline">atexit</strong> handlers, it’s crucial to ensure your code calls the required functions before exiting. Also, the use of Python eggs is being phased out in Databricks Runtime and will eventually be removed; consider using Python wheels or installing packages from PyPI <span class="No-Break">as alternatives.</span></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>Library usage modes</h2>
			<p>Databricks allows three different modes<a id="_idIndexMarker173"/> for library installation: cluster-installed, notebook-scoped, and <span class="No-Break">workspace libraries:</span></p>
			<ul>
				<li><strong class="bold">Cluster libraries</strong>: These libraries are available<a id="_idIndexMarker174"/> for use by all notebooks running on a <span class="No-Break">particular cluster.</span></li>
				<li><strong class="bold">Notebook-scoped libraries</strong>: Available for Python and R, these libraries<a id="_idIndexMarker175"/> create an environment scoped to a notebook session and do not affect other notebooks running on the same cluster. They are temporary and need to be reinstalled for <span class="No-Break">each session.</span></li>
				<li><strong class="bold">Workspace libraries</strong>: These act as local repositories<a id="_idIndexMarker176"/> from which you can create cluster-installed libraries. They could be custom code written by your organization or a specific version of an open source library that your <span class="No-Break">organization</span><span class="No-Break"><a id="_idIndexMarker177"/></span><span class="No-Break"> prefers.</span></li>
			</ul>
			<p>Let’s next move on to cover Unity <span class="No-Break">Catalog limitations.</span></p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Unity Catalog limitations</h2>
			<p>There are certain limitations<a id="_idIndexMarker178"/> when using libraries<a id="_idIndexMarker179"/> with Unity Catalog. For more details, you should refer to the <em class="italic">Cluster </em><span class="No-Break"><em class="italic">libraries</em></span><span class="No-Break"> section.</span></p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Installation sources for libraries</h2>
			<p>Cluster libraries can be installed directly<a id="_idIndexMarker180"/> from public repositories such as PyPI, Maven, or CRAN. Alternatively, they can be sourced from a cloud object storage location, a workspace library in the DBFS root, or even by uploading library files from your local machine. Libraries installed directly via upload are stored in the <span class="No-Break">DBFS root.</span></p>
			<p>For most of our use cases, we will be using notebook-scoped libraries. You can install notebook-scoped libraries using the <strong class="source-inline">%pip</strong> <span class="No-Break"><strong class="source-inline">magic</strong></span><span class="No-Break"> command.</span></p>
			<p>Here are some ways to utilize <strong class="source-inline">%pip</strong> in notebooks<a id="_idIndexMarker181"/> to install <span class="No-Break">notebook-scoped libraries:</span></p>
			<ul>
				<li><strong class="bold">PyPI</strong>: Use <strong class="source-inline">%pip install &lt;package-name&gt;</strong> for notebook-scoped libraries or select PyPI as the source for <span class="No-Break">cluster libraries</span></li>
				<li><strong class="bold">A private PyPI mirror, such as Nexus or Artifactory</strong>: Use <strong class="source-inline">%pip install &lt;package-name&gt; --index-url &lt;mirror-url&gt;</strong> for <span class="No-Break">notebook-scoped libraries</span></li>
				<li><strong class="bold">VCS, such as GitHub, with a raw source</strong>: Use <strong class="source-inline">%pip install git+https://github.com/&lt;username&gt;/&lt;repo&gt;.git </strong>for notebook-scoped libraries or select PyPI as the source and specify the repository URL as the package name for <span class="No-Break">cluster libraries</span></li>
				<li><strong class="bold">DBFS</strong>: Use <strong class="source-inline">%pip install dbfs:/&lt;path-to-package&gt;</strong> for notebook-scoped libraries or select DBFS/S3 as the source for <span class="No-Break">cluster libraries</span></li>
			</ul>
			<p>Now, let’s summarize <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/>Summary</h1>
			<p>In this chapter, we got a brief overview of all the components of the Databricks ML workspace. This will enable us to utilize these components in a more hands-on fashion so that we can train ML models and deploy them for various ML problems efficiently in the <span class="No-Break">Databricks environment.</span></p>
			<p>In the next chapter, we will start working on a customer churn prediction problem and register our first feature tables in the Databricks <span class="No-Break">feature store.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Databricks <span class="No-Break">libraries: </span><a href="https://docs.databricks.com/libraries/index.html%20"><span class="No-Break">https://docs.databricks.com/libraries/index.html</span></a></li>
				<li>Databricks <span class="No-Break">notebooks: </span><a href="https://docs.databricks.com/notebooks/index.html"><span class="No-Break">https://docs.databricks.com/notebooks/index.html</span></a></li>
			</ul>
		</div>
	</div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer032" class="Content">
			<h1 id="_idParaDest-62" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor062"/>Part 2: ML Pipeline Components and Implementation</h1>
			<p>At the end of this section, you will have a good understanding of each of the ML components that are available in the Databricks ML experience and will be comfortable using them in <span class="No-Break">your projects.</span></p>
			<p>This section has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B17875_03.xhtml#_idTextAnchor063"><em class="italic">Chapter 3</em></a>, <em class="italic">Utilizing the Feature Store</em></li>
				<li><a href="B17875_04.xhtml#_idTextAnchor076"><em class="italic">Chapter 4</em></a>, <em class="italic">Understanding MLflow Components on Databricks</em></li>
				<li><a href="B17875_05.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Create a Baseline Model Using Databricks AutoML</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer033">
			</div>
		</div>
		<div>
			<div id="_idContainer034" class="Basic-Graphics-Frame">
			</div>
		</div>
	</div>
</div>
</body></html>