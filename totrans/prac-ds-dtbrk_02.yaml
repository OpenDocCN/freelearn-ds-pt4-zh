- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview of ML on Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will give you a fundamental understanding of how to get started
    with ML on Databricks. The ML workspace is data scientist-friendly and allows
    rapid ML development by providing out-of-the-box support for popular ML libraries
    such as TensorFlow, PyTorch, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover setting up a trial Databricks account and learn about the various
    ML-specific features available at ML practitioners’ fingertips in the Databricks
    workspace. You will learn how to start a cluster on Databricks and create a new
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover these main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Databricks trial account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the ML workspace on Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the workspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering the feature store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering the model registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These topics will cover the essential features to perform effective **ML Operations**
    (**MLOps**) on Databricks. Links to the Databricks official documentation will
    also be included at relevant places if you wish to learn about a particular feature
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we can get access to a Databricks workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you’ll need access to the Databricks workspace with cluster
    creation privileges. By default, the owner of the workspace has permission to
    create clusters. We will cover clusters in more detail in the *Exploring clusters*
    sections. You can read more about the various cluster access control options here:
    [https://docs.databricks.com/security/access-control/cluster-acl.html](https://docs.databricks.com/security/access-control/cluster-acl.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Databricks trial account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, Databricks is available on all the major cloud platforms,
    namely **Google Cloud Platform** (**GCP**), **Microsoft Azure**, and **Amazon
    Web** **Services** (**AWS**).
  prefs: []
  type: TYPE_NORMAL
- en: Databricks provides an easy way to either create an account within the community
    edition or start a 14-day trial with all the enterprise features available in
    the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: To fully leverage the code examples provided in this book and explore the enterprise
    features we’ll cover, I highly recommend taking advantage of the 14-day trial
    option. This trial will grant you access to all the necessary functionalities,
    ensuring a seamless experience throughout your learning journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please go through this link to sign up for trial account: [https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account](https://www.databricks.com/try-databricks?itm_data=PricingPage-Trial#account)'
  prefs: []
  type: TYPE_NORMAL
- en: 'On filling out the introductory form, you will be redirected to a page that
    will provide you with options to start with trial deployments on either of the
    three clouds or create a Community Edition account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – How to get a free Databricks trial account](img/Figure_02.01_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – How to get a free Databricks trial account
  prefs: []
  type: TYPE_NORMAL
- en: Once your signup is successful, you will receive an email describing how to
    log in to the Databricks workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Most of the features we will cover in this chapter will be accessible with the
    14-day trial option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log into the workspace, access the persona selector tab on the left
    navigation bar. We will change our persona to **Machine Learning**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – The persona-based workspace switcher](img/Figure_02.02_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – The persona-based workspace switcher
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the new Databricks ML workspace features.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the workspace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **workspace** is within a Databricks ML environment. Each user of the Databricks
    ML environment will have a workspace. Users can create notebooks and develop code
    in isolation or collaborate with other teammates through granular access controls.
    You will be working within the workspace or repos for most of your time in the
    Databricks environment. We will learn more about repos in the *Repos* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – The Workspace tab](img/Figure_02.03_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – The Workspace tab
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the **Workspace** area is primarily intended for
    Databricks notebooks. While the workspace does support version control for notebooks
    using Git providers within Databricks, it’s worth highlighting that this version
    control capability within workspace notebooks is now considered less recommended
    compared to using repos.
  prefs: []
  type: TYPE_NORMAL
- en: Version control, in the context of software development, is a system that helps
    track changes made to files over time. It allows you to maintain a historical
    record of modifications, enabling collaboration, bug tracking, and reverting to
    previous versions if needed. In the case of Databricks, version control specifically
    refers to tracking changes made to notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance best practices, Databricks is transitioning away from relying solely
    on the version control feature within the workspace. Instead, it emphasizes the
    use of repos, which offers improved support for both Databricks and non-Databricks-specific
    files. This strategic shift provides a more comprehensive and versatile approach
    to managing code and files within the Databricks environment.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing repos, you can effectively manage and track changes not only to
    notebooks but also to other file types. This includes code files, scripts, configuration
    files, and more. Repos leverage popular version control systems such as Git, enabling
    seamless collaboration, branch management, code review workflows, and integration
    with external tools and services. Let’s look at the **Repos** feature, which was
    recently added to the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Repos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Repos is short for repository. This convenient feature allows you to version
    control your code in the Databricks environment. Using repos, you can store arbitrary
    files within a Git repository. At the time of writing, Databricks supports the
    following Git providers:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitbucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks repos provide a logging mechanism to track and record various user
    interactions with a Git repository. These interactions include actions such as
    committing code changes and submitting pull requests. The repo features are also
    available through the REST **application programming interface** (**API**) ([https://docs.databricks.com/dev-tools/api/latest/repos.html](https://docs.databricks.com/dev-tools/api/latest/repos.html)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – The Repos tab](img/Figure_02.04_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – The Repos tab
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about how to set up repos for your environment at https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks.
    Repositories are essential for setting up your CI/CD processes in the Databricks
    environment. The **Repos** feature allows users to version their code and also
    allows reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous integration/continuous deployment** (**CI/CD**) is a software
    development approach that involves automating the processes of integrating code
    changes, testing them, and deploying them to production environments. In the last
    chapter of this book, we will discuss more about the deployment paradigms in Databricks
    and CI/CD as part of your MLOps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – The supported Git providers](img/Figure_02.05_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – The supported Git providers
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at clusters, the central compute units for performing model
    training in the Databricks environment.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Clusters** are the primary computing units that will do the heavy lifting
    when you’re training your ML models. The VMs associated with a cluster are provisioned
    in Databricks users’ cloud subscriptions; however, the Databricks UI provides
    an interface to control the cluster type and its settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clusters are ephemeral compute resources. No data is stored on clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – The Clusters tab](img/Figure_02.06_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – The Clusters tab
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Pools** feature allows end users to create Databricks VM pools. One of
    the benefits of working in the cloud environment is that you can request new compute
    resources on demand. The end user pays by the second and returns the compute once
    the load on the cluster is low. This is great; however, requesting a VM from the
    cloud provider, ramping it up, and adding it to a cluster still takes some time.
    Using pools, you can pre-provision VMs to keep them in a standby state. If a cluster
    requests new nodes and has access to the pool, then if the pool has the required
    VMs available, within seconds, these nodes will be added to the cluster, helping
    reduce the cluster scale uptime. Once the cluster is done processing high load
    or is terminated, the machine borrowed from the pool is returned to the pool and
    can be used by the next cluster. More about pools can be found here: [https://docs.databricks.com/clusters/instance-pools/index.html](https://docs.databricks.com/clusters/instance-pools/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks **jobs** allow users to automate code execution on a particular
    schedule. It has a lot of other valuable configurations around how many times
    you can retry code execution in case of failure and can set up alerts in case
    of failure. You can read a lot more about jobs here: [https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html](https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html).
    This link is for a Databricks workspace deployed on AWS; however, you can click
    on the **Change cloud** tab to match your deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The dropdown for selecting documentation pertinent to your cloud](img/Figure_02.07_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – The dropdown for selecting documentation pertinent to your cloud
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s focus on the **Create** **Cluster** tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are coming from an ML background where most of the work has been done
    on your laptop or a single isolated VM, then Databricks provides an easy way to
    get started by providing a single-node mode. In this case, you will get all the
    benefits of Databricks while working on a single-node cluster. The existing non-distributed
    code should run as is on this cluster. As an example, the following code will
    run only on the driver node as is in a non-distributed manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a cluster refers to a collection of machines processing data in a
    distributed fashion. In the case of a single-node cluster, a single **VM** runs
    all the processes on multiple VMs in a regular cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is straightforward to start up a cluster in the Databricks environment.
    All the code provided in this book can be run on a single-node cluster. To spin
    up a single-node cluster, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Give a name to the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the cluster mode to **Single Node**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the latest ML runtime to **Databricks** **Runtime Version**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Create cluster**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.8 – The New Cluster screen](img/Figure_02.08_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – The New Cluster screen
  prefs: []
  type: TYPE_NORMAL
- en: This will start the process of provisioning our cluster. There are some advanced
    settings, such as adding tags, using init scripts, and connecting through JDBC
    to this cluster, that you can read about.
  prefs: []
  type: TYPE_NORMAL
- en: '**Databricks Runtime** is a powerful platform that enhances big data analytics
    by improving the performance, security, and usability of Spark jobs. With features
    such as optimized I/O, enhanced security, and simplified operations, it offers
    a comprehensive solution. It comes in various flavors, including **ML** and **Photon**,
    catering to specific needs. Databricks Runtime is the ideal choice for running
    big data analytics workloads effectively. Databricks Runtime is powered by Delta
    Lake, which seamlessly integrates batch and streaming data to enable near-real-time
    analytics. Delta Lake’s capability to track data versions over time is crucial
    for reproducing ML model training and experimentation. This ensures data consistency
    and empowers reproducibility in your workflows. You can read more about Databricks
    Runtime here: [https://docs.databricks.com/runtime/index.html](https://docs.databricks.com/runtime/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: You will be using the ML runtime as an ML practitioner on Databricks. Databricks
    Runtime ML is a pre-built ML infrastructure that’s integrated with the capabilities
    of the Databricks workspace. It provides popular ML libraries such as TensorFlow
    and PyTorch, distributed training libraries such as Horovod, and pre-configured
    GPU support. With faster cluster creation and compatibility with installed libraries,
    it simplifies scaling ML and deep learning tasks. Additionally, it offers data
    exploration, cluster management, code and environment management, automation support,
    and integrated MLflow for model development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks provides three different cluster access modes and their specific
    recommended use case patterns. All these cluster access modes can be used either
    in a multi-node (your cluster has a dedicated driver node and one or more executor
    nodes) or a single-node fashion (your cluster has a single node; both the driver
    program and executor programs run on a single node).
  prefs: []
  type: TYPE_NORMAL
- en: Single user
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This mode is recommended for single users and data applications that can be
    developed using Python, Scala, SQL, and R. Clusters are set to terminate after
    120 minutes of inactivity, and the standard cluster is the default cluster mode.
    End users can also use this cluster to execute a notebook through a Databricks
    job using a scheduled activity. It is best to segregate different data processing
    pipelines into separate standard clusters. Segregating data pipelines prevents
    the failure of one cluster from affecting another. As Databricks charges customers
    by the second, this approach is viable and widely used. A cluster with this access
    type supports ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Shared
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This mode is ideal when multiple users try to use the same cluster. It can provide
    maximum resource utilization and has lower query latency requirements in multiuser
    scenarios. Data applications can be developed using Python and SQL but not R and
    Scala. These clusters provide user isolation and also support ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: No isolation shared
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This type of cluster is intended only for admin users. We won’t cover too much
    about this type of access as this cluster type doesn’t support ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about user isolation here: [https://docs.databricks.com/notebooks/notebook-isolation.html](https://docs.databricks.com/notebooks/notebook-isolation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at single-node clusters as this is the type of cluster you
    will be using to run the code that’s been shared as part of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Single-node clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Single-node clusters do not have worker nodes, and all the Python code runs
    on the driver node. These clusters are configured to terminate after 120 minutes
    of inactivity by default and can be used to build and test small data pipelines
    and do lightweight **exploratory data analysis** (**EDA**) and ML development.
    Python, Scala, SQL, and R are supported.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use specific libraries not included in the runtime, we will explore
    the various options to install the required libraries in the *Library* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with **Jupyter** and **IPython notebooks**, then Databricks
    notebooks will look very familiar. A Databricks notebook development environment
    consists of cells where end users can interactively write code in R, Python, Scala,
    or SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks notebooks also have additional functionalities such as integration
    with the Spark UI, powerful integrated visualizations, version control, and an
    MLflow model tracking server. We can also parameterize a notebook and pass parameters
    to it at execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover notebooks in more detail as the code examples presented to you
    in this book utilize the Databricks notebook environment. Additional details about
    notebooks can be found at [https://docs.databricks.com/notebooks/index.html](https://docs.databricks.com/notebooks/index.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Databricks notebooks](img/Figure_02.09_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Databricks notebooks
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the next feature on the **Data** tab, also called the Databricks
    metastore.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, when a new workspace is deployed, it comes with a managed Hive
    **metastore**. A metastore allows you to register datasets in various formats
    such as **Comma-Separated Values** (**CSV**), **Parquet**, **Delta format**, **text**,
    or **JavaScript Object Notation** (**JSON**) as an external table ([https://docs.databricks.com/data/metastores/index.html](https://docs.databricks.com/data/metastores/index.html)).
    We will not go too much into detail about the metastore here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – The Data tab](img/Figure_02.10_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – The Data tab
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s all right if you are not familiar with the term metastore. In simple terms,
    it is similar to a relational database. In relational databases, there are databases
    and then table names and schemas. The end user can use SQL to interact with the
    data stored in databases and tables. Similarly, in Databricks, end users can decide
    to register datasets stored in cloud storage so that they’re available as tables.
    You can learn more here: [https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html](https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The Hive metastore provides a means of implementing access control by utilizing
    table access control lists for local users within the workspace. However, to enhance
    data access governance and ensure unified control over various assets such as
    deployed models and AI assets, Databricks has introduced Unity Catalog as a best
    practice solution. This enables comprehensive management and governance across
    multiple workspaces.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand Unity Catalog in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity Catalog is a unified governance solution for data and AI assets on the
    lakehouse. It provides centralized access control, auditing, lineage, and data
    discovery capabilities across Databricks workspaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Unity Catalog’s relationship to workspaces](img/Figure_02.11_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Unity Catalog’s relationship to workspaces
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key features of Unity Catalog:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define once, secure everywhere**: Unity Catalog administers data access policies
    across all workspaces and personas from a single place'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standards-compliant security model**: Unity Catalog’s security model is based
    on standard ANSI SQL and allows administrators to grant permissions in their existing
    data lake'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Built-in auditing and lineage**: Unity Catalog captures user-level audit
    logs and lineage data, tracking how data assets are created and used across all
    languages and personas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data discovery**: Unity Catalog provides a search interface to help data
    consumers find data and lets users tag and document data assets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System tables (Public Preview)**: Unity Catalog provides operational data,
    including audit logs, billable usage, and lineage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s understand what the Unity Catalog object model looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – The Unity Catalog object model](img/Figure_02.12_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – The Unity Catalog object model
  prefs: []
  type: TYPE_NORMAL
- en: 'The Unity Catalog’s hierarchy of primary data objects flows from metastore
    to table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metastore**: The top-level container for metadata. Each metastore exposes
    a three-level namespace (**catalog.schema.table**) that organizes your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Catalog**: This is the first layer of the object hierarchy and is used to
    organize your data assets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema**: Also known as databases, schemas contain tables and views.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table**: The lowest level in the object hierarchy are tables and views.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned previously, doing a deep dive into Unity Catalog is a big topic
    in itself and outside the scope of this book. Unity Catalog offers centralized
    governance, auditing, and data discovery capabilities for data and AI assets across
    Databricks workspaces. It provides a secure model based on ANSI SQL, automatic
    capture of user-level audit logs and data lineage, and a hierarchical data organization
    system. It also supports a variety of data formats, advanced identity management,
    specified admin roles for data governance, and is compatible with Databricks Runtime
    11.3 LTS or above.
  prefs: []
  type: TYPE_NORMAL
- en: For a more comprehensive understanding of Unity Catalog, go to [https://docs.databricks.com/data-governance/unity-catalog/index.html](https://docs.databricks.com/data-governance/unity-catalog/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: All the features we’ve covered so far are standard among all the Databricks
    persona-specific features.
  prefs: []
  type: TYPE_NORMAL
- en: The following three features, namely experiments, the feature store, and models,
    are critical for the ML persona.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, experiments are the central location where all the model
    training pertinent to business problems can be accessed. Users can define their
    name for the experiment or a default system-generated one and use it to train
    the different ML model training runs. Experiments in the Databricks UI come from
    integrating MLflow into the platform. We will dive deeper into MLflow in the coming
    chapters to understand more details; however, it’s important to get a sense of
    what MLflow is and some of the terminology that is MLflow-specific.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow is an open source platform for managing the end-to-end ML life cycle.
    Here are the key components of MLflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking**: This allows you to track experiments to record and compare parameters
    and results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: This component helps manage and deploy models from various ML libraries
    to a variety of model serving and inference platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projects**: This allows you to package ML code in a reusable, reproducible
    form so that you can share it with other data scientists or transfer it to production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Registry**: This centralizes a model store for managing models’ full
    life cycle stage transitions: from staging to production, with capabilities for
    versioning and annotating. Databricks provides a managed version of the Model
    Registry in Unity Catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Serving**: This allows you to host MLflow models as REST endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also certain terms specific to MLflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Run**: A run represents a specific instance of training an ML model. It comprises
    parameters, metrics, artifacts, and metadata associated with the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment**: An experiment serves as a container for organizing and tracking
    the results of ML experiments. It consists of multiple runs, allowing for easy
    comparison and analysis of different approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter**: A parameter refers to a configurable value that can be adjusted
    during the training of an ML model. These values influence the behavior and performance
    of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric**: A metric is a quantitative measure that’s used to evaluate the
    performance of an ML model. Metrics provide insights into how well the model is
    performing on specific tasks or datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact**: An artifact refers to any output generated during an ML experiment.
    This can include files, models, images, or other relevant data that captures the
    results or intermediate stages of the experiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project**: A project encompasses the code, data, and configuration necessary
    to reproduce an ML experiment. It provides a structured and organized approach
    to managing all the components required for a specific ML workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: A model represents a trained ML model that can be utilized to make
    predictions or perform specific tasks based on the learned patterns and information
    from the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model registry**: A model registry serves as a centralized repository for
    storing and managing ML models. It provides versioning, tracking, and collaboration
    capabilities for different model versions and their associated metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backend store**: The backend store is responsible for storing MLflow entities
    such as runs, parameters, metrics, and tags. It provides the underlying storage
    infrastructure for managing experiment data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact store**: The artifact store is responsible for storing artifacts
    produced during ML experiments. This can include files, models, images, or any
    other relevant data that’s generated throughout the experimentation process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flavor**: A flavor represents a standardized way of packaging an ML model,
    allowing it to be easily consumed by specific tools or platforms. Flavors provide
    flexibility and interoperability when deploying and serving models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UI**: The UI refers to the graphical interface provided by MLflow, allowing
    users to interact with and visualize experiment results, track runs, and manage
    models through an intuitive interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow also employs additional terms, but the ones mentioned here are some
    of the most commonly used. For further details, please consult the MLflow documentation:
    [https://mlflow.org/docs/latest/index.html](https://mlflow.org/docs/latest/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks AutoML is fully integrated with MLflow, so all the model training
    and the artifacts that are generated are automatically logged in the MLflow server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – The Experiments tab](img/Figure_02.13_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – The Experiments tab
  prefs: []
  type: TYPE_NORMAL
- en: End users can also utilize Databricks AutoML to start modeling a solution for
    their ML problems. Databricks has taken a different approach with its AutoML capability,
    called **glass** **box AutoML**.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks AutoML simplifies the workflow for ML practitioners by automatically
    generating comprehensive notebooks. These notebooks encompass all the necessary
    code for feature engineering and model training, covering various combinations
    of ML models and hyperparameters. This feature allows ML practitioners to thoroughly
    inspect the generated code and gain deeper insights into the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks AutoML currently supports classification, regression, and forecasting
    models. For a list of algorithms that AutoML can use to create models, go to [https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms](https://docs.databricks.com/applications/machine-learning/automl.html#automl-algorithms):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – The default experiment is linked to Python notebooks by default](img/Figure_02.14_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – The default experiment is linked to Python notebooks by default
  prefs: []
  type: TYPE_NORMAL
- en: MLflow was developed in-house at Databricks to ease the end-to-end ML life cycle
    and MLOps. Since the launch of MLflow, it has been widely adopted and supported
    by the open source community.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the feature store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **feature store** is a relatively new yet stable release in the latest Databricks
    ML workspace. Many organizations that have mature ML processes in place, such
    as Uber, Facebook, DoorDash, and many more, have internally implemented their
    feature stores.
  prefs: []
  type: TYPE_NORMAL
- en: ML life cycle management and workflows are complex. Forbes conducted a survey
    ([https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says))
    with data scientists and uncovered that managing data is the most expensive and
    time-consuming operation in their day-to-day work.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists need to spend a lot of time finding the data, cleaning it, doing
    EDA, and then performing feature engineering to train their ML models. This is
    an iterative process. The effort that needs to be put in to make the process repeatable
    is an enormous challenge. This is where feature stores come in.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Feature Store is standardized on the open source Delta format, which
    allows data scientists to govern features similar to those used to govern access
    to models, notebooks, or jobs in the Databricks environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks Feature Store is unique in a couple of ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It uses Delta Lake to store feature tables. This allows end users to read data
    from any of the supported languages and connectors outside of Databricks. More
    can be read here: [https://docs.delta.io/latest/delta-intro.html](https://docs.delta.io/latest/delta-intro.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The integrated Feature Store UI within the Databricks ML workspace provides
    end-to-end traceability and lineage of how the features were generated and which
    downstream models use it in a single unified view. We will look at this in more
    detail in [*Chapter 3*](B17875_03.xhtml#_idTextAnchor063).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks Feature Store also integrates seamlessly with MLflow. This allows
    Databricks Feature Store to utilize all the great features of MLflow’s feature
    pipelines, as well as to generate features and write them out as feature tables
    in Delta format. The Feature Store has its own generic model packaging format
    that is compatible with the MLflow Models component, which lets your models know
    exactly which features were used to train the models. This integration makes it
    possible to simplify our MLOps pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: A client can call the serving endpoint either in batch mode or online mode,
    and the model will automatically retrieve the latest features from the Feature
    Store and provide inference. We will see practical examples of this in the coming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also read more about the current state of Databricks Feature Store
    here: [https://docs.databricks.com/machine-learning/feature-store/index.html](https://docs.databricks.com/machine-learning/feature-store/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, let’s discuss the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the model registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Models** is a fully managed and integrated MLflow model registry available
    to each deployed Databricks ML workspace. The registry has its own set of APIs
    and a UI to collaborate with data scientists across the organization and fully
    manage the MLflow model. Data scientists and ML engineers can develop models in
    any of the supported ML frameworks ([https://mlflow.org/docs/latest/models.html#built-in-model-flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors))
    and package them in a generic MLfLow model format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – The Models tab](img/Figure_02.15_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – The Models tab
  prefs: []
  type: TYPE_NORMAL
- en: 'The model registry provides features to manage the versioning, tagging, and
    state transitioning between different environments (moving models from staging
    to production to archive):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – The Registered Models tab](img/Figure_02.16_B17875.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – The Registered Models tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, there is another important feature that we need to understand:
    the **Libraries** feature of Databricks. This feature allows users to utilize
    third-party or custom code available to Databricks notebooks and jobs running
    on your cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Libraries are fundamental building blocks of any programming ecosystem. They
    are akin to toolboxes, comprising pre-compiled routines that offer enhanced functionality
    and assist in optimizing code efficiency. In Databricks, libraries are used to
    make third-party or custom code available to notebooks and jobs running on clusters.
    These libraries can be written in various languages, including Python, Java, Scala,
    and R.
  prefs: []
  type: TYPE_NORMAL
- en: Storing libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to storage, libraries uploaded using the library UI are stored
    in the **Databricks File System** (**DBFS**) root. However, all workspace users
    can modify data and files stored in the DBFS root. If a more secure storage option
    is desired, you can opt to store libraries in cloud object storage, use library
    package repositories, or upload libraries to workspace files.
  prefs: []
  type: TYPE_NORMAL
- en: Managing libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Library management in Databricks can be handled via three different interfaces:
    the workspace UI, the **command-line interface** (**CLI**), or the Libraries API.
    Each option caters to different workflows and user preferences, and the choice
    often depends on individual use cases or project requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Runtime and libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Databricks Runtime comes equipped with many common libraries. To find out which
    libraries are included in your runtime, you can refer to the **System Environment**
    subsection of the Databricks Runtime release notes to check your specific runtime
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Python `atexit` functions aren’t invoked by Databricks when your notebook
    or job finishes processing. If you’re utilizing a Python library that registers
    `atexit` handlers, it’s crucial to ensure your code calls the required functions
    before exiting. Also, the use of Python eggs is being phased out in Databricks
    Runtime and will eventually be removed; consider using Python wheels or installing
    packages from PyPI as alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Library usage modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Databricks allows three different modes for library installation: cluster-installed,
    notebook-scoped, and workspace libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster libraries**: These libraries are available for use by all notebooks
    running on a particular cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notebook-scoped libraries**: Available for Python and R, these libraries
    create an environment scoped to a notebook session and do not affect other notebooks
    running on the same cluster. They are temporary and need to be reinstalled for
    each session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workspace libraries**: These act as local repositories from which you can
    create cluster-installed libraries. They could be custom code written by your
    organization or a specific version of an open source library that your organization
    prefers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s next move on to cover Unity Catalog limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Unity Catalog limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are certain limitations when using libraries with Unity Catalog. For more
    details, you should refer to the *Cluster* *libraries* section.
  prefs: []
  type: TYPE_NORMAL
- en: Installation sources for libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cluster libraries can be installed directly from public repositories such as
    PyPI, Maven, or CRAN. Alternatively, they can be sourced from a cloud object storage
    location, a workspace library in the DBFS root, or even by uploading library files
    from your local machine. Libraries installed directly via upload are stored in
    the DBFS root.
  prefs: []
  type: TYPE_NORMAL
- en: For most of our use cases, we will be using notebook-scoped libraries. You can
    install notebook-scoped libraries using the `%pip` `magic` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ways to utilize `%pip` in notebooks to install notebook-scoped
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`%pip install <package-name>` for notebook-scoped libraries or select PyPI
    as the source for cluster libraries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%pip install <package-name> --index-url <mirror-url>` for notebook-scoped
    libraries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%pip install git+https://github.com/<username>/<repo>.git` for notebook-scoped
    libraries or select PyPI as the source and specify the repository URL as the package
    name for cluster libraries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%pip install dbfs:/<path-to-package>` for notebook-scoped libraries or select
    DBFS/S3 as the source for cluster libraries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a brief overview of all the components of the Databricks
    ML workspace. This will enable us to utilize these components in a more hands-on
    fashion so that we can train ML models and deploy them for various ML problems
    efficiently in the Databricks environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start working on a customer churn prediction problem
    and register our first feature tables in the Databricks feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks libraries: [https://docs.databricks.com/libraries/index.html](https://docs.databricks.com/libraries/index.html%20)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks notebooks: [https://docs.databricks.com/notebooks/index.html](https://docs.databricks.com/notebooks/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: ML Pipeline Components and Implementation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the end of this section, you will have a good understanding of each of the
    ML components that are available in the Databricks ML experience and will be comfortable
    using them in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B17875_03.xhtml#_idTextAnchor063), *Utilizing the Feature Store*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B17875_04.xhtml#_idTextAnchor076), *Understanding MLflow Components
    on Databricks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17875_05.xhtml#_idTextAnchor085), *Create a Baseline Model Using
    Databricks AutoML*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
