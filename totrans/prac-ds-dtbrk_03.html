<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer047">
			<h1 id="_idParaDest-63" class="chapter-number"><a id="_idTextAnchor063"/>3</h1>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>Utilizing the Feature Store</h1>
			<p>In the last chapter, we <a id="_idIndexMarker182"/>briefly touched upon what a <strong class="bold">feature store</strong> is and how <strong class="bold">Databricks Feature Store</strong> is unique in its <span class="No-Break">own</span><span class="No-Break"><a id="_idIndexMarker183"/></span><span class="No-Break"> way.</span></p>
			<p>This chapter will take a more hands-on approach and utilize Databricks Feature Store to register our first feature table and discuss concepts related to Databricks <span class="No-Break">Feature Store.</span></p>
			<p>We will be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Diving into feature stores and the problems <span class="No-Break">they solve</span></li>
				<li>Discovering feature stores on the <span class="No-Break">Databricks platform</span></li>
				<li>Registering your first feature table in Databricks <span class="No-Break">Feature Store</span></li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor065"/>Technical requirements</h1>
			<p>All the code is available on the GitHub repository <a href="https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks">https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks</a> and is self-contained. To execute the notebooks, you can import the code repository directly into your Databricks workspace using <strong class="bold">Repos</strong>. We discussed Repos in the <span class="No-Break">second chapter.</span></p>
			<p>Working knowledge of <strong class="bold">Delta</strong> format is required. If you are new to Delta format, check out <a href="https://docs.databricks.com/en/delta/index.html">https://docs.databricks.com/en/delta/index.html</a> and <a href="https://docs.databricks.com/en/delta/tutorial.html">https://docs.databricks.com/en/delta/tutorial.html</a> before <span class="No-Break">going forward.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor066"/>Diving into feature stores and the problems they solve</h1>
			<p>As more teams in the organization <a id="_idIndexMarker184"/>start to use AI and ML to solve various business use cases, it becomes necessary to have a centralized, reusable, and easily discoverable feature repository. This repository is called a <span class="No-Break">feature store.</span></p>
			<p>All the curated features are in centralized, governed, access-controlled storage, such as a curated data lake. Different data science teams can be granted access to feature tables based on their needs. Like in enterprise data lakes, we can track data lineage; similarly, we can track the lineage of a feature table logged in Databricks Feature Store. We can also see all the downstream<a id="_idIndexMarker185"/> models that are consuming features from a registered <span class="No-Break">feature table.</span></p>
			<p>There are hundreds of data science<a id="_idIndexMarker186"/> teams tackling different business questions in large organizations. Each team may have its own domain knowledge and expertise. Performing feature engineering often requires heavy processing. Without a feature store, it becomes difficult for a new group of data scientists to reuse the features created and curated by another data <span class="No-Break">science team.</span></p>
			<p>We can think of feature store workflows as being similar to ETL workflows that cater to a specific type of BI or analytics use case. The workflows that write data to feature store tables cater to a particular feature-engineering process that needs to be performed on the curated dataset in your data lake before training an <span class="No-Break">ML model.</span></p>
			<p>You can schedule and monitor the execution of feature table workflows just like a regular <span class="No-Break">ETL operation.</span></p>
			<p>Feature stores also solve the problem of <em class="italic">skew</em> between model training and inference code by providing a central repository of features across the organization. The same feature-engineering logic is used during model training <span class="No-Break">and inference.</span></p>
			<p>Let’s look at how Feature Store<a id="_idIndexMarker187"/> has been built and integrated with the <span class="No-Break">Databricks workspace.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor067"/>Discovering feature stores on the Databricks platform</h1>
			<p>Each Databricks workspace<a id="_idIndexMarker188"/> has its own feature<a id="_idIndexMarker189"/> store. At the time of writing this book, <strong class="bold">Databricks Feature Store</strong> only supports the Python API. The<a id="_idIndexMarker190"/> latest Python API reference<a id="_idIndexMarker191"/> is located <span class="No-Break">at </span><a href="https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html"><span class="No-Break">https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html</span></a><span class="No-Break">.</span></p>
			<p>Databricks Feature Store is fully integrated with <strong class="bold">Managed MLFlow</strong> and other Databricks components. This allows models<a id="_idIndexMarker192"/> that are deployed by utilizing MLFlow to automatically retrieve the features at the time of training and inference. The exact steps involved in defining a feature table and using it with model training and inference are going to be covered in the <span class="No-Break">following sections.</span></p>
			<p>Let’s look at some of the key concepts and terminology associated with Databricks <span class="No-Break">Feature Store.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>Feature table</h2>
			<p>As the name suggests, a feature store<a id="_idIndexMarker193"/> stores features generated by data scientists after doing feature engineering for a <span class="No-Break">particular problem.</span></p>
			<p>These features may come from one or more clean and curated tables in the data lake. A feature table in Databricks contains two <span class="No-Break">main components:</span></p>
			<ul>
				<li><strong class="bold">Metadata</strong>: The metadata tracks the source<a id="_idIndexMarker194"/> of the data utilized to create the feature table, which notebooks and scheduled jobs write data into the feature table, and at what frequency. The metadata also tracks downstream ML models utilizing the feature table. This <span class="No-Break">provides lineage.</span></li>
				<li><strong class="bold">Generated feature data</strong>: In the case of batch and streaming<a id="_idIndexMarker195"/> inference, the underlying generated feature DataFrame is written out as a Delta table to an offline feature store. Databricks manages this offline feature store<a id="_idIndexMarker196"/> for you. In contrast, the feature table is written out to a supported <strong class="bold">relational database management system</strong> (<strong class="bold">RDBMS</strong>) for an <strong class="bold">online feature store</strong>. The online feature store <a id="_idIndexMarker197"/>is not managed by Databricks and requires some additional steps to set up. There is a link in the <em class="italic">Further reading</em> section that you can refer to in order to set up an online <span class="No-Break">feature store.</span></li>
			</ul>
			<p>Let’s briefly understand<a id="_idIndexMarker198"/> the different types of inference patterns and how Databricks Feature Store can be beneficial in each scenario before <span class="No-Break">moving forward:</span></p>
			<ul>
				<li><strong class="bold">Batch inference</strong>: Batch inference involves making predictions<a id="_idIndexMarker199"/> on a large set of data all at once, typically in intervals or scheduled runs. In Databricks, you can set up batch jobs using technologies such as <strong class="bold">Apache Spark</strong> to process and predict input data. Batch inference is well suited for scenarios<a id="_idIndexMarker200"/> where timely predictions are not critical and you can afford to wait for results. For instance, this could be used in customer segmentation, where predictions are made periodically. This scenario is supported by an offline <span class="No-Break">feature store.</span><ul><li>Databricks Feature Store enhances batch inference by providing a centralized repository for feature data. Instead of recalculating features for every batch job, Feature Store allows you to store and manage pre-computed features. This reduces computation time, ensuring consistent and accurate features for your models during each <span class="No-Break">batch run.</span></li></ul></li>
				<li><strong class="bold">Streaming inference</strong>: Streaming inference involves processing<a id="_idIndexMarker201"/> and making predictions on data as it arrives in real time, without waiting for the entire dataset to be collected. Databricks supports streaming data processing using tools such as Apache Spark’s <strong class="bold">Structured Streaming</strong>. Streaming inference<a id="_idIndexMarker202"/> is valuable when you need to respond quickly to changing data, such as in fraud detection where immediate action is crucial. This scenario is supported by an offline <span class="No-Break">feature store.</span><ul><li>Feature Store plays a key role in streaming scenarios by providing a reliable source of feature data. When new data streams in, Feature Store can supply the necessary features for predictions, ensuring consistent and up-to-date input for your models. This simplifies the streaming pipeline, as feature preparation is decoupled from the real-time <span class="No-Break">inference process.</span></li></ul></li>
				<li><strong class="bold">Real-time inference</strong>: Real-time inference takes streaming a step further by delivering instantaneous<a id="_idIndexMarker203"/> predictions as soon as new data arrives. This is essential in applications such as recommendation systems, where users expect immediate responses to their actions. This scenario requires an online <span class="No-Break">feature store.</span><ul><li>For real-time inference, Feature Store ensures that feature data is readily available for quick predictions. Feature Store’s integration into the real-time inference pipeline enables low-latency access to features, contributing to swift and accurate predictions. This is crucial in applications demanding <span class="No-Break">rapid decision-making.</span></li></ul></li>
			</ul>
			<p>Each feature table has a primary key that uniquely defines a row of data. Databricks Feature Store allows defining composite keys <span class="No-Break">as well.</span></p>
			<p>New data can be written<a id="_idIndexMarker204"/> into the feature tables using regularly executed ETL pipelines in a batch fashion or a continuous style utilizing the Structured Streaming <span class="No-Break">API (</span><a href="https://docs.databricks.com/spark/latest/structured-streaming/index.html"><span class="No-Break">https://docs.databricks.com/spark/latest/structured-streaming/index.html</span></a><span class="No-Break">).</span></p>
			<p>The workflow to register<a id="_idIndexMarker205"/> a new feature table in Databricks is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Create a database that will store our <span class="No-Break">feature tables.</span></li>
				<li>Write feature-engineering logic as a function that returns an Apache Spark DataFrame (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html">https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html</a>). This DataFrame should also produce a unique primary key for each record in the DataFrame. The primary key can have more than one column <span class="No-Break">as well.</span></li>
				<li>Instantiate an object of <strong class="source-inline">FeatureStoreClient</strong> and use <strong class="source-inline">create_table</strong> (supported in DB ML Runtime 10.2 and above) to define a feature table in the feature store. At this point, there is no data stored in the feature table. If we initialize an additional <strong class="source-inline">df</strong> argument with the value of the feature-engineered DataFrame, we can skip <span class="No-Break"><em class="italic">step 4</em></span><span class="No-Break">.</span></li>
				<li>Use the <strong class="source-inline">write_table</strong> method to write the feature-engineered dataset into the defined feature table. The <strong class="source-inline">write_table</strong> method provides modes to either completely overwrite the existing feature table or update certain records based on the defined <span class="No-Break">lookup key.</span></li>
			</ol>
			<p>The code example provided in this chapter will go through the aforementioned steps and make them clearer to understand. Before we dive deeper into the code, we need to understand some more concepts related to the <span class="No-Break">feature store.</span></p>
			<p>We will look more at reading<a id="_idIndexMarker206"/> from the feature table in the chapter on MLFlow. We will reuse the feature table we created in this chapter to predict bank <span class="No-Break">customer churn.</span></p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Offline store</h2>
			<p>The Databricks offline<a id="_idIndexMarker207"/> feature store is backed by Delta tables and is utilized for model training, batch inferencing, and <span class="No-Break">feature discovery.</span></p>
			<p>Delta tables allow users to update feature values based on the primary key in this mode. Utilizing the Delta format also provides additional advantages in the context <span class="No-Break">of ML:</span></p>
			<ul>
				<li><strong class="bold">Time travel</strong>: With Delta tables, you<a id="_idIndexMarker208"/> can access data from any historical point. This is useful when you are debugging inconsistent model performance or probing the impact of data changes on the model. Time travel facilitates a robust audit trail. You can specify <strong class="source-inline">timestampAsOf</strong> to read certain <span class="No-Break">historical data:</span><pre class="source-code">
# Reading data from a Delta table as it appeared at a specific timestampdf = spark.read.format("delta").option("timestampAsOf", "2021-01-01").load("/path/to/delta-table")</pre></li>				<li><strong class="bold">Versioning</strong>: Delta tables also track all data alterations as versions. Each version of the table represents <a id="_idIndexMarker209"/>the state of the data and metadata at a particular moment. Delta format tables automatically manage version history for a table to maintain data and metadata consistency. You can access the Delta version history using the <strong class="source-inline">history()</strong> function in the Delta <span class="No-Break">API (</span><a href="https://docs.delta.io/latest/index.html"><span class="No-Break">https://docs.delta.io/latest/index.html</span></a><span class="No-Break">):</span><pre class="source-code">
from delta.tables import *deltaTable = DeltaTable.forPath(spark, write_path)fullHistoryDF = deltaTable.history()# get the full history of the table to pick the versiondisplay(fullHistoryDF)</pre><p class="list-inset">To load data from<a id="_idIndexMarker210"/> a particular version of the table, we can specify the <span class="No-Break"><strong class="source-inline">versionAsOf</strong></span><span class="No-Break"> option:</span></p><pre class="source-code">## Specifying data version to use for model trainingversion = 1df_delta = spark.read.format('delta').option('versionAsOf', version).load(write_path)</pre></li>			</ul>
			<p>This way, you can now train models on different versions of your data and maintain <span class="No-Break">the lineage.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Online store</h2>
			<p>When using a feature<a id="_idIndexMarker211"/> store for real-time inference, the feature table needs to be stored in low-latency storage such as a <span class="No-Break">relational database.</span></p>
			<p>If you must have your features available both in online and offline feature stores, you can use your offline store as a streaming source to update your online store’s <span class="No-Break">feature</span><span class="No-Break"><a id="_idIndexMarker212"/></span><span class="No-Break"> tables.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>Training Set</h2>
			<p>While training<a id="_idIndexMarker213"/> an ML model, you may want to combine data from multiple feature tables. Each feature table needs to have a unique ID(s) or primary key(s) that is used at the time of model training and inference to join and retrieve the relevant features from multiple feature tables to the <strong class="bold">Training </strong><span class="No-Break"><strong class="bold">Set</strong></span><span class="No-Break"> construct.</span></p>
			<p>A training set makes use of an object called <strong class="source-inline">FeatureLookup</strong>, which takes as input the feature table name, feature names that we need to retrieve from the feature table, and a lookup key(s). The lookup key(s) are used to join the features from various feature tables if we define multiple <strong class="source-inline">FeatureLookup</strong> to generate a <span class="No-Break">Training Set.</span></p>
			<p>In the notebook accompanying this chapter, we will go over example code that registers a fraud detection dataset as a feature table in Databricks Feature Store. In <a href="B17875_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Create a Baseline Model Using Databricks AutoML</em>, in relation to the AutoML overview, we will take the feature table generated to build a churn prediction model and showcase various components of integrated MLFlow in the <span class="No-Break">Databricks environment.</span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>Model packaging</h2>
			<p>The <strong class="source-inline">FeatureStoreClient</strong> API (<a href="https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html">https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html</a>) provides a method called <strong class="source-inline">log_model</strong> that allows ML models<a id="_idIndexMarker214"/> to retain the references to the features<a id="_idIndexMarker215"/> utilized to train<a id="_idIndexMarker216"/> the model. These features reside in Databricks Feature Store as feature tables. The ML model can retrieve the necessary features from the feature tables based on the primary key(s) provided at the time of inference. The feature values are retrieved in the batch and streaming inference mode from the offline store. The retrieved features are combined with any new feature provided during inference before making a prediction. In the real-time inference mode, feature values are retrieved from the <span class="No-Break">online store.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">At the time of writing this book, Databricks Feature Store only supports the Python language. You can use your favorite libraries, such as <strong class="source-inline">sklearn</strong> and pandas, to do feature<a id="_idIndexMarker217"/> engineering; however, before you write the table out as a feature table, it needs to be converted to a <strong class="bold">PySpark</strong> DataFrame. PySpark is a Python wrapper on top<a id="_idIndexMarker218"/> of the Spark distributed processing <span class="No-Break">engine (</span><a href="https://spark.apache.org/docs/latest/api/python/"><span class="No-Break">https://spark.apache.org/docs/latest/api/python/</span></a><span class="No-Break">).</span></p>
			<p>Let’s dive into a hands-on example that will walk you through the process of registering your first feature table in Databricks <span class="No-Break">Feature Store.</span></p>
			<p>The dataset we will work with comes from <strong class="bold">Kaggle</strong>, and we are going to register this dataset after <a id="_idIndexMarker219"/>doing some feature engineering in Databricks <span class="No-Break">Feature Store.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Registering your first feature table in Databricks Feature Store</h1>
			<p>Before we get started, the code<a id="_idIndexMarker220"/> needs to be downloaded <a id="_idIndexMarker221"/>from the Git repository accompanying this <span class="No-Break">book (</span><a href="https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git"><span class="No-Break">https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git</span></a><span class="No-Break">).</span></p>
			<p>We will use the Databricks repository feature to clone the <span class="No-Break">GitHub repo.</span></p>
			<p>To clone the code repository, complete the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click on the <strong class="bold">Repos</strong> tab and select <span class="No-Break">your username:</span></li>
			</ol>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B17875_03_001.jpg" alt="Figure 3.1 – A screenshot displaying the Repos tab" width="1272" height="668"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A screenshot displaying the Repos tab</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In light of a recent user interface update, the 'Repos' section has been moved and can now be accessed by clicking on the 'Workspaces' icon, as illustrated in the following image. </p>
			<p class="callout"><img src="image/B17875_03_000.png" alt="" width="217" height="197"/></p>
			<p class="callout">Despite this change, the workflow outlined in this chapter <span class="No-Break">remains applicable.</span></p>
			<ol>
				<li value="2">Right-click<a id="_idIndexMarker222"/> and add <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker223"/></span><span class="No-Break"> repo:</span></li>
			</ol>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B17875_03_002.jpg" alt="Figure 3.2 – A screenshot displaying how to clone the code for this chapter (step 2)" width="1089" height="561"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – A screenshot displaying how to clone the code for this chapter (step 2)</p>
			<ol>
				<li value="3">Paste the link <a id="_idIndexMarker224"/>into the <strong class="bold">Git repo URL</strong> field <a id="_idIndexMarker225"/>and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B17875_03_003.jpg" alt="Figure 3.3 – A screenshot displaying how to clone the code for this chapter (step 3)" width="675" height="342"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – A screenshot displaying how to clone the code for this chapter (step 3)</p>
			<ol>
				<li value="4">In the cloned<a id="_idIndexMarker226"/> repository, click on <span class="No-Break"><strong class="source-inline">Chapter 03</strong></span> and, within<a id="_idIndexMarker227"/> that, the <span class="No-Break"><strong class="source-inline">churn-analysis</strong></span><span class="No-Break"> notebook:</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B17875_03_004.jpg" alt="Figure 3.4 – A screenshot displaying how to clone the code for this chapter (step 4)" width="641" height="181"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – A screenshot displaying how to clone the code for this chapter (step 4)</p>
			<p class="list-inset">When you first open the notebook, it will be in the detached stage. If you have not provisioned a cluster, please refer to the <em class="italic">Exploring clusters</em> section in <a href="B17875_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
			<p class="list-inset">You can see the notebook<a id="_idIndexMarker228"/> in the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker229"/></span><span class="No-Break"> screenshot:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B17875_03_005.jpg" alt="Figure 3.5 – A screenshot displaying the initial state of the notebook" width="776" height="226"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – A screenshot displaying the initial state of the notebook</p>
			<ol>
				<li value="5">After you have a cluster ready to use, you can attach it to <span class="No-Break">the notebook:</span></li>
			</ol>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B17875_03_006.jpg" alt="Figure 3.6 – A screenshot displaying the dropdown for attaching a notebook to a list of available clusters" width="588" height="125"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – A screenshot displaying the dropdown for attaching a notebook to a list of available clusters</p>
			<p class="list-inset">All the code has been tested<a id="_idIndexMarker230"/> on Databricks ML Runtime<a id="_idIndexMarker231"/> 10.4 LTS. I would recommend users have a cluster provisioned with ML Runtime 10.4 LTS <span class="No-Break">or above.</span></p>
			<ol>
				<li value="6">Select an option from the <span class="No-Break">drop-down menu:</span></li>
			</ol>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B17875_03_007.jpg" alt="Figure 3.7 – A screenshot displaying the notebook attached to a cluster in the ready state" width="616" height="233"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – A screenshot displaying the notebook attached to a cluster in the ready state</p>
			<p>Now, you are ready to start executing the code in <span class="No-Break">the notebook:</span></p>
			<ol>
				<li>The first few cells<a id="_idIndexMarker232"/> in the notebook describe <a id="_idIndexMarker233"/>the dataset we are working with and how to read it as a Spark DataFrame. We can also import the data as a pandas DataFrame. Databricks has a handy display function that visualizes the data loaded into a pandas or <span class="No-Break">Spark DataFrame:</span><pre class="source-code">
import osbank_df = spark.read.option("header", True).option("inferSchema", True).csv(f"file:{os.getcwd()}/data/churn.csv")display(bank_df)</pre></li>				<li>The next block of code goes over the steps to create a feature table. The first step is to define a database that will store the feature tables that we define backed by the Delta tables. We also define a method for performing some basic <span class="No-Break">feature engineering:</span><pre class="source-code">
DATABASE_NAME = "bank_churn_analysis"# setup database and tmp space that will hold our Feature tables in Delta format.spark.sql(f"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}")dbutils.fs.mkdirs(f"/tmp/{DATABASE_NAME}")</pre></li>				<li>The following code block defines a basic function that performs feature engineering on the input <span class="No-Break">Spark DataFrame.</span><p class="list-inset">Before going forward, I would like to highlight a powerful library called <strong class="source-inline">pyspark.pandas</strong>. As an ML<a id="_idIndexMarker234"/> practitioner, you might be familiar with using the pandas (<a href="https://pandas.pydata.org/docs/#">https://pandas.pydata.org/docs/#</a>) library for manipulating data. It has one big drawback in that it’s not scalable. All the processing using the pandas API happens on a single machine and if your data cannot fit on a single machine, you will be stuck. This is where Apache Spark can help. Apache Spark is built to handle massive amounts of data as it chunks large amounts of data into individual units and distributes the processing on multiple nodes of a cluster. As the volume of data you want to process increases, you can simply add more nodes to the cluster, and if your data doesn’t have any skews, the performance of your data processing pipeline will remain <span class="No-Break">the same.</span></p><p class="list-inset">However, there is a big challenge: many ML practitioners<a id="_idIndexMarker235"/> are unfamiliar with the Spark libraries. This is the core reason for developing the <strong class="source-inline">pyspark.pandas </strong>(<a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html">https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html</a>) library. This library aims to bridge the gap between the pandas library and <span class="No-Break">Apache Spark.</span></p><p class="list-inset">At the heart<a id="_idIndexMarker236"/> of the <strong class="source-inline">pyspark.pandas</strong> library lies its ability to offer a pandas-like API within<a id="_idIndexMarker237"/> the realm of Apache Spark. This innovation brings forth the best of both worlds, granting users access to a familiar DataFrame API while harnessing Spark’s scalability and performance prowess. For those well versed in pandas, the transition to <strong class="source-inline">pyspark.pandas</strong> is seamless, paving the way for streamlined adoption. However, it’s important to keep in mind that not all panda APIs are implemented in the <strong class="source-inline">pyspark.pandas</strong> API, leading to <span class="No-Break">compatibility issues.</span></p><p class="list-inset">If you really have a need to use certain pandas API functionality that is not yet available in <strong class="source-inline">pyspark.pandas</strong>, you can use a method called <strong class="source-inline">toPandas()</strong> (<a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas">https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas</a>). As a best practice, try to use the PySpark/<strong class="source-inline">pyspark.pandas</strong> API before going for the pandas API. You can read more about best practices <span class="No-Break">at </span><a href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible"><span class="No-Break">https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible</span></a><span class="No-Break">.</span></p><pre class="source-code">
import pyspark.pandas as psdef compute_features(spark_df):    # Convert to pyspark.pandas DataFrame # https://spark.apache.org/docs/3.2.0/api/python/user_guide/pandas_on_spark/pandas_pyspark.html#pyspark    ps_df = spark_df.to_pandas_on_spark() #https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.drop.html#pyspark.pandas.DataFrame.drop    # drop RowNumber &amp; Surname column    ps_df = ps_df.drop(['RowNumber', 'Surname'], axis=1)    # OHE (One hot encoding)    ohe_ps_df = ps.get_dummies(      ps_df,      columns=["Geography", "Gender"],      dtype="int",      drop_first=True    )    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r' ', '', regex=True)    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r'(', '-', regex=True)    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r')', '', regex=True)    return ohe_ps_df</pre></li>				<li>The next code block performs feature engineering, utilizing the function defined in the previous section, and <span class="No-Break">displays it:</span><pre class="source-code">
bank_features_df = compute_features(bank_df)display(bank_features_df)</pre></li>				<li>Next, we will<a id="_idIndexMarker238"/> initialize <strong class="source-inline">FeatureStoreClient</strong>, register the table, and define our feature table <a id="_idIndexMarker239"/>structure using the <span class="No-Break"><strong class="source-inline">create_table</strong></span><span class="No-Break"> function:</span><pre class="source-code">
# Our first step is to instantiate the feature store client using `FeatureStoreClient()`.from databricks.feature_store import FeatureStoreClientfs = FeatureStoreClient()bank_feature_table = fs.create_table(  name=f"{DATABASE_NAME}.bank_customer_features", # the name of the feature table  primary_keys=["CustomerId"], # primary key that will be used to perform joins  schema=bank_features_df.spark.schema(), # the schema of the Feature table  description="This customer level table contains one-hot encoded categorical and scaled numeric features to predict bank customer churn.")</pre></li>				<li>Once we have defined the structure of our feature table, we can populate data in it using the <span class="No-Break"><strong class="source-inline">write_table</strong></span><span class="No-Break"> function:</span><pre class="source-code">
fs.write_table(df=bank_features_df.to_spark(), name=f"{DATABASE_NAME}.bank_customer_features", mode="overwrite")</pre><p class="list-inset">Instead of overwriting, you can choose <strong class="source-inline">merge</strong> as an option if you want to update only <span class="No-Break">certain records.</span></p><p class="list-inset">In practice, you can populate the feature table when you call the <strong class="source-inline">create_table</strong> method itself by passing in the source Spark DataFrame as the <strong class="source-inline">feature_df</strong> parameter. This approach can be useful when you have a DataFrame ready to initialize the <span class="No-Break">feature table.</span></p></li>				<li>Now we can explore our feature table in the integrated feature store UI. We can see who created the feature table and the data sources that populated the feature table. The feature store UI has a lot of important information about our <span class="No-Break">feature table:</span></li>
			</ol>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B17875_03_008.jpg" alt="Figure 3.8 – A screenshot displaying details of the feature store" width="936" height="154"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – A screenshot displaying details of the feature store</p>
			<ol>
				<li value="8">We can examine<a id="_idIndexMarker240"/> when a feature table<a id="_idIndexMarker241"/> was created, and by whom, through Databricks’ user interface. This information is particularly valuable for tracking data provenance and understanding data lineage within the organization. Additionally, the UI displays other pertinent information such as the last time the table was refreshed, giving insights into how up to date the table’s <span class="No-Break">data is:</span></li>
			</ol>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B17875_03_009.jpg" alt="Figure 3.9 – A screenshot displaying details about the owner, primary key, creation, and last update date of the feature table" width="1026" height="199"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – A screenshot displaying details about the owner, primary key, creation, and last update date of the feature table</p>
			<p class="list-inset">Furthermore, the UI provides details about the table’s partitions and primary keys. Partitions are crucial<a id="_idIndexMarker242"/> for query optimization, as they enable more efficient data retrieval by segregating<a id="_idIndexMarker243"/> the table into different subsets based on specific column values. Primary keys, on the other hand, serve as unique identifiers for each row in the table, ensuring data integrity and facilitating <span class="No-Break">quick lookups.</span></p>
			<ol>
				<li value="9">In the production environment, if our feature table is populated regularly through a notebook, we can also visualize <span class="No-Break">historical updates:</span></li>
			</ol>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B17875_03_010.jpg" alt="Figure 3.10 – A screenshot displaying details about the source of the feature table" width="999" height="119"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – A screenshot displaying details about the source of the feature table</p>
			<ol>
				<li value="10">Lastly, we can also view the data types of every feature in the <span class="No-Break">feature table:</span></li>
			</ol>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B17875_03_011.jpg" alt="Figure 3.11 – A screenshot displaying details about the various columns of the feature table and the data types" width="499" height="759"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – A screenshot displaying details about the various columns of the feature table and the data types</p>
			<p>The notebook is heavily documented<a id="_idIndexMarker244"/> and will walk you through<a id="_idIndexMarker245"/> all the steps required to get raw data, from importing to Databricks to writing out your first <span class="No-Break">feature table.</span></p>
			<p>To understand the current limitations<a id="_idIndexMarker246"/> and other administrative options available with Databricks Feature Store, refer <span class="No-Break">to </span><a href="https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html"><span class="No-Break">https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/>Summary</h1>
			<p>In this chapter, we got a deeper understanding of feature stores, the problems they solve, and a detailed look into the feature store implementation within the Databricks environment. We also went through an exercise to register our first feature table. This will enable us to utilize the feature table to create our first ML model as we discussed in the <span class="No-Break">MLFlow chapter.</span></p>
			<p>Next, we will cover MLFlow <span class="No-Break">in detail.</span></p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor075"/>Further reading</h1>
			<ul>
				<li>Databricks, <em class="italic">Repos for Git </em><span class="No-Break"><em class="italic">Integration</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/repos.html#repos-for-git-integration"><span class="No-Break">https://docs.databricks.com/repos.html#repos-for-git-integration</span></a></li>
				<li>You can read more about the supported RDBMS <span class="No-Break">here: </span><a href="https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store"><span class="No-Break">https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store</span></a></li>
				<li>You can read more on how the feature tables are joined together with the training DataFrame <span class="No-Break">here: </span><a href="https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys"><span class="No-Break">https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys</span></a></li>
				<li>Apache Spark, <em class="italic">Apache Arrow in </em><span class="No-Break"><em class="italic">PySpark</em></span><span class="No-Break">: </span><a href="https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html"><span class="No-Break">https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html</span></a></li>
				<li>Databricks, <em class="italic">Convert PySpark DataFrames to and from pandas </em><span class="No-Break"><em class="italic">DataFrames</em></span><span class="No-Break">: (</span><span class="No-Break">https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html#convert-pyspark-dataframes-to-and-from-pandas-dataframes</span><span class="No-Break">)</span></li>
			</ul>
		</div>
	</div>
</div>
</body></html>