["```py\n    # Reading data from a Delta table as it appeared at a specific timestampdf = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2021-01-01\").load(\"/path/to/delta-table\")\n    ```", "```py\n    from delta.tables import *deltaTable = DeltaTable.forPath(spark, write_path)fullHistoryDF = deltaTable.history()# get the full history of the table to pick the versiondisplay(fullHistoryDF)\n    ```", "```py\n    ## Specifying data version to use for model trainingversion = 1df_delta = spark.read.format('delta').option('versionAsOf', version).load(write_path)\n    ```", "```py\n    import osbank_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"file:{os.getcwd()}/data/churn.csv\")display(bank_df)\n    ```", "```py\n    DATABASE_NAME = \"bank_churn_analysis\"# setup database and tmp space that will hold our Feature tables in Delta format.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")dbutils.fs.mkdirs(f\"/tmp/{DATABASE_NAME}\")\n    ```", "```py\n    import pyspark.pandas as psdef compute_features(spark_df):    # Convert to pyspark.pandas DataFrame # https://spark.apache.org/docs/3.2.0/api/python/user_guide/pandas_on_spark/pandas_pyspark.html#pyspark    ps_df = spark_df.to_pandas_on_spark() #https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.drop.html#pyspark.pandas.DataFrame.drop    # drop RowNumber & Surname column    ps_df = ps_df.drop(['RowNumber', 'Surname'], axis=1)    # OHE (One hot encoding)    ohe_ps_df = ps.get_dummies(      ps_df,      columns=[\"Geography\", \"Gender\"],      dtype=\"int\",      drop_first=True    )    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r' ', '', regex=True)    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r'(', '-', regex=True)    ohe_ps_df.columns = ohe_ps_df.columns.str.replace(r')', '', regex=True)    return ohe_ps_df\n    ```", "```py\n    bank_features_df = compute_features(bank_df)display(bank_features_df)\n    ```", "```py\n    # Our first step is to instantiate the feature store client using `FeatureStoreClient()`.from databricks.feature_store import FeatureStoreClientfs = FeatureStoreClient()bank_feature_table = fs.create_table(  name=f\"{DATABASE_NAME}.bank_customer_features\", # the name of the feature table  primary_keys=[\"CustomerId\"], # primary key that will be used to perform joins  schema=bank_features_df.spark.schema(), # the schema of the Feature table  description=\"This customer level table contains one-hot encoded categorical and scaled numeric features to predict bank customer churn.\")\n    ```", "```py\n    fs.write_table(df=bank_features_df.to_spark(), name=f\"{DATABASE_NAME}.bank_customer_features\", mode=\"overwrite\")\n    ```"]