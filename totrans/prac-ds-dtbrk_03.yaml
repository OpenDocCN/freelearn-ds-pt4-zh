- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilizing the Feature Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we briefly touched upon what a **feature store** is and
    how **Databricks Feature Store** is unique in its own way.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will take a more hands-on approach and utilize Databricks Feature
    Store to register our first feature table and discuss concepts related to Databricks
    Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Diving into feature stores and the problems they solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering feature stores on the Databricks platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering your first feature table in Databricks Feature Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code is available on the GitHub repository [https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks](https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks)
    and is self-contained. To execute the notebooks, you can import the code repository
    directly into your Databricks workspace using **Repos**. We discussed Repos in
    the second chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Working knowledge of **Delta** format is required. If you are new to Delta format,
    check out [https://docs.databricks.com/en/delta/index.html](https://docs.databricks.com/en/delta/index.html)
    and [https://docs.databricks.com/en/delta/tutorial.html](https://docs.databricks.com/en/delta/tutorial.html)
    before going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into feature stores and the problems they solve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As more teams in the organization start to use AI and ML to solve various business
    use cases, it becomes necessary to have a centralized, reusable, and easily discoverable
    feature repository. This repository is called a feature store.
  prefs: []
  type: TYPE_NORMAL
- en: All the curated features are in centralized, governed, access-controlled storage,
    such as a curated data lake. Different data science teams can be granted access
    to feature tables based on their needs. Like in enterprise data lakes, we can
    track data lineage; similarly, we can track the lineage of a feature table logged
    in Databricks Feature Store. We can also see all the downstream models that are
    consuming features from a registered feature table.
  prefs: []
  type: TYPE_NORMAL
- en: There are hundreds of data science teams tackling different business questions
    in large organizations. Each team may have its own domain knowledge and expertise.
    Performing feature engineering often requires heavy processing. Without a feature
    store, it becomes difficult for a new group of data scientists to reuse the features
    created and curated by another data science team.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of feature store workflows as being similar to ETL workflows that
    cater to a specific type of BI or analytics use case. The workflows that write
    data to feature store tables cater to a particular feature-engineering process
    that needs to be performed on the curated dataset in your data lake before training
    an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: You can schedule and monitor the execution of feature table workflows just like
    a regular ETL operation.
  prefs: []
  type: TYPE_NORMAL
- en: Feature stores also solve the problem of *skew* between model training and inference
    code by providing a central repository of features across the organization. The
    same feature-engineering logic is used during model training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how Feature Store has been built and integrated with the Databricks
    workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering feature stores on the Databricks platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each Databricks workspace has its own feature store. At the time of writing
    this book, **Databricks Feature Store** only supports the Python API. The latest
    Python API reference is located at [https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html](https://docs.databricks.com/applications/machine-learning/feature-store/python-api.html).
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Feature Store is fully integrated with **Managed MLFlow** and other
    Databricks components. This allows models that are deployed by utilizing MLFlow
    to automatically retrieve the features at the time of training and inference.
    The exact steps involved in defining a feature table and using it with model training
    and inference are going to be covered in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the key concepts and terminology associated with Databricks
    Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: Feature table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, a feature store stores features generated by data scientists
    after doing feature engineering for a particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'These features may come from one or more clean and curated tables in the data
    lake. A feature table in Databricks contains two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata**: The metadata tracks the source of the data utilized to create
    the feature table, which notebooks and scheduled jobs write data into the feature
    table, and at what frequency. The metadata also tracks downstream ML models utilizing
    the feature table. This provides lineage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generated feature data**: In the case of batch and streaming inference, the
    underlying generated feature DataFrame is written out as a Delta table to an offline
    feature store. Databricks manages this offline feature store for you. In contrast,
    the feature table is written out to a supported **relational database management
    system** (**RDBMS**) for an **online feature store**. The online feature store
    is not managed by Databricks and requires some additional steps to set up. There
    is a link in the *Further reading* section that you can refer to in order to set
    up an online feature store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s briefly understand the different types of inference patterns and how
    Databricks Feature Store can be beneficial in each scenario before moving forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch inference**: Batch inference involves making predictions on a large
    set of data all at once, typically in intervals or scheduled runs. In Databricks,
    you can set up batch jobs using technologies such as **Apache Spark** to process
    and predict input data. Batch inference is well suited for scenarios where timely
    predictions are not critical and you can afford to wait for results. For instance,
    this could be used in customer segmentation, where predictions are made periodically.
    This scenario is supported by an offline feature store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks Feature Store enhances batch inference by providing a centralized
    repository for feature data. Instead of recalculating features for every batch
    job, Feature Store allows you to store and manage pre-computed features. This
    reduces computation time, ensuring consistent and accurate features for your models
    during each batch run.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming inference**: Streaming inference involves processing and making
    predictions on data as it arrives in real time, without waiting for the entire
    dataset to be collected. Databricks supports streaming data processing using tools
    such as Apache Spark’s **Structured Streaming**. Streaming inference is valuable
    when you need to respond quickly to changing data, such as in fraud detection
    where immediate action is crucial. This scenario is supported by an offline feature
    store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Store plays a key role in streaming scenarios by providing a reliable
    source of feature data. When new data streams in, Feature Store can supply the
    necessary features for predictions, ensuring consistent and up-to-date input for
    your models. This simplifies the streaming pipeline, as feature preparation is
    decoupled from the real-time inference process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time inference**: Real-time inference takes streaming a step further
    by delivering instantaneous predictions as soon as new data arrives. This is essential
    in applications such as recommendation systems, where users expect immediate responses
    to their actions. This scenario requires an online feature store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For real-time inference, Feature Store ensures that feature data is readily
    available for quick predictions. Feature Store’s integration into the real-time
    inference pipeline enables low-latency access to features, contributing to swift
    and accurate predictions. This is crucial in applications demanding rapid decision-making.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each feature table has a primary key that uniquely defines a row of data. Databricks
    Feature Store allows defining composite keys as well.
  prefs: []
  type: TYPE_NORMAL
- en: New data can be written into the feature tables using regularly executed ETL
    pipelines in a batch fashion or a continuous style utilizing the Structured Streaming
    API ([https://docs.databricks.com/spark/latest/structured-streaming/index.html](https://docs.databricks.com/spark/latest/structured-streaming/index.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow to register a new feature table in Databricks is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a database that will store our feature tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write feature-engineering logic as a function that returns an Apache Spark DataFrame
    ([https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html)).
    This DataFrame should also produce a unique primary key for each record in the
    DataFrame. The primary key can have more than one column as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate an object of `FeatureStoreClient` and use `create_table` (supported
    in DB ML Runtime 10.2 and above) to define a feature table in the feature store.
    At this point, there is no data stored in the feature table. If we initialize
    an additional `df` argument with the value of the feature-engineered DataFrame,
    we can skip *step 4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `write_table` method to write the feature-engineered dataset into the
    defined feature table. The `write_table` method provides modes to either completely
    overwrite the existing feature table or update certain records based on the defined
    lookup key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code example provided in this chapter will go through the aforementioned
    steps and make them clearer to understand. Before we dive deeper into the code,
    we need to understand some more concepts related to the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: We will look more at reading from the feature table in the chapter on MLFlow.
    We will reuse the feature table we created in this chapter to predict bank customer
    churn.
  prefs: []
  type: TYPE_NORMAL
- en: Offline store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Databricks offline feature store is backed by Delta tables and is utilized
    for model training, batch inferencing, and feature discovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delta tables allow users to update feature values based on the primary key
    in this mode. Utilizing the Delta format also provides additional advantages in
    the context of ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '`timestampAsOf` to read certain historical data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`history()` function in the Delta API ([https://docs.delta.io/latest/index.html](https://docs.delta.io/latest/index.html)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To load data from a particular version of the table, we can specify the `versionAsOf`
    option:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This way, you can now train models on different versions of your data and maintain
    the lineage.
  prefs: []
  type: TYPE_NORMAL
- en: Online store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using a feature store for real-time inference, the feature table needs
    to be stored in low-latency storage such as a relational database.
  prefs: []
  type: TYPE_NORMAL
- en: If you must have your features available both in online and offline feature
    stores, you can use your offline store as a streaming source to update your online
    store’s feature tables.
  prefs: []
  type: TYPE_NORMAL
- en: Training Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While training an ML model, you may want to combine data from multiple feature
    tables. Each feature table needs to have a unique ID(s) or primary key(s) that
    is used at the time of model training and inference to join and retrieve the relevant
    features from multiple feature tables to the **Training** **Set** construct.
  prefs: []
  type: TYPE_NORMAL
- en: A training set makes use of an object called `FeatureLookup`, which takes as
    input the feature table name, feature names that we need to retrieve from the
    feature table, and a lookup key(s). The lookup key(s) are used to join the features
    from various feature tables if we define multiple `FeatureLookup` to generate
    a Training Set.
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook accompanying this chapter, we will go over example code that
    registers a fraud detection dataset as a feature table in Databricks Feature Store.
    In [*Chapter 5*](B17875_05.xhtml#_idTextAnchor085), *Create a Baseline Model Using
    Databricks AutoML*, in relation to the AutoML overview, we will take the feature
    table generated to build a churn prediction model and showcase various components
    of integrated MLFlow in the Databricks environment.
  prefs: []
  type: TYPE_NORMAL
- en: Model packaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `FeatureStoreClient` API ([https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html](https://docs.databricks.com/en/dev-tools/api/python/latest/feature-store/client.html))
    provides a method called `log_model` that allows ML models to retain the references
    to the features utilized to train the model. These features reside in Databricks
    Feature Store as feature tables. The ML model can retrieve the necessary features
    from the feature tables based on the primary key(s) provided at the time of inference.
    The feature values are retrieved in the batch and streaming inference mode from
    the offline store. The retrieved features are combined with any new feature provided
    during inference before making a prediction. In the real-time inference mode,
    feature values are retrieved from the online store.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, Databricks Feature Store only supports the
    Python language. You can use your favorite libraries, such as `sklearn` and pandas,
    to do feature engineering; however, before you write the table out as a feature
    table, it needs to be converted to a **PySpark** DataFrame. PySpark is a Python
    wrapper on top of the Spark distributed processing engine ([https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into a hands-on example that will walk you through the process of
    registering your first feature table in Databricks Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will work with comes from **Kaggle**, and we are going to register
    this dataset after doing some feature engineering in Databricks Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: Registering your first feature table in Databricks Feature Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get started, the code needs to be downloaded from the Git repository
    accompanying this book ([https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git](https://github.com/debu-sinha/Practical_Data_Science_on_Databricks.git)).
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Databricks repository feature to clone the GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clone the code repository, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Repos** tab and select your username:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – A screenshot displaying the Repos tab](img/B17875_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – A screenshot displaying the Repos tab
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In light of a recent user interface update, the 'Repos' section has been moved
    and can now be accessed by clicking on the 'Workspaces' icon, as illustrated in
    the following image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17875_03_000.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite this change, the workflow outlined in this chapter remains applicable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click and add the repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – A screenshot displaying how to clone the code for this chapter
    (step 2)](img/B17875_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – A screenshot displaying how to clone the code for this chapter
    (step 2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Paste the link into the **Git repo URL** field and click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – A screenshot displaying how to clone the code for this chapter
    (step 3)](img/B17875_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – A screenshot displaying how to clone the code for this chapter
    (step 3)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cloned repository, click on `Chapter 03` and, within that, the `churn-analysis`
    notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – A screenshot displaying how to clone the code for this chapter
    (step 4)](img/B17875_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – A screenshot displaying how to clone the code for this chapter
    (step 4)
  prefs: []
  type: TYPE_NORMAL
- en: When you first open the notebook, it will be in the detached stage. If you have
    not provisioned a cluster, please refer to the *Exploring clusters* section in
    [*Chapter 2*](B17875_02.xhtml#_idTextAnchor036).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the notebook in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – A screenshot displaying the initial state of the notebook](img/B17875_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – A screenshot displaying the initial state of the notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have a cluster ready to use, you can attach it to the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.6 – A screenshot displaying the dropdown for attaching a notebook
    to a list of available clusters](img/B17875_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – A screenshot displaying the dropdown for attaching a notebook to
    a list of available clusters
  prefs: []
  type: TYPE_NORMAL
- en: All the code has been tested on Databricks ML Runtime 10.4 LTS. I would recommend
    users have a cluster provisioned with ML Runtime 10.4 LTS or above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select an option from the drop-down menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – A screenshot displaying the notebook attached to a cluster in
    the ready state](img/B17875_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – A screenshot displaying the notebook attached to a cluster in the
    ready state
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you are ready to start executing the code in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first few cells in the notebook describe the dataset we are working with
    and how to read it as a Spark DataFrame. We can also import the data as a pandas
    DataFrame. Databricks has a handy display function that visualizes the data loaded
    into a pandas or Spark DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next block of code goes over the steps to create a feature table. The first
    step is to define a database that will store the feature tables that we define
    backed by the Delta tables. We also define a method for performing some basic
    feature engineering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following code block defines a basic function that performs feature engineering
    on the input Spark DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before going forward, I would like to highlight a powerful library called `pyspark.pandas`.
    As an ML practitioner, you might be familiar with using the pandas ([https://pandas.pydata.org/docs/#](https://pandas.pydata.org/docs/#))
    library for manipulating data. It has one big drawback in that it’s not scalable.
    All the processing using the pandas API happens on a single machine and if your
    data cannot fit on a single machine, you will be stuck. This is where Apache Spark
    can help. Apache Spark is built to handle massive amounts of data as it chunks
    large amounts of data into individual units and distributes the processing on
    multiple nodes of a cluster. As the volume of data you want to process increases,
    you can simply add more nodes to the cluster, and if your data doesn’t have any
    skews, the performance of your data processing pipeline will remain the same.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'However, there is a big challenge: many ML practitioners are unfamiliar with
    the Spark libraries. This is the core reason for developing the `pyspark.pandas`
    ([https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html))
    library. This library aims to bridge the gap between the pandas library and Apache
    Spark.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At the heart of the `pyspark.pandas` library lies its ability to offer a pandas-like
    API within the realm of Apache Spark. This innovation brings forth the best of
    both worlds, granting users access to a familiar DataFrame API while harnessing
    Spark’s scalability and performance prowess. For those well versed in pandas,
    the transition to `pyspark.pandas` is seamless, paving the way for streamlined
    adoption. However, it’s important to keep in mind that not all panda APIs are
    implemented in the `pyspark.pandas` API, leading to compatibility issues.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you really have a need to use certain pandas API functionality that is not
    yet available in `pyspark.pandas`, you can use a method called `toPandas()` ([https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html?highlight=topandas)).
    As a best practice, try to use the PySpark/`pyspark.pandas` API before going for
    the pandas API. You can read more about best practices at [https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=pandas#use-pandas-api-on-spark-directly-whenever-possible).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next code block performs feature engineering, utilizing the function defined
    in the previous section, and displays it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will initialize `FeatureStoreClient`, register the table, and define
    our feature table structure using the `create_table` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have defined the structure of our feature table, we can populate data
    in it using the `write_table` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Instead of overwriting, you can choose `merge` as an option if you want to update
    only certain records.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In practice, you can populate the feature table when you call the `create_table`
    method itself by passing in the source Spark DataFrame as the `feature_df` parameter.
    This approach can be useful when you have a DataFrame ready to initialize the
    feature table.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we can explore our feature table in the integrated feature store UI. We
    can see who created the feature table and the data sources that populated the
    feature table. The feature store UI has a lot of important information about our
    feature table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.8 – A screenshot displaying details of the feature store](img/B17875_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – A screenshot displaying details of the feature store
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine when a feature table was created, and by whom, through Databricks’
    user interface. This information is particularly valuable for tracking data provenance
    and understanding data lineage within the organization. Additionally, the UI displays
    other pertinent information such as the last time the table was refreshed, giving
    insights into how up to date the table’s data is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9 – A screenshot displaying details about the owner, primary key,
    creation, and last update date of the feature table](img/B17875_03_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – A screenshot displaying details about the owner, primary key, creation,
    and last update date of the feature table
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the UI provides details about the table’s partitions and primary
    keys. Partitions are crucial for query optimization, as they enable more efficient
    data retrieval by segregating the table into different subsets based on specific
    column values. Primary keys, on the other hand, serve as unique identifiers for
    each row in the table, ensuring data integrity and facilitating quick lookups.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the production environment, if our feature table is populated regularly
    through a notebook, we can also visualize historical updates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – A screenshot displaying details about the source of the feature
    table](img/B17875_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – A screenshot displaying details about the source of the feature
    table
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can also view the data types of every feature in the feature table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.11 – A screenshot displaying details about the various columns of
    the feature table and the data types](img/B17875_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – A screenshot displaying details about the various columns of the
    feature table and the data types
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is heavily documented and will walk you through all the steps required
    to get raw data, from importing to Databricks to writing out your first feature
    table.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the current limitations and other administrative options available
    with Databricks Feature Store, refer to [https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html](https://docs.databricks.com/en/machine-learning/feature-store/troubleshooting-and-limitations.html).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got a deeper understanding of feature stores, the problems
    they solve, and a detailed look into the feature store implementation within the
    Databricks environment. We also went through an exercise to register our first
    feature table. This will enable us to utilize the feature table to create our
    first ML model as we discussed in the MLFlow chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will cover MLFlow in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Databricks, *Repos for Git* *Integration*: [https://docs.databricks.com/repos.html#repos-for-git-integration](https://docs.databricks.com/repos.html#repos-for-git-integration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read more about the supported RDBMS here: [https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store](https://docs.databricks.com/applications/machine-learning/feature-store/concepts.html#online-store)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read more on how the feature tables are joined together with the training
    DataFrame here: [https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys](https://docs.databricks.com/applications/machine-learning/feature-store/feature-tables.html#create-a-trainingset-when-lookup-keys-do-not-match-the-primary-keys)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Spark, *Apache Arrow in* *PySpark*: [https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks, *Convert PySpark DataFrames to and from pandas* *DataFrames*: (https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html#convert-pyspark-dataframes-to-and-from-pandas-dataframes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
