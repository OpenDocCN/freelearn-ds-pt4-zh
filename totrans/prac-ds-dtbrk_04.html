<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer056">
			<h1 id="_idParaDest-76" class="chapter-number"><a id="_idTextAnchor076"/>4</h1>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor077"/>Understanding MLflow Components on Databricks</h1>
			<p>In the previous chapter, we learned about Feature Store, what problem it solves, and how Databricks provides the built-in Feature Store as part of the Databricks <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) workspace, which we can use to register our <span class="No-Break">feature tables.</span></p>
			<p>In this chapter, we will look into managing our model training, tracking, and experimentation. In a software engineer’s world, code development and productionization have established best practices; however, such best practices are not generally adopted in the ML engineering/data science world. While working with many Databricks customers, I observed that each data science team has its own way of managing its projects. This is where MLflow <span class="No-Break">comes in.</span></p>
			<p>MLflow is an umbrella project developed at Databricks, by Databricks engineers, to bring a standardized ML life cycle management tool to the Databricks platform. It is now an open source project with more than 500,000 daily downloads on average as of September 2023 and has broad industry and community support. MLflow provides features to manage the end-to-end ML project life cycle. Some of the features are only available <span class="No-Break">on Databricks.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Overview <span class="No-Break">of MLflow</span></li>
				<li><span class="No-Break">MLflow Tracking</span></li>
				<li><span class="No-Break">MLflow Projects</span></li>
				<li><span class="No-Break">MLflow Models</span></li>
				<li>MLflow <span class="No-Break">Model Registry</span></li>
				<li>Example code showing how to track ML model training <span class="No-Break">in Databricks</span></li>
			</ul>
			<p>These components play an essential role in standardizing and streamlining your ML project’s life cycle. When we use MLflow with Databricks, some MLflow features are more helpful than others. We’ll point out the most useful ones as we go through <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor078"/>Technical requirements</h1>
			<p>All the code is available in this book’s GitHub repository <a href="https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks">https://github.com/PacktPublishing/Practical-Machine-Learning-on-Databricks</a> and is self-contained. To execute the notebooks, you can import the code repository directly into your Databricks workspace using repos. We discussed repos in our <span class="No-Break">previous chapters.</span></p>
			<p>This chapter also assumes that you have a preliminary understanding of what user-defined functions are in Apache Spark. You can read more about them <span class="No-Break">here: </span><a href="https://docs.databricks.com/en/udf/index.html"><span class="No-Break">https://docs.databricks.com/en/udf/index.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor079"/>Overview of MLflow</h1>
			<p>The <a id="_idIndexMarker247"/>ML life cycle is complex. It starts with ingesting raw data into the data/Delta lake in raw format from various batch and streaming sources. The data engineers create data pipelines using tools such as Apache Spark with Python, R, SQL, or Scala to process a large amount of data in a scalable, performant, and <span class="No-Break">cost-effective manner.</span></p>
			<p>The data scientists then utilize the various curated datasets in the data lake to generate feature tables to train their ML models. The data scientists prefer programming languages such as Python and R for feature engineering and libraries such as scikit-learn, pandas, NumPy, PyTorch, or any other popular ML or deep learning libraries for training and tuning <span class="No-Break">ML models.</span></p>
			<p>Once the models have been<a id="_idIndexMarker248"/> trained, they need to be deployed in production either as a <strong class="bold">re</strong><strong class="bold">presentational state transfer</strong> (<strong class="bold">REST</strong>) <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) for <a id="_idIndexMarker249"/>real-time<a id="_idIndexMarker250"/> inference, or a <strong class="bold">user-defined function</strong> (<strong class="bold">UDF</strong>) for batch and stream inference on Apache Spark. We also need to apply monitoring and governance around the deployed model. In case of drift in model performance or data, we may need to retrain and redeploy <span class="No-Break">the model.</span></p>
			<p>This process is iterative and brings a lot of development challenges to organizations looking to start working on <span class="No-Break">ML projects:</span></p>
			<ul>
				<li>A zoo of software tools needs to be managed to provide a stable working environment for the data scientists. A large number of libraries need to be manually installed <span class="No-Break">and configured.</span></li>
				<li>Tracking and reproducing the results of ML experiments is also <span class="No-Break">a challenge.</span></li>
				<li>Managing the services and governance around productionizing models <span class="No-Break">is difficult.</span></li>
				<li>Scaling the training of the models with the increase in the amount of data is <span class="No-Break">a challenge.</span></li>
			</ul>
			<p>MLflow, with its <a id="_idIndexMarker251"/>components, provides a solution to each of these challenges. In the Databricks environment, MLflow is integrated with workspace components such as notebooks, Feature Store, and AutoML. This integration provides a seamless experience for data scientists and ML engineers who are looking to get productive without getting into the operational overhead of managing the installation of MLflow on <span class="No-Break">their own:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B17875_04_1.jpg" alt="Figure 4.1 – The various components of MLflow" width="1650" height="734"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – The various components of MLflow</p>
			<p>Four software <a id="_idIndexMarker252"/>components make <span class="No-Break">up MLflow:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">MLflow Tracking</strong></span></li>
				<li><span class="No-Break"><strong class="bold">MLflow Projects</strong></span></li>
				<li><span class="No-Break"><strong class="bold">MLflow Models</strong></span></li>
				<li><strong class="bold">MLflow </strong><span class="No-Break"><strong class="bold">Model Registry</strong></span></li>
			</ul>
			<p>In Databricks, all these components except MLflow Projects are integrated, fully managed, and provided as services as part of the Databricks workspace. As our primary focus is on MLflow features seamlessly integrated with Databricks, we won’t delve extensively into MLflow Projects. Rest assured, this won’t impact your ML project workflow when using Databricks. The Databricks ML workspace also offers high availability, automated updates, and access controls for all the integrated <span class="No-Break">MLflow components.</span></p>
			<p>Let’s take a look at each of these components <span class="No-Break">in detail.</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/>MLflow Tracking</h1>
			<p>MLflow Tracking allows <a id="_idIndexMarker253"/>you to track the training of your ML models. It also improves the observability of the model-training process. The MLflow Tracking feature allows you to log the generated metrics, artifacts, and the model itself as part of the model training process. MLflow Tracking also keeps track of model lineage in the Databricks environment. In Databricks, we can see the exact version of the notebook responsible for generating the model listed as <span class="No-Break">the source.</span></p>
			<p>MLflow also <a id="_idIndexMarker254"/>provides <strong class="bold">automatic logging</strong> (<strong class="bold">autolog</strong>) capabilities that automatically log many metrics, parameters, and artifacts while performing model training. We can also add our own set of metrics and artifacts to <span class="No-Break">the log.</span></p>
			<p>Using MLflow Tracking, we can chronologically track model training. Certain terms are specific to MLflow Tracking. Let’s take a look <span class="No-Break">at them:</span></p>
			<ul>
				<li><strong class="bold">Experiments</strong>: Training <a id="_idIndexMarker255"/>and tuning the ML model for a business problem is an experiment. By default, each Python notebook in Databricks has an experiment with the same name. This is called a notebook-scoped experiment. You can easily change and set the experiment’s name using the MLflow API. Defining an experiment like this will create a workspace-scoped MLflow experiment that will now be visible in your workspace. Customizing the names of MLflow experiments offers valuable benefits in ML workflows – for example, it enhances organizational clarity by helping you categorize and differentiate experiments, acting as a form of documentation that aids communication and collaboration. Custom names facilitate version control and the tracking of experiment evolution, which is particularly useful for comparing performance or revisiting past configurations. Additionally, they simplify the process of accessing specific experiments through MLflow’s user interface or programmatic queries, ultimately contributing to more efficient and effective ML project management. We will be using a custom experiment name for our <span class="No-Break">example code.</span></li>
				<li><strong class="bold">Runs</strong>: We can have <a id="_idIndexMarker256"/>multiple models training with different hyperparameters logged under each experiment. Each of the unique combinations of ML model training logged under an experiment is called a run. The accompanying MLflow Tracking UI allows us to compare and contrast the different runs and get the <span class="No-Break">best model.</span></li>
				<li><strong class="bold">Metrics</strong>: Each<a id="_idIndexMarker257"/> run will have critical offline metrics that we want to log while training our models. Unlike online metrics, which are calculated in real time as a model interacts with live data and users, offline metrics are computed retrospectively using a fixed dataset that was collected before the model’s deployment. These metrics are crucial during the model development and testing phases to gauge how well a model generalizes to unseen data and to guide model refinement. Common examples of offline metrics<a id="_idIndexMarker258"/> include <a id="_idIndexMarker259"/>accuracy, precision, recall, F1-score, <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>), and <strong class="bold">area under the receiver operating characteristic curve</strong> (<strong class="bold">AUC-ROC</strong>), among others. They provide insights into a model’s performance and can inform decisions regarding hyperparameter tuning, feature engineering, and model selection to improve overall predictive accuracy <span class="No-Break">and effectiveness.</span></li>
				<li><strong class="bold">Artifacts</strong>: MLflow<a id="_idIndexMarker260"/> artifacts play a pivotal role in MLflow’s experiment tracking system by facilitating the storage and versioning of supplementary files and data linked to ML experiments. These versatile artifacts can encompass a variety of resources, including ML model files, datasets, configuration files, data visualizations (for example, plots), documentation (for example, READMEs and Jupyter notebooks), custom scripts, and even reports summarizing experiment findings. Crucially, artifacts are versioned alongside experiment runs, ensuring precise tracking of changes over time. They support remote storage solutions and are programmatically accessible via MLflow’s API. This comprehensive approach enhances reproducibility, organization, and collaboration in ML projects, making it possible to recreate experiments accurately and access all <span class="No-Break">relevant resources.</span></li>
				<li><strong class="bold">Parameters</strong>: Parameters <a id="_idIndexMarker261"/>are user-defined configuration settings or hyperparameters associated with ML experiment runs. They play a vital role in tracking, comparing, and reproducing experiments by recording the specific configuration settings used in each run. This allows for easy visualization and analysis of how parameter values impact experiment outcomes, making it simpler to identify optimal configurations and manage <span class="No-Break">experiments effectively.</span></li>
				<li><strong class="bold">Tags</strong>: Tags are<a id="_idIndexMarker262"/> user-defined or automatically generated metadata labels that can be attached to ML experiment runs. They serve to provide context, categorization, and organization for runs, aiding in searching for, filtering, and analyzing experiments. Tags help document and distinguish different runs, making it easier to understand and manage ML projects, and they can be used for custom workflow integration <span class="No-Break">or automation.</span></li>
			</ul>
			<p>Next, we will understand one of the key components of MLflow called <span class="No-Break">MLflow Models.</span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor081"/>MLflow Models</h1>
			<p>MLflow Models is <a id="_idIndexMarker263"/>a standard packaging format for ML models. It provides a standardized abstraction on top of the ML model created by the data scientists. Each MLflow model is essentially a directory containing an <strong class="source-inline">MLmodel</strong> file in the directory’s root that can define multiple flavors that the model can be <span class="No-Break">viewed in.</span></p>
			<p>Flavors represent a fundamental concept that empowers MLflow Models by providing a standardized approach for deployment tools to comprehend and interact with ML models. This innovation eliminates the need for each deployment tool to integrate with every ML library individually. MLflow introduces several “standard” flavors, universally supported by its built-in deployment tools. For instance, the “Python function” flavor outlines how to execute the model as a Python function. However, the versatility of flavors extends beyond these standards. Libraries have the flexibility to define and employ their own flavors. As an example, MLflow’s <strong class="source-inline">mlflow.sklearn</strong> library allows you to load models as scikit-learn pipeline objects, suitable for use in scikit-learn-aware code, or as generic Python functions, catering to tools requiring a model application, such as the MLflow deployments tool with the <strong class="source-inline">-t sagemaker</strong> option for deploying models on Amazon SageMaker. So, flavors serve as a bridge between ML libraries and deployment tools, enhancing interoperability and ease <span class="No-Break">of use.</span></p>
			<p>You can register an MLflow model from a run in an experiment using the <strong class="source-inline">mlflow.&lt;model-flavor&gt;.log_mode</strong><strong class="source-inline">l</strong> method. This method serializes the underlying ML model in a specific format and persists it in the <span class="No-Break">underlying storage.</span></p>
			<p>Check out the official documentation for a complete list of ML libraries supported by MLflow Models: <a href="https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors">https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors</a>. If you have some existing models that were developed using any Python ML library, MLflow provides a method to create custom models via the <span class="No-Break"><strong class="source-inline">mlflow.pyfunc</strong></span><span class="No-Break"> module.</span></p>
			<p>Additional files are logged, such as <strong class="source-inline">conda.yaml</strong> and <strong class="source-inline">requirements.txt</strong>, that contain the library dependencies for recreating the runtime environment <span class="No-Break">when needed.</span></p>
			<p>The ML model YAML file contains the <span class="No-Break">following attributes:</span></p>
			<ul>
				<li><strong class="source-inline">time_created</strong>: The date and time in UTC ISO 8601 format describing when the model <span class="No-Break">was created.</span></li>
				<li><strong class="source-inline">flavors</strong>: This defines how downstream applications can use <span class="No-Break">this model.</span></li>
				<li><strong class="source-inline">run_id</strong>: This represents the unique identifier for the MLflow run; this model was logged <span class="No-Break">under B.</span></li>
				<li><strong class="source-inline">signature</strong>: The model signature in JSON format. This signature defines the expected format of input and output data for an ML model. It is automatically inferred from datasets representing valid input and output examples, such as the training dataset and <span class="No-Break">model predictions.</span></li>
				<li><strong class="source-inline">input_example</strong>: This is for if we provide sample records as input while training <span class="No-Break">the model.</span></li>
			</ul>
			<p>The <a id="_idIndexMarker264"/>MLflow Models API provides a method called <strong class="source-inline">mlflow.evaluate()</strong> that automatically evaluates our trained model on an evaluation dataset and logs the necessary metrics, such as accuracy, R2, and SHAP feature importance based on what kind of problem we are trying to solve. You can also create custom metrics and provide them as input to <strong class="source-inline">mlflow.evaluate(custom_metrics=[&lt;your custom metric&gt;])</strong> as a parameter. Links have been provided in the <em class="italic">Further reading</em> section if you want to learn more <span class="No-Break">about it.</span></p>
			<p>MLflow Models also provide APIs to deploy the packaged ML models as a REST endpoint for real-time inference, as a Python function that can be used to perform batch and stream inference, or as a Docker container that can then be deployed to Kubernetes, Azure ML, or AWS SageMaker. The MLflow API provides convenient methods such as <strong class="source-inline">mlflow.models.build_docker</strong> to build and configure a Docker image. You can read more about the various methods that are available here: “<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=docker#mlflow.models.build_docker">https://www.mlflow.org/docs/latest/python_api/mlflow.models.html?highlight=docker#mlflow.models.build_docker</a>”. We will look into the various available deployment options as part of Databricks integrated with MLflow in <a href="B17875_07.xhtml#_idTextAnchor108"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Model </em><span class="No-Break"><em class="italic">Deployment Approaches</em></span><span class="No-Break">.</span></p>
			<p>Now, let’s look at the next feature on the list: MLflow <span class="No-Break">Model Registry.</span></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>MLflow Model Registry</h1>
			<p>MLflow Model Registry<a id="_idIndexMarker265"/> is a tool that collaboratively manages the life cycle of all the MLflow Models in a centralized manner across an organization. In Databricks, the integrated Model Registry provides granular access control over who can transition models from one stage <span class="No-Break">to another.</span></p>
			<p>MLflow Model Registry <a id="_idIndexMarker266"/>allows multiple versions of the models in a particular stage. It enables the transition of the best-suited model between staging, prod, and archived states either programmatically or by a human-in-the-loop deployment model. Choosing one strategy over another for model deployment will depend on the use case and how comfortable teams are in automating the entire process of managing ML model promotion and testing process. We will take a deeper look into this in <a href="B17875_06.xhtml#_idTextAnchor100"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Model Versioning </em><span class="No-Break"><em class="italic">and Webhooks</em></span><span class="No-Break">.</span></p>
			<p>Model Registry also logs model descriptions, lineage, and promotion activity from one stage to another, providing <span class="No-Break">full traceability.</span></p>
			<p>We will look into the Model Registry feature more in detail in <a href="B17875_06.xhtml#_idTextAnchor100"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Model Versioning </em><span class="No-Break"><em class="italic">and Webhooks</em></span><span class="No-Break">.</span></p>
			<p>The following figure summarizes the interaction between various <span class="No-Break">MLflow components:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17875_04_2.jpg" alt="Figure 4.2 – How the various MLflow components interact with each other" width="1099" height="601"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – How the various MLflow components interact with each other</p>
			<p>You have the flexibility to choose your preferred Python ML libraries for model training within MLflow, while the MLflow Tracking server diligently logs metrics, tags, and artifacts, and then packages your model into the MLflow Models format. Once you’ve honed a candidate model ready for integration into Model Registry, it’s a straightforward process to register it there. Model Registry not only furnishes APIs but also offers governance mechanisms for smooth model transitioning between stages. Additionally, MLflow Model Registry introduces webhooks, which enable automated notifications to be triggered by specific user actions; we’ll delve into this further in<em class="italic"> </em><a href="B17875_06.xhtml#_idTextAnchor100"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Model Versioning and Webhooks</em>. In the end, downstream applications can harness APIs to fetch the latest models from the registry and deploy them in various flavors, including Python functions, Docker containers, or other supported deployment options that accommodate batch, streaming, and real-time <span class="No-Break">use cases.</span></p>
			<p>You have the freedom to independently manage your ML project life cycle by employing the features we’ve discussed thus far, even without utilizing Databricks Feature Store. However, utilizing Feature Store in ML projects offers numerous advantages, including centralized data management for streamlined access and consistency, feature reusability across projects, version control for reproducibility, data quality checks, collaborative teamwork, scalability to handle growing data complexity, real-time feature serving, model monitoring integration, regulatory compliance support, and significant time and cost savings. In essence, Feature Store enhances the efficiency and effectiveness of <a id="_idIndexMarker267"/>ML workflows by providing a structured and efficient approach to data management and <span class="No-Break">feature handling.</span></p>
			<p>Let’s look at an end-to-end code example that goes through the entire flow of training an ML model in the Databricks environment and utilizes all the features of <span class="No-Break">integrated MLflow.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor083"/>Example code showing how to track ML model training in Databricks</h1>
			<p>Before proceeding, it’s<a id="_idIndexMarker268"/> important to ensure that you’ve already cloned the code repository that accompanies this book, as outlined in <a href="B17875_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Additionally, please verify that you have executed the associated notebook for <a href="B17875_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. These preparatory steps are essential to fully engage with the content and exercises <span class="No-Break">presented here:</span></p>
			<ol>
				<li>Go to <span class="No-Break"><strong class="source-inline">Chapter 04</strong></span> and click on the <span class="No-Break"><strong class="source-inline">mlflow-without-featurestore</strong></span><span class="No-Break"> notebook:</span></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17875_04_3.jpg" alt="Figure 4.3 – The code that accompanies this chapter" width="941" height="366"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – The code that accompanies this chapter</p>
			<p class="list-inset">Make sure you have a cluster up and running and that the cluster is attached to this notebook, as you did with the notebook from <a href="B17875_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Utilizing the </em><span class="No-Break"><em class="italic">Feature Store.</em></span></p>
			<ol>
				<li value="2"><strong class="source-inline">Cmd 3</strong> demonstrates the use of notebook-scoped libraries. These can be installed using the <strong class="source-inline">%pip</strong> magic command. As best practice, keep the <strong class="source-inline">%pip</strong> command as one of the topmost cells in your notebook as it restarts the Python interpreter. We are just upgrading the version of the <strong class="source-inline">scikit-learn</strong> <span class="No-Break">library here:</span><pre class="source-code">
%pip install -U scikit-learn</pre></li>				<li>In <strong class="source-inline">Cmd 5</strong> and <strong class="source-inline">Cmd 6</strong>, we are <a id="_idIndexMarker269"/>just defining some constant values we will be using to track our ML model training. Change the <strong class="source-inline">USER_EMAIL</strong> value to the email you’ve used to log into the Databricks workspace. In this notebook, we are not going to use the Feature Store API; however, every feature table is stored as a Delta table, which can be read as a regular <span class="No-Break">hive table:</span><pre class="source-code">
from databricks.feature_store import FeatureStoreClientfrom databricks.feature_store import FeatureLookupimport typingfrom sklearn import metricsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitimport mlflowimport pandas as pd# Name of experiment where we will track all the different model training runs.EXPERIMENT_NAME = "Bank_Customer_Churn_Analysis"# Name of the modelMODEL_NAME = "random_forest_classifier"# This is the name for the entry in model registryMODEL_REGISTRY_NAME = "Bank_Customer_Churn"# The email you use to authenticate in the Databricks workspaceUSER_EMAIL = "&lt;your email&gt;"# Location where the MLflow experiment will be listed in user workspaceEXPERIMENT_NAME = f"/Users/{USER_EMAIL}/{EXPERIMENT_NAME}"# we have all the features backed into a Delta table so we will read directlyFEATURE_TABLE = "bank_churn_analysis.bank_customer_features"</pre></li>				<li>To use Mlflow, we<a id="_idIndexMarker270"/> had to import the <strong class="source-inline">mlflow</strong> package. <strong class="source-inline">mlflow.setExperiment(…)</strong> creates a named experiment to track all the MLflow runs that we will execute in this notebook. After executing this code, you should be able to see a new type of entity listed in your workspace directory where <strong class="source-inline">EXPERIMENT_NAME</strong> points to. As mentioned earlier, this creates a <span class="No-Break">workspace-scoped experiment:</span><pre class="source-code">
# set experiment namemlflow.set_experiment(EXPERIMENT_NAME)</pre></li>			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17875_04_4.jpg" alt="Figure 4.4 – The new experiment that was created in the workspace" width="683" height="444"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – The new experiment that was created in the workspace</p>
			<ol>
				<li value="5">Calling <strong class="source-inline">mlflow.start_run()</strong> starts a run under the listed experiment. The rest of the code<a id="_idIndexMarker271"/> simply trains a scikit learn model. With just a few lines of code, we are now using the features of <span class="No-Break">MLflow Tracking:</span><pre class="source-code">
with mlflow.start_run():  TEST_SIZE = 0.20  # Now we will read the data directly from the feature table  training_df = spark.table(FEATURE_TABLE)  # convert the dataset to pandas so that we can fit sklearn RandomForestClassifier on it  train_df = training_df.toPandas()  # The train_df represents the input dataframe that has all the feature columns along with the new raw input in the form of training_df.  X = train_df.drop(['Exited'], axis=1)  y = train_df['Exited']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=54, stratify=y)  # here we will are not doing any hyperparameter tuning however, in future we will see how to perform hyperparameter tuning in scalable manner on Databricks.  model = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)  signature = mlflow.models.signature.infer_signature(X_train, model.predict(X_train))  predictions = model.predict(X_test)  fpr, tpr, _ = metrics.roc_curve(y_test, predictions, pos_label=1)  auc = metrics.auc(fpr, tpr)  accuracy = metrics.accuracy_score(y_test, predictions)  # get the calculated feature importances.  importances = dict(zip(model.feature_names_in_, model.feature_importances_))  # log artifact  mlflow.log_dict(importances, "feature_importances.json")  # log metrics  mlflow.log_metric("auc", auc)  mlflow.log_metric("accuracy", accuracy)  # log parameters  mlflow.log_param("split_size", TEST_SIZE)  mlflow.log_params(model.get_params())  # set tag  mlflow.set_tag(MODEL_NAME, "mlflow demo")  # log the model itself in mlflow tracking server  mlflow.sklearn.log_model(model, MODEL_NAME, signature=signature, input_example=X_train.iloc[:4, :])</pre></li>				<li>The following code<a id="_idIndexMarker272"/> utilizes the MLflow Tracking server to log artifacts and hyperparameter values while setting <strong class="source-inline">tag</strong> to the <strong class="source-inline">sklearn</strong> model that is being logged to the MLflow <span class="No-Break">Tracking server:</span><pre class="source-code">
 mlflow.log_dict(importances, "feature_importances.json")  # log metrics  mlflow.log_metric("auc", auc)  mlflow.log_metric("accuracy", accuracy)  # log parameters  mlflow.log_param("split_size", TEST_SIZE)  mlflow.log_params(model.get_params())  # set tag  mlflow.set_tag(MODEL_NAME, "mlflow demo")  # log the model itself in mlflow tracking server  mlflow.sklearn.log_model(model, MODEL_NAME, signature=signature, input_example=X_train.iloc[:4, :])</pre><p class="list-inset">Once we’ve finished <a id="_idIndexMarker273"/>executing the code in this cell, we will be able to see the run and all its artifacts, parameters, and hyperparameters listed under <span class="No-Break">the experiment:</span></p></li>			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17875_04_5.jpg" alt="Figure 4.5 – The runs listed under the experiments" width="315" height="480"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – The runs listed under the experiments</p>
			<ol>
				<li value="7">You can check the <a id="_idIndexMarker274"/>details of each run in the Tracking UI by clicking the <span class="No-Break">shortcut icon:</span></li>
			</ol>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17875_04_6.jpg" alt="Figure 4.6 – The shortcut for accessing the integrated MLflow Tracking UI" width="650" height="275"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – The shortcut for accessing the integrated MLflow Tracking UI</p>
			<p class="list-inset">The Tracking UI displays<a id="_idIndexMarker275"/> information about each run in a lot more detail. Along with the serialized model, you can access logged artifacts, metrics, hyperparameters, the model signature, or the sample input that your model expects as input if you logged them during the run. At the top, you can see the path to the experiment under which this run is being tracked. Each run is uniquely identified with an ID that is also visible at the top. The tracking server provides lineage and links this model run back to the exact version of the notebook that was used to execute <span class="No-Break">this run:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17875_04_7.jpg" alt="Figure 4.7 – The details of the run in the Tracking UI" width="1009" height="619"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – The details of the run in the Tracking UI</p>
			<ol>
				<li value="8">Clicking on the <a id="_idIndexMarker276"/>experiment path at the top will take you to the experiment view where, if you have executed more than one run, you can select and compare various runs to get the best model or compare the best combinations of <span class="No-Break">the hyperparameters:</span></li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17875_04_8.jpg" alt="Figure 4.8 – All the runs associated with our experiment" width="999" height="352"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – All the runs associated with our experiment</p>
			<p>Now, let’s summarize <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor084"/>Summary</h1>
			<p>In this chapter, we covered the various components of MLflow and how they work together to make the end-to-end ML project life cycle easy to manage. We learned about MLflow Tracking, Projects, Models, and <span class="No-Break">Model Registry.</span></p>
			<p>This chapter covered some key components of MLFlow and their purpose. Understanding these concepts is essential in effectively managing end-to-end ML projects in the <span class="No-Break">Databricks environment.</span></p>
			<p>In the next chapter, we will look at the AutoML capabilities of Databricks in detail and how we can utilize them to create our baseline models for <span class="No-Break">ML projects.</span></p>
		</div>
	</div>
</div>
</body></html>