["```py\n    %pip install -U scikit-learn\n    ```", "```py\n    from databricks.feature_store import FeatureStoreClientfrom databricks.feature_store import FeatureLookupimport typingfrom sklearn import metricsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitimport mlflowimport pandas as pd# Name of experiment where we will track all the different model training runs.EXPERIMENT_NAME = \"Bank_Customer_Churn_Analysis\"# Name of the modelMODEL_NAME = \"random_forest_classifier\"# This is the name for the entry in model registryMODEL_REGISTRY_NAME = \"Bank_Customer_Churn\"# The email you use to authenticate in the Databricks workspaceUSER_EMAIL = \"<your email>\"# Location where the MLflow experiment will be listed in user workspaceEXPERIMENT_NAME = f\"/Users/{USER_EMAIL}/{EXPERIMENT_NAME}\"# we have all the features backed into a Delta table so we will read directlyFEATURE_TABLE = \"bank_churn_analysis.bank_customer_features\"\n    ```", "```py\n    # set experiment namemlflow.set_experiment(EXPERIMENT_NAME)\n    ```", "```py\n    with mlflow.start_run():  TEST_SIZE = 0.20  # Now we will read the data directly from the feature table  training_df = spark.table(FEATURE_TABLE)  # convert the dataset to pandas so that we can fit sklearn RandomForestClassifier on it  train_df = training_df.toPandas()  # The train_df represents the input dataframe that has all the feature columns along with the new raw input in the form of training_df.  X = train_df.drop(['Exited'], axis=1)  y = train_df['Exited']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=54, stratify=y)  # here we will are not doing any hyperparameter tuning however, in future we will see how to perform hyperparameter tuning in scalable manner on Databricks.  model = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)  signature = mlflow.models.signature.infer_signature(X_train, model.predict(X_train))  predictions = model.predict(X_test)  fpr, tpr, _ = metrics.roc_curve(y_test, predictions, pos_label=1)  auc = metrics.auc(fpr, tpr)  accuracy = metrics.accuracy_score(y_test, predictions)  # get the calculated feature importances.  importances = dict(zip(model.feature_names_in_, model.feature_importances_))  # log artifact  mlflow.log_dict(importances, \"feature_importances.json\")  # log metrics  mlflow.log_metric(\"auc\", auc)  mlflow.log_metric(\"accuracy\", accuracy)  # log parameters  mlflow.log_param(\"split_size\", TEST_SIZE)  mlflow.log_params(model.get_params())  # set tag  mlflow.set_tag(MODEL_NAME, \"mlflow demo\")  # log the model itself in mlflow tracking server  mlflow.sklearn.log_model(model, MODEL_NAME, signature=signature, input_example=X_train.iloc[:4, :])\n    ```", "```py\n     mlflow.log_dict(importances, \"feature_importances.json\")  # log metrics  mlflow.log_metric(\"auc\", auc)  mlflow.log_metric(\"accuracy\", accuracy)  # log parameters  mlflow.log_param(\"split_size\", TEST_SIZE)  mlflow.log_params(model.get_params())  # set tag  mlflow.set_tag(MODEL_NAME, \"mlflow demo\")  # log the model itself in mlflow tracking server  mlflow.sklearn.log_model(model, MODEL_NAME, signature=signature, input_example=X_train.iloc[:4, :])\n    ```"]