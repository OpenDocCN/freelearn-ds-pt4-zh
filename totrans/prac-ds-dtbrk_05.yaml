- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create a Baseline Model Using Databricks AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we understood **MLflow** and all its components. After
    running the notebook from [*Chapter 4*](B17875_04.xhtml#_idTextAnchor076), *Understanding
    MLflow Components on Databricks*, you might have recognized how easy it actually
    is to start tracking your ML model training in Databricks using the integrated
    MLflow tracking server. In this chapter, we will cover another new and unique
    feature of **Databricks** called **AutoML**.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks AutoML, like all the other features that are part of the Databricks
    workspace, is fully integrated with MLflow features and the Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks AutoML, at the time of writing of this book, supports **classification**,
    **regression**, and **forecasting** use cases using traditional ML algorithms
    and not deep learning. You can see a list of supported algorithms in the second
    section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You can use AutoML with a table registered in Databricks’ Hive metastore, feature
    tables, or even upload a new file using the import data functionality in Databricks.
    You can read more about it by clicking the link in the *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding AutoML in Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running AutoML on our churn prediction dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through the technical requirements for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To go through the chapter, we’ll need the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: we'll need the execution of the notebooks pertaining to [*Chapter 3*](B17875_03.xhtml#_idTextAnchor063),
    which involves the ingestion of raw data from a CSV file into a Delta table and
    the subsequent registration of a new feature table, to have already been completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have never worked with any AutoML framework before, you might be wondering
    what AutoML is and when and how it can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML simplifies the machine learning model development process by automating
    various tasks. It automatically generates baseline models tailored to your specific
    datasets and even offers preconfigured notebooks to kickstart your projects. This
    is particularly appealing to data scientists of all levels of expertise because
    it saves valuable time in the initial stages of model development. Instead of
    manually crafting models from scratch, AutoML provides a quick and efficient way
    to obtain baseline models, making it a valuable tool for both beginners and experienced
    data scientists alike.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML makes machine learning not only accessible to citizen data scientists
    and business subject matter experts. AutoML, while undoubtedly a powerful tool,
    also grapples with significant limitations. One notable challenge is its inherent
    black-box nature, which makes it difficult, and at times impossible, to decipher
    which hyperparameters and algorithms are most effective for a particular problem.
    This opacity presents a substantial obstacle when it comes to achieving model
    explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, many AutoML tools available in the market fall short of supporting
    essential components in the machine learning life cycle, including the critical
    step of operationalizing models for production use. This deficiency can hinder
    the seamless integration of machine learning solutions into real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that AutoML doesn’t replace the role of a data scientist.
    While it streamlines certain aspects of model development, the expertise and insights
    of a skilled data scientist remain indispensable in ensuring the success of machine
    learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Databricks AutoML actually provides one of its biggest benefits.
    Let’s take a deeper look into AutoML in Databricks and discover how you can use
    it in your model development journey.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AutoML in Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Databricks AutoML uses a glass-box approach to AutoML. When you use Databricks
    AutoML either through the UI or through the supported Python API, it logs every
    combination of model and hyperparameter (trial) as an MLflow run and generates
    Python notebooks with source code corresponding to each model trial. The results
    of all these model trials are logged into the MLflow tracking server. Each of
    the trials can be compared and reproduced. Since you have access to the source
    code, the data scientists can easily rerun a trial after modifying the code. We
    will look at this in more detail when we go over the example.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks AutoML also prepares the dataset for training and then performs model
    training and hyperparameter tuning on the Databricks cluster. One important thing
    to keep in mind here is that Databricks AutoML spreads hyperparameter tuning trials
    across the cluster. A trial is a unique configuration of hyperparameters associated
    with the model. All the training datasets should fit in a single executor, as
    Databricks AutoML will automatically sample your dataset if you have a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing the book, Databricks AutoML supports the following algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ML problems** | **Supported algorithms** |'
  prefs: []
  type: TYPE_TB
- en: '| Classification |'
  prefs: []
  type: TYPE_TB
- en: 'Scikit-learn models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Regression |'
  prefs: []
  type: TYPE_TB
- en: 'Scikit-learn models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression with stochastic gradient descent
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Forecasting |'
  prefs: []
  type: TYPE_TB
- en: Prophet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto ARIMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 – Algorithms that Databricks AutoML supports
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand some of the key capabilities provided by Databricks AutoML
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sampling is done based on the estimated memory required to load and train models
    on the training dataset. Until ML Runtime 10.5, the data sampling does not depend
    on the VM type or the amount of memory the executor is running on. In ML Runtime
    11.0 and later versions, the sampling mechanism will increase sampling fraction
    and size if a node is compute-optimized with more significant memory.
  prefs: []
  type: TYPE_NORMAL
- en: By default, in Databricks, each executor is configured to execute the same number
    of trials as there are available CPU cores. Additionally, the executor’s available
    memory is evenly distributed among these trials. However, you have the flexibility
    to modify this behavior by adjusting the `spark. task.cpus` configuration parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The default setting for `spark.task.cpus` is `1`, which means that each executor
    will run as many trials in parallel as it has CPU cores. If you change this value
    to match the number of available CPU cores on the executor, it will result in
    a different behavior. In this case, only one trial will be executed on the executor
    at a time, but that trial will have access to the full memory capacity of the
    executor. This setting can be useful if you want to provide additional resources
    to each of your trials. This will also increase the size of the sampled dataset.
    AutoML utilizes the PySparks `sampleBy` method ([https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameStatFunctions.sampleBy.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameStatFunctions.sampleBy.html))
    for performing stratified sampling for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: For regression problems, AutoML utilizes the `sample` method ([https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.sample.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.sample.html))
    when sampling is needed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sampling is not applicable for* *forecasting problems*.'
  prefs: []
  type: TYPE_NORMAL
- en: Imbalance data detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Databricks Runtime 11.2 ML and newer versions, when AutoML detects an imbalanced
    dataset for a classification use case, it takes steps to mitigate the imbalance
    within the training dataset. This is accomplished through a combination of downsampling
    the major class(es) and introducing class weights. It’s important to note that
    this balancing process is applied exclusively to the training dataset and doesn’t
    affect the test and validation datasets. This approach guarantees that the model’s
    performance is evaluated based on the original dataset with its true class distribution.
  prefs: []
  type: TYPE_NORMAL
- en: To address an imbalanced training dataset, AutoML assigns class weights that
    are inversely proportional to the extent of downsampling applied to a specific
    class. To illustrate, let’s consider a training dataset with 100 samples, where
    95 belong to `Class A` and 5 belong to `Class B`. AutoML reduces this imbalance
    by downsampling `Class A` to 70 samples, effectively reducing it by a ratio of
    70:95, or approximately 0.736\. Meanwhile, the number of samples in `Class B`
    remains at five. To ensure that the final model is properly calibrated and maintains
    the same probability distribution as the input data, AutoML adjusts the class
    weight for `Class A` by the inverse of this ratio, which is approximately 1:0.736,
    or 1.358\. The weight for `Class B` remains at one. These class weights are then
    used during the model training process as a parameter to ensure that samples from
    each class receive appropriate weighting, contributing to a balanced and accurate
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data into train/validation/test sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Databricks Runtime 10.1 ML and later versions, you have the option to designate
    a time column when performing data splits for classification and regression tasks.
    When you specify this time column, the dataset is divided into training, validation,
    and test sets based on chronological order. The data points from the earliest
    time period are allocated to the training set, followed by the next earliest for
    validation. The most recent data points are reserved for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In Databricks Runtime 10.1 ML, the time column must be of either the timestamp
    or integer data type. However, starting from Databricks Runtime 10.2 ML, you also
    have the flexibility to choose a string column for this purpose. This enhancement
    offers greater versatility in time-based data splitting for improved model training
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing semantic type detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic type detection is a powerful feature, introduced in Databricks Runtime
    versions 9.1 LTS ML and beyond, designed to augment AutoML by providing intelligent
    insights into the data types present within each column. *It is essential to note
    that semantic type detection does not apply to forecasting problems or columns
    where custom imputation methods have* *been specified.*
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML conducts a thorough analysis of columns to ascertain whether their semantic
    type differs from the data type specified in the table schema (either Spark or
    pandas). Once discrepancies are identified, AutoML takes specific actions based
    on the detected semantic type. However, it’s important to keep in mind that these
    detections may not always be 100% accurate. The following are the key adjustments
    AutoML can make:'
  prefs: []
  type: TYPE_NORMAL
- en: '**String and integer columns with date or timestamp data**: These are intelligently
    recognized as timestamp types, allowing for more precise handling of temporal
    information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**String columns representing numeric data**: When applicable, these columns
    are converted into numeric types, ensuring that mathematical operations can be
    performed seamlessly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting from Databricks Runtime 10.1 ML, AutoML extends its capabilities to
    encompass the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numeric columns containing categorical IDs**: These are identified as categorical
    features, aiding in more accurate modeling when dealing with categorical data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**String columns containing English text**: Such columns are identified as
    text features, enhancing the understanding of textual data within the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Databricks Runtime version 10.1 ML and beyond, users gain the ability to
    manually set semantic types through Python annotations. The following code snippet
    illustrates the syntax for this manual annotation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The available semantic types that you can assign manually are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical**: Appropriate for columns containing values such as IDs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numeric**: Ideal for columns containing numeric values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DateTime**: Suitable for columns with timestamp values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: Reserved for string columns containing English text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To disable semantic type detection for a specific column, you can use the special
    keyword annotation native.
  prefs: []
  type: TYPE_NORMAL
- en: Shapley value (SHAP) for model explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Shapley values** (**SHAP**) are a technique grounded in game theory used
    to estimate the significance of each feature in a machine learning model’s predictions.
    AutoML regression and classification notebooks come with built-in code to compute
    these values using the SHAP package. However, because calculating SHAP is highly
    memory-intensive, they are not enabled by default.'
  prefs: []
  type: TYPE_NORMAL
- en: To activate and compute SHAP in an AutoML notebook, you need to navigate to
    the `shap_enabled` to `True`, and then rerun the notebook. It’s worth noting that
    SHAP plots won’t be generated in version 11.1 and earlier versions of MLR if the
    dataset includes a `DateTime` column.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Store integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Databricks Runtime version 11.3 LTS ML and subsequent releases, feature tables
    from the Feature Store can be utilized to enhance the base dataset for classification
    and regression tasks. As of version 12.2 LTS ML, this capability extends to augmenting
    the input dataset for a comprehensive set of AutoML challenges, including classification,
    regression, and forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are certain limitations associated with the current state of AutoML.
    Only the following data types in your dataset are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: Numeric (`ByteType`, `ShortType`, `IntegerType`, `LongType`, `FloatType`, and
    `DoubleType`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boolean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String (categorical or English text)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timestamps (*TimestampType* and *DateType*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ArrayType[Numeric]` (Databricks Runtime 10.4 LTS ML and later versions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecimalType` (Databricks Runtime 11.3 LTS ML and later versions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You also need to ensure that the source dataset has all unique column names.
    If you utilize AutoML for time series forecasting and want Auto ARIMA, ensure
    that the interval between any two points in the time-series input dataset is the
    same. AutoML will automatically fill the missing timesteps with the previous value
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at an example use case for AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Running AutoML on our churn prediction dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a look at how to use Databricks AutoML with our bank customer churn
    prediction dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If you executed the notebooks from [*Chapter 3*](B17875_03.xhtml#_idTextAnchor063),
    *Utilizing the Feature Store*, you will have raw data available as a Delta table
    in your Hive metastore. It has the name `raw_data`. In the [*Chapter 3*](B17875_03.xhtml#_idTextAnchor063)
    code, we read a CSV file from our Git repository with raw data, wrote that as
    a Delta table, and registered it in our integrated metastore. Take a look at `cmd
    15` in your notebook. In your environment, the dataset can be coming from another
    data pipeline or uploaded directly to the Databricks workspace using the *Upload*
    *file* functionality.
  prefs: []
  type: TYPE_NORMAL
- en: To view the tables, you need to have your cluster up and running.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The location of the raw dataset](img/B17875_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The location of the raw dataset
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create our first Databricks AutoML experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that before following the next steps, you have a cluster up and running
    that has the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: Single-node or multi-node
  prefs: []
  type: TYPE_NORMAL
- en: Access mode as a single user
  prefs: []
  type: TYPE_NORMAL
- en: The Databricks runtime version is set to 13.3 LTS ML or higher
  prefs: []
  type: TYPE_NORMAL
- en: '*Worker Type/Driver Type* is any VM type with at least four cores'
  prefs: []
  type: TYPE_NORMAL
- en: '*No additional libraries should be installed on the cluster other than those
    pre-installed in Databricks Runtime for machine learning. AutoML is not compatible
    with clusters operating in shared* *access mode.*'
  prefs: []
  type: TYPE_NORMAL
- en: On the left tab, click on **Experiments**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.2 – The location of the Experiments tab](img/B17875_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – The location of the Experiments tab
  prefs: []
  type: TYPE_NORMAL
- en: On the top of this page, click on **Create AutoML Experiment**. This will bring
    you to the AutoML configuration page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – How to create a new AutoML experiment (1)](img/B17875_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – How to create a new AutoML experiment (1)
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative, you can click on **New** and then select **AutoML Experiment**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – An alternative way to create an AutoML experiment](img/B17875_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – An alternative way to create an AutoML experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you need to enter the following basic information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**For the purpose of this cluster**: This is what cluster configuration you
    want the AutoML to run on. You can reuse the same cluster we created for [*Chapter
    3*](B17875_03.xhtml#_idTextAnchor063), *Utilizing the Feature Store*, and [*Chapter
    4*](B17875_04.xhtml#_idTextAnchor076), *Understanding MLflow Components* *on Databricks.*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML problem type**: Regression, classification, or forecasting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset**: The dataset containing all the features and label/target column.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – How to create a new AutoML experiment (2)](img/B17875_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – How to create a new AutoML experiment (2)
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction target**: This is specific to the classification problem at hand.
    Once you select your dataset for running AutoML, this will auto-populate with
    all the columns and you can select your target column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Showing how to create a new AutoML experiment (3)](img/B17875_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Showing how to create a new AutoML experiment (3)
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment name**: This is the name that is used to track all your trials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, you can also select what features from your selected table need
    to be included when running the trials. In our case, `RowNumber`, `CustomerId`,
    and `Surname` don’t add any value to our analysis, so we will remove them from
    selection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.7 – How to create a new AutoML experiment (4)](img/B17875_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – How to create a new AutoML experiment (4)
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can also select how you would want to handle missing values
    in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In versions 10.4 LTS ML and higher of Databricks Runtime, you can define the
    approach for handling `null` values. Within the UI, you can select your desired
    imputation technique via the **Impute with** drop-down menu located within the
    table **Schema** section.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that AutoML automatically chooses an appropriate imputation
    strategy based on both the data type and the content of the column in question.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – How to create a new AutoML experiment (5)](img/B17875_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – How to create a new AutoML experiment (5)
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! Those are the only five things you need to do to start using glassbox
    AutoML with Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some advanced configurations as well, such as what metrics you want
    to optimize when selecting the best-performing model or what supported training
    framework you want to include to run trials on:'
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation metric serves as the primary scoring metric for runs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from Databricks Runtime 10.3 ML, it’s possible to exclude certain training
    frameworks. By default, AutoML uses frameworks listed under its algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stopping conditions are customizable. The defaults are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop after 120 minutes for forecasting experiments.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For classification and regression experiments in Databricks Runtime 10.5 ML
    and earlier versions, stop after 60 minutes or after 200 trials—whichever comes
    first. Starting from Databricks Runtime 11.0 ML, the number of trials is not a
    stopping condition.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From Databricks Runtime 10.1 ML, AutoML incorporates early stopping for classification
    and regression if the validation metric ceases to improve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also starting from Databricks Runtime 10.1 ML, you can select a time column
    for chronological data splitting in classification and regression tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can specify a DBFS location for saving the training dataset in the **Data
    directory** field. If left blank, the dataset is saved as an MLflow artifact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – How to create a new AutoML experiment (6)](img/B17875_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – How to create a new AutoML experiment (6)
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the richness of your dataset, you can seamlessly integrate an existing
    feature table. Simply scroll to the bottom of the page and click on the **Join
    Features** option. This will grant you access to a configuration panel where you
    can precisely specify which feature tables you want to merge with your existing
    dataset and establish the key or keys that will underpin this merging process,
    effectively linking the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that for the sake of this example, we will not
    be incorporating the *Feature store* table into the *merge* operation. This approach
    empowers you to bolster your dataset with additional information from selected
    feature tables, elevating its utility for analytical or machine-learning endeavors
    while omitting the Feature Store table from this particular exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we simply need to hit **start AutoML**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our AutoML experiment is now executing and is the current state visible in
    the UI. As AutoML progresses, it produces the following three artifacts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It generates a detailed data exploration notebook with source code to outline
    any skews or concerns, such as missing data or zeros. It uses the `pandas.profiling`
    package automatically to do this. You can view this notebook by clicking **View
    data** **exploration notebook**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – How to access the auto-generated exploratory data analysis
    notebook](img/B17875_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – How to access the auto-generated exploratory data analysis notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'The data exploration notebook also displays the correlation between the different
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 5.11 – Correlation graphs generated by the exploratory data analysis
    notebook](img/B17875_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Correlation graphs generated by the exploratory data analysis
    notebook
  prefs: []
  type: TYPE_NORMAL
- en: You can see the experiment trials containing source code for every run being
    performed on our dataset. The source code is listed under your workspace under
    your user directory in a folder called `databricks_automl`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.12 – The location of various notebooks with code relating to each
    trial](img/B17875_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – The location of various notebooks with code relating to each trial
  prefs: []
  type: TYPE_NORMAL
- en: The notebook with the best model is also generated from AutoML after all the
    trials have finished execution. This notebook walks you through all the steps
    performed to feature engineer and train the ML model. The trial is logged automatically
    in the tracking server. The notebook also contains code for feature transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.13 – The location of the notebook that logs the best model identified
    by AutoMl](img/B17875_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – The location of the notebook that logs the best model identified
    by AutoMl
  prefs: []
  type: TYPE_NORMAL
- en: It also utilizes SHAP ([https://pypi.org/project/shap/](https://pypi.org/project/shap/))
    to log feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – A sample SHAP value graph auto-generated as part of the best
    model notebook](img/B17875_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – A sample SHAP value graph auto-generated as part of the best model
    notebook
  prefs: []
  type: TYPE_NORMAL
- en: This notebook also explains how you can finally utilize the trained model using
    the various deployment options, which we will discuss later in [*Chapter 6*](B17875_06.xhtml#_idTextAnchor100),
    *Model Versioning* *and Webhooks*.
  prefs: []
  type: TYPE_NORMAL
- en: You can compare the various trials through the UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also utilize the Python API to kick off AutoML for classification, forecasting,
    or regression. Using the Python API, you can also retrieve the best model programmatically
    from the AutoML experiment and use it for inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is an example code for kicking off classification:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can read more about the various parameters at [https://docs.databricks.com/applications/machine-learning/automl.html#classification-and-regression](https://docs.databricks.com/applications/machine-learning/automl.html#classification-and-regression).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the importance of AutoML and how it can help data
    scientists get started and become productive with the problem at hand. We then
    covered the Databricks AutoML glassbox approach, which makes it easy to interpret
    model results and automatically capture lineage. We also learned how Databricks
    AutoML is integrated with the MLflow tracking server within the Databricks workspace.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, we will go over managing your ML model’s life cycle using
    the MLflow model registry and Webhooks in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some other useful links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks, *What is* *AutoML?*: ([https://docs.databricks.com/applications/machine-learning/automl.html#databricks-automl](https://docs.databricks.com/applications/machine-learning/automl.html#databricks-automl))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks, *Import* *data*: ([https://docs.databricks.com/data/data.html#import-data-1](https://docs.databricks.com/data/data.html#import-data-1))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: ML Governance and Deployment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will learn how to utilize the MLFlow model registry to manage model versioning
    and transition to production from various stages and use webhooks to set up alerts
    and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17875_06.xhtml#_idTextAnchor100), *Model Versioning and Webhooks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17875_07.xhtml#_idTextAnchor108), *Model Deployment Approaches*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17875_08.xhtml#_idTextAnchor122), *Automating ML Workflows Using
    Databricks Jobs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17875_09.xhtml#_idTextAnchor129), *Model Drift Detection and
    Retraining*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17875_10.xhtml#_idTextAnchor142), *Using CI/CD to Automate
    Model Retraining and Redeployment*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
