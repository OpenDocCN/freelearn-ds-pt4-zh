["```py\n    import mlflow# the name of the model in the registryregistry_model_name = \"Churn Prediction Bank\"# get the latest version of the model in staging and load it as a spark_udf.# MLflow easily produces a Spark user defined function (UDF).  This bridges the gap between Python environments and applying models at scale using Spark.model = mlflow.pyfunc.spark_udf(spark, model_uri = f\"models:/{registry_model_name}/staging\")\n    ```", "```py\n    spark_df = spark.table(\"bank_churn_analysis.raw_Data\")display(spark_df)exclude_colums = {'RowNumber', \"CustomerId\", \"Surname\", \"Exited\"}input_columns = [col for col in spark_df.columns if col not in exclude_colums]input_columns# passing non label columns to the model as inputprediction_df = spark_df.withColumn(\"prediction\", model(*input_columns))display(prediction_df)\n    ```", "```py\n    # right now we are just defining a streaming data source but this statement will not execute until we call an Spark action. Another way to exclude the columns that are not needed is by dropping them from the DataFrame.raw_streaming_df = spark.readStream.format(\"delta\").option(\"ignoreChanges\", \"true\").table(\"bank_churn_analysis.raw_Data\").drop(*(\"RowNumber\", \"CustomerId\", \"Surname\", \"Exited\"))\n    ```", "```py\n    predictions_df = raw_streaming_df.withColumn(\"prediction\", model(*raw_streaming_df.columns))display(predictions_df, streamName=stream_name)\n    ```", "```py\n    # get token from notebooktoken = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)#create authorization header for REST callsheaders = {    \"Authorization\": f\"Bearer {token}\",    \"Content-Type\": \"application/json\"  }# Next we need an enpoint at which to execute our request which we can get from the Notebook's tags collectionjava_tags = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags()# This object comes from the Java CMtags = sc._jvm.scala.collection.JavaConversions.mapAsJavaMap(java_tags)# extract the databricks instance (domain name) from the dictionaryinstance = tags[\"browserHostName\"]\n    ```", "```py\n     # Import the requests library for HTTP communicationimport requests#change the model_serving endpoint name to the one you have given.model_serving_endpoint_name = \"churn_prediction\"# Define the function 'score_model' which takes a dictionary as an inputdef score_model(data_json: dict):    # Construct the URL for the model serving endpoint    url = f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\"    # Make an HTTP POST request to score the model    response = requests.request(method=\"POST\", headers=headers, url=url, json=data_json)    # Check if the request was successful (HTTP status code 200)    if response.status_code != 200:        # If not, raise an exception detailing the failure        raise Exception(f\"Request failed with status {response.status_code}, {response.text}\")    # Return the JSON response from the model scoring endpoint    return response.json()\n    ```", "```py\n    payload = {  \"dataframe_split\": {    \"index\": [1, 2],    \"columns\": [\"CreditScore\", \"Geography\", \"Gender\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"],    \"data\": [[619, \"France\", \"Female\", 42, 2, 0.0, 1, 1, 1, 101348.88], [608, \"Spain\", \"Female\", 41, 1, 83807.86, 1, 0, 1, 112542.58]]  }}\n    ```", "```py\n    payload = {  \"record_dataframe\": [    {      \"CreditScore\": 619,      \"Geography\": \"France\",      \"Gender\": \"Female\",      \"Age\": 42,      \"Tenure\": 2,      \"Balance\": 0.0,      \"NumOfProducts\": 1,      \"HasCrCard\": 1,      \"IsActiveMember\": 1,      \"EstimatedSalary\": 101348.88    },    {      \"CreditScore\": 608,      \"Geography\": \"Spain\",      \"Gender\": \"Female\",      \"Age\": 41,      \"Tenure\": 1,      \"Balance\": 83807.86,      \"NumOfProducts\": 1,      \"HasCrCard\": 0,      \"IsActiveMember\": 1,      \"EstimatedSalary\": 112542.58    }  ]}\n    ```", "```py\n    score_model(payload){'predictions': [0, 0]}\n    ```", "```py\n        {\"instances\": [8, 9, 10]}\n        ```", "```py\n        { \"instances\": [ { \"t1\": \"a\", \"t2\": [1, 2, 3, 4, 5], \"t3\": [[1, 2], [3, 4], [5, 6]] }, { \"t1\": \"b\", \"t2\": [6, 7, 8, 9, 10], \"t3\": [[7, 8], [9, 10], [11, 12]] } ] }\n        ```", "```py\n    dbutils.fs.cp(\"local_path/to/your_dependency.whl\", \"dbfs:/path/to/your_dependency.whl\")# Installing custom library using %pip%pip install /dbfs/path/to/your_dependency.whl\n    ```", "```py\n    # Logging the modelimport mlflow.sklearncustom_requirements = [\"scikit-learn\", \"numpy\", \"/dbfs/path/to/your_dependency.whl\"]mlflow.sklearn.log_model(model, \"sklearn-model\", pip_requirements=custom_requirements)\n    ```", "```py\n    import mlflow.models.utilsmodel_uri = \"models:/<model-name>/<model-version>\"mlflow.models.utils.add_libraries_to_model(model_uri)\n    ```", "```py\n# Model URI for accessing the registered modelaccess_model_uri = \"models:/enhanced_model_with_libraries/1\"\n# Add libraries to the original model run\nadd_libraries_to_model(access_model_uri)\n# Example to add libraries to an existing run\nprev_run_id = \"some_existing_run_id\"\nadd_libraries_to_model(access_model_uri, run_id=prev_run_id)\n# Example to add libraries to a new run\nwith mlflow.start_run():\n    add_libraries_to_model(access_model_uri)\n# Example to add libraries and register under a new model name\nwith mlflow.start_run():\n    add_libraries_to_model(access_model_uri, registered_model_name=\"new_enhanced_model\")\n```", "```py\nclass CustomModel(mlflow.pyfunc.PythonModel):    def load_context(self, context):\n        self.model = torch.load(context.artifacts[\"model-weights\"])\n        from preprocessing_utils.my_custom_tokenizer import CustomTokenizer\n        self.tokenizer = CustomTokenizer(context.artifacts[\"tokenizer_cache\"])\n    def format_inputs(self, model_input):\n        # insert code that formats your inputs\n        pass\n    def format_outputs(self, outputs):\n        predictions = (torch.sigmoid(outputs)).data.numpy()\n        return predictions\n    def predict(self, context, model_input):\n        model_input = self.format_inputs(model_input)\n        outputs = self.model.predict(model_input)\n        return self.format_outputs(outputs)\n```", "```py\nmlflow.pyfunc.log_model(CustomModel(), \"model\", code_path = [\"preprocessing_utils/\"])\n```", "```py\n# Custom model classclass CustomModelWrapper(mlflow.pyfunc.PythonModel):\n    # Initialize the classifier model in the constructor\n    def __init__(self, classifier_model):\n        self.classifier_model = classifier_model\n    # Prediction method\n    def predict(self, context, model_data):\n        # Compute the probabilities and the classes\n        probs = self.classifier_model.predict_proba(model_data)\n        preds = self.classifier_model.predict(model_data)\n        # Create a DataFrame to hold probabilities and predictions\n        labels = [\"class_0\", \"class_1\", \"class_2\"]\n        result_df = pd.DataFrame(probs, columns=[f'prob_{label}' for label in labels])\n        result_df['prediction'] = [labels[i] for i in preds]\n        return result_df\n```", "```py\n# Load the wine dataset and split it into training and test setswine_data = load_wine()\nX, y = wine_data.data, wine_data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n# Initialize and fit the DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(random_state=7)\ndt_classifier.fit(X_train, y_train)\n# Create an instance of the CustomModelWrapper\ncustom_wrapper = CustomModelWrapper(dt_classifier)\n# Define the input and output schema\ninput_cols = [ColSpec(\"double\", feature) for feature in wine_data.feature_names]\noutput_cols = [ColSpec(\"double\", f'prob_{cls}') for cls in wine_data.target_names] + [ColSpec(\"string\", 'prediction')]\nmodel_sign = ModelSignature(inputs=Schema(input_cols), outputs=Schema(output_cols))\n# Prepare an example input\ninput_sample = pd.DataFrame(X_train[:1], columns=wine_data.feature_names)\ninput_sample_dict = input_sample.to_dict(orient='list')\n# Log the model using MLflow\nwith mlflow.start_run():\n    mlflow.pyfunc.log_model(\"wine_model\",\n                            python_model=custom_wrapper, input_example=input_sample_dict, signature=model_sign)\n```", "```py\nHere is an example of how to log custom artifacts with your models: mlflow.pyfunc.log_model(    artifacts={'model-weights': \"/dbfs/path/to/file\", \"tokenizer_cache\": \"./tokenizer_cache\"}\n)\n```", "```py\nclass CustomMLflowModel(mlflow.pyfunc.PythonModel):    def load_context(self, context):\n        self.model = torch.load(context.artifacts[\"model-weights\"])\n        self.tokenizer = transformers.BertweetTokenizer.from_pretrained(\n            \"model-base\",\n            local_files_only=True,\n            cache_dir=context.artifacts[\"tokenizer_cache\"]\n        )\n```", "```py\nmlflow.pyfunc.log_model(    python_model=MyModel(),\n    artifact_path=\"my_model\",\n    conda_env={\n        'name': 'my_custom_env',\n        'channels': ['defaults'],\n        'dependencies': [\n            'numpy==1.19.2',\n            'pandas==1.2.3',\n            'scikit-learn==0.24.1',\n        ],\n    }\n)\n```"]