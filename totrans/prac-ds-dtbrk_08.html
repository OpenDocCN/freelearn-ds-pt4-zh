<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer126">
			<h1 id="_idParaDest-122" class="chapter-number"><a id="_idTextAnchor122"/>8</h1>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor123"/>Automating ML Workflows Using Databricks Jobs</h1>
			<p>In the last chapter, we covered the ML deployment life cycle and the various model deployment paradigms. We also understood how the response latency, the scalability of the solution, and the way we are going to access the predictions play an important role in deciding the <span class="No-Break">deployment method.</span></p>
			<p>In this chapter, we are going to take a look at <strong class="bold">Databricks Workflows</strong> with <strong class="bold">Jobs</strong> (previously called <strong class="bold">Databricks Jobs</strong>). This functionality can be leveraged<a id="_idIndexMarker544"/> not only to schedule the retraining<a id="_idIndexMarker545"/> of our models at regular intervals but also to trigger tests to check<a id="_idIndexMarker546"/> our models when transitioning from one <strong class="bold">Model Registry</strong> stage to another using the webhook integrations we discussed in <a href="B17875_06.xhtml#_idTextAnchor100"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">.</span></p>
			<p>We will be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">Databricks Workflows</span></li>
				<li>Utilizing Databricks Workflows with Jobs to automate model training <span class="No-Break">and testing</span></li>
			</ul>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor124"/>Technical requirements</h1>
			<p>The following are the technical requirements for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Access to the Databricks workspace with <strong class="bold">Unrestricted cluster creation</strong> permission at <span class="No-Break">a minimum</span></li>
				<li>All the previous notebooks, executed <span class="No-Break">as described</span></li>
			</ul>
			<p>Now, let’s take a look at <span class="No-Break">Databricks Workflows.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Understanding Databricks Workflows</h1>
			<p>Workflows in the simplest<a id="_idIndexMarker547"/> sense are frameworks for developing and running your data <span class="No-Break">processing pipelines.</span></p>
			<p>Databricks Workflows provides a reliable, fully managed orchestration service for all your data, analytics, and AI workloads on the <strong class="bold">Databricks Lakehouse</strong> platform on any cloud. Workflows are designed to ground<a id="_idIndexMarker548"/> up with the Databricks Lakehouse platform, providing deep monitoring capabilities along with centralized observability across all your other workflows. There is no additional cost to customers for using <span class="No-Break">Databricks Workflows.</span></p>
			<p>The key benefit of using workflows is that users don’t need to worry about managing orchestration software and infrastructure. Users can simply focus on specifying the business logic that needs to be executed as part of <span class="No-Break">the workflows.</span></p>
			<p>Within Databricks Workflows, there are two ways you can make use of the <span class="No-Break">managed workflows:</span></p>
			<ul>
				<li><strong class="bold">Delta Live Tables</strong> (<strong class="bold">DLT</strong>): DLT is a declarative ETL framework to develop reliable pipelines on the Databricks<a id="_idIndexMarker549"/> Lakehouse platform. DLT allows easy monitoring of the ETL pipelines while managing the infrastructure needed to run these pipelines. It also has built-in expectations to allow validation of incoming data for each Delta table and keeps track of data lineage while providing data quality checks. DLT provides granular lineage at the table level and provides unified monitoring and alerting for all the parts of an <span class="No-Break">ETL pipeline.</span><p class="list-inset">DLT is an advanced topic in itself. Going into a lot of detail about DLT is outside the scope of this book. We will provide a link to get started with DLT in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p><p class="list-inset">The following figure illustrates what capabilities are wrapped <span class="No-Break">inside DLT:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B17875_08_01.jpg" alt="Figure 8.1 – All the capabilities that DLT provides" width="1650" height="392"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – All the capabilities that DLT provides</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Delta Pipelines are for pure declarative ETL. You cannot make API calls or send an email with them. You should use Delta pipelines for ETL. For everything else, use Workflows with Jobs. We will cover Workflows with Jobs in the next section from the perspective of triggering automated model retraining at regular intervals and performing automated validations on updated models in the <span class="No-Break">Model Registry.</span></p>
			<ul>
				<li><strong class="bold">Workflows with Jobs</strong>: A Job is a way we can use to trigger the execution of Databricks<a id="_idIndexMarker550"/> notebooks, libraries, and more, either immediately or at a fixed schedule. We will be covering this in more detail in this chapter from the perspective of automating your <span class="No-Break">ML workflow.</span></li>
			</ul>
			<p>As with almost all Databricks<a id="_idIndexMarker551"/> features, you can create Jobs either through the UI, <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), or API. You can define one or more tasks as part of a Workflow with Jobs. A task can entail executing one of the <span class="No-Break">following options:</span></p>
			<ul>
				<li>A <strong class="bold">Databricks notebook</strong> that is either in a Git repository<a id="_idIndexMarker552"/> that’s accessible in your Databricks workspace or in a location in <span class="No-Break">your workspace</span></li>
				<li>A <strong class="bold">Python script</strong> loaded in cloud<a id="_idIndexMarker553"/> storage<a id="_idIndexMarker554"/> and available through the <strong class="bold">Databricks file </strong><span class="No-Break"><strong class="bold">system</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DBFS</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Java code compiled as a JAR file</strong>, which should be installed on<a id="_idIndexMarker555"/> the cluster for this option <span class="No-Break">to work</span></li>
				<li>A <span class="No-Break"><strong class="bold">DLT</strong></span><span class="No-Break"> pipeline</span></li>
				<li>A <strong class="bold">spark-submit</strong> command, which is a utility that allows the submission<a id="_idIndexMarker556"/> of Spark or PySpark application programs to the <span class="No-Break">underlying cluster</span></li>
				<li>A <span class="No-Break"><strong class="bold">Python wheel</strong></span></li>
			</ul>
			<p>You can chain multiple tasks together as part of a Job and repair and re-run a failed or canceled job. Databricks also provides support for monitoring the status of Jobs through the UI, CLI, API, and email notifications. Links will be provided in the <em class="italic">Further reading</em> section if you want to learn more about how to create and manage Workflows with Jobs using API or CLI. Jobs is a very versatile workflow management tool that can be used to develop and chain together tasks related to your ETL data pipeline or various steps in your <span class="No-Break">ML workflows.</span></p>
			<p>Let’s delve into how you can automate the retraining of your machine learning models at regular intervals using the <em class="italic">Workflows with Jobs</em> feature in Databricks. Workflows offer fine-grained access control, allowing owners and administrators to grant permissions to other users or groups<a id="_idIndexMarker557"/> for viewing workflow run results and managing the workflow runs themselves. Next, we’ll dive deeper into how to utilize Databricks Workflows with Jobs for automating both model training <span class="No-Break">and testing.</span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor126"/>Utilizing Databricks Workflows with Jobs to automate model training and testing</h1>
			<p>In this section, we’ll delve <a id="_idIndexMarker558"/>into the powerful synergy between Databricks<a id="_idIndexMarker559"/> Workflows and Jobs to automate the training and testing of machine learning models. Before we jump into hands-on examples, it’s essential to understand the significance of automation in the ML life cycle and how Databricks uniquely addresses <span class="No-Break">this challenge.</span></p>
			<p>Automating the training and testing phases in machine learning is not just a convenience but a necessity for scalable and efficient ML operations. Manual processes are not only time-consuming but also prone to errors, making automation a critical aspect of <span class="No-Break">modern MLOps.</span></p>
			<p>This is where Databricks Workflows comes in and allows for the orchestration<a id="_idIndexMarker560"/> of complex <span class="No-Break">ML pipelines.</span></p>
			<p>Let’s take a look into an example workflow that we will automate using Workflows with Jobs. We will be going through the following logical steps shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B17875_08_02.jpg" alt="Figure 8.2 – A sample workflow of automated testing and alerting on new model promotions" width="1569" height="408"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – A sample workflow of automated testing and alerting on new model promotions</p>
			<p>All the relevant code for this part is in the <strong class="source-inline">Chaper-08</strong> folder. Let’s take a look at how we can schedule a Databricks notebook as a Workflow <span class="No-Break">with Jobs:</span></p>
			<ol>
				<li>We first navigate to the <img src="image/Icon_1.png" alt="" width="93" height="31"/> tab in the left navigation bar. Here, we can click <span class="No-Break"><strong class="bold">Create Job</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17875_08_03.jpg" alt="Figure 8.3 – The contents of the Databricks Workflows tab" width="1334" height="263"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The contents of the Databricks Workflows tab</p>
			<ol>
				<li value="2">Here, we can provide a name<a id="_idIndexMarker561"/> to the task, and then select <strong class="bold">Notebook</strong> under <strong class="bold">Type</strong>. In <strong class="bold">Source</strong>, we have two options – <strong class="bold">Workspace</strong> and <span class="No-Break"><strong class="bold">Git provider</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Workspace</strong>: Using the file<a id="_idIndexMarker562"/> browser, you can navigate<a id="_idIndexMarker563"/> to the notebook in the workspace you want to schedule as <span class="No-Break">a task:</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17875_08_04.jpg" alt="Figure 8.4 – How to browse a notebook for scheduling as a Job through exploring the Workspace option" width="933" height="412"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – How to browse a notebook for scheduling as a Job through exploring the Workspace option</p>
			<p class="list-inset">You can simply navigate to the notebook<a id="_idIndexMarker564"/> in the <strong class="source-inline">Repos</strong> folder, as shown<a id="_idIndexMarker565"/> in the following screenshot, and <span class="No-Break">hit </span><span class="No-Break"><strong class="bold">Confirm</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17875_08_05.jpg" alt="Figure 8.5 – How to browse a notebook for scheduling as a Job through the Repos functionality" width="831" height="540"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – How to browse a notebook for scheduling as a Job through the Repos functionality</p>
			<p class="callout-heading">Note</p>
			<p class="callout">One important thing to keep in mind here is that when you use the Repos feature of Databricks, it creates a local copy for your repository or a clone. If you change code in the repository without performing a Git-pull of the latest version in your local repository, your updates will not make their way to the Job that you are scheduling with the current version of the <span class="No-Break">repository code.</span></p>
			<p class="callout">For production deployments, it’s important to make use of the Git provider as the source rather than the workspace as <span class="No-Break">the source.</span></p>
			<ul>
				<li><strong class="bold">Git provider</strong>: This method simplifies the creation<a id="_idIndexMarker566"/> and management<a id="_idIndexMarker567"/> of Jobs during productionizing<a id="_idIndexMarker568"/> and automated deployments. The main benefit here is that you can version control your data pipelines without managing permissions across multiple code repositories. You will also have a single source of truth for your model pipelines. Every time the Job executes, it will pull the latest version of the notebook/code from the remote repository with the specified branch or tag. Databricks supports the following <a id="_idIndexMarker569"/>Git providers: GitHub, Bitbucket<a id="_idIndexMarker570"/> Cloud, GitLab, Azure DevOps (excluding Azure China regions), AWS CodeCommit, and <span class="No-Break">GitHub AE.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">An important thing to keep in mind is that if you use the Git provider option as the source of one of your notebooks that will be scheduled as a task, you cannot mix and match it with tasks that have notebooks using a workspace as their Source as part of the same Job workflow. This limitation is only for using the <span class="No-Break">Databricks notebooks.</span></p>
			<p class="list-inset">To add the notebook from a Git provider, enter the details of the repository you want to access the notebook from. In our case, I will use my own Git repository for this book as <span class="No-Break">an example.</span></p>
			<p class="list-inset">For <strong class="bold">Path</strong>, a couple<a id="_idIndexMarker571"/> of things need to be kept <span class="No-Break">in mind:</span></p>
			<ul>
				<li>You need to enter a path relative to the <span class="No-Break">notebook location</span></li>
				<li>Don’t add a <strong class="source-inline">/</strong> or <strong class="source-inline">./</strong> character at the beginning of the <span class="No-Break">notebook path</span></li>
				<li>Don’t include the file extension, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">.py</strong></span></li>
				<li>For adding information about your Git repository, click on <strong class="bold">Add a git reference</strong>, which will open the following window pictured in <em class="italic">Figure 8.6</em> where you can select your <span class="No-Break">Git provider.</span></li>
				<li>You can select to execute notebooks from a particular Git branch/tag or commit. In my case, I will be using the <span class="No-Break">master branch:</span></li>
			</ul>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B17875_08_06.jpg" alt="Figure 8.6 – How to set up a Databricks notebook to execute as a Job from the repository" width="947" height="392"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – How to set up a Databricks notebook to execute as a Job from the repository</p>
			<p class="list-inset">When choosing the cluster, you can either utilize the cluster that is already up and running in your workspace or define a new Jobs cluster for all the tasks of the workflow. We have covered<a id="_idIndexMarker572"/> the difference between the cluster types in <a href="B17875_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Lastly, you can also pass the parameters to your tasks. In notebooks, parameters are passed <a id="_idIndexMarker573"/>as <span class="No-Break">notebook widgets.</span><p class="list-inset">If you have a use case where you need to set up a JAR, <strong class="source-inline">spark-submit</strong> command, Python file, or Python wheel as a task, you can define the input parameters as a JSON-formatted array of strings. For Python tasks, the passed parameters can be accessed using the <strong class="source-inline">argparse</strong> (<a href="https://docs.python.org/3/library/argparse.html">https://docs.python.org/3/library/argparse.html</a>) Python module. For Python wheel tasks, you also have the option to pass in keyword arguments as key/value pairs that you can then access using the <span class="No-Break"><strong class="source-inline">argparse</strong></span><span class="No-Break"> package.</span></p></li>
				<li>In the <strong class="bold">Advanced Options</strong> tab, you have optional settings to include <strong class="bold">Dependent Libraries, Retry Policy in case of an error</strong>, and <strong class="bold">Timeouts</strong>. You can additionally add users or groups of users to notify via email in case a task starts, succeeds, or fails. This setting is also available at the <span class="No-Break">workflow level.</span><p class="list-inset">For each workflow, you can also define the <strong class="bold">Maximum concurrent</strong> runs, which has a default value of <strong class="source-inline">1</strong>. This setting is important in cases where you may require to have overlapping execution of a particular Workflow with Jobs. A request to execute a Workflow with Jobs is skipped if the concurrent running instances of the workflow that is being requested to be executed have hit the maximum number of <span class="No-Break">concurrent runs.</span></p><p class="list-inset">In our case, we don’t have any dependent task on our notebook task, so we will simply hit <strong class="bold">Create</strong>. You can also add multiple interdependent tasks as part of a workflow by clicking <span class="No-Break">the <img src="image/icon_2.png" alt="" width="102" height="56"/>icon.</span></p></li>
			</ol>
			<p>Once we have successfully created a task to execute as part of our workflow, we can see information about our workflow in the <strong class="bold">Jobs</strong> tab of the <em class="italic">Workflows</em> section. Now, we have the option to schedule the running of our workflow at regular intervals automatically or interactively using the <strong class="bold">Run </strong><span class="No-Break"><strong class="bold">now</strong></span><span class="No-Break"> button.</span></p>
			<p>We can see a graph showing the success<a id="_idIndexMarker574"/> and failure of the past executions<a id="_idIndexMarker575"/> of the workflows along with the runtime and the details of our Workflow with Jobs on the right-hand side. Take note of <strong class="bold">Job ID</strong>, as this will be used to automatically trigger our model testing notebook using the webhooks integration with the <span class="No-Break">Model Registry:</span></p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17875_08_007.jpg" alt="Figure 8.7 – The summary page to monitor the Job run history as well as the unique Job ID, Git, Schedule, Compute, and Notifications settings" width="521" height="684"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – The summary page to monitor the Job run history as well as the unique Job ID, Git, Schedule, Compute, and Notifications settings</p>
			<p>Let’s take a look at the contents<a id="_idIndexMarker576"/> of the automated testing notebook in the <strong class="source-inline">Chapter-08</strong> folder that we <span class="No-Break">just</span><span class="No-Break"><a id="_idIndexMarker577"/></span><span class="No-Break"> scheduled:</span></p>
			<ul>
				<li><strong class="source-inline">Cmd 2</strong> is simply capturing the value of the <strong class="source-inline">event_message</strong> parameter that is sent by the Model Registry webhook. It contains information about the event that triggered the execution of this notebook workflow, such as <span class="No-Break">the following:</span><ul><li><strong class="source-inline">event_timestamp</strong>: Time when the <span class="No-Break">event occurred</span></li><li><strong class="source-inline">event</strong>: Name of the event, as described in the chapter <span class="No-Break">on webhooks</span></li><li><strong class="source-inline">text</strong>: Description of the purpose of the webhook that initiated the automated <span class="No-Break">test execution</span></li><li><strong class="source-inline">to_stage</strong>: Target stage for the model to be <span class="No-Break">transitioned to</span></li><li><strong class="source-inline">version</strong>: Model version whose transition triggered <span class="No-Break">this webhook</span></li><li><strong class="source-inline">from_stage</strong>: Initial stage of the model version in the <span class="No-Break">Model Registry</span></li></ul><p class="list-inset">Depending on what type of task we are scheduling, the payload of the webhooks changes. There will be a link in the <em class="italic">Further reading</em> section if you want to learn more. The following code snippet demonstrates the process of retrieving and parsing the payload <span class="No-Break">from webhooks:</span></p><pre class="source-code">
import jsonevent_message = dbutils.widgets.get("event_message")event_message_dict = json.loads(event_message)model_name = event_message_dict.get("model_name")print(event_message_dict)print(model_name)</pre></li>				<li><strong class="source-inline">Cmd 4</strong> is simply running some utility code to interact with the MLflow REST API. It is a good practice to write modularized<a id="_idIndexMarker578"/> code for writing unit tests<a id="_idIndexMarker579"/> for <span class="No-Break">your code:</span><pre class="source-code">
import mlflowfrom mlflow.utils.rest_utils import http_requestimport jsondef client():    return mlflow.tracking.client.MlflowClient()host_creds = client()._tracking_client.store.get_host_creds()host = host_creds.hosttoken = host_creds.tokendef mlflow_endpoint(endpoint, method, body='{}'):    if method == 'GET':        response = http_request(            host_creds=host_creds, endpoint="/api/2.0/mlflow/{}".format(endpoint), method=method, params=json.loads(body))    else:        response = http_request(            host_creds=host_creds, endpoint="/api/2.0/mlflow/{}".format(endpoint), method=method, json=json.loads(body))    return response.json()</pre></li>				<li>In <strong class="source-inline">Cmd 7</strong>, we are downloading a specific model version from the MLflow Model Registry for running <span class="No-Break">our tests:</span><pre class="source-code">
import mlflowpyfunc_model = mlflow.pyfunc.load_model(model_uri=f"models:/{model_name}/{version}")</pre></li>			</ul>
			<p>The rest of the code displays how you can write arbitrary tests to test your model before promoting it to the target stage in the Model Registry. In the sample code, we are testing whether the model being tested has the required schema for the inputs or not. We are also testing the output data type for <span class="No-Break">the response.</span></p>
			<p>At the end<a id="_idIndexMarker580"/> of successfully running the test, we send a message<a id="_idIndexMarker581"/> back to the Model Registry reporting whether all the tests passed or failed for the ML engineer <span class="No-Break">to review:</span></p>
			<pre class="console">
# Leave a comment for the ML engineer who will be reviewing the testscomment = "This model passed all the tests"
comment_body = {'name': model_name, 'version': version, 'comment': comment}
mlflow_endpoint('comments/create', 'POST', json.dumps(comment_body))</pre>
			<p>With this, we are now ready to register the <strong class="source-inline">automated-test</strong> workflow with our model <span class="No-Break">training notebook.</span></p>
			<p>Let’s take a look at the model training code. Open the <span class="No-Break"><strong class="source-inline">scheduling-workflow-for-model-retraining</strong></span><span class="No-Break"> notebook.</span></p>
			<p>This notebook has code to first register a Jobs webhook with the <strong class="source-inline">TRANSITION_REQUEST_TO_STAGING_CREATED</strong> event trigger for our <strong class="source-inline">Churn Prediction Bank</strong> model in the <span class="No-Break">Model Registry.</span></p>
			<p>Let’s look<a id="_idIndexMarker582"/> at the important cells in the notebook<a id="_idIndexMarker583"/> one <span class="No-Break">by one:</span></p>
			<ol>
				<li>In <strong class="source-inline">Cmd 2</strong>, we are simply installing a notebook-scoped <strong class="source-inline">databricks-registry-webhooks</strong> library from <strong class="bold">Python Package Index</strong> (<strong class="bold">PyPI</strong>). This is an alternate way to interact with the Databricks<a id="_idIndexMarker584"/> Model Registry webhooks other than using the Databricks REST API we covered in <a href="B17875_06.xhtml#_idTextAnchor100"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">.</span></li>
				<li>In <strong class="source-inline">Cmd 3</strong>, we are simply reading our original <strong class="source-inline">raw_data</strong> table from the <strong class="source-inline">bank_churn_analysis</strong> table while excluding the features that we will not use to train <span class="No-Break">our model.</span></li>
				<li><strong class="source-inline">Cmd 5</strong> is some utility code that is dynamically extracting the token and our current workspace URL. This code can be put into its own segregated function to make it easy <span class="No-Break">for testing.</span></li>
				<li>In <strong class="source-inline">Cmd 7</strong>, we are registering the Job workflow we created in <em class="italic">step 1</em> to be triggered by a webhook on the <strong class="source-inline">TRANSITION_REQUEST_TO_STAGING_CREATED</strong> event. In the code, replace <strong class="source-inline">&lt;jobid&gt;</strong> with the <strong class="source-inline">Job Id</strong> you noted down in <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">:</span><pre class="source-code">
from databricks_registry_webhooks import RegistryWebhooksClient, JobSpecjob_spec = JobSpec(  job_id="&lt;jobid&gt;",  workspace_url="https://"+instance,  access_token=token)job_webhook = RegistryWebhooksClient().create_webhook(  model_name=model_name,  events=["TRANSITION_REQUEST_TO_STAGING_CREATED"],  job_spec=job_spec,  description="Registering webhook to automate testing of a new candidate model for staging")job_webhook</pre></li>				<li>Next, we use the <strong class="source-inline">AutoML</strong> Python<a id="_idIndexMarker585"/> API to trigger a model retraining job, with our primary<a id="_idIndexMarker586"/> metric being the <span class="No-Break"><strong class="source-inline">F1</strong></span><span class="No-Break"> score:</span><pre class="source-code">
import databricks.automlmodel = databricks.automl.classify(    new_data.select(features),    target_col=target_column,    primary_metric="f1",    timeout_minutes=5,    max_trials=30,)</pre></li>				<li>Next, we simply use the <strong class="source-inline">MLflowClient</strong> class object to register the best-performing model into our <span class="No-Break">Model Registry:</span><pre class="source-code">
import mlflowfrom mlflow.tracking.client import MlflowClientclient = MlflowClient()run_id = model.best_trial.mlflow_run_idmodel_uri = f"runs:/{run_id}/model"model_details = mlflow.register_model(model_uri, model_name)</pre></li>				<li>We now import some utility code that is just a wrapper on top of the <strong class="source-inline">MLflow REST API</strong> using the <strong class="source-inline">%run</strong> magic command. This is how you can modularize your code for easy testing <span class="No-Break">and maintainability:</span><pre class="source-code">
%run ./mlflow-util</pre></li>				<li>In <strong class="source-inline">Cmd 17</strong>, we request<a id="_idIndexMarker587"/> transitioning the new model version to Staging. Since the new model needs to be tested<a id="_idIndexMarker588"/> first before we retire our old model, we are not going to archive the existing model version in Staging just yet. The following code block demonstrates <span class="No-Break">the same.</span><pre class="source-code">
staging_request = {'name': model_name, 'version': model_details.version, 'stage': 'Staging', 'archive_existing_versions': 'false'}mlflow_endpoint('transition-requests/create', 'POST', json.dumps(staging_request))</pre></li>				<li>Lastly, we are also going to add a comment for the ML engineer to say that the model is ready <span class="No-Break">for testing:</span><pre class="source-code">
comment = "This was the best model from the most recent AutoML run. Ready for testing"comment_body = {'name': model_name, 'version': model_details.version, 'comment': comment}mlflow_endpoint('comments/create', 'POST', json.dumps(comment_body))</pre><p class="list-inset">With this, we can see that in the <strong class="bold">Models</strong> tab for our <strong class="source-inline">Churn Prediction Bank</strong> model, there is a new version of a registered model with a <span class="No-Break">pending request.</span></p><p class="list-inset">We can get even more detail about this model by<a id="_idIndexMarker589"/> clicking the <strong class="bold">Model</strong> version. This will show<a id="_idIndexMarker590"/> us the request to transition the model and also the comment left after the <span class="No-Break">model training:</span></p></li>			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17875_08_08.jpg" alt="Figure 8.8 – The message that we created using API to request model transition to Staging" width="608" height="638"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – The message that we created using API to request model transition to Staging</p>
			<ol>
				<li value="10">On requesting<a id="_idIndexMarker591"/> the transition, we can see that the automated<a id="_idIndexMarker592"/> test is now executing on our new model version. We can see more details by <span class="No-Break">clicking </span><span class="No-Break"><strong class="source-inline">automated_test</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17875_08_09.jpg" alt="Figure 8.9 – The automated testing Job for any new model that gets a request to be transitioned to Staging" width="985" height="575"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The automated testing Job for any new model that gets a request to be transitioned to Staging</p>
			<p class="list-inset">The matrix view shows the current status of our test. We can see the actual output of <span class="No-Break">our test:</span></p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17875_08_10.jpg" alt="Figure 8.10 – The matrix view for our automated testing Job run" width="855" height="432"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – The matrix view for our automated testing Job run</p>
			<ol>
				<li value="11">On successful completion<a id="_idIndexMarker593"/> of the model<a id="_idIndexMarker594"/> testing, we can check the status on the <em class="italic">model </em><span class="No-Break"><em class="italic">version</em></span><span class="No-Break"> page:</span></li>
			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B17875_08_11.jpg" alt="Figure 8.11 – The successful posting of the message into the Model Registry after successful testing of the new model" width="849" height="245"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – The successful posting of the message into the Model Registry after successful testing of the new model</p>
			<ol>
				<li value="12">Now, the ML engineer or the admin can archive<a id="_idIndexMarker595"/> the old model and approve the request to transition<a id="_idIndexMarker596"/> this model to the <span class="No-Break">Staging environment.</span><p class="list-inset">In my case, that means transitioning the model version 2 <span class="No-Break">to </span><span class="No-Break"><strong class="bold">Archived</strong></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B17875_08_12.jpg" alt="Figure 8.12 – How to transition the existing model in Staging to Archived" width="1088" height="430"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – How to transition the existing model in Staging to Archived</p>
			<ol>
				<li value="13">We can add a comment that can keep track of why this model is being archived, which will also be logged in this model <span class="No-Break">version’s activity:</span></li>
			</ol>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B17875_08_13.jpg" alt="Figure 8.13 – How to add a message to the model being Archived action" width="933" height="167"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – How to add a message to the model being Archived action</p>
			<ol>
				<li value="14">Now I can approve<a id="_idIndexMarker597"/> the model version 3 transition to Staging and add a comment. We can approve the transition of the model to Staging and the retiring of the old model in Staging<a id="_idIndexMarker598"/> when we click on the <span class="No-Break"><strong class="bold">Approve</strong></span><span class="No-Break"> button:</span></li>
			</ol>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B17875_08_14.jpg" alt="Figure 8.14 – How to approve transitioning of the new model to Staging" width="931" height="266"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – How to approve transitioning of the new model to Staging</p>
			<p class="list-inset">This is useful in cases where you have only one version of a model in Staging at a given time. The explicit retiring can be useful if you want to have simultaneous candidate models in a stage for A/B testing before selecting the <span class="No-Break">best model.</span></p>
			<p class="list-inset">So, now we have executed the end-to-end workflow where we trained a new model and also triggered automated testing before promoting the model <span class="No-Break">to Staging.</span></p>
			<p class="list-inset">The last thing to do here is to schedule the monthly retraining of <span class="No-Break">our model.</span></p>
			<ol>
				<li value="15">Go back to the <strong class="source-inline">scheduling-workflow-for-model-retraining</strong> notebook and open it. On the top right of every Databricks notebook, you have a button called <strong class="bold">Schedule</strong>. On clicking that, you can specify how often you want to execute this notebook and what type of cluster to execute it on. You can also add parameters for the notebook and <span class="No-Break">set alerts:</span></li>
			</ol>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B17875_08_15.jpg" alt="Figure 8.15 – How to set up our model for automated retraining" width="940" height="447"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – How to set up our model for automated retraining</p>
			<p>As we’ve explored the intricacies of automating machine learning workflows using Databricks Jobs, you should now have<a id="_idIndexMarker599"/> a solid understanding<a id="_idIndexMarker600"/> of how to set up and manage your automated ML retraining workflows. Next, we’ll summarize the key takeaways in the <em class="italic">Summary</em> section to help you consolidate <span class="No-Break">your knowledge.</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor127"/>Summary</h1>
			<p>In this chapter, we discussed the workflow management options available in the Databricks environment. We also looked at Workflows with Jobs functionality in more detail in relation to its utility in automating your <span class="No-Break">ML workflows.</span></p>
			<p>We went through a sample workflow of creating a notebook with tests to perform on any new model we want to transition to the Staging stage of the Model Registry. We then configured the Model Registry Jobs webhooks feature to be triggered by another automated model retraining notebook. Similar workflows can make your model tests arbitrarily complex to fit <span class="No-Break">your needs.</span></p>
			<p>In the last chapter, we will cover the concept of model drift and how to trigger a model’s <span class="No-Break">retraining automatically.</span></p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor128"/>Further reading</h1>
			<p>Here are some links to further <span class="No-Break">your understanding:</span></p>
			<ul>
				<li>Databricks, <em class="italic">What Is Delta Live </em><span class="No-Break"><em class="italic">Tables?</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/delta-live-tables/index.html"><span class="No-Break">https://docs.databricks.com/en/delta-live-tables/index.html</span></a></li>
				<li>Databricks, <em class="italic">Introduction to Databricks </em><span class="No-Break"><em class="italic">Workflows</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/workflows/index.html"><span class="No-Break">https://docs.databricks.com/en/workflows/index.html</span></a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>