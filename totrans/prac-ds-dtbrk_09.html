<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer140">
			<h1 id="_idParaDest-129" class="chapter-number"><a id="_idTextAnchor129"/>9</h1>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Model Drift Detection and Retraining</h1>
			<p>In the last chapter, we covered various workflow management options available in Databricks for automating <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) tasks. Now, we will expand upon our understanding<a id="_idIndexMarker601"/> of the ML life cycle up to now and introduce the fundamental concept of <strong class="bold">drift</strong>. We will discuss why model monitoring<a id="_idIndexMarker602"/> is essential and how you can ensure your ML models perform as expected <span class="No-Break">over time.</span></p>
			<p>At the time of writing this book, Databricks has a product that is in development that will simplify monitoring model performance and data out of the box. In this chapter, we will go through an example of how to use the existing Databricks functionalities to implement drift detection <span class="No-Break">and monitoring.</span></p>
			<p>We will be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Motivation for <span class="No-Break">model monitoring</span></li>
				<li>Introduction to <span class="No-Break">model drift</span></li>
				<li>Introduction to <span class="No-Break">Statistical Drift</span></li>
				<li>Techniques for <span class="No-Break">drift detection</span></li>
				<li>Implementing drift detection <span class="No-Break">on Databricks</span></li>
			</ul>
			<p>Let’s go through the technical requirements for <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Technical requirements</h1>
			<p>The following are the prerequisites for <span class="No-Break">the chapter:</span></p>
			<ul>
				<li>Access to a <span class="No-Break">Databricks workspace</span></li>
				<li>A running cluster with Databricks Runtime for Machine Learning (Databricks Runtime ML) with a version higher <span class="No-Break">than 10.3</span></li>
				<li>Notebooks from <a href="B17875_09.xhtml#_idTextAnchor129"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> imported into the <span class="No-Break">Databricks workspace</span></li>
				<li>Introductory knowledge of hypothesis testing and interpreting <span class="No-Break">statistical tests</span></li>
			</ul>
			<p>Let’s take a look at the motivation behind why model monitoring <span class="No-Break">is important.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>The motivation behind model monitoring</h1>
			<p>According to an article in Forbes magazine by Enrique Dans, July 21, 2019, 87% of data science projects never make it to <span class="No-Break">production (</span><a href="https://www.forbes.com/sites/enriquedans/2019/07/21/stop-experimenting-with-machine-learning-and-start-actually-usingit/?sh=1004ff0c3365"><span class="No-Break">https://www.forbes.com/sites/enriquedans/2019/07/21/stop-experimenting-with-machine-learning-and-start-actually-usingit/?sh=1004ff0c3365</span></a><span class="No-Break">).</span></p>
			<p>There are a lot of reasons<a id="_idIndexMarker603"/> why ML models fail; however, if we look purely at the reason for ML project failure in a production environment, it comes down to a lack of re-training and testing the deployed models for performance consistency <span class="No-Break">over time.</span></p>
			<p>The performance of the model keeps degrading over time. Many data scientists neglect the maintenance aspect of the models post-production. The following visualizations offer a comparative analysis between two distinct approaches to model management—one where the model is trained once and then deployed for an extended period and another where the model undergoes regular retraining with fresh data while being monitored for <span class="No-Break">performance drift:</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B17875_09_01.jpg" alt="Figure 9.1 – A comparison of model quality for a static model versus a regularly trained model" width="1650" height="715"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – A comparison of model quality for a static model versus a regularly trained model</p>
			<p class="callout-heading">Source</p>
			<p class="callout">Courtesy <span class="No-Break">of Databricks</span></p>
			<p>There are a lot of statistical tests that can help us monitor changes in our model’s performance over time, and since the field of MLOps is still in the early stages of maturity, ML practitioners struggle with how and which tests to incorporate in their ML <span class="No-Break">productionizing process.</span></p>
			<p>In this chapter, we will dive deeper into what statistical tests we recommend using to monitor ML models in the production environment. Several Python libraries can be seamlessly integrated into the Databricks environment for this purpose. Among these are open source options such as whylogs, Evidently, and Seldon Alibi Detect, which offer various functionalities ranging from data drift tracking to full-scale model health assessments. Although the primary focus of this chapter will be on leveraging statistical tests for model monitoring within Databricks, you are encouraged to explore these libraries to augment your monitoring toolkit. Databricks’ flexibility allows you to easily incorporate these libraries into your workflows if you wish to extend beyond <span class="No-Break">statistical approaches.</span></p>
			<p>We will also go over how to use everything we have learned up to now to implement model monitoring<a id="_idIndexMarker604"/> on Databricks using open <span class="No-Break">source tools.</span></p>
			<p>The example we’ll explore focuses on batch scoring using a tabular dataset. However, the underlying principles are equally applicable to streaming or real-time inference involving image or <span class="No-Break">text data.</span></p>
			<p>Organizations differ in their automation preferences, particularly when it comes to retraining models in response to detected performance drift. In this section, our primary objective is to examine various model monitoring strategies. Depending on your specific use case, you can then determine the most appropriate next steps based on <span class="No-Break">these insights.</span></p>
			<p>To better contextualize where model monitoring fits in, let’s examine the typical life cycle<a id="_idIndexMarker605"/> of an <span class="No-Break">ML project.</span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B17875_09_02.jpg" alt="Figure 9.2 – Showing the life cycle of an ML project" width="1650" height="885"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Showing the life cycle of an ML project</p>
			<p class="callout-heading">Source</p>
			<p class="callout">Courtesy <span class="No-Break">of Databricks</span></p>
			<p>The first step in any ML project<a id="_idIndexMarker606"/> is the business coming up with a business problem that they believe can benefit from the application of ML. As part of this process, the various stakeholders and the data scientists come up with the online and <span class="No-Break">offline metrics.</span></p>
			<p>Offline metrics are the ones that can be calculated when ML models are being trained, such as accuracy and F1 score. Online metrics are the ones that <span class="No-Break">are business-driven.</span></p>
			<p>After the metrics and success criteria are decided, the data scientists and ML engineers work with the data engineers to understand all the data sources that are available to them and which could be useful for the current business problem. In the preceding figure, this is shown as the <strong class="bold">Data </strong><span class="No-Break"><strong class="bold">Collection</strong></span><span class="No-Break"> phase.</span></p>
			<p>Once data collection is done, the data scientists proceed to perform the feature engineering and train various ML models, and after evaluating the model, they productionize the <span class="No-Break">best model.</span></p>
			<p>This productionizing of the model is where the model deployment and monitoring come in. The entire process, from collecting data to training and evaluating models and productionizing the best model, is iterative <span class="No-Break">in nature.</span></p>
			<p>Operationalizing the entire process previously explained is what we call MLOps. All the ML deployments look different and depend on the domain and the problem we are trying <span class="No-Break">to solve.</span></p>
			<p>When you are designing your own ML solution on Databricks, spend considerable time determining how often you will train and deploy a new model and what kind of performance monitoring and action will make sense for your <span class="No-Break">use case.</span></p>
			<p>So far, we have discovered <a id="_idIndexMarker607"/>that degrading model performance over time is one of the main causes of failed ML projects in organizations. Now, let’s take a look at some of the reasons why model performance might degrade <span class="No-Break">over time.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor133"/>Introduction to model drift</h1>
			<p>ML models can experience<a id="_idIndexMarker608"/> a decline in performance over time, which is a common issue in projects. The main reasons for this are changes in the input data that is fed into the model. These changes can occur due to various reasons, such as the underlying distribution of the data changing, an alteration in the relationship between the dependent and independent features, or changes in the source system that generates the <span class="No-Break">data itself.</span></p>
			<p>The performance degradation of deployed models over time is called Model Drift. To effectively identify instances of Model Drift, various metrics can be monitored: </p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: A declining <a id="_idIndexMarker609"/>trend in accuracy can serve as a strong indicator of model drift. </li>
				<li><strong class="bold">Precision and Recall</strong>: A noticeable decrease in these values may highlight the model's diminishing ability to make accurate and relevant predictions. </li>
				<li><strong class="bold">F1 Score</strong>: This is a harmonized metric that encapsulates both precision and recall. A drop in the F1 Score suggests that the model's overall efficacy is compromised. </li>
				<li><strong class="bold">Business Metrics</strong>: Beyond technical <a id="_idIndexMarker610"/>indicators, key business metrics like conversion rate, churn rate, and customer lifetime value can also reveal model drift by showing the model's declining impact on <span class="No-Break">business objectives.</span></li>
			</ul>
			<p>There are many categories of model drift, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Feature drift</strong>: Feature drift refers to changes<a id="_idIndexMarker611"/> in the distribution of input features or predictors<a id="_idIndexMarker612"/> used by an ML model. These changes can occur due to various reasons, such as changes in data collection processes, measurement errors, or shifts in the characteristics of the <span class="No-Break">data source:</span><ul><li>For example, if a model is trained on data collected from a specific location and time period and then deployed to a different location or time period where the feature distributions are different, the model may experience feature drift. This can result in the degradation of model performance, as the model may not be able<a id="_idIndexMarker613"/> to accurately generalize to the new <span class="No-Break">feature</span><span class="No-Break"><a id="_idIndexMarker614"/></span><span class="No-Break"> distribution.</span></li></ul></li>
				<li><strong class="bold">Label drift</strong>: Label drift refers to changes in the distribution<a id="_idIndexMarker615"/> of output labels or target <a id="_idIndexMarker616"/>variables used for training an ML model. These changes can occur due to shifts in the underlying data generation process, changes in data collection methods, or changes in the definitions <span class="No-Break">of labels:</span><ul><li>For example, if a model is trained to <a id="_idIndexMarker617"/>predict customer churn using historical data and the definition of churn changes over time, the model may experience label drift. This can result in a misalignment between the model’s predictions and the ground truth labels, leading to decreased <span class="No-Break">model performance.</span></li></ul></li>
				<li><strong class="bold">Prediction drift</strong>: Prediction drift refers to changes in the distribution<a id="_idIndexMarker618"/> of model predictions<a id="_idIndexMarker619"/> over time. This can occur due to changes in the underlying data distribution, changes in the model’s parameters, or changes in the <span class="No-Break">model’s architecture:</span><ul><li>For example, if a model is trained to predict stock prices and deployed in a dynamic financial market, the model’s predictions may drift over time as the market conditions change. Prediction drift can impact the reliability and accuracy of the model’s predictions, leading to potential business or <span class="No-Break">operational implications.</span></li></ul></li>
				<li><strong class="bold">Concept drift</strong>: Concept drift refers to changes in the underlying<a id="_idIndexMarker620"/> concepts or relationships between<a id="_idIndexMarker621"/> variables in the data distribution over time. This can occur due to changes in the data source, changes in the data generation process, or changes in the underlying phenomenon <span class="No-Break">being modeled:</span><ul><li>For example, if a model is <a id="_idIndexMarker622"/>trained to predict credit risk based on historical credit data and there are changes in the economic or regulatory environment, the model may experience concept drift. Concept drift can result in a misalignment between the model and the real-world phenomenon, leading<a id="_idIndexMarker623"/> to decreased <span class="No-Break">model</span><span class="No-Break"><a id="_idIndexMarker624"/></span><span class="No-Break"> performance.</span></li></ul></li>
			</ul>
			<p>We can summarize this information on the various types of drift and how to mitigate them <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B17875_09_03.jpg" alt="Figure 9.3 – Summarizing the various types of drift and the actions we can perform to mitigate them" width="669" height="396"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Summarizing the various types of drift and the actions we can perform to mitigate them</p>
			<p>Feature drift, label drift, and concept drift<a id="_idIndexMarker625"/> collectively fall into the category of <span class="No-Break"><strong class="bold">data drift</strong></span><span class="No-Break">.</span></p>
			<p>Understanding and addressing drift in ML models is crucial to ensure their continued performance and reliability in real-world applications. Feature drift, label drift, prediction drift, and concept drift are important types of drift that can occur in ML models. Detecting and mitigating<a id="_idIndexMarker626"/> drift requires the use of appropriate statistical tests or methods to identify and quantify the changes in data distribution <span class="No-Break">over time.</span></p>
			<p>In the next section, we'll explore another critical factor that can contribute to the deterioration of model performance over <span class="No-Break">time—Statistical Drift.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor134"/>Introduction to Statistical Drift </h1>
			<p>Statistical drift refers to changes in the underlying data distribution itself. It can affect both the input features and the target variable. This drift may or may not affect the model's performance but understanding it is crucial for broader data landscape awareness. </p>
			<p>To effectively identify instances of Statistical Drift, various metrics can be monitored: </p>
			<ul>
				<li><strong class="bold">Mean and Standard Deviation</strong>: Significant changes can indicate drift. </li>
				<li><strong class="bold">Kurtosis and Skewness</strong>: Changes signal data distribution alterations. </li>
				<li><strong class="bold">Quantile Statistics</strong>: Look at changes in 25th, 50th, and 75th percentiles for example. </li>
			</ul>
			<p>To fully grasp how Model Drift and Statistical Drift are interconnected, consider the following key points: </p>
			<ul>
				<li><strong class="bold">Cause and Effect Relationship</strong>: Statistical drift in either the features or the target variable frequently serves as a precursor to model drift. For example, should the age demographic of your customer base shift (indicative of statistical drift), a model designed to predict customer behavior could begin to falter in its performance (resulting in model drift). </li>
				<li><strong class="bold">Simultaneous Occurrence</strong>: Both forms of drift can occur concurrently. Take, for instance, an e-commerce model that experiences model drift due to seasonal variations, while also undergoing statistical drift owing to changes in customer demographics. </li>
				<li><strong class="bold">Diverse Monitoring Requirements</strong>: Each type of drift necessitates its own unique set of monitoring strategies. Model drift is commonly identified through an examination of prediction errors, whereas statistical drift is usually detected by observing shifts in the data distribution. </li>
				<li><strong class="bold">Distinct Corrective Measures</strong>: Addressing model drift typically involves retraining the model or making fine-tuned adjustments. On the other hand, statistical drift may call for more comprehensive changes in data processing protocols or adjustments in feature engineering. </li>
			</ul>
			<p>To make an informed decision on which drift detection method to employ, it's essential to weigh the pros and cons of each approach. Your choice will hinge on the specific requirements of your project and the nuances of your business domain. The table below offers a concise summary, highlighting the advantages and challenges of using Model Drift Detection Methods versus Statistical Drift Detection Methods. </p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Aspect</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Model Drift </strong><span class="No-Break"><strong class="bold">Detection Methods</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Statistical Drift </strong><span class="No-Break"><strong class="bold">Detection Methods</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Pros</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Ease of detection through performance metrics; Allows for model updates or recalibration </p>
						</td>
						<td class="No-Table-Style">
							<p>Provides broader understanding of data landscape, <span class="No-Break">Not model-specific</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Cons</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Requires continual monitoring; Can be resource-intensive </p>
						</td>
						<td class="No-Table-Style">
							<p>Difficult to quantify; May require sophisticated tests; Less obvious indicators </p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 – Summarizing the pros and cons of Model Drift Detection Methods versus Statistical Drift Detection Methods. </p>
			<p>In the next section, we will discuss the various techniques we can utilize to monitor drift in our features and model performance <span class="No-Break">over time.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/>Techniques for drift detection</h1>
			<p>To ensure effective monitoring<a id="_idIndexMarker627"/> of our model’s performance over time, we should track changes in summary statistics and distributions of both the model features and target variables. This will enable us to detect any potential data drift <span class="No-Break">early on.</span></p>
			<p>Furthermore, it’s important to monitor offline model metrics such as accuracy and F1 scores that were utilized during the initial training of <span class="No-Break">the model.</span></p>
			<p>Lastly, we should also keep an eye on online metrics or business metrics to ensure that our model remains relevant to the specific business problem we are trying <span class="No-Break">to solve.</span></p>
			<p>The following table provides an overview of various statistical tests and methods that can be employed to identify drift in your data and models. Please note that this compilation is <span class="No-Break">not all-encompassing.</span></p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Data Type </strong><span class="No-Break"><strong class="bold">to Monitor</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Sub-Category</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Statistical Measures </strong><span class="No-Break"><strong class="bold">and Tests</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Numeric Features</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Summary Statistics</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Mean</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Median</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Minimum</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Maximum</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- Missing <span class="No-Break">value count</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">Statistical Tests</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- Kolmogorov-Smirnov (<span class="No-Break">KS) test</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Levene test</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Wasserstein distance</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Categorical Features</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Summary Statistics</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Mode</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- Unique <span class="No-Break">level count</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- Missing <span class="No-Break">value count</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>Statistical Tests </p>
						</td>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Chi-Squared Test</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Target-Feature Relation</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Numeric Target</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Pearson Coefficient</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">Categorical Target</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Contingency Tables</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Model Performance </strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Regression Models</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- Mean Square <span class="No-Break">Error (MSE)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- Error <span class="No-Break">distribution plots</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">Classification Models</span></p>
						</td>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Confusion Matrix</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>- <span class="No-Break">Accuracy</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Table 9.2 – Monitoring table for detecting various types of drift</p>
			<p>Going into detail for each and every statistical<a id="_idIndexMarker628"/> test is out of the scope of this book. We will be using open source libraries to perform these tests and detect drift; however, it is still beneficial to learn the high-level steps involved in performing some of the tests we will use in the sample notebooks accompanying <span class="No-Break">this chapter.</span></p>
			<p>Let’s first understand the basics of hypothesis testing. All statistical tests use the hypothesis <span class="No-Break">testing framework.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor136"/>Hypothesis testing</h2>
			<p>When it comes to drawing conclusions<a id="_idIndexMarker629"/> about population characteristics based on sample data, hypothesis tests are invaluable. This statistical technique is designed to examine whether meaningful discrepancies exist between distinct sets of data. Let’s take a look at the core steps involved in <span class="No-Break">hypothesis testing.</span></p>
			<h3>Core steps in hypothesis testing</h3>
			<p>There are four core steps<a id="_idIndexMarker630"/> in <span class="No-Break">hypothesis testing:</span></p>
			<ol>
				<li><strong class="bold">Formulate </strong><span class="No-Break"><strong class="bold">the hypotheses</strong></span><span class="No-Break">:</span><ol><li class="upper-roman"><strong class="bold">Null Hypothesis H0</strong>: This proposes that no noteworthy differences exist between <span class="No-Break">the datasets.</span></li><li class="upper-roman"><strong class="bold">Alternate Hypothesis Ha</strong>: This claims that a significant divergence is present between <span class="No-Break">the datasets.</span></li></ol></li>
				<li><strong class="bold">Establish the level of significance</strong>: Often symbolized by <em class="italic">α</em>, this value signifies the likelihood of making an erroneous decision by rejecting the null hypothesis when it’s actually true. An <em class="italic">α</em> value of 0.05 is commonly chosen, which corresponds to a 5% risk of a Type <span class="No-Break">I error.</span></li>
				<li><strong class="bold">Compute the test statistic</strong>: This numerical measure is derived from both the sample data and the null hypothesis. It serves as the basis for determining the validity of the <span class="No-Break">null hypothesis.</span></li>
				<li><strong class="bold">Ascertain the p-value</strong>: The p-value quantifies the odds of achieving the computed test statistic—or a more extreme result if the null hypothesis holds. This metric guides the decision to either uphold or reject the <span class="No-Break">null hypothesis.</span></li>
				<li><strong class="bold">Making the </strong><span class="No-Break"><strong class="bold">final decision</strong></span><ol><li class="upper-roman">When the p−value&lt;<em class="italic"> α</em>, the null hypothesis<a id="_idIndexMarker631"/> is rejected. The data provides ample evidence to support the claim made by the <span class="No-Break">alternate hypothesis.</span></li><li class="upper-roman">When the p−value&lt;<em class="italic"> α</em>, the null hypothesis is not rejected. The evidence to back the alternate hypothesis <span class="No-Break">is insufficient.</span></li></ol></li>
			</ol>
			<p>So, now we understand the basics of hypothesis testing. Let’s go over the steps for some statistical tests that are utilized in the notebook accompanying this chapter to demonstrate <span class="No-Break">drift detection.</span></p>
			<p>To clearly visualize the outcomes of each numerical test we'll discuss, the accompanying figures will be based on a synthetic dataset. This dataset contains two distinct yet comparable sets of data points, labeled as 'Group 1' and 'Group 2': </p>
			<h3>Group 1</h3>
			<p>This ensemble includes 1,000 entries, generated using a Gaussian distribution with a zero mean and a unitary standard deviation. Essentially, it's a prototypical bell-shaped curve centered at zero. </p>
			<p><span class="No-Break">Group 2</span></p>
			<p>This assembly, too, comprises 1,000 entries. However, these are pulled from a Gaussian distribution with a mean value of one and a unit standard deviation. The distribution closely resembles that of Group 1, albeit shifted one unit to the right on the x-axis. </p>
			<h3>Defining Characteristics</h3>
			<ul>
				<li><strong class="bold">Sample Size</strong>: Each of the groups, Group 1 and Group 2, contains an equal number of entries, allowing for a balanced comparison. </li>
				<li><strong class="bold">Distribution Nature</strong>: Both groups follow a Gaussian distribution, although the central values are distinct. </li>
				<li><strong class="bold">Uniform Variance</strong>: The variances for both groups are closely matched, making them appropriate for any tests that hinge on equal <span class="No-Break">variance assumptions.</span></li>
			</ul>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Statistical tests and measurements for numeric features</h2>
			<p>In the following section, we delve<a id="_idIndexMarker632"/> into an array of statistical techniques that are crucial for understanding and analyzing numeric features in your dataset. We’ll explore various tests and metrics that can help you assess the quality, distribution, and relationships of these features, thereby aiding in effective model building <span class="No-Break">and monitoring.</span></p>
			<h3>Kolmogorov-Smirnov tests</h3>
			<p>The <strong class="bold">Kolmogorov-Smirnov</strong> (<strong class="bold">KS</strong>) two-sample assessment is a non-parametric method of evaluating<a id="_idIndexMarker633"/> whether two separate sets<a id="_idIndexMarker634"/> of data originate from identical distributions or if their distributions differ in a statistically meaningful way. This test focuses on comparing the <strong class="bold">empirical cumulative distribution functions</strong> (<strong class="bold">ECDFs</strong>) of the two datasets. To perform a KS test for detecting drift in our numeric<a id="_idIndexMarker635"/> features, we take the <span class="No-Break">following steps:</span></p>
			<ol>
				<li><span class="No-Break">Formulate hypotheses:</span><ul><li><strong class="bold">Null hypothesis (H0)</strong>: This hypothesis posits that both datasets come from the <span class="No-Break">same distribution.</span></li><li><strong class="bold">Alternative hypothesis (H1)</strong>: This hypothesis argues that the two datasets have statistically <span class="No-Break">different distributions.</span></li></ul></li>
				<li>Choose a significance level (alpha). This is the cut-off point for deciding whether to reject the null hypothesis. Typical alpha values are 0.05 <span class="No-Break">or 0.01.</span></li>
				<li>Calculate the test statistics and determine the critical value or p-value from the KS test’s critical <span class="No-Break">values table.</span></li>
				<li>Make <span class="No-Break">a decision:</span><ol><li class="upper-roman">Compare the p-value obtained in <em class="italic">step 3</em> with the chosen significance <span class="No-Break">level (alpha).</span></li><li class="upper-roman">If the p-value is less than alpha, reject the null hypothesis and conclude that the variances of the groups or samples are <span class="No-Break">not equal.</span></li></ol></li>
			</ol>
			<p>If the p-value is larger<a id="_idIndexMarker636"/> than alpha, retain the null<a id="_idIndexMarker637"/> hypothesis. This suggests that there’s insufficient evidence to say the distributions <span class="No-Break">are different.</span></p>
			<p>The following graph depicts the <strong class="bold">Empirical Cumulative Distribution Functions</strong> (<strong class="bold">ECDFs</strong>) for Group 1 and Group 2. The red dashed line indicates the maximum difference between the two ECDFs, quantifying the <strong class="bold">Kolmogorov-Smirnov</strong> (<strong class="bold">KS</strong>) statistic. The KS statistic and p-value are presented, offering a statistical measure of the drift between the <span class="No-Break">two groups.</span></p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B17875_09_04.jpg" alt="Figure 9.4 – Kolmogorov-Smirnov test visualization using the described dummy data for 'Group 1' and 'Group 2" width="619" height="345"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Kolmogorov-Smirnov test visualization using the described dummy data for 'Group 1' and 'Group 2</p>
			<p>The KS statistic is 0.4310 and the p-value is nearly 0.0, indicating that the difference between the two groups is statistically significant. In terms of "drift," this suggests that there is indeed a notable change or drift in the distribution between 'Group 1' and 'Group 2.' This is particularly relevant when monitoring models over time for changes in the data they <span class="No-Break">are processing.</span></p>
			<p>It is important to note that, depending<a id="_idIndexMarker638"/> on the use case, we may need to use Bonferroni correction<a id="_idIndexMarker639"/> to reduce <span class="No-Break">false positives.</span></p>
			<h3>Levene t-test</h3>
			<p>Levene’s test is a statistical test<a id="_idIndexMarker640"/> used to assess<a id="_idIndexMarker641"/> the equality of variances in two or more groups or samples. It is a parametric test that can be used when the assumption<a id="_idIndexMarker642"/> of equal variances (homoscedasticity) required by some other tests, such as the t-test or <strong class="bold">analysis of variance</strong> (<strong class="bold">ANOVA</strong>), may not be met. Levene’s test can be performed using the <span class="No-Break">following steps:</span></p>
			<ol>
				<li> Formulate <span class="No-Break">the hypotheses:</span><ul><li><strong class="bold">Null hypothesis (H0)</strong>: This means the variances of the groups or samples <span class="No-Break">are equal.</span></li><li><strong class="bold">Alternative hypothesis (H1)</strong>: This means the variances of the groups or samples are <span class="No-Break">not equal.</span></li></ul></li>
				<li>Choose a significance <span class="No-Break">level (alpha).</span></li>
				<li>Calculate the test statistics and compare them to <span class="No-Break">the p-value.</span></li>
				<li>Make <span class="No-Break">a decision:</span><ol><li class="upper-roman">Compare the p-value from the third step to the predetermined <span class="No-Break">alpha level.</span></li><li class="upper-roman">If the p-value is smaller than the alpha level, reject the null hypothesis. This indicates that the variances across the groups or samples are not <span class="No-Break">the same.</span></li><li class="upper-roman">If the p-value is larger than the alpha level, retain the null hypothesis. This suggests that there’s insufficient evidence to claim the variances <span class="No-Break">are different.</span></li></ol></li>
			</ol>
			<p>The box plots display the data <a id="_idIndexMarker643"/>distributions for 'Group 1' and 'Group 2.' Variances for each group are annotated, and <a id="_idIndexMarker644"/>the W statistic and p-value are presented to assess the statistical significance of <span class="No-Break">variance differences.</span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B17875_09_05.jpg" alt="Figure 9.5 – The W statistic and p-value from Levene’s test" width="820" height="476"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – The W statistic and p-value from Levene’s test</p>
			<p>The W statistic is low, <a id="_idIndexMarker645"/>and the p-value is high, suggesting <a id="_idIndexMarker646"/>that the variances between the two groups are not <span class="No-Break">significantly different.</span></p>
			<h3>Wasserstein distance</h3>
			<p>Also referred to as the earth mover’s distance, the Wasserstein<a id="_idIndexMarker647"/> metric serves as a gauge for comparing<a id="_idIndexMarker648"/> the likeness or differences between two statistical distributions. It calculates the effort needed to morph one distribution into another, where these distributions can be depicted as histograms or discrete sets of probability values. When it comes to identifying data drift, this metric can be employed to assess how much two distributions diverge from each other, either over time or across <span class="No-Break">varying conditions.</span></p>
			<p>When comparing two distributions using the Wasserstein distance, a higher Wasserstein distance value indicates greater dissimilarity or discrepancy between the distributions, while a lower Wasserstein distance value indicates greater similarity or agreement between the distributions. In the context of data drift detection, an increase in the Wasserstein distance over time or between different environments can indicate the presence of data drift, which refers to changes in the underlying data-generating process <span class="No-Break">or distribution.</span></p>
			<p>In other words, if the Wasserstein distance between two distributions increases significantly over time or between different environments, it suggests that the distributions have diverged and the data may have drifted from the original distribution. This can be an indication of changes in data characteristics, the data-generating process, or the data source. This may prompt further investigation and monitoring to ensure data quality and model performance. The threshold for what constitutes a significant increase in the Wasserstein<a id="_idIndexMarker649"/> distance as an indicator<a id="_idIndexMarker650"/> of data drift may depend on the specific problem or application. It requires careful consideration and <span class="No-Break">domain knowledge.</span></p>
			<p>The histograms in the following represent the distributions of 'Group 1' and 'Group 2.' The purple dashed lines visually signify the Wasserstein distance, quantifying the amount of 'work' required to transform one distribution into <span class="No-Break">the other.</span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B17875_09_06.jpg" alt="Figure 9.6 – visualization to represent the Wasserstein distance between 'Group 1' and 'Group 2" width="569" height="283"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – visualization to represent the Wasserstein distance between 'Group 1' and 'Group 2</p>
			<p>The Wasserstein distance provides a measure of how much the two distributions differ in terms of their location and shape. In this case, the distance value and the spread of the purple lines across the distributions suggest that there is some level of discrepancy between the two groups, but not an <span class="No-Break">extreme one..</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>Statistical tests and measurements for categorical features</h2>
			<p>In this section, we are going<a id="_idIndexMarker651"/> to briefly cover one of the most popular tests: the chi-squared test. As mentioned before, all the statistical tests follow the framework of hypothesis testing described in the <span class="No-Break">earlier section.</span></p>
			<h3>Chi-squared test</h3>
			<p>The chi-squared test serves as a statistical method<a id="_idIndexMarker652"/> for assessing the presence of a meaningful relationship<a id="_idIndexMarker653"/> between two categorical variables, typically represented in a contingency table. The general procedure involves the <span class="No-Break">following steps:</span></p>
			<ol>
				<li> Establish <span class="No-Break">the hypotheses:</span><ul><li><strong class="bold">Null Hypothesis (H0)</strong>: This suggests that no significant relationship exists between the two <span class="No-Break">categorical variables.</span></li><li><strong class="bold">Alternative Hypothesis (Ha)</strong>: This indicates that a meaningful relationship does exist between the two <span class="No-Break">categorical variables.</span></li></ul></li>
				<li><strong class="bold">Make </strong><span class="No-Break"><strong class="bold">the decision</strong></span><span class="No-Break">:</span><p class="list-inset">If the chi-squared statistics have a p-value less than the chosen significance level (alpha), we reject the null hypothesis. Otherwise, we accept the null hypothesis. </p></li>
			</ol>
			<p>To offer a clear understanding of the Chi-squared test, we will utilize illustrative visuals based on a synthetic dataset. This dataset comprises two distinct categories, termed 'Group 1' and '<span class="No-Break">Group 2':</span></p>
			<h3><strong class="bold">Group </strong>1</h3>
			<p>This collection consists of 1,000 entries. Specifically, 572 entries fall under category 'A' and 428 entries under <span class="No-Break">category 'B'.</span></p>
			<h3><strong class="bold">Group 2</strong></h3>
			<p>This set also includes 1,000 entries, with 447 entries in category 'A' and 553 entries in <span class="No-Break">category 'B'.</span></p>
			<h3>Key Attributes</h3>
			<ul>
				<li><strong class="bold">Sample Size</strong>: Both Group 1 and <a id="_idIndexMarker654"/>Group 2 have an equal number of entries (N=1,000), enabling a fair <span class="No-Break">comparative analysis.</span></li>
				<li><strong class="bold">Category Distribution</strong>: Each group contains two categories—'A' and 'B'. However, the distribution of entries across these categories varies between <span class="No-Break">the groups.</span></li>
				<li><strong class="bold">Uniform Sample Size</strong>: Since both groups have 1,000 entries, they are suitable for statistical tests like the Chi-squared test that benefit from <span class="No-Break">balanced datasets.</span></li>
			</ul>
			<p>The following bar chart showcases the frequencies of categories 'A' and 'B' for 'Group 1' and 'Group 2'. The annotated statistics in the corner provide the calculated Chi-squared statistic, p-value, and degrees of freedom, serving as quantitative measures for the <span class="No-Break">visual comparisons.</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B17875_09_07.jpg" alt="Figure 9.7 – visualization to represent the Chi-squared test results between 'Group 1' and 'Group 2" width="566" height="317"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – visualization to represent the Chi-squared test results between 'Group 1' and 'Group 2</p>
			<p>The extremely low p-value and the high Chi-squared statistic indicate a statistically significant difference between the category distributions of 'Group 1' and '<span class="No-Break">Group 2'.</span></p>
			<p>For easy reference here's a table detailing various statistical methods, their hypotheses formulation, decision criteria, and example use-cases <span class="No-Break">with metrics.</span></p>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Method</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Formulate </strong><span class="No-Break"><strong class="bold">Hypotheses</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Decision </strong><span class="No-Break"><strong class="bold">Criteria</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Example </strong><span class="No-Break"><strong class="bold">Metrics</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Feature </strong><span class="No-Break"><strong class="bold">Type</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">T-Test</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Null: Sample means identical | Alt: Sample <span class="No-Break">means differ</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Reject null if p-value is less than <span class="No-Break">significance level</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Average user session durations: 5 min vs 8 min </p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Numeric</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Chi-Squared</span></p>
							<p><span class="No-Break">Test</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Null: No relation between categories | Alt: <span class="No-Break">Categories related</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Reject null if p-value is less than significance level </p>
						</td>
						<td class="No-Table-Style">
							<p>Engagement levels across various ad channels </p>
						</td>
						<td class="No-Table-Style">
							<p>Categorical </p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">ANOVA</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Null: Group averages same | Alt: At least one group <span class="No-Break">average varies</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Reject null if p-value is less than significance level </p>
						</td>
						<td class="No-Table-Style">
							<p>Performance metrics of distinct sales units </p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Numeric</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Kolmogorov-Smirnov <span class="No-Break">Test</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Null: Data distributions match | Alt: Data distributions vary  </p>
						</td>
						<td class="No-Table-Style">
							<p>Reject null if D exceeds critical value  </p>
						</td>
						<td class="No-Table-Style">
							<p>Customer age distributions across regions  </p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Numeric</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Levene’s Test</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Null: Equal group variances | Alt: Variances differ between groups  </p>
						</td>
						<td class="No-Table-Style">
							<p>Reject null if p-value is less than <span class="No-Break">significance level</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Score variance comparison <span class="No-Break">between genders</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Numeric</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Wasserstein Distance </p>
						</td>
						<td class="No-Table-Style">
							<p> Null: Distributions are equal | Alt: Distributions differ </p>
						</td>
						<td class="No-Table-Style">
							<p>Consider domain-specific threshold for Wasserstein distance </p>
						</td>
						<td class="No-Table-Style">
							<p>Distributional shift in customer incomes over time  </p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Numeric</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.3 – A guide to statistical methods, their hypotheses, decision criteria, example use-cases, and applicable feature types.</p>
			<p>Now we have covered at a high level statistical <a id="_idIndexMarker655"/>tests involved for <a id="_idIndexMarker656"/>detecting drift for individual features. Let’s discuss what monitoring tests we can implement on <span class="No-Break">our models.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>Statistical tests and measurements on models</h2>
			<p>When it comes to monitoring model performance<a id="_idIndexMarker657"/> altogether, we can have additional monitoring in place for <span class="No-Break">the following:</span></p>
			<ul>
				<li>You can monitor the relationship between the target and <span class="No-Break">independent features:</span><ul><li><strong class="bold">Numeric target</strong>: For calculating and monitoring <span class="No-Break">Pearson coefficients</span></li><li><strong class="bold">Categorical target</strong>: For calculating and monitoring <span class="No-Break">contingency tables</span></li></ul></li>
				<li><strong class="bold">Model performance</strong>: Here, we can monitor the model’s offline metrics <span class="No-Break">over time:</span><ul><li><strong class="bold">Regression models</strong>: These comprise <strong class="bold">Mean square error</strong> (<strong class="bold">MSE</strong>), residual plots, <strong class="bold">root-mean-squared Error</strong> (<strong class="bold">RMSE</strong>), and <span class="No-Break">so on</span></li><li><strong class="bold">Classification models</strong>: These comprise accuracy, F1 scores, <strong class="bold">receiving operating characteristics</strong> (<strong class="bold">ROC</strong>) curves, and <span class="No-Break">so on</span></li><li>Tracking performance on <span class="No-Break">holdout datasets</span></li></ul></li>
				<li><strong class="bold">Time taken to train</strong>: If the time taken to train the model over time increases drastically, we may need to <span class="No-Break">investigate that</span></li>
			</ul>
			<p>Let’s take a look at an end-to-end example of detecting drift on the Databricks platform. We will be working with a synthetic dataset generated to simulate various types <span class="No-Break">of drift.</span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor140"/>Implementing drift detection on Databricks</h1>
			<p>The necessary files for this chapter<a id="_idIndexMarker658"/> are located within the <strong class="source-inline">Chapter-09</strong> folder. This example demonstrates how you can arrange your code into specific modules to keep <span class="No-Break">it organized.</span></p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B17875_09_08.jpg" alt="Figure 9.8 – A screenshot showing the layout of the files in our code base" width="544" height="210"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – A screenshot showing the layout of the files in our code base</p>
			<p>The setup notebook<a id="_idIndexMarker659"/> in the <strong class="source-inline">config</strong> folder is designed to establish the folder structure for data reading and writing. It also sets up the MLflow experiment for tracking model performance over time and manages other variables that will be utilized in our <span class="No-Break"><strong class="source-inline">model</strong></span><span class="No-Break">-</span><span class="No-Break"><strong class="source-inline">drift</strong></span><span class="No-Break"> notebook.</span></p>
			<p>The <strong class="source-inline">datagen</strong> notebook within the data folder serves the purpose of creating a synthetic dataset that effectively demonstrates the concept of model drift. This dataset encompasses time series data of online sales for an e-commerce website spanning <span class="No-Break">three months.</span></p>
			<p>In this dataset, we have a set of independent features and a target feature, along with simulated relationships between them. The independent features include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">Temperature</strong><strong class="bold"> (Numeric)</strong>: This captures the highest daily temperature <span class="No-Break">in Fahrenheit</span></li>
				<li><strong class="source-inline">Weather_Condition</strong><strong class="bold"> (Categorical)</strong>: This variable can have values such as <strong class="source-inline">'sunny'</strong>, <strong class="source-inline">'cloudy'</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">'rainy'</strong></span></li>
				<li><strong class="source-inline">Promotion_Type</strong><strong class="bold"> (Categorical)</strong>: This can be classified into categories such as <strong class="source-inline">'discount'</strong>, <strong class="source-inline">'free_gift'</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">'bundle_deal'</strong></span></li>
				<li><strong class="source-inline">Website_Traffic</strong><strong class="bold"> (Numeric)</strong>: This represents the total number of visits to <span class="No-Break">the website</span></li>
				<li><strong class="source-inline">Device_Type</strong><strong class="bold"> (Categorical)</strong>: This indicates the type of device used to access <span class="No-Break">the website</span></li>
			</ul>
			<p>The target feature is <strong class="source-inline">Daily_Sales</strong> (numeric), which represents the total sales revenue for <span class="No-Break">each day.</span></p>
			<p>In terms<a id="_idIndexMarker660"/> of relationships, the target variable, <strong class="source-inline">Daily_Sales</strong>, has specific correlations with these features. For instance, it shows a <strong class="bold">positive correlation</strong> with both <strong class="source-inline">Temperature</strong> and <strong class="source-inline">Website_Traffic</strong>. On<a id="_idIndexMarker661"/> the other hand, it has a <strong class="bold">negative correlation</strong> with <strong class="source-inline">Weather_Condition</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">Device_Type</strong></span><span class="No-Break">.</span></p>
			<p>Our approach involves training the model using the data from the initial month and subsequently simulating diverse drift patterns in the ensuing months’ data. This process allows us to effectively explore the impact of changing data distributions and patterns on the <span class="No-Break">model’s performance.</span></p>
			<p>Lastly, there are two additional notebooks present inside the <span class="No-Break"><strong class="source-inline">util</strong></span><span class="No-Break"> folder.</span></p>
			<p>The <strong class="source-inline">monitoring</strong> notebook contains a set of monitoring utility functions designed to ensure the quality and consistency of new incoming data compared to production data in a data-driven environment. These functions cover various aspects, including checking null value proportions, identifying significant differences in summary statistics, detecting variations in variances and distributions, evaluating categorical differences, and comparing model performances. The utility functions assist in maintaining data integrity, identifying potential discrepancies, and providing recommendations for transitioning models to production. Additionally, there’s a function to plot boxplots for visualizing distribution differences between incoming and <span class="No-Break">production data.</span></p>
			<p>To perform various statistical tests<a id="_idIndexMarker662"/> on data, we are going to use the <strong class="source-inline">scipy.stats</strong> package. The <strong class="source-inline">scipy.stats</strong> package is a fundamental component of the SciPy library, which is widely used in scientific and statistical computing in Python. It provides a comprehensive collection of statistical distributions, functions, and methods for performing various statistical calculations, probability density estimations, and hypothesis testing. With <strong class="source-inline">scipy.stats</strong>, you can easily work with continuous and discrete probability distributions, generate random samples, compute statistical measures, and perform tests to <span class="No-Break">analyze data.</span></p>
			<p>This package encompasses a wide range of statistical techniques, such as calculating probabilities, quantiles, and moments and performing goodness-of-fit tests. The distributions available in <strong class="source-inline">scipy.stats</strong> include common ones such as normal, exponential, uniform, and many more, as well as less common and specialized distributions used in various <span class="No-Break">research fields.</span></p>
			<p>In addition to statistical distributions, <strong class="source-inline">scipy.stats</strong> offers functions for hypothesis testing, correlation analysis, linear<a id="_idIndexMarker663"/> regression, and non-parametric tests. You can read more about it on the official <span class="No-Break">website (</span><a href="https://docs.scipy.org/doc/scipy/reference/stats.html"><span class="No-Break">https://docs.scipy.org/doc/scipy/reference/stats.html</span></a><span class="No-Break">).</span></p>
			<p>The <strong class="source-inline">training</strong> notebook<a id="_idIndexMarker664"/> focuses on training and managing ML models using scikit-learn within the MLflow environment. It includes utility functions for MLflow Model Registry interactions, Delta table-handling, and a detailed ML workflow. The workflow encompasses data loading, preprocessing, training a <strong class="source-inline">RandomForestRegressor</strong> model, evaluating performance, and logging relevant metrics and artifacts to MLflow for model tracking. The script’s purpose is to streamline the end-to-end process of training, evaluating, and tracking ML models in a <span class="No-Break">production setting.</span></p>
			<p>With all the supporting notebooks and high-level code explanations out of the way, let’s go through the main driver notebook called <strong class="source-inline">model-drift</strong>, which calls all the other notebooks. You can go over the supporting notebooks in your <span class="No-Break">own time.</span></p>
			<p>Let’s jump right into the <span class="No-Break"><strong class="source-inline">model-drift</strong></span><span class="No-Break"> notebook:</span></p>
			<ol>
				<li>The initial cells are just calling the other supporting notebooks to have the required libraries, directory structure, raw dataset, and MLflow experiment ready for <span class="No-Break">our example:</span><pre class="source-code">
%run ./config/setup%run ./util/training%run ./util/monitoring%run ./data/datagen</pre></li>				<li>Next, we use the Databricks notebook widgets to allow us to parametrize our notebook to set certain thresholds for our hypothesis tests and model <span class="No-Break">performance metrics:</span><pre class="source-code">
# Remove all existing widgetsdbutils.widgets.removeAll() # Create three widgets for the stats threshold limit, p-threshold, and min model R2 thresholddbutils.widgets.text("stats_threshold_limit", "0.5")dbutils.widgets.text("p_threshold", "0.05")dbutils.widgets.text("min_model_r2_threshold", "0.005")# Get the values of the widgets# stats_threshold_limit: how much we should allow basic summary stats to shiftstats_threshold_limit = float(dbutils.widgets.get("stats_threshold_limit"))# p_threshold: the p-value below which to reject null hypothesisp_threshold = float(dbutils.widgets.get("p_threshold"))# min_model_r2_threshold: minimum model improvementmin_model_r2_threshold = float(dbutils.widgets.get("min_model_r2_threshold"))</pre></li>				<li>Ingest the first month’s data<a id="_idIndexMarker665"/> from the raw synthetic dataset stored as a CSV file into a <span class="No-Break">Delta table:</span><pre class="source-code">
# Ensure we start with no existing Delta tabledbutils.fs.rm(months_gold_path, True) # Incoming Month 1 Dataraw_data = spark.read.csv(raw_good_data_path, header=True, inferSchema=True)# Filter the DataFrame to only include data for January 2023raw_data_month1 = raw_data.filter(raw_data["Date"].between("2023-01-01", "2023-01-31"))import pyspark.sql.functions as F# Create inital version of the Gold Delta table we will use for training - this will be updated with subsequent "months" of dataraw_data_month1.withColumn("month", F.lit("month_1")).write.format("delta").mode("overwrite").partitionBy("month").save(months_gold_path)</pre></li>				<li>Train the baseline model as an MLflow run<a id="_idIndexMarker666"/> using our custom method, <strong class="source-inline">train_sklearn_rf_model</strong>. This run will be available to view under <span class="No-Break">our experiment:</span><pre class="source-code">
# read gold data for month 1 from the Delta tablemonth1_gold_delta_table = DeltaTable.forPath(spark, path=months_gold_path)month1_gold_df = month1_gold_delta_table.toDF()# Set the month number - used for naming the MLflow run and tracked as a parameter month = 1# Specify name of MLflow runrun_name = f"month_{month}"target_col = "Daily_Sales"cat_cols = [col[0] for col in month1_gold_df.dtypes if col[1]=="string" and col[0]!='month']num_cols= [col[0] for col in month1_gold_df.dtypes if ((col[1]=="int" or col[1]=="double") and col[0]!="Daily_Sales") ]print(f"category columns : {cat_cols}")print(f"numeric columns : {num_cols}")print(f"target column : {target_col}")# Define the parameters to pass in the RandomForestRegressor modelmodel_params = {"n_estimators": 500,               "max_depth": 5,               "max_features": "log2"}# Define a dictionary of parameters that we would like to use during preprocessingmisc_params = {"month": month,              "target_col": target_col,             "cat_cols": cat_cols,           "num_cols": num_cols}# Trigger model training and logging to MLflowmonth1_run = train_sklearn_rf_model(run_name,                        months_gold_path,  model_params, misc_params)month_1_run_id = month1_run.info.run_id</pre></li>				<li>Register the baseline model<a id="_idIndexMarker667"/> we trained on the first month’s data in the model registry and change its state <span class="No-Break">to production:</span><pre class="source-code">
# Register model to MLflow Model Registrymonth_1_model_version = mlflow.register_model(model_uri=f"runs:/{month_1_run_id}/model", name=mlflow_experiment_name)# Transition model to Productionmonth_1_model_version = transition_model(month_1_model_version, stage="Production")print(month_1_model_version)</pre></li>				<li>Let’s delve into the run that has been generated as a result of training the baseline model. Locate<a id="_idIndexMarker668"/> the flask icon situated on the right-hand side of the notebook and proceed to click on it to access the run. It’s important to note that the name of the run might vary given that it is <span class="No-Break">randomly generated.</span></li>
			</ol>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B17875_09_09.jpg" alt="Figure 9.9 – A screenshot showing run tracking the baseline model training using the first month’s data" width="318" height="602"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – A screenshot showing run tracking the baseline model training using the first month’s data</p>
			<p class="list-inset">In addition to the metrics, our custom<a id="_idIndexMarker669"/> model training method contains code that logs the summary statistics of the dataset that we used for training <span class="No-Break">the model.</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B17875_09_10.jpg" alt="Figure 9.10 – A screenshot showing the logged summary statistics for the training dataset" width="1008" height="461"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – A screenshot showing the logged summary statistics for the training dataset</p>
			<ol>
				<li value="7">Along with this, we also logged<a id="_idIndexMarker670"/> the exact version of the Delta table that we used to train the model. This will provide us with reproducibility and lineage in the future if we need to analyze how this model was trained and what features were used to <span class="No-Break">train it.</span></li>
			</ol>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B17875_09_11.jpg" alt="Figure 9.11 – The logging of the Delta table version and the size of the training and test sets used for this run" width="1057" height="604"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – The logging of the Delta table version and the size of the training and test sets used for this run</p>
			<p class="list-inset">In the rest of the notebook, we simply use this baseline to compare <span class="No-Break">model performance.</span></p>
			<ol>
				<li value="8">For the second month’s data, we simulate upstream data errors by introducing missing values for <strong class="source-inline">website_traffic</strong> for certain promotion types and changing the measurement<a id="_idIndexMarker671"/> of the temperature from Fahrenheit to Celsius. By performing the test that checks the null proportion for all numeric columns, we are able to capture that <strong class="source-inline">web_traffic</strong> has an unusual amount of <span class="No-Break">missing values:</span><pre class="source-code">
print("\nCHECKING PROPORTION OF NULLS.....")check_null_proportion(month_2_pdf, null_proportion_threshold=.5)</pre></li>			</ol>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B17875_09_12.jpg" alt="Figure 9.12 – A screenshot showing the results of the null value check on the new data" width="869" height="117"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – A screenshot showing the results of the null value check on the new data</p>
			<ol>
				<li value="9">To detect drift in the data, we use the <strong class="source-inline">calculate_summary_stats</strong> method defined in our monitoring<a id="_idIndexMarker672"/> notebook to calculate summary statistics on the second month’s data. Then, we use the other utility methods, such as <strong class="source-inline">load_summary_stats_pdf_from_run</strong>, to read the summary statistics from our base run to compare the second month’s <span class="No-Break">data with:</span><pre class="source-code">
# Incoming Month 2 Dataraw_data_month2 = spark.read.csv(raw_month2_bad_data_path, header=True, inferSchema=True)# Filter the DataFrame to only include data for Feb 2023raw_data_month2 = raw_data_month2.filter(raw_data_month2["Date"].between("2023-02-01", "2023-02-28"))# Print the filtered DataFrameraw_data_month2.show(5)# Compute summary statistics on new incoming data# we will keep only the columns that we monitored for the last mode training data# convert to pandas dataframe should be used with care as if the size of data is larger than what can fit on driver node then this can cause failures.# In the case of data size being large use proper sampling technique to estimate population summary statistics.month_2_pdf = raw_data_month2.toPandas().drop(['Date'], axis=1)summary_stats_month_2_pdf = calculate_summary_stats(month_2_pdf)summary_stats_month_2_pdf# Get the original MLflow run associated with the model registered under Productioncurrent_prod_run = get_run_from_registered_model(mlflow_experiment_name, stage="Production")# Load in original versions of Delta table used at training time for current Production modelcurrent_prod_pdf = load_delta_table_from_run(current_prod_run).toPandas()# Load summary statistics pandas DataFrame for data which the model currently in Production was trained and evaluated againstcurrent_prod_stats_pdf = load_summary_stats_pdf_from_run(current_prod_run, project_local_tmp_dir)print("\nCHECKING PROPORTION OF NULLS.....")check_null_proportion(month_2_pdf, null_proportion_threshold=.5)statistic_list = ["mean", "median", "std", "min", "max"]unique_feature_diff_array_month_2 = check_diff_in_summary_stats(summary_stats_month_2_pdf,                                                                 current_prod_stats_pdf,                                                                 num_cols + [target_col],                                                                stats_threshold_limit,                                                                statistic_list)unique_feature_diff_array_month_2</pre><p class="list-inset">Our summary statistics<a id="_idIndexMarker673"/> comparison tests are able to capture the drift in the data and highlight that the temperature has changed drastically in the <span class="No-Break">new data.</span></p></li>			</ol>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B17875_09_13.jpg" alt="Figure 9.13 – Screenshot showcasing the output generated by the statistical tests comparing the summary statistics between the new month’s data and the baseline data" width="1014" height="204"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – Screenshot showcasing the output generated by the statistical tests comparing the summary statistics between the new month’s data and the baseline data</p>
			<ol>
				<li value="10">Moving forward, we employ the Levine test to assess variations in variance coupled with a corrected Bonferroni test to ascertain <span class="No-Break">statistical significance:</span><pre class="source-code">
print("\nCHECKING VARIANCES WITH LEVENE TEST.....")check_diff_in_variances(current_prod_pdf, month_2_pdf, num_cols, p_threshold)print("\nCHECKING KS TEST.....")check_dist_ks_bonferroni_test(current_prod_pdf, month_2_pdf, num_cols + [target_col], p_threshold)</pre></li>				<li>As the next step in the production stage, you may want to collaborate with the upstream data provider team to understand the root cause of the temperature values have changed so drastically in comparison to our <span class="No-Break">base dataset.</span></li>
			</ol>
			<p>The presented tests serve as illustrative instances, underscoring the seamless integration of drift detection code within your ML workflow on Databricks. It’s worth noting that these tests can be effortlessly triggered automatically, mirroring the demonstration in <a href="B17875_08.xhtml#_idTextAnchor122"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Automating ML Workflows Using </em><span class="No-Break"><em class="italic">Databricks Jobs</em></span><span class="No-Break">.</span></p>
			<p>As you progress through the remainder of the notebook, you’ll encounter additional examples that illuminate the method of tracking model metrics over time to detect deterioration. Furthermore, you’ll gain insights into the programmatic management of model promotion up to the production stage or other stages, a process contingent on the outcomes derived<a id="_idIndexMarker674"/> from <span class="No-Break">your tests.</span></p>
			<p>Let’s summarize what we learned in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor141"/>Summary</h1>
			<p>In this chapter, we extensively explored the significance of monitoring both models and data, emphasizing the crucial role of drift detection. Our understanding deepened as we delved into the spectrum of statistical tests at our disposal, which are adept at identifying diverse forms of drift encompassing numerical and <span class="No-Break">categorical features.</span></p>
			<p>Moreover, we engaged in a comprehensive walk-through, exemplifying the application of these concepts. Through a simulated model drift scenario using a synthetic e-commerce dataset, we harnessed the power of various statistical tests from the <strong class="source-inline">scipy.stats</strong> package to accurately pinpoint instances <span class="No-Break">of drift.</span></p>
			<p>As we venture into the next chapter, our focus will pivot toward elucidating the organization within the Databricks workspace and delving into the realm of <strong class="bold">continuous integration/continuous </strong><span class="No-Break"><strong class="bold">deployment</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CI/CD</strong></span><span class="No-Break">).</span></p>
		</div>
	</div>
</div>
</body></html>