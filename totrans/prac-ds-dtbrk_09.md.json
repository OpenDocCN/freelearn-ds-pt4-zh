["```py\n    %run ./config/setup%run ./util/training%run ./util/monitoring%run ./data/datagen\n    ```", "```py\n    # Remove all existing widgetsdbutils.widgets.removeAll() # Create three widgets for the stats threshold limit, p-threshold, and min model R2 thresholddbutils.widgets.text(\"stats_threshold_limit\", \"0.5\")dbutils.widgets.text(\"p_threshold\", \"0.05\")dbutils.widgets.text(\"min_model_r2_threshold\", \"0.005\")# Get the values of the widgets# stats_threshold_limit: how much we should allow basic summary stats to shiftstats_threshold_limit = float(dbutils.widgets.get(\"stats_threshold_limit\"))# p_threshold: the p-value below which to reject null hypothesisp_threshold = float(dbutils.widgets.get(\"p_threshold\"))# min_model_r2_threshold: minimum model improvementmin_model_r2_threshold = float(dbutils.widgets.get(\"min_model_r2_threshold\"))\n    ```", "```py\n    # Ensure we start with no existing Delta tabledbutils.fs.rm(months_gold_path, True) # Incoming Month 1 Dataraw_data = spark.read.csv(raw_good_data_path, header=True, inferSchema=True)# Filter the DataFrame to only include data for January 2023raw_data_month1 = raw_data.filter(raw_data[\"Date\"].between(\"2023-01-01\", \"2023-01-31\"))import pyspark.sql.functions as F# Create inital version of the Gold Delta table we will use for training - this will be updated with subsequent \"months\" of dataraw_data_month1.withColumn(\"month\", F.lit(\"month_1\")).write.format(\"delta\").mode(\"overwrite\").partitionBy(\"month\").save(months_gold_path)\n    ```", "```py\n    # read gold data for month 1 from the Delta tablemonth1_gold_delta_table = DeltaTable.forPath(spark, path=months_gold_path)month1_gold_df = month1_gold_delta_table.toDF()# Set the month number - used for naming the MLflow run and tracked as a parameter month = 1# Specify name of MLflow runrun_name = f\"month_{month}\"target_col = \"Daily_Sales\"cat_cols = [col[0] for col in month1_gold_df.dtypes if col[1]==\"string\" and col[0]!='month']num_cols= [col[0] for col in month1_gold_df.dtypes if ((col[1]==\"int\" or col[1]==\"double\") and col[0]!=\"Daily_Sales\") ]print(f\"category columns : {cat_cols}\")print(f\"numeric columns : {num_cols}\")print(f\"target column : {target_col}\")# Define the parameters to pass in the RandomForestRegressor modelmodel_params = {\"n_estimators\": 500,               \"max_depth\": 5,               \"max_features\": \"log2\"}# Define a dictionary of parameters that we would like to use during preprocessingmisc_params = {\"month\": month,              \"target_col\": target_col,             \"cat_cols\": cat_cols,           \"num_cols\": num_cols}# Trigger model training and logging to MLflowmonth1_run = train_sklearn_rf_model(run_name,                        months_gold_path,  model_params, misc_params)month_1_run_id = month1_run.info.run_id\n    ```", "```py\n    # Register model to MLflow Model Registrymonth_1_model_version = mlflow.register_model(model_uri=f\"runs:/{month_1_run_id}/model\", name=mlflow_experiment_name)# Transition model to Productionmonth_1_model_version = transition_model(month_1_model_version, stage=\"Production\")print(month_1_model_version)\n    ```", "```py\n    print(\"\\nCHECKING PROPORTION OF NULLS.....\")check_null_proportion(month_2_pdf, null_proportion_threshold=.5)\n    ```", "```py\n    # Incoming Month 2 Dataraw_data_month2 = spark.read.csv(raw_month2_bad_data_path, header=True, inferSchema=True)# Filter the DataFrame to only include data for Feb 2023raw_data_month2 = raw_data_month2.filter(raw_data_month2[\"Date\"].between(\"2023-02-01\", \"2023-02-28\"))# Print the filtered DataFrameraw_data_month2.show(5)# Compute summary statistics on new incoming data# we will keep only the columns that we monitored for the last mode training data# convert to pandas dataframe should be used with care as if the size of data is larger than what can fit on driver node then this can cause failures.# In the case of data size being large use proper sampling technique to estimate population summary statistics.month_2_pdf = raw_data_month2.toPandas().drop(['Date'], axis=1)summary_stats_month_2_pdf = calculate_summary_stats(month_2_pdf)summary_stats_month_2_pdf# Get the original MLflow run associated with the model registered under Productioncurrent_prod_run = get_run_from_registered_model(mlflow_experiment_name, stage=\"Production\")# Load in original versions of Delta table used at training time for current Production modelcurrent_prod_pdf = load_delta_table_from_run(current_prod_run).toPandas()# Load summary statistics pandas DataFrame for data which the model currently in Production was trained and evaluated againstcurrent_prod_stats_pdf = load_summary_stats_pdf_from_run(current_prod_run, project_local_tmp_dir)print(\"\\nCHECKING PROPORTION OF NULLS.....\")check_null_proportion(month_2_pdf, null_proportion_threshold=.5)statistic_list = [\"mean\", \"median\", \"std\", \"min\", \"max\"]unique_feature_diff_array_month_2 = check_diff_in_summary_stats(summary_stats_month_2_pdf,                                                                 current_prod_stats_pdf,                                                                 num_cols + [target_col],                                                                stats_threshold_limit,                                                                statistic_list)unique_feature_diff_array_month_2\n    ```", "```py\n    print(\"\\nCHECKING VARIANCES WITH LEVENE TEST.....\")check_diff_in_variances(current_prod_pdf, month_2_pdf, num_cols, p_threshold)print(\"\\nCHECKING KS TEST.....\")check_dist_ks_bonferroni_test(current_prod_pdf, month_2_pdf, num_cols + [target_col], p_threshold)\n    ```"]