<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer153">
			<h1 id="_idParaDest-142" class="chapter-number"><a id="_idTextAnchor142"/>10</h1>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor143"/>Using CI/CD to Automate Model Retraining and Redeployment</h1>
			<p>Having explored various statistical tests in <a href="B17875_09.xhtml#_idTextAnchor129"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, courtesy of diverse open source libraries on Databricks and their integration with MLflow, we will now focus on an integral component of MLOps on Databricks. In this chapter, we will look at how Databricks unifies DevOps, DataOps, and ModelOps all in a <span class="No-Break">single platform.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introduction <span class="No-Break">to MLOps</span></li>
				<li>Fundamentals of MLOps and <span class="No-Break">deployment patterns</span></li>
			</ul>
			<p>Let’s understand what <span class="No-Break">MLOps is.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor144"/>Introduction to MLOps</h1>
			<p>MLOps serves as a multidisciplinary approach<a id="_idIndexMarker675"/> that merges the principles of DevOps, ModelOps, and DataOps to facilitate the end-to-end life cycle of ML projects. It aims to streamline the transition from model development to deployment, while also ensuring effective monitoring and management. In this framework, we have <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">DevOps</strong>: This focuses on the continuous integration<a id="_idIndexMarker676"/> and deployment of code, aiming for quicker releases and more <span class="No-Break">reliable software</span></li>
				<li><strong class="bold">ModelOps</strong>: This specializes in managing ML<a id="_idIndexMarker677"/> models, ensuring they are effectively trained, validated, <span class="No-Break">and deployed</span></li>
				<li><strong class="bold">DataOps</strong>: This deals with data management<a id="_idIndexMarker678"/> practices, encompassing everything from data collection and preprocessing to storage <span class="No-Break">and analytics</span></li>
			</ul>
			<p>MLOps improves<a id="_idIndexMarker679"/> the performance, stability, and long-term efficiency of ML systems. There are two primary risks that MLOps can help mitigate for your use case <span class="No-Break">and industry:</span></p>
			<ul>
				<li><strong class="bold">Technical risks</strong>: These result from poorly managed models<a id="_idIndexMarker680"/> that are not performing as expected. Without MLOps implementing your infrastructure and pipeline to train new models and redeploy them in the production environment, it may be <span class="No-Break">very fragile.</span></li>
				<li><strong class="bold">Compliance risks</strong>: If you are part of a regulated industry<a id="_idIndexMarker681"/> and must keep track of the new regulations and compliances to ensure you are not violating them, MLOps can help <span class="No-Break">mitigate them.</span></li>
			</ul>
			<p>Through automation, MLOps can also reduce and catch errors before getting to the production environment and reduce the time to market to launch and maintain products reliant on the most updated models for your <span class="No-Break">use case.</span></p>
			<p>Now, let’s look at Databricks as a platform, which allows you to reduce the risks outlined previously and helps improve the long-term efficiency of your teams and your <span class="No-Break">ML projects.</span></p>
			<p>One unique part of Databricks is that it is a <em class="italic">data-centric AI platform</em>. As part of this AI platform, Databricks<a id="_idIndexMarker682"/> uniquely provides all the necessary components needed to manage the data, models, and code that are part of the <span class="No-Break">ML projects.</span></p>
			<p>To elucidate how Databricks facilitates MLOps, the following figure illustrates the platform’s integration capabilities with various tools and services on the Databricks <span class="No-Break">Lakehouse platform:</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B17875_10_01.jpg" alt="Figure 10.1 – Databricks’ data-centric platform and its components" width="1599" height="677"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Databricks’ data-centric platform and its components</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Courtesy <span class="No-Break">of Databricks.</span></p>
			<p>Next, we’ll explore how Delta Lake serves as a pivotal technology that bridges the gap between robust data storage and <span class="No-Break">ML readiness.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>Delta Lake – more than just a data lake</h2>
			<p>When it comes to managing<a id="_idIndexMarker683"/> complex data ecosystems, Databricks offers Delta Lake, a comprehensive open source storage layer that we discussed briefly in <a href="B17875_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. For more specialized reading, there are other detailed books on this topic, written by my esteemed colleagues, listed in the <em class="italic">Further reading</em> section of <span class="No-Break">this chapter.</span></p>
			<p>Delta Lake stands out for enhancing the reliability, scalability, and performance of big data<a id="_idIndexMarker684"/> processing frameworks, particularly Apache Spark. Developed by Databricks, it equips data lakes with <strong class="bold">Atomicity, Consistency, Isolation,</strong> and<strong class="bold"> Durability</strong> (<strong class="bold">ACID</strong>) transactions and robust schema enforcement capabilities. This is particularly crucial because clean and reliable data<a id="_idIndexMarker685"/> is not merely an advantage but a prerequisite for any serious data engineering or <span class="No-Break">ML initiative.</span></p>
			<h3>Why the need for cleaner data and robust data engineering pipelines?</h3>
			<p>Having clean data<a id="_idIndexMarker686"/> in Delta Lake and robust data engineering pipelines<a id="_idIndexMarker687"/> is not just a matter of operational efficiency but a strategic imperative. Data quality directly impacts ML model accuracy, predictive power, and, ultimately, business outcomes. Inconsistent or noisy data can mislead algorithms, leading to incorrect insights and poor decisions. By enforcing strict schema and providing ACID transactions, Delta Lake elevates data lakes from being simple storage repositories to agile, data-ready platforms that can handle the intricacies of ML <span class="No-Break">algorithms effectively.</span></p>
			<p>Efficient pipelines are equally important. They accelerate data flow from the point of ingestion to insights and model deployment. Slow or broken pipelines can bottleneck ML projects, costing organizations both time and money. Delta Lake’s transactional capabilities and metadata management help build pipelines that are not just efficient but also resilient <span class="No-Break">and future-proof.</span></p>
			<h4>Role of access control in ML modeling</h4>
			<p>As ML becomes integral<a id="_idIndexMarker688"/> to business processes, the requirement<a id="_idIndexMarker689"/> for secure and controlled data access intensifies. Delta Lake’s <strong class="bold">role-based access controls</strong> (<strong class="bold">RBACs</strong>) come into play here, integrating seamlessly with organizational identity management systems. This ensures that sensitive data is only accessible to authorized personnel, thereby adding a security layer that helps in meeting regulatory compliance requirements and safeguarding the integrity of <span class="No-Break">ML models.</span></p>
			<p>The key features<a id="_idIndexMarker690"/> of Delta Lake include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">ACID transactions</strong>: Delta Lake ensures atomicity, consistency, isolation, and durability for data operations, allowing concurrent reads and writes. It provides transactional guarantees, so you can confidently perform complex <span class="No-Break">data manipulations.</span></li>
				<li><strong class="bold">Schema evolution</strong>: Delta Lake supports schema enforcement, allowing you to specify and evolve a schema for your data. It enforces data quality by rejecting writes with incompatible schemas and provides schema evolution capabilities to handle schema changes <span class="No-Break">over time.</span></li>
				<li><strong class="bold">Time travel</strong>: Delta Lake maintains full historical versions of data, enabling you to query and analyze data at any point in time. You can easily track changes and compare different versions of the data, which is valuable for auditing, debugging, and <span class="No-Break">reproducing analyses.</span></li>
				<li><strong class="bold">Optimized data processing</strong>: Delta Lake leverages advanced indexing and caching mechanisms to optimize query performance. It uses statistics and optimizations to skip unnecessary data during query execution, resulting in faster <span class="No-Break">response times.</span></li>
				<li><strong class="bold">Data lake metadata management</strong>: Delta Lake stores metadata in a transaction log, enabling automatic schema discovery and efficient management of table metadata. It provides data lineage information, making it easier to understand the flow and transformation <span class="No-Break">of data.</span></li>
			</ul>
			<p>Delta Lake is highly compatible with Apache Spark, allowing you to leverage Spark’s robust analytics capabilities on top of your data lake. It has gained popularity in data lake architectures, enabling data engineers and scientists to build robust, scalable, and reliable data <span class="No-Break">processing pipelines.</span></p>
			<p>Next, let’s explore the seamless integration of MLflow within the Databricks platform, which offers robust capabilities for end-to-end model management. We’ll also delve into the emerging domain <span class="No-Break">of ModelOps.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>Comprehensive model management with Databricks MLflow</h2>
			<p>For managing models, Databricks<a id="_idIndexMarker691"/> provides managed MLflow, which we have<a id="_idIndexMarker692"/> already covered in the previous chapters in <span class="No-Break">great depth.</span></p>
			<p>MLflow is an open source platform<a id="_idIndexMarker693"/> designed to simplify the ML life cycle. It provides a comprehensive set of tools and APIs for managing, tracking, and deploying ML models. MLflow was developed by Databricks and has gained significant adoption within the <span class="No-Break">ML community.</span></p>
			<p>MLflow consists of four <span class="No-Break">main components:</span></p>
			<ul>
				<li><strong class="bold">Tracking</strong>: MLflow Tracking allows you to log<a id="_idIndexMarker694"/> and track experiments, parameters, metrics, and artifacts associated with your ML projects. It provides a unified interface to record and compare different experiment runs, making it easier to reproduce results and iterate on models. Tracking also supports integration with ML frameworks, such as TensorFlow, PyTorch, <span class="No-Break">and scikit-learn.</span></li>
				<li><strong class="bold">Projects</strong>: MLflow Projects provides a standard format for packaging and sharing ML code. With<a id="_idIndexMarker695"/> MLflow Projects, you can define your ML code as a reusable project, including the code, dependencies, and configuration. This enables reproducibility and collaboration by ensuring your code can be easily executed in <span class="No-Break">different environments.</span></li>
				<li><strong class="bold">Models</strong>: MLflow Models enables you to manage<a id="_idIndexMarker696"/> and deploy ML models in various formats. It provides a simple model format that allows you to package models with their associated metadata and dependencies. You can then deploy these models in various deployment environments, such as batch scoring, real-time serving, or <span class="No-Break">cloud platforms.</span></li>
				<li><strong class="bold">Model Registry</strong>: MLflow Model Registry is an optional component that adds model versioning, stage<a id="_idIndexMarker697"/> transitions, and collaboration features to MLflow Models. It allows you to keep track of different versions of your models, promotes models through different stages (for example, staging to production), and manages access control for different <span class="No-Break">team members.</span></li>
			</ul>
			<p>MLflow supports multiple programming languages, including Python, R, and Java. It can be used both in local development environments and distributed clusters, making it suitable for different <span class="No-Break">deployment scenarios.</span></p>
			<p>As we transition from discussing<a id="_idIndexMarker698"/> model management<a id="_idIndexMarker699"/> with Databricks MLflow, let’s delve into the synergy between DevOps and MLOps, and how these principles are adapted and extended for robust ML pipelines within the <span class="No-Break">Databricks ecosystem.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>Integrating DevOps and MLOps for robust ML pipelines with Databricks</h2>
			<p>Databricks integrates<a id="_idIndexMarker700"/> with well-known Git providers<a id="_idIndexMarker701"/> such as GitHub, GitLab, and Azure DevOps for managing and executing DevOps workflows for our <span class="No-Break">ML projects.</span></p>
			<p>DevOps combines software <strong class="bold">development</strong> (<strong class="bold">Dev</strong>) and IT <strong class="bold">operations</strong> (<strong class="bold">Ops</strong>) to foster collaboration, automation, and continuous delivery. It aims to streamline software systems’ development, deployment, <span class="No-Break">and maintenance.</span></p>
			<p>By incorporating DevOps principles, MLOps brings an added layer of efficiency to the life cycle of ML models. It fosters cohesive collaboration across every stage of the process – from developing and validating models to their deployment, monitoring, retraining, <span class="No-Break">and redeployment.</span></p>
			<p>Within the sphere of MLOps, <strong class="bold">continuous integration and continuous delivery</strong> (<strong class="bold">CI/CD</strong>) emerge as critical elements. They underpin<a id="_idIndexMarker702"/> automation and drive continuous learning within ML systems. The ultimate goal of CI/CD is to seamlessly integrate data with source code versions, execute parallel tasks initiated by pertinent events, compile artifacts, and propagate releases to the <span class="No-Break">production stage.</span></p>
			<p>Continuous learning through combining CI and CD principles is essential for the success of an ML system. Without it, the system risks stagnation and becoming a fruitless <strong class="bold">Proof of Concept</strong> (<strong class="bold">POC</strong>). Consistent learning and adaptation enable an ML model to provide valuable <span class="No-Break">business insights.</span></p>
			<p>To use ML models<a id="_idIndexMarker703"/> that continually improve, you need<a id="_idIndexMarker704"/> to understand CI, CD, and related methods. They work together and depend on each other, as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B17875_10_02.jpg" alt="Figure 10.2 – The relationship between continuous integration, continuous delivery, and continuous deployment" width="1650" height="686"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – The relationship between continuous integration, continuous delivery, and continuous deployment</p>
			<p>Let's understand these methods in a bit <span class="No-Break">more detail:</span></p>
			<ul>
				<li><strong class="bold">Continuous integration</strong>: In MLOps, CI is not merely about testing and validating code<a id="_idIndexMarker705"/> but also extends to testing and validating data, data schemas, and ML models. This ensures a more robust and reliable integration process tailored to <span class="No-Break">ML needs.</span></li>
				<li><strong class="bold">Continuous delivery</strong>: Beyond deploying a single software package or service, CD in an MLOps context<a id="_idIndexMarker706"/> is about deploying an entire system, which often includes an ML training pipeline and a model <span class="No-Break">prediction service.</span></li>
				<li><strong class="bold">Continuous deployment</strong>: Similar to traditional DevOps, CD in MLOps goes one step further by fully automating<a id="_idIndexMarker707"/> the release process and deploying new changes to production without <span class="No-Break">human intervention.</span></li>
				<li><strong class="bold">Continuous training</strong>: Unique to ML systems, CT focuses on automatically retraining<a id="_idIndexMarker708"/> and serving models, ensuring they adapt and improve <span class="No-Break">over time.</span></li>
			</ul>
			<p>At the time of writing<a id="_idIndexMarker709"/> this book, Databricks is working on a new feature called the MLOps Stack, which provides a template to structure complex ML projects for CI/CD integration with <span class="No-Break">Git providers.</span></p>
			<p>For further details regarding the MLOps Stack, you are encouraged to peruse <em class="italic">MLOps Stack</em> on <span class="No-Break">GitHub (</span><a href="https://github.com/databricks/mlops-stack"><span class="No-Break">https://github.com/databricks/mlops-stack</span></a><span class="No-Break">).</span></p>
			<p>We will not be covering the MLOps Stack in this chapter; instead, we will cover another approach to building your MLOps pipelines<a id="_idIndexMarker710"/> on Databricks based on utilizing what we have learned<a id="_idIndexMarker711"/> <span class="No-Break">so far.</span></p>
			<p>Let’s dive deeper and understand the fundamentals of MLOps and the various <span class="No-Break">deployment paradigms.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Fundamentals of MLOps and deployment patterns</h1>
			<p>To effectively<a id="_idIndexMarker712"/> manage MLOps, it’s essential<a id="_idIndexMarker713"/> to first familiarize ourselves with its underlying terminology and structure. This includes understanding the roles and responsibilities associated with various operational environments – namely, <strong class="bold">development</strong> (<strong class="bold">dev</strong>), staging, and <strong class="bold">production</strong> (<strong class="bold">prod</strong>). Let’s dissect what these <a id="_idIndexMarker714"/>environments signify in a practical<a id="_idIndexMarker715"/> <span class="No-Break">MLOps framework.</span></p>
			<p>Within any ML project, there are three <span class="No-Break">pivotal assets:</span></p>
			<ul>
				<li><strong class="bold">Code base</strong>: This serves as the project’s blueprint. It contains<a id="_idIndexMarker716"/> all the source code related to data preprocessing, model training, evaluation, <span class="No-Break">and deployment.</span></li>
				<li><strong class="bold">Data</strong>: This includes the datasets<a id="_idIndexMarker717"/> that are used for training, validating, and testing the model. The quality and availability of this data directly influence the <span class="No-Break">model’s efficacy.</span></li>
				<li><strong class="bold">Trained model</strong>: This is the culmination of your ML workflow, a model that has been<a id="_idIndexMarker718"/> trained, evaluated, and prepared <span class="No-Break">for inference.</span></li>
			</ul>
			<p>Each of these assets goes through distinct phases – development, testing, and deployment – which are often segregated into <span class="No-Break">separate environments:</span></p>
			<ul>
				<li><strong class="bold">Development environment (dev)</strong>: This is where the initial code is written and tested. It’s generally<a id="_idIndexMarker719"/> the most accessible in terms of code and data but has the least stringent quality and <span class="No-Break">testing requirements.</span></li>
				<li><strong class="bold">Staging environment</strong>: This serves as an intermediate space for additional testing <a id="_idIndexMarker720"/>and quality assurance before the project moves <span class="No-Break">to production.</span></li>
				<li><strong class="bold">Production environment (prod)</strong>: This is the most restrictive environment where the finalized<a id="_idIndexMarker721"/> assets are deployed. It has the highest quality and security requirements and is the least accessible for direct interactions. The following figure provides a visual representation of the key assets in MLOps, as well as the organizational structure of different environments. It illustrates the life cycle of these assets as they progress through<a id="_idIndexMarker722"/> development, testing, and, <span class="No-Break">ultimately, production:</span></li>
			</ul>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B17875_10_03.jpg" alt="Figure 10.3 – The various assets related to an ML project and its environments" width="1650" height="578"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The various assets related to an ML project and its environments</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding figure is courtesy <span class="No-Break">of Databricks.</span></p>
			<p>The following figure<a id="_idIndexMarker723"/> illustrates the accessibility levels and quality requirements<a id="_idIndexMarker724"/> across <span class="No-Break">these environments:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B17875_10_04.jpg" alt="Figure 10.4 – The various environments and their openness to accessibility" width="1650" height="552"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The various environments and their openness to accessibility</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding figure is courtesy <span class="No-Break">of Databricks.</span></p>
			<p>With Databricks, you have the flexibility to structure these dev, staging, and prod environments in various ways to meet your project’s <span class="No-Break">specific needs.</span></p>
			<p>It’s important<a id="_idIndexMarker725"/> to note that the theoretical separation of dev, staging, and prod environments <a id="_idIndexMarker726"/>serves as a guideline for best practices in MLOps. However, the real-world implementation can vary significantly based on your organizational needs, workflow, and <span class="No-Break">technological capabilities.</span></p>
			<p>In the following section, we will delve into multiple approaches for deploying Databricks workspaces to better align your dev, staging, and prod environments with your specific <span class="No-Break">organizational requirements.</span></p>
			<p>The following figure showcases three distinct deployment patterns designed to set up your dev, QA, and prod <span class="No-Break">environments effectively:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17875_10_05.jpg" alt="Figure 10.5 – Various Databricks environment deployment approaches" width="1650" height="640"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Various Databricks environment deployment approaches</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This is the source for the preceding figure: <em class="italic">The Big Book </em><span class="No-Break"><em class="italic">of MLOps</em></span><span class="No-Break">.</span></p>
			<p>Let’s understand<a id="_idIndexMarker727"/> these patterns<a id="_idIndexMarker728"/> one <span class="No-Break">by one.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor149"/>Navigating environment isolation in Databricks – multiple strategies for MLOps</h2>
			<p>To devise a robust<a id="_idIndexMarker729"/> MLOps strategy, you need to consider not only the type of assets involved but also the environment where they reside – dev, staging, or prod. Each environment offers varying levels of accessibility, testing rigor, and data security, informed by organizational size, governance policies, and <span class="No-Break">security requirements.</span></p>
			<h3>Multiple cloud accounts</h3>
			<p>For large organizations<a id="_idIndexMarker730"/> governed by stringent rules and regulations, separating dev, staging, and prod environments across distinct cloud accounts is a common practice. In such a setup, each cloud account will host its own Databricks workspace. This architecture ensures isolation at both the cloud account and network levels, while also potentially increasing costs due to duplicated resources and <span class="No-Break">data storage.</span></p>
			<h3>A single cloud account with multiple Databricks workspaces</h3>
			<p>Alternatively, smaller organizations<a id="_idIndexMarker731"/> or projects may opt for a single cloud account containing multiple Databricks workspaces. Each workspace is deployed into its own network and is isolated at that level. While more cost-effective, this approach still allows for sufficient isolation and can align well with organizational data <span class="No-Break">governance policies.</span></p>
			<h3>A single cloud account with a single Databricks workspace</h3>
			<p>Even within a single<a id="_idIndexMarker732"/> cloud account, Databricks provides the capability to enforce strict isolation between different roles and projects. Features such as RBAC, permissions, and native data governance tools such as Unity Catalog allow for effective segregation of access within a <span class="No-Break">single workspace.</span></p>
			<p>Having explored various approaches for organizing our dev, staging, and prod environments in Databricks, it’s time to turn our attention to another pivotal aspect of MLOps: the asynchronous nature of life cycles in ML projects. This stands in contrast to traditional software DevOps, where code and application updates usually happen <span class="No-Break">in lockstep.</span></p>
			<p>Consider a deployed <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>) as a case in point. The sheer complexity and size of such models<a id="_idIndexMarker733"/> can make retraining a formidable challenge. You may find that while the data engineering code sees monthly iterations, the training code for the model itself remains static for an <span class="No-Break">extended duration.</span></p>
			<p>On the flip side, think about a churn prediction model. Here, automatic retraining might be scheduled monthly using fresh datasets. If the newly trained model outperforms its predecessor, it immediately gets moved to production, all without requiring any changes to the existing <a id="_idIndexMarker734"/><span class="No-Break">code base.</span></p>
			<h3>Navigating asynchronous life cycles</h3>
			<p>Given the incongruent update<a id="_idIndexMarker735"/> cycles for ML models and code, adopting strategies to manage these asynchronicities becomes imperative. You might employ techniques such as canary deployments for safer model rollouts, or opt for blue-green deployments to ensure smoother rollbacks. Automated monitoring systems and alert mechanisms are equally important, serving as early warning systems for model degradation or operational issues, thus allowing for <span class="No-Break">quick remediation.</span></p>
			<h3>Fiscal and regulatory considerations</h3>
			<p>Beyond technical aspects, MLOps also encompasses financial and compliance variables. Cost considerations<a id="_idIndexMarker736"/> can’t be overlooked – both for data storage and computational resources. Furthermore, data lineage is essential for keeping tabs on data movement through your pipeline, which not only aids in debugging but is invaluable for compliance and auditing purposes. Similarly, data versioning is indispensable when it comes to model reproducibility, an especially crucial feature for models undergoing <span class="No-Break">frequent retraining.</span></p>
			<p>With this nuanced understanding, we are better equipped to manage the complexities that arise from asynchronous updates in the ML life cycle, in the context of Databricks or any <span class="No-Break">MLOps platform.</span></p>
			<p>Now, let’s take a look at the various ML deployment paradigms that you <span class="No-Break">can utilize.</span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor150"/>Understanding ML deployment patterns</h1>
			<p>The ultimate goal of any ML project<a id="_idIndexMarker737"/> is to get our ML model into production. Depending on what kind of use case we are catering to and how sophisticated our ML engineering team is, there are two broad ML <span class="No-Break">deployment approaches:</span></p>
			<ul>
				<li>The deploy <span class="No-Break">models approach</span></li>
				<li>The deploy <span class="No-Break">code approach</span></li>
			</ul>
			<p>Let’s understand these approaches one <span class="No-Break">by one.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>The deploy models approach</h2>
			<p>The model deployment<a id="_idIndexMarker738"/> workflow adheres to a structured<a id="_idIndexMarker739"/> methodology, beginning in a development environment where code for training the ML model is both crafted and refined. After the model undergoes training and the optimal version is ascertained, it is formally registered within a specialized model registry. This is followed by a battery of integration tests to evaluate its performance and reliability. Upon successfully passing these assessments, the model is first elevated to a staging environment for further validation. Once it meets all requisite criteria, it is then deployed into the <span class="No-Break">production environment.</span></p>
			<p>The following figure offers a graphical depiction of this <span class="No-Break">multi-stage approach:</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B17875_10_06.jpg" alt="Figure 10.6 – The deploy models approach" width="1650" height="371"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – The deploy models approach</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This is the source for the preceding figure: <em class="italic">The Big Book </em><span class="No-Break"><em class="italic">of MLOps</em></span><span class="No-Break">.</span></p>
			<p>Throughout this book, all the notebooks we have utilized so far focus on this particular deployment approach. It is a popular choice among companies and teams, especially when the ML team comprises individuals with a background in data science rather than traditional software engineering. This approach offers simplicity and serves as a great starting point for <span class="No-Break">ML projects.</span></p>
			<p>The following figure showcases<a id="_idIndexMarker740"/> the entire end-to-end MLOps life cycle<a id="_idIndexMarker741"/> for the deploy <span class="No-Break">models approach:</span></p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B17875_10_07.jpg" alt="Figure 10.7 – The reference architecture and workflow for deploying a model from development to production using the deploy models approach" width="1174" height="486"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – The reference architecture and workflow for deploying a model from development to production using the deploy models approach</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding figure is courtesy <span class="No-Break">of Databricks.</span></p>
			<p>In this MLOps workflow, data engineers, data scientists, and ML engineers collaborate to perform various steps to ensure the successful development, deployment, and monitoring of ML models. Here is a breakdown of the responsibilities and tasks performed by <span class="No-Break">each role:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Data engineers</strong></span><span class="No-Break">:</span><ul><li>Collect data<a id="_idIndexMarker742"/> from diverse sources such as databases, cloud storage, and sensors, ensuring reliability <span class="No-Break">and quality</span></li><li>Cleanse and preprocess the data, handling tasks such as removing duplicates, handling missing values, and transforming data into a format suitable for <span class="No-Break">ML algorithms</span></li><li>Store and manage data in a data warehouse or Delta Lake, ensuring accessibility and efficient utilization by data scientists and <span class="No-Break">ML engineers</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Data scientists</strong></span><span class="No-Break">:</span><ul><li>Explore and analyze the data to gain<a id="_idIndexMarker743"/> insights into its characteristics, identifying relevant patterns <span class="No-Break">and relationships.</span></li><li>Generate and register features into feature tables <span class="No-Break">for reuse.</span></li><li>Develop and train ML models, employing various algorithms and techniques to achieve accurate predictions and desired outcomes. All the model runs and experiments are logged automatically into the MLflow tracking server <span class="No-Break">on Databricks.</span></li><li>Evaluate and assess the performance of the trained models using appropriate metrics and <span class="No-Break">validation techniques.</span></li><li>Select the most suitable model for deployment based on performance and business requirements. The best model is then registered in the Model Registry as a <span class="No-Break">candidate model.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">ML engineers</strong></span><span class="No-Break">:</span><ul><li>Deploy ML models<a id="_idIndexMarker744"/> into production environments, making them available for making predictions or decisions in <span class="No-Break">real time</span></li><li>Monitor the performance of deployed models, ensuring they operate optimally and detect any anomalies or drift in <span class="No-Break">their behavior</span></li><li>Update and retrain models as new data becomes available, maintaining model relevance<a id="_idIndexMarker745"/> <span class="No-Break">and accuracy</span></li></ul></li>
			</ul>
			<p>All the notebooks that we covered as part of this book show <span class="No-Break">this workflow.</span></p>
			<p>Now that we understand how the deploy models approach for MLOps works, let’s take a look at the deploy <span class="No-Break">code approach.</span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor152"/>The deploy code approach</h2>
			<p>In the deploy code approach, we version control<a id="_idIndexMarker746"/> not only the code to train the ML models<a id="_idIndexMarker747"/> but also the code to create the feature tables. This approach works well when you have strict regulations to separate data access in <span class="No-Break">each environment.</span></p>
			<p>The data scientists develop code in the dev environment for feature engineering and model training. After a good candidate model is found, the dev branch code is committed to the staging branch, where automated unit tests are run. Again, we train the model in staging and perform our performance benchmark test. Once everything else looks good, we push the code to the main branch and the prod environment. Here, again, we retrain the model on the data <span class="No-Break">in production:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B17875_10_08.jpg" alt="Figure 10.8 – The reference architecture and workflow for deploying a model from development to production using the deploy code approach" width="1650" height="341"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – The reference architecture and workflow for deploying a model from development to production using the deploy code approach</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This is the source for the preceding figure: <em class="italic">The Big Book </em><span class="No-Break"><em class="italic">of MLOps</em></span><span class="No-Break">.</span></p>
			<p>The development process involves several stages, starting with the creation of code for the training model and feature<a id="_idIndexMarker748"/> engineering in the dev environment. The following figure showcases<a id="_idIndexMarker749"/> the deploy code workflow step by step in the <span class="No-Break">dev environment:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B17875_10_09.jpg" alt="Figure 10.9 – The deploy code workflow within the development environment" width="848" height="634"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – The deploy code workflow within the development environment</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Courtesy <span class="No-Break">of Databricks</span></p>
			<p>Let’s understand these steps one <span class="No-Break">by one:</span></p>
			<ol>
				<li><strong class="bold">Data access points</strong>: In development settings, data scientists typically have read-only permissions for production data. For compliance reasons, access may be limited to sanitized or duplicate versions of this data. A separate development storage is also available for read-write operations, facilitating <span class="No-Break">experimental work.</span></li>
				<li><strong class="bold">Preliminary data investigation</strong> (<strong class="bold">PDI</strong>): Data scientists use an iterative, interactive method<a id="_idIndexMarker750"/> for data exploration, leveraging notebooks, visual charts, and Databricks SQL. This step is often a standalone process and not usually part of <span class="No-Break">deployable pipelines.</span></li>
				<li><strong class="bold">Source code management</strong>: All ML system code resides in a version control repository. Data scientists work<a id="_idIndexMarker751"/> on a development branch within this Git repository. Code can be synchronized<a id="_idIndexMarker752"/> with the Databricks workspace via <span class="No-Break">Databricks Repos.</span></li>
				<li><strong class="bold">Enhance feature datasets</strong>: This pipeline ingests data from both raw and existing feature tables, outputting it to tables within Feature Store. This step includes two <span class="No-Break">main tasks:</span><ol><li class="upper-roman"><strong class="bold">Quality assurance</strong>: Here, the data is validated to ensure it meets <span class="No-Break">quality standards.</span></li><li class="upper-roman"><strong class="bold">Feature construction</strong>: Code is written or updated by data scientists to generate new features. Data may be pulled from Feature Store or other Lakehouse tables. These dev feature tables are used to build experimental models, and upon promotion to production, they update the corresponding <span class="No-Break">production tables.</span></li></ol><p class="list-inset">Management can be separate for feature pipelines if they are governed by <span class="No-Break">different teams.</span></p></li>
				<li><strong class="bold">Model training pipeline</strong>: Data scientists build pipelines for model training on either read-only production data or development-specific data. These pipelines may utilize feature tables from both the dev and <span class="No-Break">prod environments:</span><ol><li class="upper-roman"><strong class="bold">Tuning and training</strong>: The training process sources data from feature stores and varying levels of Lakehouse tables while logging parameters, metrics, and artifacts in the MLflow <span class="No-Break">tracking system.</span></li><li class="upper-roman"><strong class="bold">Model storing</strong>: After training and tuning have been finalized, the model is stored on the MLflow tracking server, capturing its association with the input data and <span class="No-Break">the code.</span></li></ol><p class="list-inset">When executed in staging or production, the model can be retrieved and registered for ongoing management <span class="No-Break">and testing.</span></p></li>
				<li><strong class="bold">Code finalization</strong>: Once the development work on pipelines for features, training, and inference is complete, either the data scientist or the ML engineer commits these changes to the version control system from the <span class="No-Break">development branch.</span></li>
			</ol>
			<p>Let’s move on and understand the workflow<a id="_idIndexMarker753"/> in the staging environment. The staging environment<a id="_idIndexMarker754"/> serves as the final testing ground for ML code before it transitions to production. It encompasses comprehensive testing of all pipeline components, including model training and feature engineering. ML engineers employ a CI pipeline to execute unit and integration tests. Successful completion results in a release branch, triggering the CI/CD system to initiate the <span class="No-Break">production stage.</span></p>
			<p>The following diagram provides a step-by-step visual guide to the workflow within the <span class="No-Break">staging environment:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B17875_10_10.jpg" alt="Figure 10.10 – The deploy code workflow within the staging environment" width="867" height="790"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – The deploy code workflow within the staging environment</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Courtesy <span class="No-Break">of Databricks</span></p>
			<p>Let’s delve<a id="_idIndexMarker755"/> into each of these<a id="_idIndexMarker756"/> steps <span class="No-Break">in detail:</span></p>
			<ol>
				<li><strong class="bold">Initiate merge process</strong>: The journey toward deployment commences when an ML engineer submits a merge request to the source control’s staging branch, often<a id="_idIndexMarker757"/> the “main” branch. This action sets off a <span class="No-Break"><strong class="bold">CI</strong></span><span class="No-Break"> workflow.</span></li>
				<li><strong class="bold">Execute unit tests</strong>: Within the CI framework, the source code is automatically compiled, and unit tests are initiated. Should these tests not succeed, the merge request gets dismissed. Note that unit tests operate in isolation from data or <span class="No-Break">external services.</span></li>
				<li><strong class="bold">Conduct integration</strong> <strong class="bold">tests</strong>: Following the unit tests, the CI mechanism proceeds to administer <a id="_idIndexMarker758"/>integration tests. These tests validate the compatibility and functionality of all pipelines, which encompasses feature engineering, model training, inference, and monitoring. The staging environment is designed to mirror the production setting as closely <span class="No-Break">as feasible.</span><p class="list-inset">To economize on test duration, concessions may be made between the thoroughness of testing<a id="_idIndexMarker759"/> and execution speed. For instance, smaller data subsets<a id="_idIndexMarker760"/> could be used, or fewer training cycles run. Depending on the model’s intended application, comprehensive load testing might be conducted at <span class="No-Break">this stage.</span></p><p class="list-inset">After successful completion of integration tests in the staging branch, the code becomes eligible for <span class="No-Break">production deployment.</span></p></li>
				<li><strong class="bold">Commit to the staging branch</strong>: Should the tests be successful, the code merges into the staging branch. In case of test failure, the CI/CD system alerts the relevant parties and updates the merge (or pull) request with <span class="No-Break">the results.</span><p class="list-inset">Periodic integration tests can be scheduled on the staging branch, especially if it receives frequent updates from <span class="No-Break">multiple contributors.</span></p></li>
				<li><strong class="bold">Establish a release branch</strong>: Once the code has been validated and is ready for production deployment, the ML engineer forms a release branch. This action prompts the CI/CD system to refresh the <span class="No-Break">production tasks.</span></li>
			</ol>
			<p>Lastly, let’s understand the production <span class="No-Break">environment’s workflow.</span></p>
			<p>In the production environment, ML engineers oversee the deployment of ML pipelines that handle feature<a id="_idIndexMarker761"/> computation, model training, and testing, as well as prediction<a id="_idIndexMarker762"/> publishing and performance monitoring. A retraining mechanism operates on production data to keep the model up to date and optimized. Performance benchmarks are rigorously evaluated to ensure that the new model meets or exceeds the set standards. Data scientists typically lack write and compute permissions in this environment but maintain visibility into test outcomes, logs, model artifacts, and pipeline statuses to aid in diagnosing any <span class="No-Break">production issues.</span></p>
			<p>The following diagram offers a comprehensive, step-by-step visualization of the workflow processes in the <span class="No-Break">production environment:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B17875_10_11.jpg" alt="Figure 10.11 – The deploy code workflow within the production environment" width="871" height="800"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – The deploy code workflow within the production environment</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Courtesy <span class="No-Break">of Databricks</span></p>
			<p>Let’s go through<a id="_idIndexMarker763"/> this workflow<a id="_idIndexMarker764"/> step <span class="No-Break">by step:</span></p>
			<ol>
				<li><strong class="bold">Refresh feature data</strong>: This phase involves ingesting new data from production and updating tables in Feature Store. This can be either a batch or real-time process and can be invoked by different triggers, such as schedules or <span class="No-Break">continuous runs.</span></li>
				<li><span class="No-Break"><strong class="bold">Model training</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Tuning and training</strong>: The pipeline trains the production model on complete data and logs relevant metrics and parameters through autologging. Unlike the development stage, only top-performing algorithms and hyperparameters are considered to optimize time <span class="No-Break">and performance.</span></li><li><strong class="bold">Model assessment</strong>: The quality of the model<a id="_idIndexMarker765"/> is tested against a separate dataset<a id="_idIndexMarker766"/> from production. Test results and custom metrics <span class="No-Break">are recorded.</span></li><li><strong class="bold">Model registration</strong>: Upon successful training, the model is registered with an initial status of “None” in <span class="No-Break">Model Registry.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Automated deployment</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Compliance verification</strong>: The pipeline performs mandatory compliance checks, which can include human review for complex evaluations. The results <span class="No-Break">are logged.</span></li><li><strong class="bold">Performance validation</strong>: Models in the staging phase are compared against those in production to avert <span class="No-Break">performance decay.</span></li><li><strong class="bold">Transition to production</strong>: The model is advanced to the production stage, either manually or automatically, following satisfactory <span class="No-Break">performance comparisons.</span></li></ul></li>
				<li><strong class="bold">Real-time serving</strong>: MLflow enables the model to be deployed for low-latency use cases. The deployed model fetches features and returns predictions for each <span class="No-Break">incoming request.</span></li>
				<li><strong class="bold">Batch or stream inference</strong>: For higher throughput or latency requirements, batch or stream-based inferences are processed. Predictions can be saved in various storage options, including message queues such as <span class="No-Break">Apache Kafka.</span></li>
				<li><span class="No-Break"><strong class="bold">Ongoing monitoring</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Data feeding</strong>: Logs from different inference types <span class="No-Break">are ingested</span></li><li><strong class="bold">Performance and drift metrics</strong>: Various quality and performance metrics <span class="No-Break">are calculated</span></li><li><strong class="bold">Metric reporting</strong>: Metrics are saved for further analysis and <span class="No-Break">alerting purposes</span></li></ul></li>
				<li><strong class="bold">Retraining triggers</strong>: Models can be automatically<a id="_idIndexMarker767"/> retrained based on a schedule or triggered by <span class="No-Break">performance</span><span class="No-Break"><a id="_idIndexMarker768"/></span><span class="No-Break"> degradation.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Automating the retraining process can be complex and may require manual intervention to resolve issues identified through monitoring, such as data drift or <span class="No-Break">performance degradation.</span></p>
			<p>The following figure summarizes the various steps that are performed in various environments for the deploy code approach <span class="No-Break">to ModelOps:</span></p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B17875_10_12.jpg" alt="Figure 10.12 – The various steps performed in various environments for the deploy code approach to ModelOps" width="908" height="364"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – The various steps performed in various environments for the deploy code approach to ModelOps</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding figure is courtesy <span class="No-Break">of Databricks.</span></p>
			<p>Overall, you have three environments. At the top, you have your Git workflow provider, which manages transitioning code from one environment to another. At the bottom, you have the data access layer or feature tables with data across <span class="No-Break">different environments.</span></p>
			<p>The important point to keep<a id="_idIndexMarker769"/> in mind here is that the trained model itself will have its own stages<a id="_idIndexMarker770"/> in Model Registry in the production environment. We retrain the model again in each environment and hydrate the respective feature tables based on the <span class="No-Break">updated code.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This approach may resonate more with individuals who have a background in traditional software engineering and are acquainted with DevOps principles. However, at the time of writing this book, there is no officially established method for implementing the deploy code approach of MLOps on the Databricks platform using the currently generally available tools. Although we discussed the concepts of the deploy code approach in this section, we won’t be covering this as part of the <span class="No-Break">provided code.</span></p>
			<p>MLOps Stack is going to address<a id="_idIndexMarker771"/> this model deployment paradigm when it becomes generally<a id="_idIndexMarker772"/> available. We will update this book once the new feature <span class="No-Break">is available.</span></p>
			<p>Now, let’s wrap up this chapter and summarize our <span class="No-Break">key learnings.</span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor153"/>Summary</h1>
			<p>In this chapter, we covered the basics of MLOps, the different deployment approaches on Databricks, and their <span class="No-Break">reference architectures.</span></p>
			<p>Selecting a model deployment approach should be based on your team’s proficiency in implementing DevOps processes for ML projects. It’s important to acknowledge that there is no universal solution as each approach we have discussed has its own advantages and disadvantages. However, it is possible to create a customized hybrid ModelOps architecture within the <span class="No-Break">Databricks environment.</span></p>
			<p>By considering your team’s strengths and expertise, you can determine the most suitable deployment approach for your project. It’s essential to assess scalability, maintainability, ease of deployment, and integration with existing infrastructure. Evaluating these aspects will help you make an informed decision and optimize the model <span class="No-Break">deployment process.</span></p>
			<p>In Databricks, you have the flexibility to tailor your ModelOps architecture to your project’s requirements. Leveraging the capabilities of Databricks, you can combine the best elements from different deployment approaches to create a customized and efficient workflow. This hybrid approach allows you to leverage the strengths of different methodologies while mitigating <span class="No-Break">their limitations.</span></p>
			<p>Remember, the ultimate goal is to establish a robust and streamlined model deployment process that aligns with your team’s capabilities and project needs. By carefully considering your options and utilizing the resources in the Databricks environment, you can create a ModelOps architecture that maximizes efficiency and productivity for your <span class="No-Break">ML projects.</span></p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor154"/>Further reading</h1>
			<p>Please go through the following sources and their links to learn more about the topics that were covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li><em class="italic">The Big Book of </em><span class="No-Break"><em class="italic">MLOps</em></span><span class="No-Break">: </span><a href="http://bit.ly/big-book-of-mlops"><span class="No-Break">bit.ly/big-book-of-mlops</span></a></li>
				<li><em class="italic">MLOps Stack on </em><span class="No-Break"><em class="italic">GitHub</em></span><span class="No-Break">: </span><a href="https://github.com/databricks/mlops-stack"><span class="No-Break">https://github.com/databricks/mlops-stack</span></a></li>
				<li>Damji, J. S., Wenig, B., Das, T., and Lee, D. (2020). <em class="italic">Learning Spark</em> (<span class="No-Break">2nd ed.)</span></li>
			</ol>
		</div>
	</div>
</div>
</body></html>