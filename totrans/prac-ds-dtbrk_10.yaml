- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using CI/CD to Automate Model Retraining and Redeployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having explored various statistical tests in [*Chapter 9*](B17875_09.xhtml#_idTextAnchor129),
    courtesy of diverse open source libraries on Databricks and their integration
    with MLflow, we will now focus on an integral component of MLOps on Databricks.
    In this chapter, we will look at how Databricks unifies DevOps, DataOps, and ModelOps
    all in a single platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamentals of MLOps and deployment patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s understand what MLOps is.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLOps serves as a multidisciplinary approach that merges the principles of
    DevOps, ModelOps, and DataOps to facilitate the end-to-end life cycle of ML projects.
    It aims to streamline the transition from model development to deployment, while
    also ensuring effective monitoring and management. In this framework, we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DevOps**: This focuses on the continuous integration and deployment of code,
    aiming for quicker releases and more reliable software'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ModelOps**: This specializes in managing ML models, ensuring they are effectively
    trained, validated, and deployed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataOps**: This deals with data management practices, encompassing everything
    from data collection and preprocessing to storage and analytics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLOps improves the performance, stability, and long-term efficiency of ML systems.
    There are two primary risks that MLOps can help mitigate for your use case and
    industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Technical risks**: These result from poorly managed models that are not performing
    as expected. Without MLOps implementing your infrastructure and pipeline to train
    new models and redeploy them in the production environment, it may be very fragile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance risks**: If you are part of a regulated industry and must keep
    track of the new regulations and compliances to ensure you are not violating them,
    MLOps can help mitigate them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through automation, MLOps can also reduce and catch errors before getting to
    the production environment and reduce the time to market to launch and maintain
    products reliant on the most updated models for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at Databricks as a platform, which allows you to reduce the
    risks outlined previously and helps improve the long-term efficiency of your teams
    and your ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: One unique part of Databricks is that it is a *data-centric AI platform*. As
    part of this AI platform, Databricks uniquely provides all the necessary components
    needed to manage the data, models, and code that are part of the ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To elucidate how Databricks facilitates MLOps, the following figure illustrates
    the platform’s integration capabilities with various tools and services on the
    Databricks Lakehouse platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Databricks’ data-centric platform and its components](img/B17875_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Databricks’ data-centric platform and its components
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Courtesy of Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explore how Delta Lake serves as a pivotal technology that bridges
    the gap between robust data storage and ML readiness.
  prefs: []
  type: TYPE_NORMAL
- en: Delta Lake – more than just a data lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to managing complex data ecosystems, Databricks offers Delta Lake,
    a comprehensive open source storage layer that we discussed briefly in [*Chapter
    1*](B17875_01.xhtml#_idTextAnchor014). For more specialized reading, there are
    other detailed books on this topic, written by my esteemed colleagues, listed
    in the *Further reading* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Delta Lake stands out for enhancing the reliability, scalability, and performance
    of big data processing frameworks, particularly Apache Spark. Developed by Databricks,
    it equips data lakes with **Atomicity, Consistency, Isolation,** and **Durability**
    (**ACID**) transactions and robust schema enforcement capabilities. This is particularly
    crucial because clean and reliable data is not merely an advantage but a prerequisite
    for any serious data engineering or ML initiative.
  prefs: []
  type: TYPE_NORMAL
- en: Why the need for cleaner data and robust data engineering pipelines?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having clean data in Delta Lake and robust data engineering pipelines is not
    just a matter of operational efficiency but a strategic imperative. Data quality
    directly impacts ML model accuracy, predictive power, and, ultimately, business
    outcomes. Inconsistent or noisy data can mislead algorithms, leading to incorrect
    insights and poor decisions. By enforcing strict schema and providing ACID transactions,
    Delta Lake elevates data lakes from being simple storage repositories to agile,
    data-ready platforms that can handle the intricacies of ML algorithms effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient pipelines are equally important. They accelerate data flow from the
    point of ingestion to insights and model deployment. Slow or broken pipelines
    can bottleneck ML projects, costing organizations both time and money. Delta Lake’s
    transactional capabilities and metadata management help build pipelines that are
    not just efficient but also resilient and future-proof.
  prefs: []
  type: TYPE_NORMAL
- en: Role of access control in ML modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As ML becomes integral to business processes, the requirement for secure and
    controlled data access intensifies. Delta Lake’s **role-based access controls**
    (**RBACs**) come into play here, integrating seamlessly with organizational identity
    management systems. This ensures that sensitive data is only accessible to authorized
    personnel, thereby adding a security layer that helps in meeting regulatory compliance
    requirements and safeguarding the integrity of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of Delta Lake include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ACID transactions**: Delta Lake ensures atomicity, consistency, isolation,
    and durability for data operations, allowing concurrent reads and writes. It provides
    transactional guarantees, so you can confidently perform complex data manipulations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema evolution**: Delta Lake supports schema enforcement, allowing you
    to specify and evolve a schema for your data. It enforces data quality by rejecting
    writes with incompatible schemas and provides schema evolution capabilities to
    handle schema changes over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time travel**: Delta Lake maintains full historical versions of data, enabling
    you to query and analyze data at any point in time. You can easily track changes
    and compare different versions of the data, which is valuable for auditing, debugging,
    and reproducing analyses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized data processing**: Delta Lake leverages advanced indexing and caching
    mechanisms to optimize query performance. It uses statistics and optimizations
    to skip unnecessary data during query execution, resulting in faster response
    times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data lake metadata management**: Delta Lake stores metadata in a transaction
    log, enabling automatic schema discovery and efficient management of table metadata.
    It provides data lineage information, making it easier to understand the flow
    and transformation of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delta Lake is highly compatible with Apache Spark, allowing you to leverage
    Spark’s robust analytics capabilities on top of your data lake. It has gained
    popularity in data lake architectures, enabling data engineers and scientists
    to build robust, scalable, and reliable data processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the seamless integration of MLflow within the Databricks
    platform, which offers robust capabilities for end-to-end model management. We’ll
    also delve into the emerging domain of ModelOps.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensive model management with Databricks MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For managing models, Databricks provides managed MLflow, which we have already
    covered in the previous chapters in great depth.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow is an open source platform designed to simplify the ML life cycle. It
    provides a comprehensive set of tools and APIs for managing, tracking, and deploying
    ML models. MLflow was developed by Databricks and has gained significant adoption
    within the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow consists of four main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking**: MLflow Tracking allows you to log and track experiments, parameters,
    metrics, and artifacts associated with your ML projects. It provides a unified
    interface to record and compare different experiment runs, making it easier to
    reproduce results and iterate on models. Tracking also supports integration with
    ML frameworks, such as TensorFlow, PyTorch, and scikit-learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projects**: MLflow Projects provides a standard format for packaging and
    sharing ML code. With MLflow Projects, you can define your ML code as a reusable
    project, including the code, dependencies, and configuration. This enables reproducibility
    and collaboration by ensuring your code can be easily executed in different environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: MLflow Models enables you to manage and deploy ML models in various
    formats. It provides a simple model format that allows you to package models with
    their associated metadata and dependencies. You can then deploy these models in
    various deployment environments, such as batch scoring, real-time serving, or
    cloud platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Registry**: MLflow Model Registry is an optional component that adds
    model versioning, stage transitions, and collaboration features to MLflow Models.
    It allows you to keep track of different versions of your models, promotes models
    through different stages (for example, staging to production), and manages access
    control for different team members.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow supports multiple programming languages, including Python, R, and Java.
    It can be used both in local development environments and distributed clusters,
    making it suitable for different deployment scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: As we transition from discussing model management with Databricks MLflow, let’s
    delve into the synergy between DevOps and MLOps, and how these principles are
    adapted and extended for robust ML pipelines within the Databricks ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating DevOps and MLOps for robust ML pipelines with Databricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Databricks integrates with well-known Git providers such as GitHub, GitLab,
    and Azure DevOps for managing and executing DevOps workflows for our ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps combines software **development** (**Dev**) and IT **operations** (**Ops**)
    to foster collaboration, automation, and continuous delivery. It aims to streamline
    software systems’ development, deployment, and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating DevOps principles, MLOps brings an added layer of efficiency
    to the life cycle of ML models. It fosters cohesive collaboration across every
    stage of the process – from developing and validating models to their deployment,
    monitoring, retraining, and redeployment.
  prefs: []
  type: TYPE_NORMAL
- en: Within the sphere of MLOps, **continuous integration and continuous delivery**
    (**CI/CD**) emerge as critical elements. They underpin automation and drive continuous
    learning within ML systems. The ultimate goal of CI/CD is to seamlessly integrate
    data with source code versions, execute parallel tasks initiated by pertinent
    events, compile artifacts, and propagate releases to the production stage.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous learning through combining CI and CD principles is essential for
    the success of an ML system. Without it, the system risks stagnation and becoming
    a fruitless **Proof of Concept** (**POC**). Consistent learning and adaptation
    enable an ML model to provide valuable business insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use ML models that continually improve, you need to understand CI, CD, and
    related methods. They work together and depend on each other, as shown in *Figure
    10**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The relationship between continuous integration, continuous
    delivery, and continuous deployment](img/B17875_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – The relationship between continuous integration, continuous delivery,
    and continuous deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand these methods in a bit more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous integration**: In MLOps, CI is not merely about testing and validating
    code but also extends to testing and validating data, data schemas, and ML models.
    This ensures a more robust and reliable integration process tailored to ML needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous delivery**: Beyond deploying a single software package or service,
    CD in an MLOps context is about deploying an entire system, which often includes
    an ML training pipeline and a model prediction service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous deployment**: Similar to traditional DevOps, CD in MLOps goes
    one step further by fully automating the release process and deploying new changes
    to production without human intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous training**: Unique to ML systems, CT focuses on automatically
    retraining and serving models, ensuring they adapt and improve over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing this book, Databricks is working on a new feature called
    the MLOps Stack, which provides a template to structure complex ML projects for
    CI/CD integration with Git providers.
  prefs: []
  type: TYPE_NORMAL
- en: For further details regarding the MLOps Stack, you are encouraged to peruse
    *MLOps Stack* on GitHub ([https://github.com/databricks/mlops-stack](https://github.com/databricks/mlops-stack)).
  prefs: []
  type: TYPE_NORMAL
- en: We will not be covering the MLOps Stack in this chapter; instead, we will cover
    another approach to building your MLOps pipelines on Databricks based on utilizing
    what we have learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deeper and understand the fundamentals of MLOps and the various deployment
    paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of MLOps and deployment patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively manage MLOps, it’s essential to first familiarize ourselves with
    its underlying terminology and structure. This includes understanding the roles
    and responsibilities associated with various operational environments – namely,
    **development** (**dev**), staging, and **production** (**prod**). Let’s dissect
    what these environments signify in a practical MLOps framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within any ML project, there are three pivotal assets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code base**: This serves as the project’s blueprint. It contains all the
    source code related to data preprocessing, model training, evaluation, and deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data**: This includes the datasets that are used for training, validating,
    and testing the model. The quality and availability of this data directly influence
    the model’s efficacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trained model**: This is the culmination of your ML workflow, a model that
    has been trained, evaluated, and prepared for inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these assets goes through distinct phases – development, testing, and
    deployment – which are often segregated into separate environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Development environment (dev)**: This is where the initial code is written
    and tested. It’s generally the most accessible in terms of code and data but has
    the least stringent quality and testing requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Staging environment**: This serves as an intermediate space for additional
    testing and quality assurance before the project moves to production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production environment (prod)**: This is the most restrictive environment
    where the finalized assets are deployed. It has the highest quality and security
    requirements and is the least accessible for direct interactions. The following
    figure provides a visual representation of the key assets in MLOps, as well as
    the organizational structure of different environments. It illustrates the life
    cycle of these assets as they progress through development, testing, and, ultimately,
    production:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The various assets related to an ML project and its environments](img/B17875_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – The various assets related to an ML project and its environments
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure is courtesy of Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates the accessibility levels and quality requirements
    across these environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The various environments and their openness to accessibility](img/B17875_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – The various environments and their openness to accessibility
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure is courtesy of Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: With Databricks, you have the flexibility to structure these dev, staging, and
    prod environments in various ways to meet your project’s specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the theoretical separation of dev, staging, and
    prod environments serves as a guideline for best practices in MLOps. However,
    the real-world implementation can vary significantly based on your organizational
    needs, workflow, and technological capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will delve into multiple approaches for deploying
    Databricks workspaces to better align your dev, staging, and prod environments
    with your specific organizational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure showcases three distinct deployment patterns designed
    to set up your dev, QA, and prod environments effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Various Databricks environment deployment approaches](img/B17875_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Various Databricks environment deployment approaches
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source for the preceding figure: *The Big Book* *of MLOps*.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand these patterns one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating environment isolation in Databricks – multiple strategies for MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To devise a robust MLOps strategy, you need to consider not only the type of
    assets involved but also the environment where they reside – dev, staging, or
    prod. Each environment offers varying levels of accessibility, testing rigor,
    and data security, informed by organizational size, governance policies, and security
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple cloud accounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For large organizations governed by stringent rules and regulations, separating
    dev, staging, and prod environments across distinct cloud accounts is a common
    practice. In such a setup, each cloud account will host its own Databricks workspace.
    This architecture ensures isolation at both the cloud account and network levels,
    while also potentially increasing costs due to duplicated resources and data storage.
  prefs: []
  type: TYPE_NORMAL
- en: A single cloud account with multiple Databricks workspaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alternatively, smaller organizations or projects may opt for a single cloud
    account containing multiple Databricks workspaces. Each workspace is deployed
    into its own network and is isolated at that level. While more cost-effective,
    this approach still allows for sufficient isolation and can align well with organizational
    data governance policies.
  prefs: []
  type: TYPE_NORMAL
- en: A single cloud account with a single Databricks workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even within a single cloud account, Databricks provides the capability to enforce
    strict isolation between different roles and projects. Features such as RBAC,
    permissions, and native data governance tools such as Unity Catalog allow for
    effective segregation of access within a single workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having explored various approaches for organizing our dev, staging, and prod
    environments in Databricks, it’s time to turn our attention to another pivotal
    aspect of MLOps: the asynchronous nature of life cycles in ML projects. This stands
    in contrast to traditional software DevOps, where code and application updates
    usually happen in lockstep.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a deployed **large language model** (**LLM**) as a case in point. The
    sheer complexity and size of such models can make retraining a formidable challenge.
    You may find that while the data engineering code sees monthly iterations, the
    training code for the model itself remains static for an extended duration.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, think about a churn prediction model. Here, automatic retraining
    might be scheduled monthly using fresh datasets. If the newly trained model outperforms
    its predecessor, it immediately gets moved to production, all without requiring
    any changes to the existing code base.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating asynchronous life cycles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the incongruent update cycles for ML models and code, adopting strategies
    to manage these asynchronicities becomes imperative. You might employ techniques
    such as canary deployments for safer model rollouts, or opt for blue-green deployments
    to ensure smoother rollbacks. Automated monitoring systems and alert mechanisms
    are equally important, serving as early warning systems for model degradation
    or operational issues, thus allowing for quick remediation.
  prefs: []
  type: TYPE_NORMAL
- en: Fiscal and regulatory considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond technical aspects, MLOps also encompasses financial and compliance variables.
    Cost considerations can’t be overlooked – both for data storage and computational
    resources. Furthermore, data lineage is essential for keeping tabs on data movement
    through your pipeline, which not only aids in debugging but is invaluable for
    compliance and auditing purposes. Similarly, data versioning is indispensable
    when it comes to model reproducibility, an especially crucial feature for models
    undergoing frequent retraining.
  prefs: []
  type: TYPE_NORMAL
- en: With this nuanced understanding, we are better equipped to manage the complexities
    that arise from asynchronous updates in the ML life cycle, in the context of Databricks
    or any MLOps platform.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the various ML deployment paradigms that you can utilize.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ML deployment patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ultimate goal of any ML project is to get our ML model into production.
    Depending on what kind of use case we are catering to and how sophisticated our
    ML engineering team is, there are two broad ML deployment approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: The deploy models approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deploy code approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s understand these approaches one by one.
  prefs: []
  type: TYPE_NORMAL
- en: The deploy models approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model deployment workflow adheres to a structured methodology, beginning
    in a development environment where code for training the ML model is both crafted
    and refined. After the model undergoes training and the optimal version is ascertained,
    it is formally registered within a specialized model registry. This is followed
    by a battery of integration tests to evaluate its performance and reliability.
    Upon successfully passing these assessments, the model is first elevated to a
    staging environment for further validation. Once it meets all requisite criteria,
    it is then deployed into the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure offers a graphical depiction of this multi-stage approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The deploy models approach](img/B17875_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – The deploy models approach
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source for the preceding figure: *The Big Book* *of MLOps*.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, all the notebooks we have utilized so far focus on this
    particular deployment approach. It is a popular choice among companies and teams,
    especially when the ML team comprises individuals with a background in data science
    rather than traditional software engineering. This approach offers simplicity
    and serves as a great starting point for ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure showcases the entire end-to-end MLOps life cycle for the
    deploy models approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – The reference architecture and workflow for deploying a model
    from development to production using the deploy models approach](img/B17875_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – The reference architecture and workflow for deploying a model
    from development to production using the deploy models approach
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure is courtesy of Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this MLOps workflow, data engineers, data scientists, and ML engineers collaborate
    to perform various steps to ensure the successful development, deployment, and
    monitoring of ML models. Here is a breakdown of the responsibilities and tasks
    performed by each role:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data engineers**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect data from diverse sources such as databases, cloud storage, and sensors,
    ensuring reliability and quality
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleanse and preprocess the data, handling tasks such as removing duplicates,
    handling missing values, and transforming data into a format suitable for ML algorithms
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Store and manage data in a data warehouse or Delta Lake, ensuring accessibility
    and efficient utilization by data scientists and ML engineers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data scientists**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore and analyze the data to gain insights into its characteristics, identifying
    relevant patterns and relationships.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate and register features into feature tables for reuse.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop and train ML models, employing various algorithms and techniques to
    achieve accurate predictions and desired outcomes. All the model runs and experiments
    are logged automatically into the MLflow tracking server on Databricks.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate and assess the performance of the trained models using appropriate
    metrics and validation techniques.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the most suitable model for deployment based on performance and business
    requirements. The best model is then registered in the Model Registry as a candidate
    model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML engineers**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy ML models into production environments, making them available for making
    predictions or decisions in real time
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the performance of deployed models, ensuring they operate optimally
    and detect any anomalies or drift in their behavior
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update and retrain models as new data becomes available, maintaining model relevance
    and accuracy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All the notebooks that we covered as part of this book show this workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how the deploy models approach for MLOps works, let’s
    take a look at the deploy code approach.
  prefs: []
  type: TYPE_NORMAL
- en: The deploy code approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the deploy code approach, we version control not only the code to train the
    ML models but also the code to create the feature tables. This approach works
    well when you have strict regulations to separate data access in each environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data scientists develop code in the dev environment for feature engineering
    and model training. After a good candidate model is found, the dev branch code
    is committed to the staging branch, where automated unit tests are run. Again,
    we train the model in staging and perform our performance benchmark test. Once
    everything else looks good, we push the code to the main branch and the prod environment.
    Here, again, we retrain the model on the data in production:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The reference architecture and workflow for deploying a model
    from development to production using the deploy code approach](img/B17875_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – The reference architecture and workflow for deploying a model
    from development to production using the deploy code approach
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the source for the preceding figure: *The Big Book* *of MLOps*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The development process involves several stages, starting with the creation
    of code for the training model and feature engineering in the dev environment.
    The following figure showcases the deploy code workflow step by step in the dev
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – The deploy code workflow within the development environment](img/B17875_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – The deploy code workflow within the development environment
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Courtesy of Databricks
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand these steps one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data access points**: In development settings, data scientists typically
    have read-only permissions for production data. For compliance reasons, access
    may be limited to sanitized or duplicate versions of this data. A separate development
    storage is also available for read-write operations, facilitating experimental
    work.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Preliminary data investigation** (**PDI**): Data scientists use an iterative,
    interactive method for data exploration, leveraging notebooks, visual charts,
    and Databricks SQL. This step is often a standalone process and not usually part
    of deployable pipelines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Source code management**: All ML system code resides in a version control
    repository. Data scientists work on a development branch within this Git repository.
    Code can be synchronized with the Databricks workspace via Databricks Repos.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enhance feature datasets**: This pipeline ingests data from both raw and
    existing feature tables, outputting it to tables within Feature Store. This step
    includes two main tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Quality assurance**: Here, the data is validated to ensure it meets quality
    standards.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature construction**: Code is written or updated by data scientists to
    generate new features. Data may be pulled from Feature Store or other Lakehouse
    tables. These dev feature tables are used to build experimental models, and upon
    promotion to production, they update the corresponding production tables.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Management can be separate for feature pipelines if they are governed by different
    teams.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model training pipeline**: Data scientists build pipelines for model training
    on either read-only production data or development-specific data. These pipelines
    may utilize feature tables from both the dev and prod environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tuning and training**: The training process sources data from feature stores
    and varying levels of Lakehouse tables while logging parameters, metrics, and
    artifacts in the MLflow tracking system.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model storing**: After training and tuning have been finalized, the model
    is stored on the MLflow tracking server, capturing its association with the input
    data and the code.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When executed in staging or production, the model can be retrieved and registered
    for ongoing management and testing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Code finalization**: Once the development work on pipelines for features,
    training, and inference is complete, either the data scientist or the ML engineer
    commits these changes to the version control system from the development branch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s move on and understand the workflow in the staging environment. The staging
    environment serves as the final testing ground for ML code before it transitions
    to production. It encompasses comprehensive testing of all pipeline components,
    including model training and feature engineering. ML engineers employ a CI pipeline
    to execute unit and integration tests. Successful completion results in a release
    branch, triggering the CI/CD system to initiate the production stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram provides a step-by-step visual guide to the workflow
    within the staging environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The deploy code workflow within the staging environment](img/B17875_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – The deploy code workflow within the staging environment
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Courtesy of Databricks
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve into each of these steps in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initiate merge process**: The journey toward deployment commences when an
    ML engineer submits a merge request to the source control’s staging branch, often
    the “main” branch. This action sets off a **CI** workflow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execute unit tests**: Within the CI framework, the source code is automatically
    compiled, and unit tests are initiated. Should these tests not succeed, the merge
    request gets dismissed. Note that unit tests operate in isolation from data or
    external services.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Conduct integration** **tests**: Following the unit tests, the CI mechanism
    proceeds to administer integration tests. These tests validate the compatibility
    and functionality of all pipelines, which encompasses feature engineering, model
    training, inference, and monitoring. The staging environment is designed to mirror
    the production setting as closely as feasible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To economize on test duration, concessions may be made between the thoroughness
    of testing and execution speed. For instance, smaller data subsets could be used,
    or fewer training cycles run. Depending on the model’s intended application, comprehensive
    load testing might be conducted at this stage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After successful completion of integration tests in the staging branch, the
    code becomes eligible for production deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Commit to the staging branch**: Should the tests be successful, the code
    merges into the staging branch. In case of test failure, the CI/CD system alerts
    the relevant parties and updates the merge (or pull) request with the results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Periodic integration tests can be scheduled on the staging branch, especially
    if it receives frequent updates from multiple contributors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Establish a release branch**: Once the code has been validated and is ready
    for production deployment, the ML engineer forms a release branch. This action
    prompts the CI/CD system to refresh the production tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, let’s understand the production environment’s workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the production environment, ML engineers oversee the deployment of ML pipelines
    that handle feature computation, model training, and testing, as well as prediction
    publishing and performance monitoring. A retraining mechanism operates on production
    data to keep the model up to date and optimized. Performance benchmarks are rigorously
    evaluated to ensure that the new model meets or exceeds the set standards. Data
    scientists typically lack write and compute permissions in this environment but
    maintain visibility into test outcomes, logs, model artifacts, and pipeline statuses
    to aid in diagnosing any production issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram offers a comprehensive, step-by-step visualization of
    the workflow processes in the production environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – The deploy code workflow within the production environment](img/B17875_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – The deploy code workflow within the production environment
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Courtesy of Databricks
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through this workflow step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Refresh feature data**: This phase involves ingesting new data from production
    and updating tables in Feature Store. This can be either a batch or real-time
    process and can be invoked by different triggers, such as schedules or continuous
    runs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tuning and training**: The pipeline trains the production model on complete
    data and logs relevant metrics and parameters through autologging. Unlike the
    development stage, only top-performing algorithms and hyperparameters are considered
    to optimize time and performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model assessment**: The quality of the model is tested against a separate
    dataset from production. Test results and custom metrics are recorded.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model registration**: Upon successful training, the model is registered with
    an initial status of “None” in Model Registry.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated deployment**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compliance verification**: The pipeline performs mandatory compliance checks,
    which can include human review for complex evaluations. The results are logged.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance validation**: Models in the staging phase are compared against
    those in production to avert performance decay.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transition to production**: The model is advanced to the production stage,
    either manually or automatically, following satisfactory performance comparisons.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time serving**: MLflow enables the model to be deployed for low-latency
    use cases. The deployed model fetches features and returns predictions for each
    incoming request.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Batch or stream inference**: For higher throughput or latency requirements,
    batch or stream-based inferences are processed. Predictions can be saved in various
    storage options, including message queues such as Apache Kafka.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ongoing monitoring**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data feeding**: Logs from different inference types are ingested'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance and drift metrics**: Various quality and performance metrics
    are calculated'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric reporting**: Metrics are saved for further analysis and alerting purposes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retraining triggers**: Models can be automatically retrained based on a schedule
    or triggered by performance degradation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Automating the retraining process can be complex and may require manual intervention
    to resolve issues identified through monitoring, such as data drift or performance
    degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure summarizes the various steps that are performed in various
    environments for the deploy code approach to ModelOps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – The various steps performed in various environments for the
    deploy code approach to ModelOps](img/B17875_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – The various steps performed in various environments for the deploy
    code approach to ModelOps
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure is courtesy of Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, you have three environments. At the top, you have your Git workflow
    provider, which manages transitioning code from one environment to another. At
    the bottom, you have the data access layer or feature tables with data across
    different environments.
  prefs: []
  type: TYPE_NORMAL
- en: The important point to keep in mind here is that the trained model itself will
    have its own stages in Model Registry in the production environment. We retrain
    the model again in each environment and hydrate the respective feature tables
    based on the updated code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This approach may resonate more with individuals who have a background in traditional
    software engineering and are acquainted with DevOps principles. However, at the
    time of writing this book, there is no officially established method for implementing
    the deploy code approach of MLOps on the Databricks platform using the currently
    generally available tools. Although we discussed the concepts of the deploy code
    approach in this section, we won’t be covering this as part of the provided code.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps Stack is going to address this model deployment paradigm when it becomes
    generally available. We will update this book once the new feature is available.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s wrap up this chapter and summarize our key learnings.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of MLOps, the different deployment approaches
    on Databricks, and their reference architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a model deployment approach should be based on your team’s proficiency
    in implementing DevOps processes for ML projects. It’s important to acknowledge
    that there is no universal solution as each approach we have discussed has its
    own advantages and disadvantages. However, it is possible to create a customized
    hybrid ModelOps architecture within the Databricks environment.
  prefs: []
  type: TYPE_NORMAL
- en: By considering your team’s strengths and expertise, you can determine the most
    suitable deployment approach for your project. It’s essential to assess scalability,
    maintainability, ease of deployment, and integration with existing infrastructure.
    Evaluating these aspects will help you make an informed decision and optimize
    the model deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: In Databricks, you have the flexibility to tailor your ModelOps architecture
    to your project’s requirements. Leveraging the capabilities of Databricks, you
    can combine the best elements from different deployment approaches to create a
    customized and efficient workflow. This hybrid approach allows you to leverage
    the strengths of different methodologies while mitigating their limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the ultimate goal is to establish a robust and streamlined model deployment
    process that aligns with your team’s capabilities and project needs. By carefully
    considering your options and utilizing the resources in the Databricks environment,
    you can create a ModelOps architecture that maximizes efficiency and productivity
    for your ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following sources and their links to learn more about
    the topics that were covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Big Book of* *MLOps*: [bit.ly/big-book-of-mlops](http://bit.ly/big-book-of-mlops)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*MLOps Stack on* *GitHub*: [https://github.com/databricks/mlops-stack](https://github.com/databricks/mlops-stack)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Damji, J. S., Wenig, B., Das, T., and Lee, D. (2020). *Learning Spark* (2nd
    ed.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
