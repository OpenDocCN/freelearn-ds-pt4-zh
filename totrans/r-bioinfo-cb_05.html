<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Metagenomics</h1>
                </header>
            
            <article>
                
<p class="mce-root">The use of high throughput sequencing has turbocharged metagenomics from a field focused on studying variation in single sequences such as the 16S <strong>ribosomal RNA</strong> (<strong>rRNA</strong>) sequence to studying entire genomes of the many species that may be present in a sample. The task of identifying species or taxa and their abundances in a sample is computationally challenging and requires the bioinformatician to deal with the preparation of sequences, assignment to taxa, comparisons of taxa, and quantifications. Packages for this have been developed by a wide range of specialist laboratories that have created new tools and new visualizations specific to working with sequences in metagenomics.</p>
<p class="mce-root">In this chapter, we'll look at recipes to carry out some complex analyses in metagenomics with R:</p>
<ul>
<li>Loading in hierarchical taxonomic data using <kbd>phyloseq</kbd></li>
<li>Rarefying counts to correct for sample differences using <kbd>metacoder</kbd></li>
<li>Reading amplicon data from raw reads with <kbd>dada2</kbd></li>
<li>Visualizing taxonomic abundances with heat trees in <kbd>metacoder</kbd></li>
<li>Computing sample diversity with <kbd>vegan</kbd></li>
<li>Splitting sequence files into operational taxonomic units</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The sample data you'll need is available from this book's GitHub repository at <a href="https://github.com/PacktPublishing/R-Bioinformatics-Cookbook">https://github.com/PacktPublishing/R-Bioinformatics-Cookbook</a><a href="https://github.com/danmaclean/R_Bioinformatics_Cookbook">.</a> If you want to use the code examples as they are written, then you will need to make sure that this data is in a sub-directory of whatever your working directory is.</p>
<p class="mce-root"/>
<p>Here are the R packages that you'll need. Most of these will install with <kbd>install.packages()</kbd><em>;</em> others are a little more complicated:</p>
<ul>
<li><kbd>ape</kbd> </li>
<li> <kbd>Bioconductor</kbd>
<ul>
<li><kbd>dada2</kbd> </li>
<li><kbd>phyloseq</kbd></li>
</ul>
</li>
<li><kbd>corrplot</kbd></li>
<li><kbd>cowplot</kbd></li>
<li><kbd>dplyr</kbd></li>
<li><kbd>kmer</kbd></li>
<li><kbd>magrittr</kbd></li>
<li><kbd>metacoder</kbd></li>
<li><kbd>RColorBrewer</kbd></li>
<li><kbd>vegan</kbd></li>
</ul>
<p>Bioconductor is huge and has its own installation manager. You can install it with the following code:<a href="https://www.bioconductor.org/install/"/></p>
<pre>if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()</pre>
<div class="packt_infobox"><span> </span><span>Further information is available at the following link:</span><span> </span><span><a href="https://www.bioconductor.org/install/">https://www.bioconductor.org/install/</a>.</span></div>
<p>Normally, in R, a user will load a library and use the functions directly by name. This is great in interactive sessions but it can cause confusion when many packages are loaded. To clarify which package and function I'm using at a given moment, I will occasionally use the<span> </span><kbd>packageName::functionName()</kbd> convention. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox"><span><span>Sometimes, in the middle of a recipe, I'll interrupt the code so you can see some intermediate output or the structure of an object that's important to understand. Whenever that happens, you'll see a code block where each line begins with <kbd>##</kbd> (double hash) symbols. Consider the following command:<br/></span></span>
<p><kbd>letters[1:5]</kbd></p>
<span><span>This will give us the following output:</span></span>
<p class="mce-root"><kbd>## a b c d e</kbd></p>
<p><span> Note that the output lines are prefixed with <kbd>##</kbd>.</span></p>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading in hierarchical taxonomic data using phyloseq</h1>
                </header>
            
            <article>
                
<p>Metagenomics pipelines often start with large sequencing datasets that are processed in powerful and fully featured programs such as QIIME and <kbd>mothur</kbd>. In these cases, it is the results from these tools that we wish to prepare into reports or further specific analysis with R. In this recipe, we'll look at how we can get the output from QIIME and <kbd>mothur</kbd> into R. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this short recipe, we need the <kbd>phyloseq</kbd> package from <kbd>Bioconductor</kbd> and files in the <kbd>datasets/ch5</kbd> folder of this book's data repository. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Loading in hierarchical taxonomic data using <kbd>phyloseq</kbd> can be done using the following steps:</p>
<ol>
<li>Load the library:</li>
</ol>
<pre style="padding-left: 60px">library(phyloseq)</pre>
<p class="mce-root"/>
<ol start="2">
<li>Import the QIIME <kbd>.biom</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">biom_file &lt;- file.path(getwd(), "datasets", "ch5", "rich_sparse_otu_table.biom")
qiime &lt;- import_biom(biom_file)</pre>
<ol start="3">
<li>Access different parts of the <kbd>phyloseq</kbd> object:</li>
</ol>
<pre style="padding-left: 60px" class="r">tax_table(qiime)<br/>## Taxonomy Table:     [5 taxa by 7 taxonomic ranks]:
##          Rank1         Rank2               Rank3                   
## GG_OTU_1 "k__Bacteria" "p__Proteobacteria" "c__Gammaproteobacteria"
## GG_OTU_2 "k__Bacteria" "p__Cyanobacteria"  "c__Nostocophycideae"<br/><br/><br/>otu_table(qiime)<br/>## OTU Table:          [5 taxa and 6 samples]
##                      taxa are rows
##          Sample1 Sample2 Sample3 Sample4 Sample5 Sample6
## GG_OTU_1       0       0       1       0       0       0
## GG_OTU_2       5       1       0       2       3       1<br/><br/><br/>sample_data(qiime)<br/>##         BarcodeSequence  LinkerPrimerSequence BODY_SITE Description
## Sample1    CGCTTATCGAGA CATGCTGCCTCCCGTAGGAGT       gut   human gut</pre>
<ol start="4">
<li>Import the <kbd>mothur</kbd> data files:</li>
</ol>
<pre style="padding-left: 60px">mothur &lt;- import_mothur(
  mothur_list_file = file.path(getwd(), "datasets", "ch5", "esophagus.fn.list"),
  mothur_group_file = file.path(getwd(), "datasets", "ch5", "esophagus.good.groups"),
  mothur_tree_file = file.path(getwd(), "datasets", "ch5", "esophagus.tree")
)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Access the <kbd>otu</kbd> object in the <kbd>phyloseq</kbd> object:</li>
</ol>
<pre style="padding-left: 60px">otu_table(mothur)<br/>## OTU Table:          [591 taxa and 3 samples]
##                      taxa are rows
##          B C  D
## 9_6_14   2 0  0
## 9_6_25   1 0  0</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this straightforward recipe, we create objects and use accessor functions.</p>
<p>In <em>Step 1</em>, we load the <kbd>phyloseq</kbd> library as is customary.</p>
<p>Then, in <em>Step 2</em>, we define a file and use it as the first argument to the <kbd>import_biom()</kbd> function. This function can read the modern <kbd>biom</kbd> format output from QIIME in uncompressed JSON and compressed HDF5 forms. The type is detected automatically. We get back a fully populated <kbd>phyloseq</kbd> object.</p>
<p>In <em>Step 3</em>, we use the accessor functions to get the subsections of the object, the taxonomies with <kbd>tax_table()</kbd>, the OTU with <kbd>otu_table()</kbd>, and the sample data with <kbd>sample_data()</kbd>; these can all be used downstream easily as they are matrix-like objects.</p>
<p>We change track in <em>Step 4</em> and work with the <kbd>mothur</kbd> output. We need a list file and group file at least, which we specify as file paths in the <kbd>mothur_list_file</kbd> and <kbd>mothur_group_file</kbd> arguments. Here, we also specify a Newick format tree with the <kbd>mothur_tree_file</kbd> argument.</p>
<p>Again, we can use the <kbd>phyloseq accessor</kbd> function, <kbd>otu_table()</kbd>, to get the OTU. With the minimal <kbd>mothur</kbd> data, we specify here that we can't get the sample data or taxonomy table. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>If you have data generated from an older version of QIIME in the proprietary format, you can use the <kbd>import_qiime()</kbd> function. There is also an accessor function for the tree object if you attach one—<kbd>phy_tree()</kbd>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The websites and wiki pages of the QIIME and <kbd>mothur</kbd> programs do a great job of showing how to work with the data from their pipelines in R, particularly <kbd>mothur</kbd>. If you'd <span>like analysis ideas for this data, try them out.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Rarefying counts and correcting for sample differences using metacoder</h1>
                </header>
            
            <article>
                
<p>In metagenomics, a common question is to ask which species are present in a sample and what is the difference between two or more samples. Since samples can be made up of different amounts of observations—which, in a metagenomic sense, means the different amounts of reads that were generated—then the taxonomic richness of the sample will increase with the depth of sequencing. To assess the diversity of different taxa represented in samples fairly, metagenomicists will often perform rarefaction on the counts to ensure the samples all have constant depths. Essentially, this means reducing the sample depth down to whatever the lowest sample depth is. We'll perform rarefaction on OTU counts from a <kbd>biom</kbd> file in this recipe.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this recipe, you'll need the <kbd>metacoder</kbd> package and <kbd>datasets/ch5/rich_high_count_otu.biom</kbd>, which is an example <kbd>biom</kbd> file with six samples (labeled <kbd>Sample1</kbd>–<kbd>Sample6</kbd>) and five OTUs. This is, of course, a very small file, useful only to learn how the code works. Real metagenomic datasets are much larger.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Rarefying counts and correcting for sample differences using <kbd>metacoder</kbd> can be done using the following steps:</p>
<ol>
<li>Load the library and files:</li>
</ol>
<pre style="padding-left: 60px">library(metacoder)<br/>biom_file &lt;- file.path(getwd(), "datasets", "ch5", "rich_high_count_otu.biom")
taxdata &lt;- parse_qiime_biom(biom_file)</pre>
<ol start="2">
<li>Create a histogram of counts in the samples:</li>
</ol>
<pre style="padding-left: 60px">sample_ids &lt;- paste0("Sample", 1:6)<br/>hist_data &lt;- colSums(taxdata$data$otu_table[, sample_ids])
hist(hist_data, prob= TRUE, breaks=3)
lines(density(hist_data, adjust = 2), col="red")</pre>
<ol start="3">
<li>Call the rarefaction function and filter out low OTUs that may be created:</li>
</ol>
<pre style="padding-left: 60px">taxdata$data$rarefied_otus &lt;- rarefy_obs(taxdata, "otu_table", other_cols = TRUE)<br/>low_otu_index &lt;- rowSums(taxdata$data$rarefied_otus[, sample_ids]) &lt;=20<br/>taxdata &lt;- filter_obs(taxdata, "rarefied_otus", ! low_otu_index)
taxdata$data$rarefied_otus</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The overall pattern here is to get the file loaded, check the distribution of sample OTU counts, and apply rarefaction.</p>
<p>The first step is to get the library loaded and the example file imported. We do this by preparing the <kbd>rich_high_count_otu.biom</kbd> file, which we pass to the <kbd>parse_qiime()</kbd> function. This <kbd>metacoder</kbd> function simply reads in biome files and returns a <kbd>taxmap</kbd> object (another type of object for holding taxonomic data) that we can use in the <kbd>metacoder</kbd> functions.</p>
<p>Next, we wish to inspect the distribution of sample OTU counts, which we do by preparing a histogram. We make a character vector of sample names with the <kbd>paste()</kbd> function and use that to extract by named index the counts containing columns from within <kbd>otu_table</kbd>. This subset of columns is passed into the <kbd>colSums()</kbd> function, which gets the total counts for each sample in the <kbd>hist_data</kbd> vector. We can now create a histogram of those counts with <kbd>hist()</kbd> and add the density curve with <kbd>lines()</kbd> and the <kbd>density()</kbd> function on <kbd>hist_data</kbd>. Note that the resulting plot (in the following histogram) looks sparse because of the small number of samples in the small example file. The lowest numbers here give us an idea of the lowest sequenced sample. If there are stand-out low samples, it may be wise to remove those columns first:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-579 image-border" src="Images/bf69048e-291b-494e-8d88-a2f0db900a49.png" style="width:32.08em;height:22.58em;" width="639" height="449"/></p>
<p>Now, we can perform rarefaction. We use the <kbd>rarefy_obs()</kbd> function on <kbd>taxdata</kbd>; the second argument (with the <kbd>"otu_table"</kbd> value) is the name of the slot in the <kbd>taxdata</kbd> object that contains the OTU counts. As rarefaction reduces counts, we now need to remove any that have fallen too far across all samples. Hence, we use the <kbd>rowSums()</kbd> function and indexing by sample name on the <kbd>taxdata$data$rarefied_otus</kbd> object to get a logical vector indicating which OTUs have a total count lower than 20. Finally, we use the <kbd>filter_obs()</kbd> function <span>on <kbd>taxdata</kbd>; the second argument (</span>with the <kbd>"rarefied_otus"</kbd> value<span>) is the name of the slot in the <kbd>taxdata</kbd> object that contains the rarefied OTU counts. The <kbd>!</kbd> character is used to invert the logical vector of low OTUs because <kbd>filter_obs()</kbd> keeps the rows that pass and we wish to remove them. The final output from this is a rarefied set of OTU counts.</span></p>
<p><span>Note how, in the following output, OTU row 3 has been removed through low counts:</span></p>
<pre>## # A tibble: 4 x 8
##   taxon_id otu_id   Sample1 Sample2 Sample3 Sample4 Sample5 Sample6
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 ax       GG_OTU_1      24    1004     847    1979    1070    1170
## 2 ay       GG_OTU_2     872       0     704     500    1013     689
## 3 ba       GG_OTU_4     875    1144    1211     217       0    1180
## 4 ax       GG_OTU_5    1270     893     276     338     953       0</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We can estimate a useful count level with rarefaction curves. With these, the counts are randomly sampled at varying sample sizes and the number of species in the OTUs is counted. The point at which the number of species stops increasing lets us know we have enough reads and aren't getting any more value from dealing with extra counts. The <kbd>rarecurve()</kbd> <span>function </span>in the <kbd>vegan</kbd> package will do this for us. We provide an OTU table (note that this function needs the samples in rows so we must rotate our <kbd>taxdata</kbd> OTU table with the <kbd>t()</kbd> function). Then, we pass the minimum sample size for the <kbd>sample</kbd> argument. We use the <kbd>colSums()</kbd> and <kbd>min()</kbd> functions to get the lowest sample OTU count for this. The output looks like the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-580 image-border" src="Images/c1c83f1c-0ad4-4d48-9de7-319c2dc02658.png" style="width:38.08em;height:23.17em;" width="656" height="399"/></p>
<p><span>Here, we can clearly see that samples over 20,000 do not increase the richness of species.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reading amplicon data from raw reads with dada2</h1>
                </header>
            
            <article>
                
<p>A long-standing technique in metagenomics, particularly for those interested in bacterial microbiome studies, uses the sequencing of cloned copies (amplicons) of the 16S or 18S rRNA genes to create species profiles. These approaches can take advantage of lower throughput sequencing and the knowledge of the target sequence to classify each cloned sequence, simplifying the tricky task of assigning taxa to reads. In this recipe, we'll make use of the <kbd>dada2</kbd> package to run an amplicon analysis from raw <kbd>fastq</kbd> sequence reads. We'll perform quality control and OTU assignment steps and use an interesting machine learning method to classify sequences.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this recipe, we need the Bioconductor <kbd>dada2</kbd> package and the CRAN <kbd>cowplot</kbd> package. We'll use some metagenomic sequence reads from the Short Read Archive experiment <em>SRR9040914</em>, in which the water from a tidal marine lake at a tourist center was examined for species composition because people were tossing coins into it and making wishes. We will use twenty <kbd>fastq</kbd> files of 2,500 files each, each compressed and available in this book's repository at <kbd>datasets/ch5/fq</kbd>. This is a small subset of the full set of Illumina reads. We'll also need the <kbd>datasets/ch5/rdp_train_set_14.fa</kbd> <span>file, </span>which is one of the sets of 16S sequences maintained as training sets by the <kbd>dada</kbd> team. See more training sets at <a href="http://benjjneb.github.io/dada2/training.html">http://benjjneb.github.io/dada2/training.html</a><em>. </em></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Reading amplicon data from raw reads with <kbd>dada2</kbd> can be done using the following steps:</p>
<ol>
<li>Load the libraries and prepare a plot for each <kbd>fastq</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">library(dada2)
library(cowplot)

fq_dir &lt;- file.path(getwd(), "datasets", "ch5", "fq")
read_files &lt;- list.files(fq_dir, full.names = TRUE,  pattern = "fq.gz")

quality_plots &lt;- lapply(read_files, plotQualityProfile)<br/>plot_grid(plotlist = quality_plots)</pre>
<p class="mce-root"/>
<ol start="2">
<li>Quality trimming and dereplicating the files:</li>
</ol>
<pre style="padding-left: 60px">for (fq in read_files ){
  out_fq &lt;- paste0(fq, ".trimmed.filtered")
  fastqFilter(fq, out_fq, trimLeft=10, truncLen=250,
                      maxN=0, maxEE=2, truncQ=2,
                      compress=TRUE)
}

trimmed_files &lt;-  list.files(fq_dir, full.names = TRUE, pattern = "trimmed.filtered")
derep_reads &lt;- derepFastq(trimmed_files)</pre>
<ol start="3">
<li>Estimate the <kbd>dada2</kbd> model from a subset of samples:</li>
</ol>
<pre style="padding-left: 60px">trimmed_files &lt;-  list.files(fq_dir, full.names = TRUE, pattern = "trimmed.filtered")
derep_reads &lt;- derepFastq(trimmed_files)

dd_model &lt;- dada(derep_reads[1:5], err=NULL, selfConsist=TRUE)</pre>
<ol start="4">
<li>Infer the sequence composition of the samples using the parameters estimated in <em>Step 3</em>:</li>
</ol>
<pre style="padding-left: 60px">dada_all &lt;- dada(derep_reads, err=dd_model[[1]]$err_out, pool=TRUE)</pre>
<ol start="5">
<li>Assign taxonomy to the sequences:</li>
</ol>
<pre style="padding-left: 60px">sequence_tb &lt;-makeSequenceTable( dada_all )
taxonomy_tb &lt;- assignTaxonomy(sequence_tb, refFasta = file.path(getwd(), "datasets", "ch5", "rdp_train_set_14.fa")) 
taxonomy_tb[1, 1:6]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We first make a vector of file paths to all of the <kbd>fastq</kbd> files we wish to use by passing the <kbd>fq_dir</kbd> variable containing the <kbd>fastq</kbd> directory to the <kbd>list.files()</kbd> function. Then, we use the looping function, <kbd>lapply()</kbd>, to iterate over each <kbd>fastq</kbd> file path and run the <kbd>dada</kbd> function, <kbd>plotQualityProfile()</kbd>, with each file in turn. Each resulting plot object is saved into the list object, <kbd>quality_plots</kbd>. The <kbd>cowplot</kbd> function, <kbd>plot_grid()</kbd>, will plot all these in a grid when a list of plots is passed to the <kbd>plotlist</kbd> argument.</p>
<p class="mce-root"/>
<p>We get the plot in the following diagram. Note how the <kbd>fastq</kbd> quality scores are poor in the first 10 or so nucleotides and after about 260 nucleotides in. These will be the trimming points for the next step:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-581 image-border" src="Images/2dd7cd9b-9132-4592-bb02-73eec82e7e6c.png" style="width:96.83em;height:97.25em;" width="1162" height="1167"/></p>
<p class="mce-root"/>
<p>To carry out trimming, we run a loop over the <kbd>fastq</kbd> files in <kbd>read_files</kbd>. In each iteration of the loop, we create an output <kbd>fastq</kbd> filename, <kbd>out_fq</kbd>, by pasting the text <kbd>"trimmed.filtered"</kbd> onto the filename (since we will save the trimmed reads to a new file, rather than memory), then run the <kbd>fastqFilter()</kbd> <span>trimming function, </span>passing it the input filename, <kbd>fq</kbd>; the <kbd>out_fq</kbd> filename; and the trim parameters. At the end of this loop, we have a folder full of trimmed read files. The names of these are loaded into a vector with the <kbd>list.files()</kbd> function again—this time, matching only files with the <kbd>"trimmed.filtered"</kbd> <span>pattern.</span> All of these files are loaded into memory and dereplicated using the <kbd>derepFaistq()</kbd> function. We then calculate the parameters for the compositional inference step using the <kbd>dada()</kbd> function on a proportion of the files. We pass the first five sets of dereplicated files using indexing on the <kbd>derep_reads</kbd> object. By setting <kbd>err</kbd> to <kbd>NULL</kbd> and <kbd>selfConsist</kbd> to <kbd>TRUE</kbd>, we force <kbd>dada()</kbd> to estimate parameters from the data, saving the results in the <kbd>dd_model</kbd> variable.</p>
<p>We next run the <kbd>dada()</kbd> function on all of the data, setting the <kbd>err</kbd> parameter to that estimated previously and stored in <kbd>dd_model</kbd>. This step calculates the final sequence composition for the whole data. </p>
<p>Finally, we can make the sequence table with the results of the <kbd>dada()</kbd> function and use that to find OTUs using <kbd>assignTaxonomy()</kbd>. This function uses a naive Bayes classifier to assign sequences to taxa, based on the classification in the training set provided in the <kbd>rdp_train_set_14.fa</kbd> file<em>.</em> The output of this function is the classification of each sequence. A single row of the resulting table, <kbd>taxonomy_tb</kbd>, looks like this:</p>
<pre>## Kingdom Phylum <br/>## "Bacteria" "Cyanobacteria/Chloroplast" <br/>## Class Order <br/>## "Chloroplast" "Chloroplast" <br/>## Family Genus <br/>## "Bacillariophyta" NA</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The functions used in this recipe, <kbd>fastqFilter()</kbd> and <kbd>derepFastQ()</kbd>, also have variants for paired reads.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visualizing taxonomic abundances with heat trees in metacoder</h1>
                </header>
            
            <article>
                
<p>However we arrive at estimates of taxonomic abundance, it is usually helpful to create a visualization that summarizes the broad trends in the data in a single figure. One expressive and easy to interpret visualization is a heat tree. These are renderings of phylogenetic trees of the taxons of interest with data mapped onto visual elements of the render. For example, the number of times a taxon is seen may be expressed by changing the color or thickness of a tree branch. Different datasets can be easily compared by examining trees of each for differences. In this recipe, we'll construct a heat tree and customize it.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We need the input <kbd>.biom</kbd> file in <kbd>datasets/ch5/rich_high_count_otu.biom</kbd> and the <kbd>metacoder</kbd> and <kbd>RColorBrewer</kbd> <span>packages.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Visualizing taxonomic abundances with heat trees in <kbd>metacoder</kbd> can be done using the following steps:</p>
<ol>
<li>Load the libraries and input files:</li>
</ol>
<pre style="padding-left: 60px">library(metacoder)
library(RColorBrewer)
biom_file &lt;- file.path(getwd(), "datasets", "ch5", "rich_high_count_otu.biom")
taxdata &lt;- parse_qiime_biom(biom_file)</pre>
<ol start="2">
<li>Pass customization options to the tree-drawing function:</li>
</ol>
<pre style="padding-left: 60px">heat_tree(taxdata, <br/>    node_label = taxon_names, <br/>    node_size = n_obs, <br/>    node_color = n_supertaxa, <br/>    layout = "gem", <br/>    title = "sample heat tree", <br/>    node_color_axis_label = "Number of Supertaxa", <br/>    node_size_axis_label = "Number of OTUs", <br/>    node_color_range = RColorBrewer::brewer.pal(5, "Greens")<br/>)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Initially, we load the libraries and use the <kbd>parse_qiime_biom()</kbd> function to get a <kbd>metacoder taxmap</kbd> object from the <kbd>biom</kbd> file. </p>
<p>We then use the <kbd>heat_tree()</kbd> function to render the tree. It's enough to pass just the <kbd>taxdata taxmap</kbd> object—this will give a default tree—all of the other arguments specify customizations of the tree. <kbd>node_label</kbd> specifies the column in the <kbd>taxdata</kbd> object to use for the node labels; here, we use <kbd>taxon_names</kbd>, notably without enclosing quotes since the function uses non-standard evaluation in the same way that you may be familiar with from the <kbd>tidyverse</kbd> and <kbd>ggplot</kbd> functions. <kbd>node_size</kbd> controls node size based on the column given. Here, <kbd>n_obs</kbd> and <kbd>node_color</kbd> give the parameter that affects the variation of the color of the nodes (note that this isn't the set of colors—it's the things that should be colored the same/differently). Next, the <kbd>layout</kbd> argument tells the function how to spread the branches of the tree in the render. Of the next three argument titles, <kbd>node_color_axis</kbd> and <kbd>node_size_axis_label</kbd> are simply labels for the plot. Finally, <kbd>node_color_range</kbd> gets a vector of color identifiers that are used to draw with. Here, we use the <kbd>RColorBrewer</kbd> package function, <kbd>brewer.pal()</kbd>, which returns such things. Its first parameter is the number of colors to return, and the second the name of the palette to choose from. With all of these set, we get the following plot from our small input file:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-582 image-border" src="Images/e5c0ea89-48c3-4989-965e-33a3ae341008.png" style="width:40.83em;height:27.33em;" width="1040" height="697"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Computing sample diversity with vegan</h1>
                </header>
            
            <article>
                
<p>A common task in ecological and metagenomics studies is to estimate the species (or taxonomical) diversity within a sample or between samples to see which has more or less. There are multiple measures for both within and between sample diversity, including the Simpson and Bray indices. In this recipe, we'll look at functions that can go from the common OTU table and return measures of diversity.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We'll need the sample <kbd>.biom</kbd> input file,<span> </span><kbd>datasets/ch5/rich_high_count_otu.biom</kbd><span>, and the <kbd>vegan</kbd> package.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Computing sample diversity with <kbd>vegan</kbd> can be done using the following steps:</p>
<ol>
<li>Load in the libraries and prepare an OTU table from the sample file:</li>
</ol>
<pre style="padding-left: 60px">library(vegan)
biom_file &lt;- file.path(getwd(), "datasets", "ch5", "rich_high_count_otu.biom")
taxdata &lt;- metacoder::parse_qiime_biom(biom_file)
otu_table &lt;- taxdata$data$otu_table[, paste0("Sample", 1:6)]</pre>
<ol start="2">
<li>Calculate the alpha diversity:</li>
</ol>
<pre style="padding-left: 60px">alpha_diversity &lt;- diversity(otu_table, MARGIN=2, index="simpson")
barplot(alpha_diversity)</pre>
<ol start="3">
<li>Calculate the beta diversity:</li>
</ol>
<pre style="padding-left: 60px">between_sample &lt;- vegdist(t(otu_table), index = "bray")
<br/>between_sample_m &lt;- as.matrix(between_sample, ncol = 6)
corrplot::corrplot(between_sample_m, method="circle", type = "upper", diag = FALSE )</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The first step is very straightforward. Here, we use the <kbd>metacoder parse_qiime_biom()</kbd> function to load in our <kbd>biom</kbd> file and then use subsetting on the resulting <kbd>taxdata$data$otu_table</kbd> slot to extract a simple OTU table into <kbd>otu_table</kbd>.</p>
<p>We can now call the <kbd>diversity()</kbd> function from <kbd>vegan</kbd>. The <kbd>index</kbd> argument is set to <kbd>"simpson"</kbd>, so the function will use the Simpson index for within-sample diversity. The <kbd>MARGIN</kbd> argument tells the function whether the samples are in rows or columns: 1 for rows and 2 for columns. The <kbd>diversity()</kbd> function returns a named vector that is easy to visualize with the <kbd>barplot()</kbd> function, giving us this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-583 image-border" src="Images/48f8dc6b-3b3f-41a3-a48e-935484d9100e.png" style="width:34.42em;height:22.08em;" width="600" height="384"/></p>
<p>We can now run the between-sample diversity measure using the <kbd>vegdist()</kbd> function; again, the <kbd>index</kbd> argument sets the index to use, and here, we choose the Bray index. <kbd>vegdist()</kbd> expects the sample data to be rows, so we use the <kbd>t()</kbd> function to rotate <kbd>otu_table</kbd>. The resulting object is stored in <kbd>between_sample</kbd>— it's a pairwise correlation table and we can visualize it in <kbd>corrplot</kbd>. To do this, we need to convert it into a matrix with <kbd>as.matrix()</kbd>; the <kbd>ncol</kbd> argument should match the number of samples so that you get a column for each sample. The returned matrix, <kbd>between_sample_m</kbd>, can be passed to the <kbd>corrplot()</kbd> function to render it. By setting <kbd>method</kbd> to <kbd>circle</kbd>, <kbd>type</kbd> to <kbd>upper</kbd>, and <kbd>diag</kbd> to <kbd>false</kbd>, we get a plot with only the upper diagonal of the matrix, without the self-versus-self comparisons reducing redundancy in the plot.</p>
<p>The output looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-584 image-border" src="Images/3789c397-ec25-4c32-b3e1-778e1a903158.png" style="width:33.25em;height:30.75em;" width="530" height="490"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">See also...</h1>
                </header>
            
            <article>
                
<p>The correlation plot in this recipe explicitly <span>shows </span>correlations for a few samples but can become unwieldy on very large experiments. At this stage, you may want to consider PCA or some other multidimensional scaling approach.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Splitting sequence files into OTUs</h1>
                </header>
            
            <article>
                
<p>Perhaps the most common task with cleaned trimmed reads for a metagenomic shotgun experiment is to divide the sequences into OTUs. This can be achieved in many ways; in this recipe, we'll look at a method that splits sequences into subsequences of a given length and performs a type of hierarchical clustering on them to create groups.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The key package here is the <kbd>kmer</kbd> package and we'll use one of the sample <kbd>fastq</kbd> sequence files in the <kbd>datasets/ch5/fq</kbd><span> folder. We'll also make use of the <kbd>dplyr</kbd> and <kbd>magrittr</kbd> packages for convenience.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Splitting sequence files into OTUs can be done using the following steps:</p>
<ol>
<li>Load the data and compute the OTUs:</li>
</ol>
<pre style="padding-left: 60px">library(kmer)
library(magrittr)<br/>library(ape)<br/>seqs &lt;- ape::read.fastq(file.path(getwd(), "datasets", "ch5","fq", "SRR9040914ab.fq.gz")
otu_vec &lt;- otu(seqs, k = 6, threshold = 0.99 )<br/><br/><br/></pre>
<ol start="2">
<li class="mce-root">Count the frequency of each OTU cluster:</li>
</ol>
<pre style="padding-left: 60px">data.frame(<br/>    seqid = names(otu_vec), <br/>    cluster = otu_vec, <br/>    row.names = NULL) %&gt;% <br/>dplyr::group_by(cluster) %&gt;% <br/>dplyr::summarize(count = dplyr::n() )</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>After loading the libraries, we use the <kbd>read.fastq()</kbd> function from <kbd>ape</kbd> to get a <kbd>DNAbin</kbd> object representing the sequences. The key function, <kbd>otu()</kbd>, from the <kbd>kmer</kbd> package can use the <kbd>DNAbin seqs</kbd> object directly to create k-mers of the length, <kbd>k</kbd>, and perform hierarchical clustering on them. The threshold argument sets the OTU identity cut-off. This function returns a named vector in which the names are the sequence IDs and the value for each is the ID of the cluster it belongs to.</p>
<p>We can then use <kbd>otu_vec</kbd> to build an intermediate data frame with <kbd>data.frame</kbd>, using the names attribute to set a <kbd>seqid</kbd> column and putting the cluster membership into a column called <kbd>cluster</kbd>. We drop row names by setting <kbd>row.names</kbd> to <kbd>NULL</kbd>. We then use <kbd>magrittr</kbd> piping with the <kbd>%&gt;%</kbd> operator to group the data frame on clusters with <kbd>dplyr::group()</kbd> and create a summary data frame with <kbd>dplyr::summarize()</kbd>. By setting the count to the result of the <kbd>dplyr::n()</kbd> function, we get the number of times each cluster appeared in the named vector—or, how many reads were assigned into each cluster.</p>


            </article>

            
        </section>
    </div>



  </body></html>