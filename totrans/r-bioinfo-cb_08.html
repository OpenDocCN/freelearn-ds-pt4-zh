<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with Databases and Remote Data Sources</h1>
                </header>
            
            <article>
                
<p><span>Large-scale model organism sequencing projects, such as the <strong>Human Genome Project</strong> (<strong>HGP</strong>), or the 1,001 plant genomes sequencing projects have made a huge amount of genomics data publicly available. Likewise, open access data sharing by individual laboratories has made the raw sequencing data of genomes and transcriptomes widely available, too. Working with this data programmatically can mean having to parse or bring locally some seriously large or complicated files. As such, much effort has gone into making these resources as accessible as possible through APIs and other queryable interfaces, such as BioMart. In this chapter, we'll look at some recipes that will allow us to search for annotations without having to download whole genome files and find relevant information across databases. We'll look at how to pull raw reads from experiments from within your code and take the opportunity to look at how to apply quality control to this downloaded data.</span></p>
<p>The following recipes will be covered in this chapter:</p>
<ul>
<li>Retrieving gene and genome annotations from BioMart</li>
<li>Retrieving and working with SNPs</li>
<li>Getting gene ontology information</li>
<li>Finding experiments and reads from SRA/ENA</li>
<li>Performing quality control and filtering on high-throughput sequence reads</li>
<li>Completing read-to-reference alignment with external programs</li>
<li>Visualizing quality control plots of read-to-reference alignments</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The sample data you'll need is available from this book's GitHub repository at <a href="https://github.com/PacktPublishing/R-Bioinformatics-Cookbook">https://github.com/PacktPublishing/R-Bioinformatics-Cookbook</a><a href="https://github.com/danmaclean/R_Bioinformatics_Cookbook">.</a> If you want to use the code examples as they are written, then you will need to make sure that this data is in a sub-directory of whatever your working directory is.</p>
<p>Here are the R packages that you'll need. In general, you can install these with<span> </span><kbd>install.packages("package_name")</kbd>. The packages listed under <kbd>Bioconductor</kbd> need to be installed with the dedicated installer. That's described as follows in this section. If you need to do anything further, installation will be described in the recipes in which the packages are used:</p>
<ul>
<li><kbd>Bioconductor</kbd>
<ul>
<li><kbd>biomaRt</kbd></li>
<li><kbd>ramwas</kbd></li>
<li><kbd>ShortRead</kbd></li>
<li><kbd>SRAdb</kbd></li>
</ul>
</li>
</ul>
<p>Bioconductor is huge and has its own installation manager. You can install the manager with the following code:<a href="https://www.bioconductor.org/install/"/></p>
<pre>if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")</pre>
<p>Then, you can install the packages with this code:</p>
<div>
<pre>BiocManager::install("package_name")</pre></div>
<div class="packt_infobox"><span> </span><span>Further information is available at</span><span> </span><span><a href="https://www.bioconductor.org/install/">https://www.bioconductor.org/install/</a>.</span></div>
<div>
<p>Normally, in R, a user will load a library and use the functions directly by name. This is great in interactive sessions but it can cause confusion when many packages are loaded. To clarify which package and function I'm using at a given moment, I will occasionally use the<span> </span><kbd>packageName::functionName()</kbd> convention. </p>
<div class="packt_infobox"><span><span>Sometimes, in the middle of a recipe, I'll interrupt the code so you can see some intermediate output or the structure of an object that's important to understand. Whenever that happens, you'll see a code block where each line begins with <kbd>##</kbd> (double hash) symbols. Consider the following command:<br/></span></span>
<p><kbd>letters[1:5]</kbd></p>
<p><span>This will give us the following output:</span></p>
<p class="mce-root"><kbd>## a b c d e</kbd></p>
<p><span> Note that the output lines are prefixed with <kbd>##</kbd>.</span></p>
</div>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Retrieving gene and genome annotation from BioMart</h1>
                </header>
            
            <article>
                
<p>Once a draft of a genome sequence is prepared, a lot of bioinformatics work goes into finding the genes and other functional features or important loci that are in a genome. These annotations are numerous, difficult to perform and verify, typically take lots of expertise and time, and are not something we would want to repeat. So, genome project consortia will typically share their annotations in some way, often through public databases of some sort. BioMart is a common data structure and API through which annotation data is made available. In this recipe, we'll look at how to programmatically access such databases so we can get annotations for genes that we are interested in.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this recipe, we need the <kbd>Bioconductor</kbd> package called <kbd>biomaRt</kbd> and a working internet connection. We'll also need to know the BioMart server to connect to—there are about 40 worldwide, providing information about all sorts of things. The most widely accessed are the Ensembl databases and these are the default in these packages. You can see a list of all of the BioMarts here: <a href="http://www.biomart.org/notice.html">http://www.biomart.org/notice.html</a>. The code we'll develop will apply to any of these BioMarts with a little modification of table names and URLs, as appropriate.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Retrieving gene and genome annotation from BioMart can be done using the following steps:</p>
<ol>
<li>List marts in the selected example database—<kbd>gramene</kbd>:</li>
</ol>
<pre style="padding-left: 60px">library(biomaRt)<br/>listMarts(host = "ensembl.gramene.org")</pre>
<ol start="2">
<li class="mce-root">Create a connection to the selected mart:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">gramene_connection &lt;- useMart(biomart = "ENSEMBL_MART_PLANT", host = "ensembl.gramene.org")</pre>
<ol start="3">
<li>List datasets in that mart:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">data_sets &lt;-  listDatasets(gramene_connection)<br/>head(data_sets)<br/><br/>data_set_connection &lt;- useMart("atrichopoda_eg_gene", biomart = "ENSEMBL_MART_PLANT", host = "ensembl.gramene.org")</pre>
<ol start="4">
<li>List the datatypes we can actually retrieve:</li>
</ol>
<pre style="padding-left: 60px">attributes &lt;- listAttributes(data_set_connection)<br/>head(attributes)</pre>
<ol start="5">
<li>Get a vector of all chromosome names:</li>
</ol>
<pre style="padding-left: 60px">chrom_names &lt;- getBM(attributes = c("chromosome_name"), mart = data_set_connection )<br/>head(chrom_names)</pre>
<ol start="6">
<li>Create some filters to query data:</li>
</ol>
<pre style="padding-left: 60px">filters &lt;- listFilters(data_set_connection)<br/>head(filters)</pre>
<ol start="7">
<li>Get gene IDs on the first chromosome:</li>
</ol>
<pre style="padding-left: 60px">first_chr &lt;- chrom_names$chromosome_name[1]<br/>genes &lt;- getBM(attributes = c("ensembl_gene_id", "description"), filters = c("chromosome_name"), values = c(first_chr), mart = data_set_connection )head(genes)<br/>head(genes)</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The recipe revolves around doing a series of different lookups on the database, each time receiving a little more information to work with.</p>
<p>In <em>Step 1</em>, we use the <kbd>listMarts()</kbd> function to get a list of all of the BioMarts available at the specified host URL. Change the URL as appropriate when you want to connect to a different server. We get a dataframe of the available marts and use that information.</p>
<p>In <em>Step 2</em>, we create a connection object called <kbd>gramene_connection</kbd> with the <kbd>useMart()</kbd> function, passing in the server URL and the specific BioMart from <em>Step 1</em>.</p>
<p>In <em>Step</em> <em>3</em>, we pass <kbd>gramene_connection</kbd> to the <kbd>listDatasets()</kbd> function to retrieve the datasets in this <kbd>biomart</kbd>. Having selected one of the datasets (<kbd>atrichopda_eg_gene</kbd>), we can run the <kbd>useMart()</kbd> function to create a connection to the datasets in that <kbd>biomart</kbd>, naming the object <kbd>data_set_connection</kbd>.</p>
<p>In <em>Step 4</em>, we're nearly done working out which datasets we can use. Here, we use <kbd>data_set_connection</kbd>, which we created in the <kbd>listAttributes()</kbd> function, to get a list of the types of information we can retrieve from this dataset.</p>
<p>At <em>Step 5</em>, we finally get some actual information with the main function, <kbd>getBM()</kbd>. We set the <kbd>attributes</kbd> argument to the names of the data we want to get back; here, we get all values for <kbd>chromosome_name</kbd> and save them in a vector, <kbd>chrom_names</kbd>.</p>
<p>In <em>Step 6</em>, we set up filters—the restrictions on which values to receive. We first ask the <kbd>data_set_connection</kbd> object which filters we can use with the <kbd>listFilters()</kbd> function. Notice from the returned <kbd>filters</kbd> object that we can filter on <kbd>chromosome_name</kbd>, so we'll use that.</p>
<p>In <em>Step 7</em>, we set up a full query. Here, we intend to get all genes on the first chromosome. Note that we already have a list of chromosomes from <em>Step 5</em>, so we take the first element of the <kbd>chrom_names</kbd> object to use in the filter, saving it in <kbd>first_chr</kbd>. To perform the query, we use the <kbd>getBM()</kbd> function, with the <kbd>ensembl_gene_id</kbd> and <kbd>description</kbd> <span>attributes. W</span>e set the <kbd>filter</kbd> argument to the data type we wish to filter on and set the <kbd>values</kbd> argument to the value of the filter we wish to keep. We also pass the <kbd>data_set_connection</kbd> object as the BioMart to use. The resulting <kbd>genes</kbd> object contains <kbd>ensembl_gene_id</kbd> and descriptions on the first chromosome, as follows:</p>
<pre>## ensembl_gene_id           description<br/>## 1 AMTR_s00001p00009420    hypothetical protein <br/>## 2 AMTR_s00001p00015790    hypothetical protein <br/>## 3 AMTR_s00001p00016330    hypothetical protein <br/>## 4 AMTR_s00001p00017690    hypothetical protein <br/>## 5 AMTR_s00001p00018090    hypothetical protein <br/>## 6 AMTR_s00001p00019800    hypothetical protein</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Retrieving and working with SNPs</h1>
                </header>
            
            <article>
                
<p>SNPs and other polymorphisms are important genomic features and we often want to retrieve known SNPs in particular genomic regions. Here, we'll look at doing that in two different BioMarts that hold different types of data for their SNPs. In the first part, we'll use Gramene again to look at retrieving plant SNPs. In the second part, we'll look at how to find information on human SNPs in the main Ensembl database.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready </h1>
                </header>
            
            <article>
                
<p>As before, we'll need only the <kbd>biomaRt</kbd> package from <kbd>Bioconductor</kbd> and a working internet connection.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Retrieving and working with SNPs can be done using the following steps:</p>
<ol>
<li>Get the list of datasets, attributes, and filters from Gramene:</li>
</ol>
<pre style="padding-left: 60px">library(biomaRt)<br/>listMarts(host = "ensembl.gramene.org")<br/>gramene_connection &lt;- useMart(biomart = "ENSEMBL_MART_PLANT_SNP", host = "ensembl.gramene.org")<br/>data_sets &lt;- listDatasets(gramene_connection)<br/>head(data_sets)<br/>data_set_connection &lt;- useMart("athaliana_eg_snp", biomart = "ENSEMBL_MART_PLANT_SNP", host = "ensembl.gramene.org")<br/><br/>listAttributes(data_set_connection)<br/>listFilters(data_set_connection)</pre>
<ol start="2">
<li>Query for the actual SNP information:</li>
</ol>
<pre style="padding-left: 60px">snps &lt;- getBM(attributes = c("refsnp_id", "chr_name", "chrom_start", "chrom_end"), filters = c("chromosomal_region"), values = c("1:200:200000:1"), mart = data_set_connection )<br/>head(snps)<br/><br/></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><em>Step 1</em> will be familiar from the previous recipe's <em>steps 1</em> to <em>6</em>, in which we make the initial connections and get them to list the datasets, attributes, and filters we can use in this BioMart; it's the same pattern and is repeated every time we use the BioMart (until we get to know it by heart). </p>
<p>In <em>Step 2</em>, we use the information gathered to pull the SNPs in the region of interest. Again, we use the <kbd>getBM()</kbd> function and use a <kbd>chromosomal_region</kbd> <span>filter. </span>This allows us to specify a value describing a particular locus on the genome. The <kbd>value</kbd> argument gets a <kbd>Chromosome:Start:Stop:Strand</kbd> formatted string; specifically, <kbd>1:200:20000:1</kbd>, which will return all SNPs on chromosome 1, between nucleotide 200 and 20,000 on the positive strand (note that the positive DNA strand identifier is <kbd>1</kbd>, and the negative DNA strand identifier is <kbd>-1</kbd>). </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more... </h1>
                </header>
            
            <article>
                
<p>Finding human SNPs from Ensembl follows pretty much the same pattern. The only difference is that, because Ensembl is the default server, we can omit the server information from the <kbd>useMart()</kbd> functions. A similar query for humans would look like this:</p>
<pre>data_set_connection &lt;- useMart("hsapiens_snp", biomart = "ENSEMBL_MART_SNP")<br/>human_snps &lt;- getBM(attributes = c("refsnp_id", "allele", "minor_allele", "minor_allele_freq"), filters = c("chromosomal_region"), value = c("1:200:20000:1"), mart = data_set_connection)<br/><br/></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>If you have the <kbd>dbSNP refsnp ID</kbd> numbers, it is possible to query these directly using the <kbd>rnsps</kbd> package and the <kbd>ncbi_snp_query()</kbd> function. Simply pass this function a vector of valid <kbd>refsnp</kbd> IDs.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting gene ontology information</h1>
                </header>
            
            <article>
                
<p>The <strong>Gene Ontology</strong> (<strong>GO</strong>) is a very useful restricted vocabulary of annotation terms for genes and gene products that describe the biological process, molecular function, or cellular component of an annotated entity. As such, the terms are extremely useful as data in such things as gene-set enrichment analysis and other functional -omics approaches. In this recipe, we'll look at how we can prepare a list of gene IDs in a genomic region and get the GO IDs and descriptions for them all.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>As we're still using the <kbd>biomaRt</kbd> package, we'll just need that and a working internet connection.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Getting gene ontology information can be done using the following steps:</p>
<ol>
<li>Make connections to the Ensembl BioMart and find the appropriate attributes and filters:</li>
</ol>
<pre style="padding-left: 60px">library(biomaRt)<br/><br/>ensembl_connection &lt;- useMart(biomart = "ENSEMBL_MART_ENSEMBL")<br/> listDatasets(ensembl_connection)<br/><br/>data_set_connection &lt;- useMart("hsapiens_gene_ensembl", biomart = "ENSEMBL_MART_ENSEMBL")<br/><br/>att &lt;- listAttributes(data_set_connection)<br/>fil &lt;- listFilters(data_set_connection)<br/><br/></pre>
<ol start="2">
<li>Get a list of genes and, using their IDs, get their GO annotations:</li>
</ol>
<pre style="padding-left: 60px">genes &lt;- getBM(attributes = c("ensembl_gene_id"), filters = c("chromosomal_region"), value = c("1:200:2000000:1"), mart = data_set_connection)<br/><br/>go_ids &lt;- getBM(attributes = c("go_id", "goslim_goa_description"), filters = c("ensembl_gene_id"), values = genes$ensembl_gene_id, mart = data_set_connection )</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>As in the previous two recipes, <em>Step 1</em> involves finding the right values for the biomart, datasets, attributes, and filters.</p>
<p>In <em>Step 2</em>, we use the <kbd>getBM()</kbd> function to get <kbd>ensembl_gene_id</kbd> attributes in a particular chromosome region, saving the result in the <kbd>genes</kbd> object. We then use that function again using <kbd>ensembl_gene_id</kbd> as a filter and <kbd>go_id</kbd> and <kbd>goslim_goa_description</kbd> to get the actual GO annotation for just the selected genes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Finding experiments and reads from SRA/ENA</h1>
                </header>
            
            <article>
                
<p>The <strong>Short Read Archive</strong> (<strong>SRA</strong>) and the <strong>European Nucleotide Archive</strong> (<strong>ENA</strong>) are databases of records of raw high-throughput-DNA sequence data. Each is a mirrored version of the same sets of high-throughput sequence data, submitted by scientists in all fields of biology from all over the world. The free availability of high-throughput sequence data through these databases means that we can conceive of and execute new analyses on existing datasets. By performing searches on the databases, we can identify sequence data that we may wish to work with. In this recipe, we'll look at using the <kbd>SRAdb</kbd> package to query the datasets on SRA/ENA and retrieve the data for selected sets programmatically.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The two essential items for this recipe are the <kbd>SRAdb</kbd> package from <kbd>Bioconductor</kbd> and a working internet connection. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>Finding experiments and reads from SRA/ENA can be done using the following steps:</span></p>
<ol>
<li>Download the SQL database and make the connection:</li>
</ol>
<pre style="padding-left: 60px">library(SRAdb)<br/>sqlfile &lt;- file.path(system.file('extdata', package='SRAdb'), 'SRAmetadb_demo.sqlite')<br/>sra_con &lt;- dbConnect(SQLite(),sqlfile)</pre>
<p class="mce-root"/>
<ol start="2">
<li>Get the study information:</li>
</ol>
<pre style="padding-left: 60px">dbGetQuery(sra_con, "select study_accession, study_description from study where study_description like '%coli%' ")</pre>
<ol start="3">
<li>Get information on what is contained in that study:</li>
</ol>
<pre style="padding-left: 60px">sraConvert( c('ERP000350'), sra_con = sra_con )</pre>
<ol start="4">
<li>Get a list of the files available:</li>
</ol>
<pre style="padding-left: 60px">listSRAfile( c("ERR019652","ERR019653"), sra_con, fileType = 'sra' )</pre>
<ol start="5">
<li>Download the sequence files:</li>
</ol>
<pre style="padding-left: 60px">getSRAfile( c("ERR019652","ERR019653"), sra_con, fileType = 'fastq', destDir = file.path(getwd(), "datasets", "ch8") )</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>After loading the library, the first step sets up a local SQL file, called <kbd>sqlfile</kbd>. The file contains all of the information about the studies on SRA. In our example, we are using a small version from within the package itself (hence, we're extracting it with the <kbd>system.file()</kbd> function); the real file is &gt;50GB in size so we won't use it now but it can be retrieved using this replacement code: <kbd>sqlfile &lt;- getSRAdbfile()</kbd>. Once we have a <kbd>sqlfile</kbd> object, we can create a connection to the database with the <kbd>dbConnect()</kbd> function. We save the connection in the object named <kbd>sra_con</kbd> for reuse.</p>
<p>We then perform a query on the <kbd>sqlfile</kbd> <span>database </span>using the <kbd>dbGetQuery()</kbd> function. The first argument to this is the database file, and the second is a full query in SQL format. The query written is pretty self-explanatory; we're looking to return <kbd>study_accession</kbd> and <kbd>study_description</kbd> when the description contains the term <kbd>coli</kbd>. Much more complicated queries are possible—if you're prepared to write them in SQL. A tutorial on that is far beyond the scope of this recipe but there are numerous books dedicated to the subject; you should try <em>SQL for Data Analytics</em> by Upom Malik, Matt Goldwasser, and Benjamin Johnston, Packt Publishing: <a href="https://www.packtpub.com/big-data-and-business-intelligence/sql-data-analysis">https://www.packtpub.com/big-data-and-business-intelligence/sql-data-analysis</a>. The query returns a dataframe object that looks like this:</p>
<pre>## study_accession    study_description<br/>## ERP000350    Transcriptome sequencing of E.coli K12 in LB media in early exponential phase and transition to stationary phase<br/><br/></pre>
<p class="mce-root"/>
<p><em>Step 3</em> uses the accession number we extracted to get all of the related submission, sample, and experiment and run information related to the study with the <kbd>sraConvert()</kbd> function. This returns something like the following table—we can see the run IDs for this study, showing the actual files containing the sequence:</p>
<pre>##    study submission    sample experiment       run<br/>## 1 ERP000350 ERA014184 ERS016116 ERX007970 ERR019652 <br/>## 2 ERP000350 ERA014184 ERS016115 ERX007969 ERR019653 </pre>
<p>In <em>Step 4</em>, we use the <kbd>listSRAfile()</kbd> function to get the actual FTP address on the server for the specific sequences in a run. This provides the address of the SRA format file, a compressed and convenient format should you wish to know that:</p>
<pre>     run     study    sample experiment    ftp<br/>## 1 ERR019652 ERP000350 ERS016116 ERX007970 <span>ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR019/ERR019652/ERR019652.sra <br/></span>## 2 ERR019653 ERP000350 ERS016115 ERX007969 <span>ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR019/ERR019653/ERR019653.sra</span></pre>
<p>But in <em>Step 5</em>, we use the <kbd>getSRAfile()</kbd> function, setting the <kbd>fileType</kbd> argument to <kbd>fastq</kbd> to get the data in the standard <kbd>fastq</kbd> format. The files are downloaded into the folder specified in the <kbd>destDir</kbd> argument.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Don't forget to refresh the local SQL database regularly and to use the full version with this code: <kbd><span>sqlfile &lt;- getSRAdbfile()</span></kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Performing quality control and filtering on high-throughput sequence reads</h1>
                </header>
            
            <article>
                
<p>When we have a new set of sequence reads to work with, whether that be from a new experiment or a database, we need to perform a quality control step that will remove any sequence adapters, remove reads with a poor sequence, or trim down a poor sequence, as appropriate. In this recipe, we'll look at doing that within R using the <kbd>Bioconductor ShortRead</kbd> package. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You'll need the <kbd>ShortRead</kbd> package and you'll need to run the code for the <em>Finding experiments and reads from SRA/ENA</em> recipe in this chapter. Two files are created in the last step of that recipe and we'll use one of those. Once that code is run, the file should be in <kbd>datasets/ch8/ERRR019652.fastq.gz</kbd> of this book's repository.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Performing quality control and filtering on high-throughput sequence reads can be done using the following steps:</p>
<ol>
<li>Load the library and connect to a file:</li>
</ol>
<pre style="padding-left: 60px">library(ShortRead)<br/>fastq_file &lt;- readFastq(file.path(getwd(), "datasets", "ch8", "ERR019652.fastq.gz") )</pre>
<ol start="2">
<li>Filter reads with any nucleotide with quality lower than 20:</li>
</ol>
<pre style="padding-left: 60px">qualities &lt;- rowSums(as(quality(fastq_file), "matrix") &lt;= 20) <br/>fastq_file &lt;- fastq_file[qualities == 0] </pre>
<ol start="3">
<li>Trim the right-hand side of the read:</li>
</ol>
<pre style="padding-left: 60px">cut_off_txt &lt;- rawToChar(as.raw(40))<br/>trimmed &lt;- trimTails(fastq_file, k =2, a= cut_off_txt)</pre>
<ol start="4">
<li>Set up a custom filter to remove <em>N</em> and homomeric runs:</li>
</ol>
<pre style="padding-left: 60px">custom_filter_1 &lt;- nFilter(threshold=0)<br/>custom_filter_2 &lt;- polynFilter(threshold = 10, nuc = c("A", "T", "C", "G"))<br/>custom_filter &lt;- compose(custom_filter_1, custom_filter_2)<br/>passing_rows &lt;- custom_filter(trimmed)<br/>trimmed &lt;- trimmed[passing_rows]</pre>
<ol start="5">
<li>Write out the retained reads:</li>
</ol>
<pre style="padding-left: 60px">writeFastq(trimmed, file = file.path(getwd(), "datasets", "ch8", "ERR019652.trimmed.fastq.gzip"), compress = TRUE)</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The first step loads in the reads to a <kbd>ShortReadQ</kbd> object that represents the DNA read and its associated quality scores; this special object allows us to work on the sequence and qualities in one go.</p>
<p>The second step lets us find any reads where all quality scores are above 20. The code here is a little idiomatic so take some time to unpack it. First, we use the <kbd>quality()</kbd> function on <kbd>fastq_file</kbd> to extract the qualities alone, then pass that to the <kbd>as()</kbd> function, asking for a matrix. On that resultant matrix, we calculate the sum of each row with <kbd>rowSums()</kbd> and finally get a logical vector, <kbd>qualities</kbd>, from a comparison to see which of the <kbd>rowSums()</kbd> values is less than 20. In the next line, we use the <kbd>qualities</kbd> vector to subset <kbd>fastq_file</kbd> and remove the lower quality reads.</p>
<p>In <em>Step 3</em>, we trim the right-hand side of a read (to correct places where the read quality falls below a threshold). The main function here is <kbd>trimTails()</kbd>, which takes two arguments: <kbd>k</kbd>, the number of failing letters required to start trimming, and <kbd>a</kbd>, the letter to start trimming at. This, of course, means that the Phred numeric quality score we think of (such as in <em>Step 2</em>, where we just used 20) needs to be converted into its ASCII equivalent as per the text encoding of the quality score. That's what happens in the first line; the number 40 is converted into raw bytes with <kbd>as.raw()</kbd> and then into a character in <kbd>rawToChar()</kbd>. The resulting text can be used by storing it in the <kbd>cut_off_txt</kbd> <span>variable.</span></p>
<p><em>Step 4</em> applies some custom filters. The first <span>line</span>, <kbd>custom_filter_1</kbd>, creates a filter for sequences containing bases called <em>N</em>, the threshold argument allowing sequences to contain zero <em>N</em>s. The second, <kbd>custom_filter_2</kbd>, creates a filter for homopolymeric reads of homopolymers of length equal or longer than the threshold. The <kbd>nuc</kbd> argument specifies which nucleotides are to be considered. Once the filters are specified, we must join them into a single filter using the <kbd>compose()</kbd> function, which returns a filter function we call <kbd>custom_filter()</kbd> and then call on the trimmed object. It returns an <kbd>SRFFilterResult</kbd> object that can be used to subset the reads.</p>
<p>Finally, in <em>Step 5</em>, we use the <kbd>writeFastQ()</kbd> function to write the retained reads to a file.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Completing read-to-reference alignment with external programs</h1>
                </header>
            
            <article>
                
<p>The alignment of high-throughput reads is an important prerequisite for a lot of the recipes in this book, including RNAseq and SNP/INDEL calling. We looked at them in depth in <a href="ff091bc9-a002-4a63-b0fe-c0b9f9baf7d1.xhtml">Chapter 1</a>, <em>Performing Quantitative RNAseq</em>, and <a href="951477d3-d812-45e7-a324-0ffb1dc3ebf4.xhtml">Chapter 2</a>, <em>Finding Genetic Variants with HTS Data</em>, but we didn't cover how to actually perform alignment. We wouldn't normally do this within R; the programs needed to make these alignments are powerful and run from the command line as independent processes. But R can control these external processes, so we'll look at how to run an external process so you can control them from within an R wrapper script, ultimately allowing you to develop end-to-end analysis pipelines.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready...</h1>
                </header>
            
            <article>
                
<p>We'll use base R only in this recipe, so you don't need to install any packages. You will need the reference genome FASTA file in <kbd>datasets/ch8/ecoli_genome.fa</kbd> and the <kbd>datasets/ch8/ERR019653.fastq,gz</kbd> file that we created in the <em>Finding experiments and reads from SRA/ENA</em> recipe. <span>This recipe also requires a working copy of BWA and <kbd>samtools</kbd> on your system. The web pages for these pieces of software are at <a href="http://samtools.sourceforge.net/">http://samtools.sourceforge.net/</a> </span><span>and <a href="http://bio-bwa.sourceforge.net/">http://bio-bwa.sourceforge.net/</a>. If you have <kbd>conda</kbd> installed, you can install it with</span> <kbd>conda install -c bioconda bwa</kbd><span> and <kbd>conda install -c bioconda samtools</kbd>. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Complete read-to-reference alignment with external programs using the following steps:</p>
<ol>
<li>Set up the files and executable paths:</li>
</ol>
<pre style="padding-left: 60px">bwa &lt;- "/Users/macleand/miniconda2/bin/bwa"<br/>samtools &lt;- "/Users/macleand/miniconda2/bin/samtools"<br/>reference &lt;- file.path(getwd(), "datasets", "ch8", "ecoli_genome.fa")</pre>
<ol start="2">
<li>Prepare the <kbd>index</kbd> command and run:</li>
</ol>
<pre style="padding-left: 60px">command &lt;- paste(bwa, "index", reference)<br/>system(command, wait = TRUE)</pre>
<ol start="3">
<li>Prepare the <kbd>alignment</kbd> command and run:</li>
</ol>
<pre style="padding-left: 60px">reads &lt;- file.path(getwd(), "datasets", "ch8", "ERR019653.fastq.gz")<br/>output &lt;- file.path(getwd(), "datasets", "ch8", "aln.bam")<br/>command &lt;- paste(bwa, "mem", reference, reads, "|", samtools, "view -S -b &gt;", output)<br/>system(command, wait = TRUE)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The first step is simple: we just create a few variables that hold directory paths to the programs and files we will use. <kbd>bwa</kbd> and <kbd>samtools</kbd> hold the path to those programs on the system. Note that the paths on your system are almost definitely different. On Linux and macOS systems, you can find the path using the <kbd>which</kbd> command in the Terminal, on Windows machines, you can try the <kbd>where</kbd> command or equivalent.</p>
<p>In <em>Step 2</em>, we outline the basic pattern for running a system command. First, with the <kbd>paste()</kbd> function, we create the command as a string and save it in a variable called <kbd>command</kbd>. Here, we're preparing a command line that creates the index we need before performing read alignment with BWA. Then, we use the command as the first argument in the <kbd>system()</kbd> function, which actually executes the command. The command is started as a brand new process in the background and, by default, control is returned to the R script as soon as the process begins. If you intend to work immediately within R upon output from the background process, then you need to set the <kbd>system()</kbd> argument wait to <kbd>TRUE</kbd>, so that the R process only continues once the background process is complete.</p>
<p>In <em>Step 3</em>, we extend the pattern, creating reads and output variables and putting together a much longer command line, showing that any valid command line can be composed. We then repeat the <kbd>system</kbd> command. This process results in a final BAM file in <kbd>datasets/ch8/aln.bam</kbd>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visualizing the quality control of read-to-reference alignments</h1>
                </header>
            
            <article>
                
<p>Once the alignment of reads has been performed, it is usually wise to check the quality of the alignment and ensure that there is nothing unexpected about the pattern of reads and things such as expected insert distances. This can be especially useful in draft reference genomes where unusual alignments of high-throughput reads can reveal misassemblies of the reference or other structural rearrangements. In this recipe, we'll use a package called <kbd>ramwas</kbd>, which has some easily accessed plots we can create to assess alignment.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready...</h1>
                </header>
            
            <article>
                
<p>For this recipe, we'll need the prepared <kbd>bam_list.txt</kbd> and <kbd>sample_list.txt</kbd> information files in the <kbd>datasets/ch8</kbd> directory of this book's repository. We'll need the small <kbd>ERR019652.small.bam</kbd> and <kbd>ERR019653.small.bam</kbd> files from the same place.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Visualizing the quality control of read-to-reference alignments can be done using the following steps:</p>
<ol>
<li>Set up the parameters for the run:</li>
</ol>
<pre style="padding-left: 60px">library(ramwas)<br/>param &lt;- ramwasParameters( dirbam = ".", filebamlist = "bam_list.txt", <br/>                            dirproject = file.path(getwd(), "datasets", "ch8"), <br/>                            filebam2sample = "sample_list.txt")</pre>
<ol start="2">
<li>Perform the QC:</li>
</ol>
<pre style="padding-left: 60px">ramwas1scanBams(param)<br/>qc &lt;- readRDS(file.path(getwd(), "datasets", "ch8", "rds_qc", "ERR019652.small.qc.rds")$qc</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>View the plots:</li>
</ol>
<pre style="padding-left: 60px">plot(qc$hist.score1)<br/>plot(qc$bf.hist.score1)<br/>plot(qc$hist.length.matched)<br/>plot(qc$bf.hist.length.matched)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><em>Step 1</em> sets up a parameter-containing object using the <kbd>ramwasParameters()</kbd> function. We simply provide information files (<kbd>bam_list.txt</kbd> and <kbd>sample_list.txt</kbd>) saying where the BAM files to be used are and the samples they contain, respectively. The <kbd>dirproject</kbd> argument specifies the place on the system to which the results should be written. Note the results from this are written to disk; they don't come directly back to memory.</p>
<p><em>Step 2</em> uses the parameters to run the QC with the <kbd>ramwas1scanBams()</kbd> function. The results are written to disk so we load the resulting RDS file back in using the base R <kbd>readRDS()</kbd> function. The <kbd>qc</kbd> <span>object </span>has a lot of members that represent different quality control aspects of alignment.</p>
<p><em>Step 3</em> uses the generic <kbd>plot</kbd> function to create graphs of some of the QC statistics in the <kbd>qc</kbd> object.</p>


            </article>

            
        </section>
    </div>



  </body></html>