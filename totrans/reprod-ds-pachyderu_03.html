<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer023">
			<h1 id="_idParaDest-36"><em class="italic"><a id="_idTextAnchor037"/>Chapter 2</em>: Pachyderm Basics</h1>
			<p><strong class="bold">Pachyderm</strong> is a data science platform that enables data scientists to create an end-to-end machine learning workflow that covers the most important stages of a machine learning life cycle, starting from data ingestion all the way into production. </p>
			<p>If you are familiar with <strong class="bold">Git</strong>, a version control and life cycle system for code, you will find many similarities between the most important Git and Pachyderm concepts. Version control systems such as Git and its hosted version <strong class="bold">GitHub</strong> have become an industry standard for thousands of developers worldwide. Git enables you to keep a history of changes in your code and go back when needed. Data scientists deserve a platform that will let them track the versions of their experiments, reproduce results when needed, and investigate and correct bias that might crawl into one of the stages of the data science life cycle. Pachyderm provides benefits similar to Git that enable data scientists to reproduce their experiments and effortlessly manage the complete life cycle of the data science workflow.</p>
			<p>This chapter is intended to describe the basics of Pachyderm architecture and its main concepts. We will cover the following topics:</p>
			<ul>
				<li>Reviewing Pachyderm architecture</li>
				<li>Learning about version control primitives</li>
				<li>Discovering pipeline elements</li>
			</ul>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor038"/>Reviewing Pachyderm architecture</h1>
			<p>This section walks you through the distributed Pachyderm architecture and the internals of the Pachyderm solution. But before we dive into the nitty-gritty details of Pachyderm infrastructure, let's answer <a id="_idIndexMarker113"/>the question that a lot of you might have on your mind after reading the introduction—<em class="italic">why can't I use Git or any other version control system?</em> We'll address this question with Git in mind as it is the most popular and widely used software version control system, but all of the arguments apply to any other similar version control system for source code. After we review how Pachyderm is different <a id="_idIndexMarker114"/>and similar to Git, we will review the Pachyderm internals, Kubernetes, and container runtimes.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor039"/>Why can't I use Git for my data pipelines?</h2>
			<p><em class="italic">So, if Pachyderm </em><em class="italic"><a id="_idIndexMarker115"/></em><em class="italic">is similar to Git, why can't I store everything in Git rather </em><em class="italic"><a id="_idIndexMarker116"/></em><em class="italic">than have multiple tools that I have to learn and support?</em>, you might ask. </p>
			<p>While Git is a great open source technology for software engineers, it is not tailored to solve the reproducibility problem in data science. This is mainly because Git was designed to track versions of files rather than establishing connections between data, code, and parameters – the essential parts of each data model. </p>
			<p>While you can keep your code files in Git, doing so with the training data is not possible for the following reasons:</p>
			<ul>
				<li>File format support: Git is great for source code, such as text files, scripts, and so on, but it does not version non-binary files, such as image or video files. Git will detect that a binary file has changed, but it does not provide detailed information about the changes like it does in the case of text files. In addition, Git replaces the whole binary file, which makes for a dramatic increase in your repository when you update it.</li>
				<li>File size support: Often, image and video files are very large, and Git has a file size limitation of 100 MB per file. For best performance, Git recommends keeping your repository size below 1 GB, which might not be enough for some machine learning and deep learning projects.</li>
				<li>Cloning a large Git repository and all the associated history of each file can be problematic and takes a long time. </li>
			</ul>
			<p>Consider these limitations when you plan the tooling for your machine learning project. </p>
			<p>In addition to standard Git repositories, Git offers storage for large files, called <strong class="bold">Git Large File Storage</strong> (<strong class="bold">LFS</strong>). However, this option likely won't provide the needed functionality either. Git LFS has <a id="_idIndexMarker117"/>the following disadvantages for machine learning projects:</p>
			<ul>
				<li>Git LFS requires a server. You either need to set up and maintain your own Git LFS server or store it on a third-party online platform. Hosting your own server is problematic and requires additional support, and your preferred cloud provider might not be supported.</li>
				<li>Online Git LFS <a id="_idIndexMarker118"/>servers have a limited free tier. For projects that <a id="_idIndexMarker119"/>use high-resolution images or videos, this likely means that the limit will be reached fairly soon, and an upgrade to a paid version will be required.</li>
				<li>Designed for marketing <a id="_idIndexMarker120"/>department purposes, Git LFS seems like a good solution for marketing departments to store assets. Everyone else seems to prefer standard Git for their non-machine-learning projects.</li>
			</ul>
			<p>Now that you know why source code version control systems are not the best solutions to work with data, let's review Pachyderm architecture.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor040"/>Pachyderm architecture diagram</h2>
			<p>The following diagram <a id="_idIndexMarker121"/>shows the Pachyderm architecture:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="Images/B17085_02_001.jpg" alt="Figure 2.1 – Pachyderm architecture&#13;&#10;" width="1346" height="683"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Pachyderm architecture</p>
			<p>Let's review all the <a id="_idIndexMarker122"/>components of this diagram one by one. </p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/>Kubernetes</h2>
			<p>Kubernetes, an open source distributed workflow scheduling system, is at Pachyderm's core. If you are not familiar with Kubernetes, here is a quick overview.</p>
			<p><strong class="bold">Kubernetes</strong> is the latest stage of infrastructure evolution, which started with traditional hardware <a id="_idIndexMarker123"/>deployments where data center operators deployed all applications on one physical server. The applications would fight over resources, which resulted in poor performance in some applications. This didn't scale well and was difficult to manage. Later on, we saw the rise of <strong class="bold">virtual machine</strong> technology that made the management of physical servers much easier and allowed us to run multiple applications <a id="_idIndexMarker124"/>on one physical server with better scalability. Many data centers still run a lot of virtual machines. <strong class="bold">Containers</strong>, the technology on which <a id="_idIndexMarker125"/>Kubernetes is based, made the infrastructure even leaner by allowing containers to share common operating system components and removing the redundant copies of the same software parts. Containers can be easily ported from one cloud platform to another, provide better resource allocation, and overall are easier to manage and support.</p>
			<p>The function of Kubernetes is to manage containers and collections of containers called <strong class="bold">Pods</strong>. Today, Kubernetes is the <a id="_idIndexMarker126"/>dominant container technology in the open source world. Big tech companies <a id="_idIndexMarker127"/>might have their own types of schedulers – for example, Google has a project called <strong class="bold">Borg</strong>, but the majority of others are using one or <a id="_idIndexMarker128"/>another version of Kubernetes.</p>
			<p>Companies <a id="_idIndexMarker129"/>such as Amazon, Microsoft, and Google offer <a id="_idIndexMarker130"/>hosted versions of Kubernetes called, respectively, <strong class="bold">AWS Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), <strong class="bold">Microsoft Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), and <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>). Kubernetes <a id="_idIndexMarker131"/>can also be installed on-premises by using an <a id="_idIndexMarker132"/>automated software provisioning tool, such as Ansible, or using one of the specifically designed Kubernetes installation tools, such as <strong class="bold">Kubespray</strong>.</p>
			<p>Some companies, such as DigitalOcean, provide a Kubernetes installation and management system that can deploy Kubernetes to a cloud platform of your choice. </p>
			<p>Now that we know what Kubernetes is, let's review how Pachyderm leverages Kubernetes to provide a scalable, reproducible data science solution.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor042"/>Helm</h2>
			<p>The tool that deploys Pachyderm on top of Kubernetes is called <strong class="bold">Helm</strong>. Helm is an open source package manager that <a id="_idIndexMarker133"/>enables you to deploy any Kubernetes-compatible packages on a Kubernetes cluster. The main advantage of Helm is that it makes the deployment of <a id="_idIndexMarker134"/>packages easy and allows a lot of flexibility through Helm Charts. A <strong class="bold">Helm Chart</strong> is a YAML file where you specify parameters for your Pachyderm cluster. Pachyderm provides sample Helm Charts for all types of installations, including local and cloud deployments. You can find an example of the <strong class="source-inline">values.yaml</strong> that you can use with the Pachyderm Helm Chart for local Pachyderm installation at <a href="https://github.com/pachyderm/pachyderm/blob/master/doc/docs/master/reference/helm_values.md">https://github.com/pachyderm/pachyderm/blob/master/doc/docs/master/reference/helm_values.md</a>.</p>
			<p>In the <strong class="source-inline">values.yaml</strong> file, you can define everything you want to deploy with your Pachyderm cluster. For <a id="_idIndexMarker135"/>example, you can deploy the Pachyderm UI called Console, or not; you can configure storage buckets in your cloud provider; you can specify which version of Pachyderm to install, and so on.</p>
			<p>Helm Charts are stored <a id="_idIndexMarker136"/>in Artifact Hub, located <a id="_idIndexMarker137"/>at <a href="https://artifacthub.io/">https://artifacthub.io/</a>. You can find Pachyderm at <a href="https://artifacthub.io/packages/helm/pachyderm/pachyderm">https://artifacthub.io/packages/helm/pachyderm/pachyderm</a>. We will discuss the Pachyderm Helm Chart in more detail in <a href="B17085_05_Final_SB_Epub.xhtml#_idTextAnchor123"><em class="italic">Chapter 5</em></a>, <em class="italic">Installing Pachyderm on a Cloud Platform</em>.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/>Pachyderm internals</h2>
			<p>When you deploy Pachyderm <a id="_idIndexMarker138"/>locally or on a cloud platform of your choice, the <a id="_idIndexMarker139"/>underlying Kubernetes orchestrator deploys the following components:</p>
			<ul>
				<li><strong class="source-inline">Pachyderm Console</strong> (<strong class="source-inline">UI</strong>): The Pachyderm in-browser <strong class="bold">User Interface</strong> (<strong class="bold">UI</strong>) that provides <a id="_idIndexMarker140"/>basic Pachyderm functionality.</li>
				<li><strong class="source-inline">Pachd</strong>: The Pachyderm daemon container that is responsible for managing all the main Pachyderm logical operations.</li>
				<li><strong class="source-inline">postgress</strong>: Pachyderm requires a <strong class="bold">PostgreSQL</strong> instance to store metadata. PostgreSQL is a relational <a id="_idIndexMarker141"/>database popular in many open source and commercial solutions. While for testing purposes, an instance of PostgreSQL is shipped with the Pachyderm build, for production deployments and workloads, a separate cloud-based instance is strongly recommended.</li>
				<li><strong class="source-inline">etcd</strong>: In older versions of Pachyderm, <strong class="source-inline">etcd</strong> was the key-value store that stored information about nodes and administrative metadata. In versions of Pachyderm 2.0.0 and later, <strong class="source-inline">etcd</strong> only stores a small portion of metadata, while the majority is stored in PostgreSQL.</li>
			</ul>
			<p>Each of these components is deployed as a Kubernetes <strong class="bold">Pod</strong>—the smallest deployment unit of Kubernetes, which can <a id="_idIndexMarker142"/>have one or a collection of containers. Unlike in other applications, in Pachyderm, each Pod has only one container. </p>
			<p>In addition to these containers that are deployed during the installation, every <strong class="bold">Pachyderm pipeline</strong>, a computational <a id="_idIndexMarker143"/>component that trains your model, is deployed as a separate Kubernetes Pod. These Pods are called <strong class="bold">workers</strong>. You can deploy an unlimited number of pipelines <a id="_idIndexMarker144"/>in your Pachyderm cluster <a id="_idIndexMarker145"/>and assign a custom amount of resources to each Pod. Pods are completely isolated from each other.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor044"/>Other components</h2>
			<p><em class="italic">Figure 2.1</em> has the following components that we have not yet listed:</p>
			<ul>
				<li><strong class="bold">Load Balancer or Ingress Controller</strong>: A networking component of Kubernetes that exposes HTTP/HTTPS routes from the cluster to the outside world. By default, Kubernetes <a id="_idIndexMarker146"/>provides a <strong class="bold">ClusterIP</strong> service that enables the services inside the cluster to talk to each other. There is no <a id="_idIndexMarker147"/>way to access cluster services from the outside world. In a production deployment, such as on a cloud platform, you need to deploy either an ingress controller or a load balancer to be able to access your Pachyderm cluster from the internet. In a local, testing deployment, Pachyderm uses port-forwarding. <strong class="bold">NodePort</strong> is another way to configure external access. However, it is not <a id="_idIndexMarker148"/>recommended for production deployments and, therefore, has been omitted from the current description. Pachyderm provides a Traefik ingress option to install through its Helm Chart.</li>
				<li><strong class="bold">Pachyderm CLI or pachctl</strong>: <strong class="source-inline">pachctl</strong> is Pachyderm's <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>), which enables <a id="_idIndexMarker149"/>users to perform advanced <a id="_idIndexMarker150"/>operations with Pachyderm pipelines <a id="_idIndexMarker151"/>and configure infrastructure as needed. </li>
				<li><strong class="bold">API</strong>: Pachyderm supports programmatical access through a variety of language clients, including Python, Go, and <a id="_idIndexMarker152"/>others. Many users will find it useful to use one of the supported clients or even build their own.</li>
				<li><strong class="bold">Metadata storage</strong>: etcd collects administrative metadata, which has to be stored either on a local <a id="_idIndexMarker153"/>disk or a Kubernetes <strong class="bold">Persistent Volume</strong> (<strong class="bold">PV</strong>), which can be on a cloud platform or any other platform <a id="_idIndexMarker154"/>of your choice.</li>
				<li><strong class="bold">Object Storage</strong>: A location to store your <strong class="bold">Pachyderm File System</strong> (<strong class="bold">PFS</strong>) and the pipeline-related <a id="_idIndexMarker155"/>files. Pachyderm supports S3 object storage, such <a id="_idIndexMarker156"/>as MinIO, Google S3, Azure Blob storage, and Amazon S3. </li>
			</ul>
			<p>In <em class="italic">Figure 2.1</em>, the user accesses the Pachyderm cluster through a load balancer or ingress controller by using either the Pachyderm dashboard, <strong class="source-inline">pachctl</strong>, or through the API by using a language client. The user's call is sent to <strong class="source-inline">pachd</strong>. <strong class="source-inline">pachd</strong> processes the request and updates the state of the pipeline and <strong class="source-inline">etcd</strong> accordingly. After the pipeline runs, it outputs the result to the configured storage location, from where it can be accessed by other third-party applications. </p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor045"/>Container runtimes</h2>
			<p>A container runtime is software that executes the code in a container. There are many open source container runtimes, such as Docker, container, CRI-O, and others. Pachyderm supports the <a id="_idIndexMarker157"/>most popular container runtime—Docker. The fact that Pachyderm supports only one type of container runtime does not mean limited functionality. Most users will find Docker containers sufficient for all their needs.</p>
			<p>To run a container, you need to have a <strong class="bold">container image</strong> and a place to store it. A container image is a file that <a id="_idIndexMarker158"/>contains the immutable code of a program that can be executed by the container runtime. Container images are stored in <strong class="bold">container registries</strong>, storage or repositories that host container images. Some container registries are public <a id="_idIndexMarker159"/>and others are private. All Pachyderm components <a id="_idIndexMarker160"/>are packaged as container images and stored in <strong class="bold">Docker Hub</strong>, a public container registry, meaning that anyone can download those images free of charge.</p>
			<p>All your Pachyderm pipelines will have to be packaged as Docker containers and stored in a container registry. Your organization might require you to store your packages in a private container registry to protect the intellectual property of your pipeline. But if you are working on an open source or student project, you can store your container images in Docker Hub for free. </p>
			<p>Now that we have <a id="_idIndexMarker161"/>reviewed the Pachyderm architecture, let's look at Pachyderm version control primitives that are foundational in understanding how Pachyderm works.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor046"/>Learning about version control primitives</h1>
			<p>As we saw in a previous section, Pachyderm bears similarities to the code version control software <a id="_idIndexMarker162"/>called Git. If you have participated in developing an open source project before, you <a id="_idIndexMarker163"/>are likely familiar with Git through the use of a hosted Git version, such as GitHub, GitLab, or Gerrit. </p>
			<p>Like with Git, you store your data in <strong class="bold">repositories</strong>, upload your data with a <strong class="bold">commit</strong>, and can have multiple <strong class="bold">branches</strong> in your repositories. Pachyderm stores the history of your commits and allows you to track changes or the <strong class="bold">history</strong> of your data back to its origins. </p>
			<p>Pachyderm version control primitives enable you to go back in time and run your pipeline against previous versions of your changes. This can be very powerful in tracking bias and mistakes that crawl into your pipeline changes.</p>
			<p>Let's look at these concepts in more detail.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor047"/>Repository</h2>
			<p>A Pachyderm <strong class="bold">repository</strong> is a filesystem in which you store your data and where you store versions <a id="_idIndexMarker164"/>of your data. Pachyderm <a id="_idIndexMarker165"/>distinguishes between the <strong class="bold">input repository</strong> and <strong class="bold">output repository</strong>.</p>
			<p>An input repository is a filesystem that you create and where you upload your data for further processing <a id="_idIndexMarker166"/>either by using a CLI, the UI, or automatically by using the API.</p>
			<p>An output repository is a filesystem that Pachyderm creates automatically, which has the same name as <a id="_idIndexMarker167"/>the pipeline. This is the location where Pachyderm outputs the results of the calculation and from where results can be exported for serving as a model. </p>
			<p>An important distinction from the Git repositories is that Pachyderm repository history is stored in a centralized location, which eliminates the risk of merge conflicts. Therefore, there is no equivalent of the <strong class="source-inline">.git</strong> history file in Pachyderm.</p>
			<p>The following diagram demonstrates how internal and external repositories work within a pipeline:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="Images/B17085_02_002.jpg" alt="Figure 2.2 – Pachyderm input and output repositories&#13;&#10;" width="1299" height="571"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Pachyderm input and output repositories</p>
			<p>The preceding diagram <a id="_idIndexMarker168"/>shows a simple use case with just one pipeline. However, your <a id="_idIndexMarker169"/>workflow could be much more complex with multiple pipelines being intertwined and working together to achieve the needed result. </p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor048"/>Branch</h2>
			<p>A <strong class="bold">branch</strong> is a Pachyderm line of development that tracks a set of changes within a repository. By default, a <a id="_idIndexMarker170"/>repository has no branches. The <strong class="bold">master branch</strong> that we will use in this book is just <a id="_idIndexMarker171"/>an example of how you can name your primary branch in Pachyderm, and it's not enforced. Typically, you create a branch on a repository when you <a id="_idIndexMarker172"/>upload initial data in the repository. You can create multiple branches to organize your work, but you likely won't use them as extensively as in Git. Often, just one branch is enough for all the work. You could create a separate branch for a different experiment. All branches are visible to all users, so you won't be able to have a local branch, experiment in it, and then merge it with the master branch as you do in Git.</p>
			<p>To create a branch, run the following: </p>
			<p class="source-code"><strong class="bold">pachctl create branch &lt;repo&gt;@&lt;branch&gt;</strong></p>
			<p>The repository <a id="_idIndexMarker173"/>must exist before you can create a branch.</p>
			<p>To confirm <a id="_idIndexMarker174"/>that a branch was created, run the following:</p>
			<p class="source-code"><strong class="bold">pachctl list branch &lt;repo&gt;</strong></p>
			<p>The final part of this section will deal with the commit concept.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor049"/>Commit</h2>
			<p>A <strong class="bold">commit</strong> is a single <a id="_idIndexMarker175"/>immutable changeset of your data. For example, when new data is received from a <a id="_idIndexMarker176"/>streaming platform, such as <strong class="bold">Kafka</strong>, it is written to the Pachyderm repository as <a id="_idIndexMarker177"/>a new commit with an individual hash identifier. </p>
			<p>The following diagram illustrates the concept of the commit on a branch in Pachyderm:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="Images/B17085_02_003.jpg" alt="Figure 2.3 – A diagram showing Pachyderm commits&#13;&#10;" width="1276" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – A diagram showing Pachyderm commits</p>
			<p>Data changes are stored in the repository history. The <strong class="bold">HEAD</strong>, or the <strong class="source-inline">HEAD</strong> commit of the branch, moves every <a id="_idIndexMarker178"/>time a new commit is submitted to the repository. As soon as the new data is submitted, the pipeline runs the code against these latest changes unless a different behavior is explicitly configured.</p>
			<p>Now that we know the most important Pachyderm version control primitives, let's learn about the Pachyderm pipeline in more detail.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor050"/>Discovering pipeline elements</h1>
			<p>This section walks you <a id="_idIndexMarker179"/>through the main Pachyderm pipeline concepts. The <strong class="bold">Pachyderm Pipeline System</strong> (<strong class="bold">PPS</strong>) is the centerpiece <a id="_idIndexMarker180"/>of Pachyderm functionality.</p>
			<p>A Pachyderm <strong class="bold">pipeline</strong> is a sequence of computational tasks that data undergoes before it outputs <a id="_idIndexMarker181"/>the final result. For example, it could be a series of image processing tasks, such as labeling each image or applying a photo filter. Or it could be a comparison between two datasets or a finding similarities task. </p>
			<p>A pipeline performs the following three steps:</p>
			<ol>
				<li>Downloads the data from a specified location.</li>
				<li>Applies the transformation steps specified by your cod.</li>
				<li>Outputs the result to a specified location.</li>
			</ol>
			<p>The following diagram shows how a Pachyderm pipeline works:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="Images/B17085_02_004.jpg" alt="Figure 2.4 – Pachyderm pipeline&#13;&#10;" width="1478" height="740"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Pachyderm pipeline</p>
			<p>Each Pachyderm pipeline has an input and output repository. An input repository is a filesystem within Pachyderm where it is being placed from an outside source. This is the location inside <a id="_idIndexMarker182"/>of the pipeline Pod under the <strong class="source-inline">/pfs</strong> directory. The data can either be pulled by a pipeline or pushed to the input repository by the data source system. After the data goes through the transformation, it is placed into the output Pachyderm repository, which is located in the /<strong class="source-inline">pfs/out</strong> directory. From the output repository, the results can be further consumed by third-party applications or other pipelines.</p>
			<p>Every time new data lands in the input repository, Pachyderm starts a pipeline <strong class="bold">job</strong>. The job processes the newly arrived data and places the results in the output repository.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor051"/>Types of pipelines</h2>
			<p>A machine learning pipeline continuously performs the tasks that the data scientists coded on the new data. For example, you might want to compare or join two types of files together or apply certain parameters to them. To simplify these tasks, Pachyderm offers predefined pipelines that can do those things automatically. </p>
			<p>Pachyderm offers <a id="_idIndexMarker183"/>the following types of pipelines:</p>
			<ul>
				<li>Standard pipeline</li>
				<li>Cron pipeline</li>
				<li>Spout pipeline</li>
				<li>Service pipeline</li>
			</ul>
			<p>To create a <a id="_idIndexMarker184"/>pipeline, you need to write a <strong class="bold">pipeline specification</strong>. A pipeline <a id="_idIndexMarker185"/>specification is a file in the <strong class="bold">YAML Ain't Markup Language</strong> (<strong class="bold">YAML</strong>) or <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) format that <a id="_idIndexMarker186"/>describes what the pipeline needs to do. We will talk about pipeline specifications in more detail in the next chapter. </p>
			<h3>Standard pipelines</h3>
			<p>A <strong class="bold">standard pipeline</strong>, or simply a pipeline, is the most straightforward way to schedule work in a Pachyderm cluster. This type of pipeline is triggered when new data lands in the Pachyderm input <a id="_idIndexMarker187"/>repository. The pipeline spawns or resumes a <a id="_idIndexMarker188"/>Kubernetes Pod to run your code against the newly landed data and outputs the result to the output repository. The output repository is created automatically and has the same name as the pipeline.</p>
			<p>The simplest standard pipeline must have the following components:</p>
			<ul>
				<li><strong class="source-inline">name</strong>: A descriptive name for your pipeline. It is good practice to give your pipeline a name that <a id="_idIndexMarker189"/>reflects the task it accomplishes. For example, if your code analyzes the sentiment of users' feeds on social media, you might want to call it <strong class="source-inline">sentiment-analysis</strong> or something similar.</li>
				<li><strong class="source-inline">transform</strong>: The transformation section of a pipeline contains the information about the Docker image that the pipeline needs to pull and use and the code that it needs to run in the pipeline container. For example, if you are doing sentiment analysis, you probably will <a id="_idIndexMarker190"/>use <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) tools such as <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>) or <strong class="bold">Stanford CoreNLP</strong>. Therefore, you can use <a id="_idIndexMarker191"/>either an already available Docker image for those <a id="_idIndexMarker192"/>tools or build your own custom one.</li>
				<li><strong class="source-inline">input</strong>: An input repository from which the pipeline grabs data for processing. You will need to upload the data to an input repository either through the CLI, UI, or API. </li>
			</ul>
			<p>The following text is an example of a pipeline specification that performs sentiment analysis. The pipeline is called <strong class="source-inline">sentiment-analyzer</strong>, uses the Docker image called <strong class="source-inline">ntlk-image</strong>, downloads data from the input repository called <strong class="source-inline">feed</strong>, and then runs the code stored in the file called <strong class="source-inline">transform_code.py</strong>:</p>
			<p class="source-code"># sentiment-analyzer.yml</p>
			<p class="source-code">---</p>
			<p class="source-code">pipeline:</p>
			<p class="source-code">  name: sentiment-analyzer</p>
			<p class="source-code">transform:</p>
			<p class="source-code">  image: nltk-image</p>
			<p class="source-code">  cmd:</p>
			<p class="source-code">  - python3</p>
			<p class="source-code">  - "/transform_code.py"</p>
			<p class="source-code">input:</p>
			<p class="source-code">  pfs:</p>
			<p class="source-code">    repo: feed</p>
			<p class="source-code">    glob: "/"</p>
			<p>The <strong class="source-inline">glob</strong> parameter <a id="_idIndexMarker193"/>defines how to break down data <a id="_idIndexMarker194"/>into processable chunks that can be spread across multiple Pachyderm workers for better performance. We will discuss this parameter in more detail in the next section.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can either put your transformation code in a file like in the example above or specify it directly in the <strong class="source-inline">cmd</strong> field. See <a href="B17085_03_Final_SB_Epub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a>, <em class="italic">Pachyderm Pipeline Specification</em>, for more information.</p>
			<h3>Cron pipelines</h3>
			<p>A <strong class="bold">Cron</strong> pipeline, or cron, is a pipeline that runs a specific task periodically. If you are familiar with the <strong class="bold">UNIX</strong> software <a id="_idIndexMarker195"/>utility cron, the Cron pipeline uses a <a id="_idIndexMarker196"/>similar logic—it runs periodically according to the specified schedule and performs the same task every time. Unlike the standard pipeline that runs every time new data arrives in the input repository, the Cron pipeline runs according to a schedule. You might want to use this pipeline to periodically scrape a website or a table.</p>
			<p>A Cron pipeline specification would look very similar to the standard pipeline apart from the input section. You need to specify the time interval in the input section for your pipeline.</p>
			<p>The following text is an example of a Cron pipeline in YAML format that scrapes a website called <strong class="source-inline">my-website</strong> once a day at midnight:</p>
			<p class="source-code"># scrape-my-website.yml</p>
			<p class="source-code">---</p>
			<p class="source-code">pipeline:</p>
			<p class="source-code">  name: scrape-my-website</p>
			<p class="source-code">transform:</p>
			<p class="source-code">  image: web-scraper</p>
			<p class="source-code">  cmd:</p>
			<p class="source-code">  - python3</p>
			<p class="source-code">  - "/scraper.py"</p>
			<p class="source-code">input:</p>
			<p class="source-code">  cron:</p>
			<p class="source-code">    repo: mywebsite</p>
			<p class="source-code">    spec: "@daily"</p>
			<p>All the <a id="_idIndexMarker197"/>information about the website URL will <a id="_idIndexMarker198"/>go into your <strong class="source-inline">scraper.py</strong> file.</p>
			<h3>Spout pipelines</h3>
			<p>A <strong class="bold">s</strong><strong class="bold">pout</strong> pipeline, or spout, as the name suggests, is designed to stream data from an outside source, such <a id="_idIndexMarker199"/>as a publish/subscribe <a id="_idIndexMarker200"/>messaging system. In your infrastructure, you might have multiple message queue systems. With a Pachyderm spout, you can consolidate inputs from all of these systems and inject them into a Pachyderm spout pipeline. The spout code is not triggered by any event; instead, it runs continuously, listening for new messages. </p>
			<p>Unlike in a standard or a Cron pipeline, spouts do not have an input repository, but instead, they listen on a specified address. The port and host can be specified either as an <strong class="source-inline">env</strong> variable in the pipeline specification or inside of the container. </p>
			<p>Often, a spout is used together with a Pachyderm <strong class="bold">Service</strong> pipeline to expose the results of the pipeline because the data in the spout's output repository cannot be accessed with standard Pachyderm commands.</p>
			<p>The following text is an <a id="_idIndexMarker201"/>example of a spout pipeline in YAML format that connects to an Amazon <strong class="bold">Simple Queue Service</strong> (<strong class="bold">SQS</strong>) host and applies a Python script, <strong class="source-inline">myscript.py</strong>, to the messages:</p>
			<p class="source-code">---</p>
			<p class="source-code">pipeline:</p>
			<p class="source-code">  name: spout-pipeline</p>
			<p class="source-code">spout: {}</p>
			<p class="source-code">transform:</p>
			<p class="source-code">  cmd:</p>
			<p class="source-code">  - python3</p>
			<p class="source-code">  - myscript.py</p>
			<p class="source-code">  image: 'myimage'</p>
			<p class="source-code">  env:</p>
			<p class="source-code">    HOST: sqs-host</p>
			<p class="source-code">    TOPIC: mytopic</p>
			<p class="source-code">    PORT: '1111'</p>
			<p>The <strong class="source-inline">spout</strong> section is left <a id="_idIndexMarker202"/>empty, but this is the place where you can <a id="_idIndexMarker203"/>combine the spout with a service pipeline to expose the results of the pipeline to the outside world.</p>
			<h3>Service pipelines</h3>
			<p>A <strong class="bold">service</strong> pipeline, or service, is a pipeline that can expose your output repository to the outside world. Unlike <a id="_idIndexMarker204"/>other pipelines, it does not perform any <a id="_idIndexMarker205"/>modifications to your data. The only function of this pipeline is to serve the results for your pipeline as an API, for example, in the form of a dashboard. A service pipeline is often combined with a spout pipeline and is sometimes called <strong class="bold">spout-service</strong>. However, it can use <a id="_idIndexMarker206"/>the results from any other pipeline output repository as its own input repository to expose the results.</p>
			<p>A pipeline specification for this pipeline misses the transformation section. The following text is an example of a service pipeline in YAML format:</p>
			<p class="source-code">---</p>
			<p class="source-code">pipeline:</p>
			<p class="source-code">  name: expose-service</p>
			<p class="source-code">input:</p>
			<p class="source-code">  pfs:</p>
			<p class="source-code">    glob: "/"</p>
			<p class="source-code">    repo: final-results</p>
			<p class="source-code">service:</p>
			<p class="source-code">  external_port: 31111</p>
			<p class="source-code">  internal_port: 8888</p>
			<p>Next, we'll <a id="_idIndexMarker207"/>see how Pachyderm works <a id="_idIndexMarker208"/>with datum.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor052"/>Datum</h2>
			<p><strong class="bold">A datum</strong> is the smallest piece of data that a Pachyderm worker can process. Folders and files in your input repository can be processed all as one datum or as multiple datums. This behavior is <a id="_idIndexMarker209"/>controlled by the <strong class="source-inline">glob</strong> parameter in the pipeline specification. Pachyderm processes each datum independently. If you have more than one worker running your pipeline, Pachyderm can schedule datums to run on separate workers for faster processing and, in the end, all datums are merged back together in the output repository.</p>
			<p class="callout-heading">Datums</p>
			<p class="callout">This is the term used by the Pachyderm folks. Yes, usually, the plural is data, but <strong class="bold">datums</strong> is how it appears throughout Pachyderm's documentation!</p>
			<p>Breaking down your data into multiple datums provides the following advantages:</p>
			<ul>
				<li>Improves performance by scaling your pipeline to multiple Pachyderm workers</li>
				<li>Enables you to <a id="_idIndexMarker210"/>process only specific files and folders</li>
			</ul>
			<p>For example, say you have a repository with the following folder structure:</p>
			<p class="source-code">data</p>
			<p class="source-code">├── folder1</p>
			<p class="source-code">│   ├── file1</p>
			<p class="source-code">│   ├── file2</p>
			<p class="source-code">│   └── file3</p>
			<p class="source-code">├── folder2</p>
			<p class="source-code">│   ├── file1</p>
			<p class="source-code">│   └── subfolder1</p>
			<p class="source-code">│       └── file1</p>
			<p class="source-code">└── folder3</p>
			<p class="source-code">    ├── subfolder1</p>
			<p class="source-code">    │   ├── file1</p>
			<p class="source-code">    │   └── file2</p>
			<p class="source-code">    └── subfolder2</p>
			<p class="source-code">        ├── file1</p>
			<p class="source-code">        └── file2 </p>
			<p>For this folder structure, you can set the following types of glob patterns:</p>
			<ul>
				<li><strong class="source-inline">/</strong>: Process all files <a id="_idIndexMarker211"/>and folders as a single datum. This single datum includes all files and folders in the <strong class="source-inline">/</strong> directory.</li>
				<li><strong class="source-inline">/*</strong>: Process each folder as a separate datum. The resulting number of datums is three, including the following:<p class="source-code"><strong class="bold">/folder1</strong></p><p class="source-code"><strong class="bold">/folder2</strong></p><p class="source-code"><strong class="bold">/folder3</strong></p></li>
				<li><strong class="source-inline">/*/*</strong>: Process each filesystem object on the <strong class="source-inline">/*/*</strong> level as a separate datum. The resulting number of datums is seven, including the following:<p class="source-code">/folder1/file1</p><p class="source-code">/folder1/file2</p><p class="source-code">/folder1/file3</p><p class="source-code">/folder2/file1</p><p class="source-code">/folder2/subfolder1</p><p class="source-code">/folder3/subfolder1</p><p class="source-code">/folder3/subfolder2</p></li>
				<li><strong class="source-inline">/*/*/*</strong>: Process each filesystem on the third level. The resulting number of datums is five, including the following:<p class="source-code"><strong class="bold">/folder2/subfolder1/file1</strong></p><p class="source-code"><strong class="bold">/folder3/subfolder1/file1</strong></p><p class="source-code"><strong class="bold">/folder3/subfolder1/file2</strong></p><p class="source-code"><strong class="bold">/folder3/subfolder2/file1</strong></p><p class="source-code"><strong class="bold">/folder3/subfolder2/file2</strong></p></li>
				<li><strong class="source-inline">/**</strong>: Process each file, folder, and subfolder as a separate datum. The resulting number is 15, which includes all files and folders listed in the preceding code extract:<p class="source-code"><strong class="bold">/folder1/</strong></p><p class="source-code"><strong class="bold">/folder1/file1</strong></p><p class="source-code"><strong class="bold">/folder1/file2</strong></p><p class="source-code"><strong class="bold">/folder1/file3 </strong></p><p class="source-code"><strong class="bold">/folder2/</strong></p><p class="source-code"><strong class="bold">/folder2/file1/</strong></p><p class="source-code"><strong class="bold">/folder2/subfolder1/</strong></p><p class="source-code"><strong class="bold">...</strong></p></li>
				<li><strong class="source-inline">/&lt;filesystem&gt;/*</strong>: Process only the files and folders that match the naming pattern. For example, if you decide to process only the data in <strong class="source-inline">folder1</strong>, you will set your glob <a id="_idIndexMarker212"/>pattern to <strong class="source-inline">/folder1/*</strong>. Similarly, you can set just the first few letters of a directory name as a glob pattern.</li>
			</ul>
			<p>These are the most commonly used glob patterns. Extended glob patterns are available with the Pachyderm's <strong class="source-inline">ohmyglob</strong> library. For more information, see <a href="https://github.com/pachyderm/ohmyglob">https://github.com/pachyderm/ohmyglob</a>.</p>
			<h3>Scaling your pipeline with datums</h3>
			<p>One of the biggest advantages of breaking your data into multiple datums is the ability to scale your work <a id="_idIndexMarker213"/>across multiple pipeline workers to significantly <a id="_idIndexMarker214"/>improve the performance and the processing time of your pipeline. By default, Pachyderm deploys one worker node for each pipeline, but you can increase this number as needed by specifying the <strong class="source-inline">parallelism_spec</strong> parameter in the pipeline specification.</p>
			<p>The following diagram demonstrates how datums are scaled across multiple workers:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="Images/B17085_02_005.jpg" alt="Figure 2.5 – Scaling a pipeline&#13;&#10;" width="1122" height="446"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Scaling a pipeline</p>
			<p>In the preceding diagram, the input data is broken down into three workers, which simultaneously start <a id="_idIndexMarker215"/>the processing. After all of the datums are <a id="_idIndexMarker216"/>processed, they are merged into the final output result. It is important to emphasize that datums only exist inside of a Pachyderm pipeline. They cannot be accessed, mounted, or modified in any way.</p>
			<h3>Pachyderm inputs</h3>
			<p>In Pachyderm, you likely will create multiple repositories to store training data, testing data, and parameters. Pachyderm provides an automated way to combine files in different repositories in <a id="_idIndexMarker217"/>the pipeline to be processed together through the use of <strong class="bold">inputs</strong>. If you have used SQL before, you might find that these types of inputs remind you of SQL operators. However, there is one major difference that distinguishes SQL operators from Pachyderm inputs. While in SQL you can create matching pairs of rows in each table, Pachyderm inputs work on the file level only. This means that you can only combine files or directories inside Pachyderm repositories based on their names rather than the contents of the files. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This section provides examples of how you can create various Pachyderm inputs. These examples are for your reference only and you might want to use some of them in your future Pachyderm work after you install and configure Pachyderm. We will cover the installation and setup in the <em class="italic">Installing Pachyderm locally</em> and <em class="italic">Deploying Pachyderm on a cloud platform</em> sections of <a href="B17085_03_Final_SB_Epub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a>, <em class="italic">Pachyderm Pipeline Specification</em>.</p>
			<h4>Cross inputs</h4>
			<p>A <strong class="source-inline">cross</strong> input, or cross, is an input in which each file from one repository is combined with a file in the other <a id="_idIndexMarker218"/>repository. The set of matched files <a id="_idIndexMarker219"/>is determined by the glob pattern, and all that data is visible to the pipeline code at the time of the pipeline run.</p>
			<p>The input part of your pipeline file in YAML format might look like this:</p>
			<p class="source-code">…</p>
			<p class="source-code">input:</p>
			<p class="source-code">  cross:</p>
			<p class="source-code">  - pfs:</p>
			<p class="source-code">      glob: "/*"</p>
			<p class="source-code">      repo: data</p>
			<p class="source-code">  - pfs:</p>
			<p class="source-code">      glob: "/"</p>
			<p class="source-code">      repo: parameters</p>
			<p class="source-code">…</p>
			<p>For example, you have two Pachyderm repositories, <strong class="source-inline">data</strong> and <strong class="source-inline">parameters</strong>, with the following structure and files:</p>
			<p class="source-code">data</p>
			<p class="source-code">├── image1.png</p>
			<p class="source-code">├── image2.png</p>
			<p class="source-code">├── image3.png</p>
			<p class="source-code">└── image4.png</p>
			<p class="source-code">parameters</p>
			<p class="source-code">├── param1.csv</p>
			<p class="source-code">└── param2.csv</p>
			<p>If you create a pipeline that creates a cross-product of these repositories with a glob pattern set to <strong class="source-inline">/*</strong>, you will <a id="_idIndexMarker220"/>get eight datums in total—each file from <a id="_idIndexMarker221"/>the <strong class="source-inline">data</strong> repository will be combined with each file in the <strong class="source-inline">parameters</strong> repository. This is how the files will be processed:</p>
			<p class="source-code">data@4e5897...:/image1.png, parameters@d1a0da...:/param1.csv</p>
			<p class="source-code">data@4e5897...:/image2.png, parameters@d1a0da...:/param1.csv      </p>
			<p class="source-code">data@4e5897...:/image3.png, parameters@d1a0da...:/param1.csv      </p>
			<p class="source-code">data@4e5897...:/image4.png, parameters@d1a0da...:/param1.csv       </p>
			<p class="source-code">data@4e5897...:/image1.png, parameters@d1a0da...:/param2.csv       </p>
			<p class="source-code">data@4e5897...:/image2.png, parameters@d1a0da...:/param2.csv      </p>
			<p class="source-code">data@4e5897...:/image3.png, parameters@d1a0da...:/param2.csv      </p>
			<p class="source-code">data@4e5897...:/image4.png, parameters@d1a0da...:/param2.csv      </p>
			<p>In the preceding output, each pair of files and each line is one datum.</p>
			<h4>Union inputs</h4>
			<p>A union input, or union, is an <a id="_idIndexMarker222"/>input that enables you to combine datums from one repository with datums in <a id="_idIndexMarker223"/>another repository. The total number of datums is the sum of datums in all repositories. If we take the same repository structure as described in the <em class="italic">Cross inputs</em> section and, instead of creating a cross-product, just add them to each other by using the <strong class="source-inline">union</strong> input, we will have a total of five datums. </p>
			<p>The <strong class="source-inline">input</strong> part of your pipeline file in YAML format might look like this:</p>
			<p class="source-code">…</p>
			<p class="source-code">input:</p>
			<p class="source-code">  union:</p>
			<p class="source-code">  - pfs:</p>
			<p class="source-code">      glob: "/*"</p>
			<p class="source-code">      repo: data</p>
			<p class="source-code">  - pfs:</p>
			<p class="source-code">      glob: "/"</p>
			<p class="source-code">      repo: parameters</p>
			<p class="source-code">…</p>
			<p>The following output demonstrates the list of datums for a pipeline with the <strong class="source-inline">/*</strong> glob pattern set for both repositories:</p>
			<p class="source-code">data@4e58978ca1304f16a0a5dfe3715363b4:/image1.png      </p>
			<p class="source-code">data@4e58978ca1304f16a0a5dfe3715363b4:/image2.png</p>
			<p class="source-code">data@4e58978ca1304f16a0a5dfe3715363b4:/image3.png </p>
			<p class="source-code">data@4e58978ca1304f16a0a5dfe3715363b4:/image4.png</p>
			<p class="source-code">model@1de402b7be004a289f6d7185b2329b21:/</p>
			<p>Your code <a id="_idIndexMarker224"/>processes each file individually and recognizes <a id="_idIndexMarker225"/>only one file with every run, ignoring all other files.</p>
			<h4>Join inputs</h4>
			<p>A <strong class="bold">join input</strong>, or join, is a Pachyderm <a id="_idIndexMarker226"/>input that enables you to match <a id="_idIndexMarker227"/>pairs of files based on a specific naming pattern defined by a <strong class="bold">capturing group</strong>. A capturing group is a regular expression that is defined <a id="_idIndexMarker228"/>within parentheses. The capturing group must be specified in the <strong class="source-inline">join_on</strong> parameter in your Pachyderm pipeline. </p>
			<p>Pachyderm <a id="_idIndexMarker229"/>separates <strong class="bold">inner joins</strong> and <strong class="bold">outer joins</strong>. The difference between the two is that an inner join matches <a id="_idIndexMarker230"/>pairs of files, skipping files that do not match the specified pattern. An outer join acts the same way as an inner join, but also includes files that do not match the pattern in the pipeline run. If this sounds a bit confusing, the following example should clarify how the pipeline works.</p>
			<p>Let's say you have two repositories, <strong class="source-inline">data</strong> and <strong class="source-inline">parameters</strong>, and the <strong class="source-inline">parameters</strong> repo has the following structure:</p>
			<p class="source-code">NAME                 TYPE SIZE</p>
			<p class="source-code">/param-0101-2021.txt file 4B</p>
			<p class="source-code">/param-0102-2021.txt file 4B</p>
			<p class="source-code">/param-0103-2021.txt file 4B</p>
			<p class="source-code">/param-0104-2021.txt file 4B</p>
			<p class="source-code">/param-0105-2021.txt file 4B</p>
			<p>The <strong class="source-inline">data</strong> repository has the following structure:</p>
			<p class="source-code">NAME                TYPE SIZE</p>
			<p class="source-code">/data-0101-2021.txt file 2B</p>
			<p class="source-code">/data-0102-2021.txt file 2B</p>
			<p class="source-code">/data-0103-2021.txt file 2B</p>
			<p class="source-code">/data-0104-2021.txt file 2B</p>
			<p class="source-code">/data-0105-2021.txt file 2B</p>
			<p class="source-code">/data-0106-2021.txt file 2B</p>
			<p class="source-code">/data-0107-2021.txt file 2B</p>
			<p>In your pipeline, you <a id="_idIndexMarker231"/>might want to match the files by their date. To do so, you would have to specify a <a id="_idIndexMarker232"/>capturing group of <strong class="source-inline">$1</strong> and a glob pattern, <strong class="source-inline">/</strong>. Here is an example of a pipeline specification in YAML format that matches these file paths:</p>
			<p class="source-code">---</p>
			<p class="source-code"> pipeline:</p>
			<p class="source-code">   name: describe</p>
			<p class="source-code"> input:</p>
			<p class="source-code">   join:</p>
			<p class="source-code">   - pfs:</p>
			<p class="source-code">       glob: "/data-(*).txt"</p>
			<p class="source-code">       repo: data</p>
			<p class="source-code">       join_on: "$1"</p>
			<p class="source-code">   - pfs:</p>
			<p class="source-code">       glob: "/param-(*).txt"</p>
			<p class="source-code">       repo: parameters</p>
			<p class="source-code">       join_on: "$1"</p>
			<p class="source-code"> transform:</p>
			<p class="source-code">   image: mydockerhub/describe</p>
			<p class="source-code">   cmd:</p>
			<p class="source-code">   - python3</p>
			<p class="source-code">   - "/describe.py"</p>
			<p>This capturing group <a id="_idIndexMarker233"/>in combination with the <strong class="source-inline">/</strong> glob pattern <a id="_idIndexMarker234"/>matches five pairs of files:</p>
			<p class="source-code">data@2c95b1...:/data-0101-2021.txt, parameters@b7acec...:/param-0101-2021.txt </p>
			<p class="source-code">data@2c95b1...:/data-0102-2021.txt, parameters@b7acec...:/param-0102-2021.txt</p>
			<p class="source-code">data@2c95b1...:/data-0103-2021.txt, parameters@b7acec...:/param-0103-2021.txt</p>
			<p class="source-code">data@2c95b1...:/data-0104-2021.txt, parameters@b7acec...:/param-0104-2021.txt </p>
			<p class="source-code">data@2c95b1...:/data-0105-2021.txt, parameters@b7acec...:/param-0105-2021.txt</p>
			<p><strong class="source-inline">/data-0106-2021.txt</strong> and <strong class="source-inline">/data-0107-2021.txt</strong> do not have matching pairs, and therefore, Pachyderm would skip them in this run. However, if you want to include these in the files in the pipeline run, you can specify <strong class="source-inline">outer_join: true</strong> in the <strong class="source-inline">data</strong> repository input to include these files in the pipeline run without a pair. This abstract shows how you can add this parameter:</p>
			<p class="source-code">…</p>
			<p class="source-code">input:</p>
			<p class="source-code">   join:</p>
			<p class="source-code">   - pfs:</p>
			<p class="source-code">       glob: "/data-(*).txt"</p>
			<p class="source-code">       repo: data</p>
			<p class="source-code">       join_on: "$1"</p>
			<p class="source-code">       outer_join: true</p>
			<p class="source-code">…</p>
			<p>Then the list of datums in your pipeline will look like this:</p>
			<p class="source-code">data@2c95b1...:/data-0101-2021.txt, parameters@b7acec...:/param-0101-2021.txt </p>
			<p class="source-code">data@2c95b1...:/data-0102-2021.txt, parameters@b7acec...:/param-0102-2021.txt</p>
			<p class="source-code">data@2c95b1...:/data-0103-2021.txt, parameters@b7acec...:/param-0103-2021.txt</p>
			<p class="source-code">data@2c95b1...:/data-0104-2021.txt, parameters@b7acec...:/param-0104-2021.txt </p>
			<p class="source-code">data@2c95b1...:/data-0105-2021.txt, parameters@b7acec...:/param-0105-2021.txt</p>
			<p class="source-code">data@2c95b1...:/data-0106-2021.txt</p>
			<p class="source-code">data@2c95b1...:/data-0107-2021.txt</p>
			<p><strong class="source-inline">data-0106-2021.txt</strong> and <strong class="source-inline">data-0107-2021</strong> are <a id="_idIndexMarker235"/>included in the pipeline <a id="_idIndexMarker236"/>run without a pair.</p>
			<h4>Group inputs</h4>
			<p>A <strong class="bold">group input</strong>, or <strong class="bold">group</strong>, is a Pachyderm input that enables you to group matching files within one or more <a id="_idIndexMarker237"/>repositories based on a specific naming pattern. This type of input is similar to joins but it can address a broader use case. In a group input, you <a id="_idIndexMarker238"/>must specify a replacement group under the <strong class="source-inline">group_by</strong> pipeline parameter. If we took the same pipeline that we used in the inner join input example and replaced <strong class="source-inline">join</strong> with <strong class="source-inline">group</strong> and <strong class="source-inline">join_on</strong> with <strong class="source-inline">group_by</strong>, we would get the exact same result as we got with the inner join input.</p>
			<p>The main differences from the join input include the following:</p>
			<ul>
				<li>A group input creates one large datum of the files that match a naming pattern, while a join crosses two files that match a pattern and typically creates more datums.</li>
				<li>A group input enables you to match pairs in a single repository, while a join requires at least two repositories.</li>
				<li>A group input is primarily useful in time series scenarios when you need to group files based on their timestamps. </li>
			</ul>
			<p>Therefore, you <a id="_idIndexMarker239"/>could create a <strong class="source-inline">cron</strong> pipeline with the replacement group <a id="_idIndexMarker240"/>that matches a specific timestamp.</p>
			<p>For example, say you have repository data with the following structure:</p>
			<p class="source-code">/data-0101-2020.txt file 2B</p>
			<p class="source-code">/data-0101-2021.txt file 2B</p>
			<p class="source-code">/data-0102-2020.txt file 2B</p>
			<p class="source-code">/data-0102-2021.txt file 2B</p>
			<p class="source-code">/data-0103-2020.txt file 2B</p>
			<p class="source-code">/data-0103-2021.txt file 2B</p>
			<p class="source-code">/data-0104-2021.txt file 2B</p>
			<p class="source-code">/data-0105-2021.txt file 2B</p>
			<p class="source-code">/data-0106-2021.txt file 2B</p>
			<p class="source-code">/data-0107-2021.txt file 2B</p>
			<p>Your pipeline in YAML format might look like this:</p>
			<p class="source-code">---</p>
			<p class="source-code"> pipeline:</p>
			<p class="source-code">   name: test-group</p>
			<p class="source-code"> transform:</p>
			<p class="source-code">   cmd:</p>
			<p class="source-code">   - python3</p>
			<p class="source-code">   - "/test.py"</p>
			<p class="source-code"> input:</p>
			<p class="source-code">   group:</p>
			<p class="source-code">   - pfs:</p>
			<p class="source-code">       repo: data</p>
			<p class="source-code">       glob: "/data-(*)-(*).txt"</p>
			<p class="source-code">       group_by: "$1"</p>
			<p>You have two <a id="_idIndexMarker241"/>replacement groups listed in the pipeline. Replacement group one <a id="_idIndexMarker242"/>matches the month and day in the name of the text file. For example, in the file <strong class="source-inline">/data-0104-2021.txt</strong>, it is <strong class="source-inline">0104</strong>. The second replacement group matches the year in the timestamp. In the same file, it is <strong class="source-inline">2021</strong>.</p>
			<p>If you specify the first matching group in your pipeline, the resulting list of datums will include three pairs and seven datums in total:</p>
			<p class="source-code">data@...:/data-0101-2020.txt, data@..:/data-0101-2021.txt</p>
			<p class="source-code">data@...:/data-0102-2020.txt, data@...:/data-0102-2021.txt</p>
			<p class="source-code">data@...:/data-0103-2020.txt, data@...:/data-0103-2021.txt</p>
			<p class="source-code">data@...:/data-0104-2021.txt</p>
			<p class="source-code">data@...:/data-0105-2021.txt                                                         </p>
			<p class="source-code">data@...:/data-0106-2021.txt                                                            </p>
			<p class="source-code">data@...:/data-0107-2021.txt</p>
			<p>In the preceding output, the files with the matching day and month, such as <strong class="source-inline">0101</strong>, are grouped in one datum. If you change the <strong class="source-inline">group_by</strong> parameter to use the second replacement group, meaning grouping by year, the list of datums will include two datums, grouping the files by year:</p>
			<p class="source-code">-  data@ecbf241489bf452dbb4531f59d0948ea:/data-0101-2020.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0102-2020.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0103-2020.txt                                                                                                                                                                                                                                             </p>
			<p class="source-code">-  data@ecbf241489bf452dbb4531f59d0948ea:/data-0101-2021.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0102-2021.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0103-2021.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0104-2021.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0105-2021.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0106-2021.txt, data@ecbf241489bf452dbb4531f59d0948ea:/data-0107-2021.txt</p>
			<p>In this section, we have <a id="_idIndexMarker243"/>learned about the most important Pachyderm <a id="_idIndexMarker244"/>pipeline primitives and how they differ from each other.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor053"/>Summary</h1>
			<p>In this chapter, we have learned about the most important Pachyderm version control primitives, including repositories, branches, and commits. We reviewed that although they are similar to those of the Git version control system, they have a few very important differences. </p>
			<p>We've learned about the types of Pachyderm pipelines and inputs and how a pipeline can be scaled and optimized through the use of glob patterns and datums.</p>
			<p>In the next chapter, we will review the Pachyderm pipeline specification in more detail and will learn how to use the various pipeline settings to run a pipeline in the most efficient way.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor054"/>Further reading</h1>
			<ul>
				<li>Git documentation: <a href="https://git-scm.com/">https://git-scm.com/</a> </li>
				<li>Kubernetes documentation: <a href="https://kubernetes.io/docs/home">https://kubernetes.io/docs/home</a> </li>
				<li>Helm documentation: <a href="https://helm.sh/docs/">https://helm.sh/docs/</a></li>
			</ul>
		</div>
	</div></body></html>