["```py\n    helm uninstall pachd\n    ```", "```py\nrelease \"pachd\" uninstalled\n```", "```py\n    minikube delete\n    ```", "```py\nDeleting \"minikube\" in docker ...\nDeleting container \"minikube\" ...\n…\n```", "```py\n    minikube start --cpus 4 --memory 8192\n    ```", "```py\n…\nStarting control plane node minikube in cluster minikube\nCreating docker container (CPUs=4, Memory=8192MB) ...\n…\nDone! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n```", "```py\n    helm install --set deployTarget=LOCAL pachd ./pachyderm\n    ```", "```py\nNAME: pachd\nLAST DEPLOYED: Thu Aug 19 13:03:36 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\n...\n```", "```py\n    pachctl config import-kube local --overwrite\n    pachctl config set active-context local\n    ```", "```py\n---\n pipeline:\n   name: data-clean\n description: A pipeline that tokenizes the text.\n input:\n   pfs:\n     glob: \"/data.txt\"\n     repo: data\n transform:\n   cmd:\n   - python3\n   - \"/data-clean.py\"\n   image: svekars/nlp-example:1.0\n```", "```py\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n```", "```py\nwith open('/pfs/data/data.txt', \"r\", encoding='utf-8') as f:\n    data=f.read().replace('\\n', '')\nurl = urlopen(data).read()\nsoup = BeautifulSoup(url, 'html.parser')\ncontent = soup.get_text(strip=True)\nparagraphs = soup.find_all('p')\n```", "```py\nf = open('pfs/out/text.txt', 'w', encoding='utf-8')\n for i in paragraphs:\n     all_text = i.get_text()\n      f.writelines(all_text)\nf.close()\n```", "```py\ntokens = []\nfor i in paragraphs:\n    tokens += word_tokenize(i.text)\n    with open('/pfs/out/tokens.txt', 'w', encoding='utf-8') as filehandle:\n        for item in tokens:\n            filehandle.write('%s\\n' % item)\n```", "```py\nstopwords = stopwords.words(\"english\")\nno_stopwords = []\nfor word in tokens:\n     if not word in stopwords:\n         no_stopwords.append(word)\n         appendFile = open('/pfs/out/no_stopwords.txt', 'a', encoding='utf-8')\n         appendFile.write(word)\n         appendFile.write(\"\\n\")\n         appendFile.close()\n```", "```py\nno_punctuation = []\nfor word in no_stopwords:\n      if word.isalpha():\n          no_punctuation.append(word)\n          appendFile = open('/pfs/out/no_punctuation.txt', 'a', encoding='utf-8')\n          appendFile.write(word)\n          appendFile.write(\"\\n\")\n          appendFile.close()\n```", "```py\nport_stem = PorterStemmer()\nstemmed = []\nfor word in no_punctuation:\n    stemmed_word = port_stem.stem(word)\n    stemmed.append(stemmed_word)\n    appendFile = open('/pfs/out/stemmed.txt', 'a', encoding='utf-8')\n    appendFile.write(stemmed_word)\n    appendFile.write(\"\\n\")\n    appendFile.close()\n```", "```py\nlemmatizer = WordNetLemmatizer()\nlemmatized = []\nfor word in no_punctuation:\n    l_text = lemmatizer.lemmatize(word)\n    lemmatized.append(l_text)\n    appendFile = open('/pfs/out/lematized.txt', 'a', encoding='utf-8')\n    appendFile.write(l_text)\n    appendFile.write(\"\\n\")\n    appendFile.close()\n```", "```py\n    pachctl version\n    ```", "```py\nCOMPONENT           VERSION\npachctl             2.0.0\npachd               2.0.0\n```", "```py\n    pachctl create repo data\n    ```", "```py\n    pachctl list repo\n    ```", "```py\nNAME CREATED   SIZE (MASTER) DESCRIPTION\ndata 10 seconds ago ≤ 0B\n```", "```py\n    pachctl put file -f data.txt data@master\n    ```", "```py\ndata.txt 49.00b / 49.00 b [==============] 0s 0.00 b/s\n```", "```py\n    pachctl list file data@master\n    ```", "```py\nNAME      TYPE SIZE\n/data.txt file 49B\n```", "```py\n    pachctl create pipeline -f data-clean.yaml\n    ```", "```py\n    pachctl list pipeline\n    ```", "```py\nNAME       VERSION INPUT          CREATED       STATE / LAST JOB DESCRIPTION\ndata-clean 1       data:/data.txt 4 seconds ago running / -      A pipeline that tokenizes the text.\n```", "```py\n    pachctl list repo\n    ```", "```py\nNAME       CREATED        SIZE (MASTER) DESCRIPTION\ndata-clean 15 seconds ago ≤ 315.8KiB    Output repo for pipeline data-clean.\ndata       1 minute ago ≤ 49B\n```", "```py\n    pachctl list file data-clean@master\n    ```", "```py\nNAME                TYPE SIZE\n/lemmatized.txt     file 42.25KiB\n/no_punctuation.txt file 42.99KiB\n/no_stopwords.txt   file 47.88KiB\n/stemmed.txt        file 38.01KiB\n/text.txt           file 75.18KiB\n/tokens.txt         file 69.54KiB\n```", "```py\n    pachctl get file data-clean@master:/lemmatized.txt | open -f  -a  TextEdit.app\n    ```", "```py\n---\npipeline:\n   name: pos-tag\n description: A pipeline that performs POS tagging.\n input:\n   pfs:\n     glob: \"/lemmatized.txt\"\n     repo: data-clean\n transform:\n   cmd:\n   - python3\n   - \"/pos-tag.py\"\n   image: svekars/nlp-example:1.0\n```", "```py\nimport spacy\nimport en_core_web_sm\nfrom spacy import displacy\nimport IPython\nfrom pathlib import Path\nimport spacy.attrs\nfrom contextlib import redirect_stdout\n```", "```py\nsp = spacy.load('en_core_web_sm')\ntextfile = sp(open('pfs/data-clean/lematized.txt', \"r\", encoding='utf-8').read())\nwith open('/pfs/out/pos-table.txt', 'w') as f:\n     with redirect_stdout(f):\n         for word in textfile:\n             print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n```", "```py\nwith open('/pfs/out/pos-number.txt', 'w') as file:\n     with redirect_stdout(file):\n         count_tags = textfile.count_by(spacy.attrs.IDS['POS'])\n         for i, count in count_tags.items():\n             tags = textfile.vocab[i].text\n             print(tags, count)\n```", "```py\n image = displacy.render(textfile, style='dep', options={\"compact\": True, \"distance\": 70})\n f = open('/pfs/out/pos-tag-dependency.svg', \"w\")\n f.write(image)\n f.close()\n```", "```py\n    pachctl version\n    ```", "```py\nCOMPONENT           VERSION\npachctl             2.0.0\npachd               2.0.0\n```", "```py\n    pachctl create pipeline -f pos-tag.yaml\n    ```", "```py\n    pachctl list pipeline\n    ```", "```py\nNAME       VERSION INPUT   CREATED       STATE / LAST JOB  DESCRIPTION\npos-tag    1       data-clean:/lemmatized.txt 2 seconds ago running / -       A pipeline that performs POS tagging.\ndata-clean 1       data:/data.txt             1 minute ago running / success A pipeline that tokenizes the text.\n```", "```py\n    pachctl list repo\n    ```", "```py\nNAME       CREATED           SIZE (MASTER) DESCRIPTION\npos-tag    22 seconds ago  ≤ 10.82MiB    Output repo for pipeline pos-tag.\ndata-clean 2 minutes ago  ≤ 315.8KiB    Output repo for pipeline data-clean.\ndata       3 minutes ago ≤ 49B\n```", "```py\n    pachctl list file pos-tag@master \n    ```", "```py\nNAME                    TYPE SIZE\n/pos-number.txt         file 132B\n/pos-table.txt          file 564.1KiB\n/pos-tag-dependency.svg file 10.27MiB\n```", "```py\n    pachctl get file pos-tag@master:/pos-number.txt \n    ```", "```py\nADP 154\nSPACE 6291\nNOUN 3110\nNUM 67\nADJ 789\nPROPN 346\nVERB 1053\nADV 374\nDET 98\nAUX 69\nPRON 130\nPART 9\nSCONJ 59\nCCONJ 15\nINTJ 7\nX 11 \n```", "```py\n    pachctl get file pos-tag@master:/pos-tag-dependency.svg > pos-tag-dependency.svg | open -f pos-tag-dependency.svg -a \"Google Chrome\"\n    ```", "```py\nSnap Inc. - 0 - 9 - ORG - Companies, agencies, institutions, etc.\nFirst Quarter 2021 - 20 - 38 - DATE - Absolute or relative dates or periods\n```", "```py\n---\n pipeline:\n   name: ner\n description: A NER pipeline\n input:\n   pfs:\n     glob: \"/text.txt\"\n     repo: data-clean\n transform:\n   cmd:\n   - python3\n   - \"/ner.py\"\n    image: svekars/nlp-example:1.0\n```", "```py\nimport spacy\nfrom spacy import displacy\nfrom contextlib import redirect_stdout\n```", "```py\nsp = spacy.load(\"en_core_web_lg\")\ndef display_entities(text):\n     with open ('/pfs/out/ner-list.txt', 'w') as f:\n         with redirect_stdout(f):\n             if text.ents:\n                 for i in text.ents:\n                     print(i.text+' - '+str(i.start_char)+' - '+str(i.end_char)+' - '+i.label_+' - '+str(spacy.explain(i.label_)))\n text = sp(open('/pfs/data-clean/text.txt', \"r\", encoding='utf-8').read())\n display_entities(text)\n```", "```py\n with open ('/pfs/out/ner-labels.html', 'w') as f:\n      with redirect_stdout(f):\n          for i in text.ents:\n              html=displacy.render(text, style=\"ent\", page=True)\n              print(html)\n```", "```py\n    pachctl version\n    ```", "```py\nCOMPONENT           VERSION\npachctl             2.0.0\npachd               2.0.0\n```", "```py\n    pachctl create pipeline -f ner.yaml\n    ```", "```py\n    pachctl list pipeline\n    ```", "```py\nNAME       VERSION INPUT                     CREATED           STATE / LAST JOB  DESCRIPTION\nner        1       data-clean:/text.txt      5 seconds ago     running / running A NER pipeline\npos-tag    1       data-clean:/lematized.txt 1 minutes ago running / success A pipeline that performs POS tagging.\ndata-clean 1       data:/data.txt            2 minutes ago       running / success A pipeline that tokenizes the text.\n```", "```py\n    pachctl list repo\n    ```", "```py\nNAME       CREATED        SIZE (MASTER) DESCRIPTION\nner        36 seconds ago ≤ 43.49MiB    Output repo for pipeline ner.\npos-tag    2 minutes ago  ≤ 10.82MiB    Output repo for pipeline pos-tag.\ndata-clean 3 minutes ago ≤ 315.8KiB    Output repo for pipeline data-clean.\ndata       4 minutes ago ≤ 49B\n```", "```py\n    pachctl list file ner@master \n    ```", "```py\nNAME             TYPE SIZE\n/ner-labels.html file 43.47MiB\n/ner-list.txt    file 19.36KiB\n```", "```py\n    pachctl get file ner@master:/ner-list.txt | awk 'FNR <= 10'\n    ```", "```py\none - 36 - 39 - CARDINAL - Numerals that do not fall under another type\nHudson - 108 - 114 - LOC - Non-GPE locations, mountain ranges, bodies of water\nDutch - 186 - 191 - NORP - Nationalities or religious or political groups\nthe Tappan Zee - 203 - 217 - EVENT - Named hurricanes, battles, wars, sports events, etc.\nSt. Nicholas - 303 - 315 - ORG - Companies, agencies, institutions, etc.\nGreensburgh - 417 - 428 - PERSON - People, including fictional\nTarry Town - 498 - 508 - GPE - Countries, cities, states\nformer days - 547 - 558 - DATE - Absolute or relative dates or periods\nabout two miles - 891 - 906 - QUANTITY - Measurements, as of weight or distance\nfirst - 1330 - 1335 - ORDINAL - \"first\", \"second\", etc. NUM 67\n```", "```py\n    pachctl get file ner@master:/ner-labels.html > ner-labels.html | open -f ner-labels.html -a \"Google Chrome\"\n    ```", "```py\n---\n pipeline:\n   name: retrain\n description: A pipeline that retrains the NER model.\n input:\n   pfs:\n     glob: \"/training-data.json\"\n     repo: data\n transform:\n   cmd:\n   - python3\n   - \"/ner-improved.py\"\n   image: svekars/nlp-example:1.0\n```", "```py\nimport spacy\nimport random\nfrom spacy.util import minibatch\nfrom spacy.training import Example\nfrom contextlib import redirect_stdout\nimport simplejson as json\n```", "```py\nnlp=spacy.load(\"en_core_web_lg\")\nner=nlp.get_pipe(\"ner\")\ndata = open(\"/pfs/data/training-data.json\")\ndata = json.loads(data.read())\n```", "```py\n optimizer = nlp.create_optimizer()\n other_pipes = [p for p in nlp.pipe_names if p != \"ner\"]\n with nlp.disable_pipes(*other_pipes):\n     for i in range(30):\n         random.shuffle(data)\n         losses = {}\n         for text, annotations in data:\n             doc = nlp.make_doc(text)\n             example = Example.from_dict(doc, annotations)\n             nlp.update([example], drop=0.1, sgd=optimizer, losses=losses)\n         print(losses)\n```", "```py\ntest_text = 'Headless Horseman came to see Ichabod Crane.'\ndoc = nlp(test_text)\nwith open ('/pfs/out/ner-improved.txt', 'w') as f:\n    with redirect_stdout(f):\n        for i in doc.ents:\n            print(i.label_, \" -- \", i.text)\npickle.dump(nlp, open('/pfs/out/ner-improved-model.p', 'wb'))\n```", "```py\n    pachctl version\n    ```", "```py\n  COMPONENT           VERSION\npachctl             2.0.0\npachd               2.0.0\n```", "```py\n    pachctl put file -f training-data.json data@master\n    ```", "```py\n    pachctl create pipeline -f retrain.yaml\n    ```", "```py\n    pachctl list pipeline\n    ```", "```py\nNAME       VERSION INPUT                     CREATED        STATE / LAST JOB  DESCRIPTION\nretrain    1       data:/training-data.json   About a minute ago running / success A pipeline that retrains the NER model.\nner        1       data-clean:/text.txt       2 minutes ago      running / success A NER pipeline\npos-tag    1       data-clean:/lemmatized.txt 3 minutes ago      running / success A pipeline that performs POS tagging.\ndata-clean 1       data:/data.txt             5 minutes ago      running / success A pipeline that tokenizes the text.\n```", "```py\n    pachctl list repo\n    ```", "```py\nNAME       CREATED        SIZE (MASTER) DESCRIPTION\nretrain    About a minute ago ≤ 821.8MiB    Output repo for pipeline retrain.\nner        2 minutes ago      ≤ 43.49MiB    Output repo for pipeline ner.\npos-tag    3 minutes ago      ≤ 10.82MiB    Output repo for pipeline pos-tag.\ndata-clean 5 minutes ago      ≤ 315.8KiB    Output repo for pipeline data-clean.\ndata       6 minutes ago      ≤ 205B\n```", "```py\n    pachctl list file retrain@master \n    ```", "```py\nNAME                TYPE SIZE\n/ner-improved-model.p file 821.8MiB\n/ner-improved.txt   file 56B\n```", "```py\n    pachctl get file retrain@master:/ner-improved.txt\n    ```", "```py\nPERSON  --  Headless Horseman\nPERSON  --  Ichabod Crane \n```", "```py\n---\n  pipeline:\n    name: my-model\n  description: A retrained NER pipeline\n  input:\n    cross:\n    - pfs:\n        repo: data-clean\n        glob: \"/text.txt\"\n    - pfs:\n        repo: retrain\n        glob: \"/ner-improved-model.p\"\n  transform:\n    cmd:\n    - python3\n    - \"/ner-my-model.py\"\n    image: svekars/nlp-example:1.0\n```", "```py\nnlp = pickle.load(open('/pfs/retrain/ner-improved-model.p', 'rb')))\n```", "```py\nwith open ('/pfs/out/person-label-count.txt', 'w') as f:\n     with redirect_stdout(f):\n         person_label=[]\n         for i in text.ents:\n             if i.label_ =='PERSON':\n                 person_label.append(i.text)\n         count = Counter(person_label)\n         for key, counter in count.most_common():\n```", "```py\n    pachctl version\n    ```", "```py\n  COMPONENT           VERSION\npachctl             2.0.0\npachd               2.0.0\n```", "```py\n    pachctl create pipeline -f my-model.yaml\n    ```", "```py\n    pachctl list pipeline\n    ```", "```py\nNAME       VERSION INPUT                                                  CREATED        STATE / LAST JOB   DESCRIPTION\nmy-model   1       (data-clean:/text.txt ⨯ retrain:/ner-improved-model.p) 6 seconds ago  running / running A retrained NER pipeline\nretrain    1       data:/training-data.json                               9 minutes ago  running / success A pipeline that retrains the NER model.\nner        1       data-clean:/text.txt                                   10 minutes ago running / success A NER pipeline\npos-tag    1       data-clean:/lemmatized.txt                             11 minutes ago running / success A pipeline that performs POS tagging.\ndata-clean 1       data:/data.txt                                         13 minutes ago running / success A pipeline that tokenizes the text.\n```", "```py\n    pachctl list repo\n    ```", "```py\nNAME       CREATED        SIZE (MASTER) DESCRIPTION\nmy-model   About a minute ago ≤ 31.44MiB    Output repo for pipeline my-model.\nretrain    10 minutes ago     ≤ 821.8MiB    Output repo for pipeline retrain.\nner        11 minutes ago     ≤ 43.49MiB    Output repo for pipeline ner.\npos-tag    12 minutes ago     ≤ 10.82MiB    Output repo for pipeline pos-tag.\ndata-clean 14 minutes ago     ≤ 315.8KiB    Output repo for pipeline data-clean.\ndata       15 minutes ago     ≤ 205B\n```", "```py\n    pachctl list file my-model@master \n    ```", "```py\nNAME                      TYPE SIZE\n/ner-improved-labels.html file 26.14MiB\n/ner-improved-list.txt    file 13.43KiB\n/person-label-count.txt   file 654B \n```", "```py\n    pachctl get file my-model@master:/person-label-count.txt | awk 'FNR <= 15'\n    ```", "```py\nIchabod: 35\nBrom Bones: 9\nIchabod Crane: 8\nVan Tassel: 5\nHans Van: 5\nGalloping Hessian: 4\nAndré: 4\nHeadless Horseman: 3\nBrom: 3\nHans Van Ripper: 3\nBrouwer: 3\nTarry Town: 2\nCotton Mather: 2\nMather: 2\nBaltus Van: 2\n```", "```py\n    pachctl get file my-model@master:/ner-improved-labels.html > ner-improved-labels.html | open -f ner-labels.html -a \"Google Chrome\"\n    ```", "```py\n    pachctl delete pipeline –all && pachctl delete repo --all\n    ```", "```py\n    pachctl list repo && pachctl list pipeline\n    ```", "```py\nNAME CREATED SIZE (MASTER) DESCRIPTION\nNAME VERSION INPUT CREATED STATE / LAST JOB DESCRIPTION\n```"]