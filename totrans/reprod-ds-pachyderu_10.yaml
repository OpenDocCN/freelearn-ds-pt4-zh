- en: '*Chapter 8*: Creating an End-to-End Machine Learning Workflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we learned about Pachyderm basics and how to install Pachyderm
    locally and on a cloud platform. We've deployed our first pipeline, learned how
    to update a pipeline, and performed some fundamental Pachyderm operations, such
    as splitting. I hope by now you are convinced that Pachyderm is an extremely versatile
    tool that gives you a lot of flexibility and power in managing your machine learning
    pipelines. To make it even more obvious, we will deploy a much more complex example
    than the ones that we have deployed so far. We hope this chapter will be especially
    fun for you to work on and will expand your understanding of data infrastructure
    quirks even more.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will deploy a multistep **Natural Language Processing**
    (**NLP**) workflow that will demonstrate how to use Pachyderm at scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter includes the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: NLP example overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating repositories and pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an NER pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining an NER model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have the following components installed and configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a local macOS installation, you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: macOS Mojave, Catalina, Big Sur, or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Desktop for Mac 10.14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube` v1.9.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a local Windows installation, you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Windows Pro 64-bit v10 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windows Subsystem for Linux** (**WSL**) 2 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft PowerShell v6.2.1 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyper-V
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube` v1.9.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an **Amazon Elastic Kubernetes Service** (**Amazon EKS**) installation,
    you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` v.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eksctl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws-iam-authenticator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a Microsoft Azure cloud installation, you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` v.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jq` 1.5 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a **Google Kubernetes Engine** (**GKE**) cloud installation, you need the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud SDK v124.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` v.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum virtual hardware requirements for a cloud or local virtual machine
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of CPUs**: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory**: 8,192 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk**: 20 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know the technical requirements needed to accomplish the tasks in
    this chapter, we can deploy a Pachyderm instance with sufficient resources to
    run the example described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting virtual machine parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run the example described in this section, you must make sure that the virtual
    machine that runs Pachyderm has enough memory and CPU to accommodate the pipeline
    requirements. This applies to both cloud and local environments.
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Pachyderm on a cloud platform, make sure that you have deployed
    Kubernetes on a virtual machine flavor that adheres to the minimum hardware requirements
    listed in the *Technical requirements* section for this chapter. Then, redeploy
    your Pachyderm cluster as described in [*Chapter 5*](B17085_05_Final_SB_Epub.xhtml#_idTextAnchor123),
    *Installing Pachyderm on a Cloud Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Pachyderm in `minikube` on your local computer, make sure
    that the `minikube` virtual machine is large enough. If you have a `minikube`
    machine deployed as described in [*Chapter 4*](B17085_04_Final_SB_Epub.xhtml#_idTextAnchor096),
    *Installing Pachyderm Locally*, you need to delete it and deploy a new `minikube`
    virtual machine with a larger CPU and memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Uninstall the old Pachyderm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the existing `minikube` virtual machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following system response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After your old machine is deleted, start a new virtual machine with the following
    parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, redeploy your Pachyderm cluster as described in [*Chapter 4*](B17085_04_Final_SB_Epub.xhtml#_idTextAnchor096),
    *Installing Pachyderm Locally*. For simplicity, here are the commands that you
    need to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following system response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following commands to connect to `pachd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have Pachyderm deployed with enough resources to run the example
    in this chapter, let's review the NLP pipeline that we will create.
  prefs: []
  type: TYPE_NORMAL
- en: NLP example overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review the end-to-end machine learning workflow that
    will help us understand how to schedule it in Pachyderm.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this functionality, we will create an NLP pipeline that will
    perform various text optimizations against the *The Legend of Sleepy Hollow* short
    story book written by *Washington Irving*.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let's review what NLP is and the typical stages that are involved
    in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP is a machine learning technique that enables you to analyze natural text,
    namely speech or written text. This branch of artificial intelligence has existed
    for many years, but with the advancement of computer and internet technologies,
    it has found new implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how can you use NLP in your business or academic research? There are many
    ways, but the most common ones include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech recognition**: Technology that enables computers to understand human
    voice and speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chatbots**: Software that can answer questions and learn from provided answers.
    Older chatbots were based on rules defined by standard software engineering techniques,
    meaning that they could not evolve and could only produce mediocre responses.
    Newer bots are much more advanced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation**: Technology that automates the task of translating
    text and speech from one language to another. The most common example is, of course,
    Google Translate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text extraction, summarization, and classification**: A very needed technique
    in our world of information overload. NLP enables you to create pipelines that
    provide insights about a text, such as a summary of a research paper or information
    about the keywords used on a page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: A famous technique that helps classify information
    by its positive or negative tone. The most famous implementation of this technology
    is the Gmail email classifier that sorts your email into three categories: **Primary**,
    **Promotions**, and **Social** emails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the main examples of NLP. However, this list is not complete. NLP
    is also used in bioinformatics to analyze genetic data, in finance for understanding
    market events and trends, in healthcare to understand patient information, and
    in many other areas.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the areas in which NLP is applicable, let's review the main
    phases that are involved in building an NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the NLP phases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in the previous section, NLP is used to solve a variety of
    text- and speech-related tasks. Like with other areas of machine learning, when
    you need to solve an NLP problem, you need to build a pipeline. There are a few
    definitions of an NLP pipeline, but typically, phases of an NLP pipeline include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text preprocessing or** **cleaning**: This stage includes operations such
    as word and sentence segmentation, tokenization, removal of stop words and punctuation,
    converting words to lowercase, and lemmatization or stemming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structure analysis**: This stage goes deeper into analyzing what the text
    is about. It includes operations such as **Part-of-Speech** (**POS**) tagging,
    dependency parsing, and chunking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: This stage deals with answering specific questions
    about your data and finding relationships between your text entities. It might
    include tasks such as **Named Entity Recognition** (**NER**) and **Named Entity
    Disambiguation** (**NED**) or **linking** and sentiment analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modeling**: This stage is where you train your model on training data and
    test it to further put it into a production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on what your use case is, your pipeline might include all or some
    of these stages and they might also be in a different order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates an example NLP pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – NLP pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_08_001.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – NLP pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the phases of an NLP pipeline, let's look more closely at the
    example that we will implement.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the NLP example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our example, we will be using the text of *The Legend of Sleepy Hollow*
    by *Washington Irving* and in the end, we will create and train an NER pipeline
    that will help us to answer the question of who the main characters in the book
    are. To create this multistep workflow, we will need to create the following pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning pipeline**: This pipeline will download the text from a provided
    URL and clean it of any HTML tags, headings, and other irrelevant content. Then,
    it tokenizes the text, removes stop words and punctuation, and then does stemming
    and lemmatization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**POS tagging pipeline**: This pipeline will add POS tags to the cleaned text
    from the previous pipeline based on the position of the words in the sentences
    and context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NER pipeline**: This pipeline will run a pretrained model against our text
    and attempt to label the results with the correct entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NER training pipeline**: This pipeline will train a new NER model based on
    provided training data to correct the results of the first NER pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved NER pipeline**: This pipeline will run the new NER model against
    our text and output the list of characters in the story to a text file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a diagram of our full NLP pipeline workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Pachyderm NLP pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_08_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Pachyderm NLP pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the pipeline steps, let's create all the needed repositories
    and pipelines step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Creating repositories and pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create all the pipelines that we reviewed in the previous
    section. The six-step workflow will clean the data, apply POS tagging, perform
    NER, train a new custom mode based on the provided data, run the improved pipeline,
    and output the results to the final repo.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create the data cleaning pipeline that will strip the text
    from the elements we won't need for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You need to download all files for this example from [https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter08-End-to-End-Machine-Learning-Workflow](https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter08-End-to-End-Machine-Learning-Workflow).
    The Docker image is stored at [https://hub.docker.com/repository/docker/svekars/nlp-example](https://hub.docker.com/repository/docker/svekars/nlp-example).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the data cleaning pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data cleaning is typically performed before any other types of tasks. For this
    pipeline, we have created a Python script that uses the **Natural Language Toolkit**
    (**NLTK**) platform to perform the data cleaning task. NLTK is an open source
    set of libraries that enables you to complete a variety of NLP-related tasks,
    including tokenization, stemming, removing stop words, and lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the pipeline specification that we will use for this pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This pipeline performs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes a URL provided in the `data.txt` file from the `data` repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the `svekars/nlp-example:1.0` image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs the `data-clean.py` script added to the `svekars/nlp-example:1.0` image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might have noticed that the glob pattern used in the pipeline specification
    uses only one file—`data.txt`. This is a file where we will have a URL to the
    *The Legend of Sleepy Hollow* text located at the *Project Gutenberg: Free eBooks*
    website. To access the website, go to [https://gutenberg.org](https://gutenberg.org).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve reviewed the pipeline, let''s look closer at what our script
    does. Here is a list of the components that we will be importing in the `data-clean.py`
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We need `BeautifulSoup` to parse the HTML file with our text. We use `urlopen`
    to open the URL inside of the `data.txt` file. We need NLTK with `stopwords`,
    `word_tokenize`, `PorterStemmer`, `WordNet`, and `WordNetLemmatizer` to perform
    various NLP operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of code opens the `data.txt` file that we have placed in the
    data repository, reads the file, and uses the `BeautifulSoup` HTML parser to parse
    the text. In the `paragraphs` line, we strip the text from all other HTML elements
    but the `<p>` HTML tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part of the script saves the downloaded text to the text file in
    the output repository. We will need our downstream pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next part of the code, we use the `word_tokenize` NLTK method to break
    the text into individual tokens and save them into the `tokens.txt` file in the
    output repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of the code takes the tokenized text from previously, removes
    the stop words from it, and saves the result in the `no_stopwords.txt` file in
    the output repository. The **stop words** are the words that include articles,
    pronouns, and other commonly used words that don''t add a lot of value to the
    text and can be ignored for research purposes to save processing time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of the code removes punctuation from the text that was already
    tokenized and stripped of stop words. The code saves the results into a separate
    file called `no_punctuation.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is stemming. `grouping` and `grouped` would be reduced to just `group`.
    Sometimes, this technique might be considered too aggressive and lemmatization
    can be used instead. The stemmed output is saved to `stemmed.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of this script is lemmatization, which uses NLTK''s WordNet
    Lemmatizer database to perform lemmatization on the text that was tokenized and
    stripped of stop words and punctuation. This last piece of code saves the results
    to the `lematized.txt` file. We will use that file in our next pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know what our pipeline does, let's create it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the `data-clean.py` pipeline, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the data repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that that data repository was created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'From the directory where you have your `data.txt` file, put it into the data
    repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This file has just one line—a link to the text of *The Legend of Sleepy Hollow*
    on the Gutenberg website: [https://www.gutenberg.org/files/41/41-h/41-h.htm](https://www.gutenberg.org/files/41/41-h/41-h.htm).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the file was placed in the repository with the `file` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `data-clean` pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No output is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the pipeline was created and is starting or running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After a minute or so, the pipeline should finish running and upload the results
    to the output repository. Check the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Pachyderm automatically created an output repository called
    `data-clean` and uploaded 315.8 KiB to the master branch of that repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s list the files in the repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the pipeline has uploaded six files into the output repository.
    We have deliberately saved them in separate files so that you can see the difference
    between them. You can view the contents of each file and compare them. For example,
    to open the `lemmatized.txt` file on macOS, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Lemmatized words'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_08_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Lemmatized words
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have created a pipeline that cleans our text. In the next
    section, we will create our next pipeline, which will apply POS tags to our lemmatized
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the POS tagging pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: POS tagging is an NLP technique that labels each word with a relevant part of
    speech. This process is used in many NLP problems, such as text disambiguation
    and text-to-speech conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this task, we have used **spaCy**, a free library that performs POS tagging,
    NER, and other tasks. For example, say you have the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Whoever is happy will make others happy too.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of what POS tagging with spaCy looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – POS tagging example](img/Table_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – POS tagging example
  prefs: []
  type: TYPE_NORMAL
- en: We will use spaCy to find POS tags in our lemmatized text from the `data-clean`
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what our POS tagging pipeline specification will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This pipeline performs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes the `lemmatized.txt` file from the `data-clean` repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the `svekars/nlp-example:1.0` Docker image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs the `pos-tag.py` script against our lemmatized text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs a table with all POS tags found in the text inthe `pos_table.txt` file,
    a file with the total number for each POS tag, to `pos_number.txt` and creates
    a dependency graph saved as a `pos-tag-dependency.svg` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have reviewed what the pipeline does, let's take a look at the `pos-tag.py`
    script.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script imports the following components and libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We need `spacy` and its modules to perform POS tagging, count them, and visualize
    the results. We import the `en_core_web_sm` pretrained spaCy model to do the tagging
    task. We need IPython as a spaCy dependency. Finally, we are using `pathlib` and
    `redirect_stdout` to save the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the code imports a pretrained spaCy model called `en_core_web_sm`.
    POS tagging requires you to use either a pretrained model or your own custom model.
    The `en_core_web_sm` model does a good job of tagging POSes. Therefore, we will
    just use it. Then, the script opens our `lematized.txt` file, tags all the words
    in the file, and prints out the result to the `pos-table.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of the code counts the number of each tag in the processed text
    and outputs the results to the `pos-number.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the last part of the script generates a dependency graph and saves
    it as an SVG image, `pos-tag-dependency.svg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's create our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a POS tagging pipeline, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the POS tagging pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No system output is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the pipeline was created and is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the output that this command returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'When the pipeline finishes running, check the output repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Pachyderm has created an output repository called `pos-tag` and uploaded 10.82
    MiB to the master branch of this repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at files that were uploaded to the output repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns the following system output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the number of each tag we have in our text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s take a look at the dependency graph. If you are on macOS, run
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Google Chrome opens the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17085_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – POS dependency graph
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You might need to scroll down in your browser to see the graph. Because we ran
    POS tagging against the whole book, this graph is very long. You'll need to scroll
    horizontally to see it all.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we configured a POS tagging pipeline by using Pachyderm and
    spaCy, as well as visualizing it with a dependency graph. Next, we'll configure
    an NER pipeline that will help us find the main characters of the story.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an NER pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NER is an information extraction technique that recognizes entities in text
    and puts them in certain categories, such as person, location, and organization.
    For example, say we have the following phrase:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Snap Inc. Announces First Quarter 2021 Financial Results*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use spaCy''s `en_core_web_lg` against this phrase, you will get the
    following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Name recognition can be useful in a variety of tasks. In this section, we will
    use it to retrieve the main characters of *The Legend of Sleepy Hollow*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what our NER pipeline specification will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This pipeline performs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes the original text of *The Legend of Sleepy Hollow* from the `data-clean`
    repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the `svekars/nlp-example:1.0` Docker image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs the `ner.py` script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs the results to the `ner` repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s look at what the `ner.py` script does. Here is the list of components
    the script imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We need `spacy` to perform NER and the `displacy` module to visualize the results.
    `redirect_stdout` is a handy way to redirect printed output to a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the code imports spaCy''s pretrained model called `en_core_web_lg`.
    This model seems to perform better on NER tasks than its counterpart that we used
    in the POS tagging pipeline. Then, the script grabs the original text stored in
    the `text.txt` file in the `data-clean` repository and performs the NER task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the script visualizes the results with `displacy` and saves them in
    the HTML format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know what our script does, let's create the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the NER pipeline, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the POS tagging pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No system output is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the pipeline was created and is starting or running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'When the pipeline finishes running, check the output repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The command returns this system output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Pachyderm has created an output repository called `ner` and uploaded 43.49 MiB
    to the master branch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the files that were uploaded to the output repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This repository has two files. One is the list of all instances of entities
    found in the file and the other is the visualization for all the entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the first 10 lines of the `ner-list.txt` file to the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the NER model has identified many entities in the text. Let's
    open the HTML file to view all the entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the HTML file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The file will open in Google Chrome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – NER labels'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_08_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – NER labels
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the spaCy model has identified many entities in the text correctly.
    However, if you start browsing, you'll notice that it has skipped some of them.
    For example, it did not tag *Headless Horseman* as **PERSON** in some cases. This
    is a known accuracy problem of pretrained models. In the next section, we will
    correct this by retraining our model to use these entities.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining an NER model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inaccuracy in NER pipeline results is a common problem. The only way to fix
    it is to retrain an existing model or train your own model completely from scratch.
    Training a model from scratch is a difficult and lengthy operation. In our case,
    we don't need to necessarily train a completely new model but instead, we can
    retrain the existing model to understand the missing context. To accomplish this
    task, we will put training data into the `data-clean` repository, create a training
    pipeline that will train on that data, save our model to an output repository,
    and then run the retrained model against our original text again.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Pachyderm terms, this means that we will create two pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: The first pipeline, called `retrain`, will train our model and output the new
    model to the `train` output repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second pipeline, called `my-model`, will use the new model to analyze our
    text and upload the results to the `my-model` repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's create the retrain pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the retrain pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this pipeline, we will create the following pipeline specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This pipeline takes the `training-data.json` file that has our training data
    and runs the `ner-improved.py` script to improve the existing model. The results
    are saved to the `retrain` repository. For this example, we do not need a lot
    of training examples, but in a real-life use case, you would have to have hundreds
    of examples to improve the accuracy of your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of the components the `ner-improved.py` script imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We need `spacy` with `minibatch` and `Example.from_dict` methods to train the
    model. We use `random` to shuffle the files into a different order for better
    training. The `simplejson` Python decoder is needed to read the training data
    file in the JSON format and `redirect_stdout` is needed to save the results in
    an output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the script loads the spaCy model, reads the training data
    file, and opens a spaCy NER pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This part of the code uses an optimizer, which performs the gradient descent
    calculation. Then, the script specifies that only the NER pipeline needs to be
    trained and all others should be ignored. The next `for` loop performs the actual
    training, updates the model, and prints the losses. We will train our model for
    30 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the script tests the retrained pipeline against a test text
    and outputs the results to the `ner-improved.txt` text. The retrained model is
    saved using `pickle.dump` as in the `ner-improved-model.p` file directory in the
    `output` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Put `training-data.json` in the data repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the retrain pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No system output is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the pipeline was created and is starting or running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'After some time, check the output repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Pachyderm uploaded 816.7 MiB to the `retrain` repository. This
    is our retrained model that takes this much space.
  prefs: []
  type: TYPE_NORMAL
- en: 'List the files in the `retrain` repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the `ner-improved.txt` file, which should have the results of
    running the retrained model against the test text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Great! *Headless Horseman* and *Ichabod Crane* are defined as **PERSON**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have retrained our model, let's deploy our final pipeline, which
    will give us improved NER and output all the characters of the story into one
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the retrained pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our retrained pipeline needs to be a cross-pipeline to combine our new retrained
    model with our text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This pipeline will take the same `text.txt` file as in the original NER pipeline
    and create a cross-product of our retrained model with that text. It will output
    the results to the `my-model` repository. The resulting files will have an HTML
    file with better NER tagging and a text file with the list of characters in *The
    Legend of Sleepy Hollow*.
  prefs: []
  type: TYPE_NORMAL
- en: '`ner-my-model.py` is very similar to the original `ner.py` script with the
    following differences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It loads our improved model by using `pickle.load` instead of the original
    spaCy model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'It counts the total number of instances for each `/pfs/out/person-label-count.txt`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: It saves the HTML visualization to `ner-improved-labels.html`. It saves all
    entities to `ner-improved-list.txt` in the `my-model` repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create our final pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your terminal and verify that Pachyderm is up and running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `my-model` pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command does not return any output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the pipeline was created and is starting or running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `my-model` pipeline is starting. It is different from all other
    pipelines we've created for this example. Because we are saving our model in the
    `retrain` repository and we need to combine it with the text in the data repository,
    a standard pipeline won't be able to accomplish this. That's why we create a cross-pipeline
    that combines two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the output repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Pachyderm uploaded 26.15 MiB to the `my-model` repository. This is the result
    of our computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'List the files in the `my-model` repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the `person-label-count.txt` file, which should provide the
    total count for each unique **PERSON** instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output is still not entirely accurate because we see instances
    of *Ichabod* and *Ichabod Crane* separately. If we provide more training data,
    we can improve these results. However, you already see the most often-listed characters
    in this list and can understand that *Ichabod Crane* is likely the main character
    of the story.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the HTML file to view the highlighted version of the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes our experiment with the spaCy NER model. You can add more training
    data to see how the accuracy will improve with more training examples.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's clean up our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After you are done experimenting, you might want to clean up your cluster so
    that you start your next experiment with a fresh install. To clean up the environment,
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete all pipelines and repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that no repositories and pipelines exist in your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: You have successfully cleaned up your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to build a complex machine learning workflow
    with the NER pipeline example. We have learned how to clean the data with the
    NTLK library, how to do POS tagging, and finally, how to retrain a spaCy model
    inside Pachyderm and output results for preview. You can do much more and tweak
    this example further to achieve better accuracy of NER by adding more training
    data and tweaking the model training parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to do hyperparameter tuning in Pachyderm
    on an example of housing price prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLTK documentation: [https://www.nltk.org/](https://www.nltk.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'spaCy documentation: [https://spacy.io/api/doc](https://spacy.io/api/doc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Legend of Sleepy Hollow* on the Gutenberg project website: [https://www.gutenberg.org/files/41/41-h/41-h.htm](https://www.gutenberg.org/files/41/41-h/41-h.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
