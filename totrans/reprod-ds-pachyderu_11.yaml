- en: '*Chapter 9*: Distributed Hyperparameter Tuning with Pachyderm'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B17085_08_Final_SB_Epub.xhtml#_idTextAnchor184), *Creating
    an End-to-End Machine Learning Workflow*, we implemented an **End-to-End** (**E2E**)
    **Machine Learning** (**ML**) workflow based on a **Named-Entity Recognition**
    (**NER**) pipeline example. This was a multi-step pipeline that included many
    computational stages, including data cleaning, **Part-Of-Speech** (**POS**) tagging,
    model training, and running the new model against various data. Our goal was to
    find the main characters in the story, which we successfully achieved.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore various strategies that can be implemented
    to select optimal parameters for an ML problem. This technique is called **hyperparameter
    tuning** or optimization. In the second part of this chapter, we will implement
    a hyperparameter tuning pipeline based on a house price prediction example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter includes the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing hyperparameter tuning techniques and strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a hyperparameter tuning pipeline in Pachyderm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have specific components installed and configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a local macOS installation, you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: macOS Mojave, Catalina, Big Sur, or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Desktop for Mac 10.14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube` v1.9.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a local Windows installation, you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Windows Pro 64-bit v10 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windows Subsystem for Linux** (**WSL**) 2 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft PowerShell v6.2.1 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyper-V
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube` v1.9.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` v1.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an **Amazon Elastic Kubernetes Service** (**Amazon EKS**) installation,
    you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` v.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eksctl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws-iam-authenticator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a Microsoft Azure cloud installation, you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` v.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Azure Command-Line Interface** (**Azure CLI**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jq` 1.5 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a **Google Kubernetes Engine** (**GKE**) cloud installation, you should
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud **Software Development Kit** (**SDK**) v124.0.0\. or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` v.18 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pachctl` 2.0.0 or later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pachyderm 2.0.0 or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be able to run the pipelines in this chapter, you do not need any special
    hardware. If you are running your Pachyderm cluster locally, any modern laptop
    should support all operations in this section. If you are running Pachyderm in
    a cloud platform, you will need to have a **Persistent Volume** (**PV**). See
    [*Chapter 5*](B17085_05_Final_SB_Epub.xhtml#_idTextAnchor123), *Installing Pachyderm
    on a Cloud Platform*, for more details.
  prefs: []
  type: TYPE_NORMAL
- en: All scripts and data described in this chapter are available at [https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter09-Distributed-Hyperparameter-Tuning-with-Pachyderm](https://github.com/PacktPublishing/Reproducible-Data-Science-with-Pachyderm/tree/main/Chapter09-Distributed-Hyperparameter-Tuning-with-Pachyderm).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the technical requirements for this chapter, let's
    take a closer look at our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing hyperparameter tuning techniques and strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning** or **hyperparameter optimization** is a technique
    that ML professionals use to determine the best parameters to solve a specific
    ML problem. In different problems, you''d need to tune different types of parameters,
    such as weights in neural networks, or the number of trees in the Random Forest
    algorithm, or the learning rate of your model. Ultimately, selecting the best
    parameters helps you determine which method is best to solve a problem. A data
    scientist needs to understand the tunable parameters in the algorithm they use
    to be able to optimize them correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ML algorithms that help solve the hyperparameter optimization
    problem. Let's review the most common ones.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Grid search** is the simplest algorithm and is sometimes called a *brute-force*
    approach to hyperparameter optimization. This method calculates the optimum values
    of hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: In Grid search, you typically define such hyperparameters as learning rate,
    dropout rate, or batch size. Then, you define a range of possible values. After
    that, the algorithm runs and searches for all possible configurations.
  prefs: []
  type: TYPE_NORMAL
- en: A disadvantage of Grid search is that it is computationally expensive and is
    typically used on a smaller set of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Still, Grid search is a popular hyperparameter tuning algorithm and is the easiest
    one to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates Grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Grid search'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Grid search
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at another hyperparameter optimization technique called **Random
    search**.
  prefs: []
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Random search** is similar to Grid search, but instead of checking all possible
    combinations, it selects them randomly, which often results in better performance
    and less computational time and resources. In many cases, Random search has proven
    to find the best combination faster than the Grid search method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram illustrates Random search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Random search'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Random search
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what Random search and Grid search methods are, let's learn
    about a slightly more complex hyperparameter tuning method called Bayesian optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian optimization is a hyperparameter tuning technique that finds the minimum
    of a function. The main difference between Bayesian optimization and Grid search/Random
    search is that it keeps track of previous iterations and evaluation results and
    therefore uses probability (*P*) to predict the best combination.
  prefs: []
  type: TYPE_NORMAL
- en: A model trained with Bayesian optimization provides better results with more
    available data. Because it takes into account past results, such a model can find
    the best results with fewer iterations. Based on previous iterations, Bayesian
    optimization builds a posterior model that is closer to reality.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram demonstrates the concept of Bayesian optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Bayesian optimization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – Bayesian optimization
  prefs: []
  type: TYPE_NORMAL
- en: We have learned about the three main hyperparameter optimization techniques.
    There are more available, but these three seem to be the most popular and widely
    used. Now, let's look into model evaluation metrics that we can use to determine
    the performance of our models. Because the problem we are going to discuss later
    in this chapter is a regression problem, we will only consider regression evaluation
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Regression evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just picking the right algorithm is only half of our success. We need to use
    an evaluation metric that determines the performance of our model. Evaluation
    metrics can be applied for various parameters to determine the best parameters.
    They can also be applied to multiple algorithms so that they can be compared and
    presented for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the house price prediction example is a regression problem, we will
    only consider regression evaluation metrics. The most common evaluation metrics
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**R-squared** (**R2**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean Square Error** (**MSE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean Absolute Error** (**MAE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics are known statistical methods for evaluating performance.
  prefs: []
  type: TYPE_NORMAL
- en: R2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R2 is an evaluation metric that is used in statistics to determine the variance
    in a dependent variable or how close the data is to the regression line. The parameter
    is measured as a percentage. If you get an R2 value of 100%, this means that the
    data fits the regression model perfectly. However, other values are acceptable,
    including 75%, 50%, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to express R2 in a formula, but the simplest one looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17085_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you are not that familiar with math, don't worry about this too much as in
    our code, we use the `scikit-learn` module to calculate R2.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a general understanding of R2, let's learn about another evaluation
    metric called MSE.
  prefs: []
  type: TYPE_NORMAL
- en: MSE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MSE is an evaluation metric that estimates the difference between the predicted
    values with the observed value. MSE is highly influenced by outliers, values that
    are outside of the standard range. Therefore, you must remove outliers before
    you evaluate your model.
  prefs: []
  type: TYPE_NORMAL
- en: The lower the MSE value, the closer the result is to the real value.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have a model that predicts salary based on years of service,
    your model MSE might be 200, meaning that the predicted value was **US Dollars**
    (**USD**) $200 higher than the actual value. Depending on the sample size, the
    overall scale, and expected precision, this number might have different significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula is used to calculate MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17085_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following values are used in this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n*: A sample size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*observed*: The actual value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*predicted*: The value that the model has predicted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with R2, don't worry about the formula too much as we will use the `scikit-learn`
    MSE module to calculate MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's learn about MAE.
  prefs: []
  type: TYPE_NORMAL
- en: MAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MAE is another evaluation metric often used with regression models. It calculates
    an average of total errors in your model or an absolute difference between real
    and forecasted values. If we were to express MAE in the simplest formula, it would
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17085_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This metric is provided for your information only. We will not use it in our
    calculation. However, you can try it if you'd like by using the `scikit-learn`
    MAE module as well.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these metrics, there are a number of other metrics that can help
    you evaluate your model, including **Root MSE** (**RMSE**) and **Adjusted R2**.
    However, the ones that we mentioned previously are the most commonly used and
    the easiest to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the methodology, evaluation metrics, and the
    algorithms we will use to configure hyperparameter tunning in Pachyderm, let's
    review the actual example, the model, the code, and the pipeline specifications.
    By the end of the next section, we will have a working hyperparameter tuning example
    in Pachyderm.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a hyperparameter tuning pipeline in Pachyderm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore our hyperparameter tuning pipeline and will
    create all the required attributes in Pachyderm to run our example.
  prefs: []
  type: TYPE_NORMAL
- en: Example overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The house price prediction challenge is one of the classic ML examples of hyperparameter
    tunning optimization. It might not sound that complicated and may even be easy
    to predict based on your empirical experience. Likely, you know the area where
    you live pretty well and can estimate the price of houses based on square footage,
    number of rooms, adjacent land plot, and other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This information can be represented in a form of a **two-dimensional** (**2D**)
    array or a table with mentioned parameters. Here is an example of such a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Sample housing data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_Table_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Sample housing data
  prefs: []
  type: TYPE_NORMAL
- en: Based on this information, you can predict the price of a house with similar
    characteristics without using any ML model and just by looking at these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: But imagine that all the data you have is a **Comma-Separated Values** (**CSV**)
    file with thousands of rows and 60+ columns. You do not know anything about the
    area, and you haven't ever lived there. Imagine that you want to predict house
    prices continuously based on data that changes all the time. That's where creating
    a highly performing ML model comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will use a dataset that is available for free on `train.csv`
    version of this dataset as we will modify it to clean the data.
  prefs: []
  type: TYPE_NORMAL
- en: The `train.csv` dataset includes *81* columns and *1,461* rows. You can view
    the `data_description.txt` file to review the column descriptions. The columns
    include the various parameters that affect the price of a house. Each row represents
    an example of a house sale with a specific sale price.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an extract from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Extract from the housing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Extract from the housing dataset
  prefs: []
  type: TYPE_NORMAL
- en: We will attempt to create a model that we will train on our training data to
    predict house prices and will evaluate the performance of the model by using the
    R2 and MSE evaluation metrics that we discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Hyperparameter tuning pipeline](img/B17085_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Hyperparameter tuning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic idea of our pipeline, let's review each step of our
    pipeline workflow in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an exploratory analysis pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first pipeline explores the data and gives us some basic information about
    the dataset that we are using. Here is the pipeline specification of the exploratory
    analysis pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This pipeline takes all data from the data repository located under `/*` and
    runs the `data-explore.py` script against it. The pipeline uses the `hyperparameter-example:1.0`
    Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review what''s the `data-explore.py` script does. The script imports
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will use `pandas` to manipulate CSV tables and represent them as `DataFrame`
    structures. `pandas` is an open source Python library that is widely used by data
    scientists for data operations, specifically 2D tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: We will use, familiar to us from previous sections, `matplotlib` in combination
    with the `seaborn` library to visualize the results of our computations. `seaborn`
    is based on `matplotlib` but provides more sophisticated and visually appealing
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the `data-explore.py` script reads the `housing-train.csv`
    file as a `DataFrame` from the `data` repository and computes a correlation matrix
    between all columns in our dataset by using the `pandas.DataFrame.corr()` method.
    The code then creates a heatmap of the created correlation matrix and saves it
    in the pipeline output repository. The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part of the code saves the types of data objects in a column. Typically,
    you use a different approach to analyzing and manipulating numerical and categorical
    data, which is why getting this information might be important. The code saves
    this information in a `data-types.csv` file and is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The third part of the script checks the columns for missing data, creates a
    table with percentages of columns that have missing data, and saves the table
    to `no-data.csv`, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create this pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Pachyderm is up and running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns the following output (your version of `pachctl` and `pachd`
    might be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a data repository by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command does not return any output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Move the `housing-train.csv` file to the master branch of the `data` repository
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system response should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the file was added to the repository with the `file` type by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system response should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `data-explore` pipeline by using the `data-explore.yaml` file, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command does not return any response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the pipeline was created by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the system response that you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Wait for the pipeline to finish running and display a `success` status for the
    last job.
  prefs: []
  type: TYPE_NORMAL
- en: 'List the repositories, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see that the `data-explore` pipeline uploaded `3.361MiB` of data
    to the `data-explore` repository, as indicated in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explore the data in the repository by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following three files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s open the `/data-types.csv file`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The file includes data types for each column, as we can see in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at what''s in the `no-data.csv` file by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you could also open the file in an application on your computer.
    For example, in macOS, you can open it in the `Numbers` application, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This file contains information about columns and the percentage of missing
    data in these columns. This is very useful for data cleaning. Some columns have
    more than 80% of missing data. These columns can be removed so that they don''t
    interfere with our calculations. We will do that in our next pipeline. Here is
    a list of columns with the majority of data missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Columns with missing data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Columns with missing data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the heatmap by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following heatmap that shows us a correlation between all
    columns in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Heatmap of all parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Heatmap of all parameters
  prefs: []
  type: TYPE_NORMAL
- en: This heatmap has too many parameters and is difficult to read. But even on this
    heatmap, we can see that some parameters affect the sale price more than others.
    For example, it looks as though the `OverallQuality` parameter affects the price
    the most, as well as the `GrLivArea` parameter (for *great living area*). We will
    try to narrow down our dataset to these parameters in our next pipeline step.
  prefs: []
  type: TYPE_NORMAL
- en: We have explored the dataset and got a basic understanding of the data. Now,
    let's review our next pipeline, which will clean up the data based on our findings.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data cleaning pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next step is to create a pipeline that cleans our data. This pipeline will
    clean the data according to our findings in the previous section. Here is the
    pipeline specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is a standard Pachyderm pipeline that takes the data from the `data` repository
    and runs the `data-clean.py` script against that data. The data, in this case,
    is our `housing-train.csv` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our `data-clean.py` script. The script imports the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These components are similar to the ones in the `data-explore` pipeline. The
    new component that is being imported is `pandas.Series`, which we need to be able
    to save our data to a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of our script reads the data from the `housing-train.csv` dataset
    as a `DataFrame`. Then, we drop the columns that have more than 40% of columns
    with missing data and save the columns that we have dropped in the `col_drop.csv`
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a new correlation matrix that only includes the parameters
    that affect the `SalePrice` column with a coefficient of 0.5 or larger. We plot
    a new heatmap and save it in the `heatmap2.png` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we remove the columns that are not part of the new correlation matrix
    and save them in a new dataset called `cleaned-data.csv` in the pipeline output
    repository, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create this data cleaning pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Pachyderm is up and running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Your version of Pachyderm might be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `data-clean` pipeline, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No system response is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Pachyderm has successfully created the pipeline by running the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the system response that you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You need to wait for the pipeline to change its status to `success`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the pipeline has successfully finished running, list the repositories
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see that the `data-clean` pipeline added `780.4KiB` of data, as
    indicated in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the data in the repository by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the columns that were dropped by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The file includes data types for each column, as we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: These were the columns that had more than 40% columns empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve also removed all the columns that we mapped in our new correlation matrix
    and that have a correlation coefficient of less than 0.5\. Run the following command
    to see what our new correlation matrix looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the new heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Refined heatmap'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Refined heatmap
  prefs: []
  type: TYPE_NORMAL
- en: This heatmap makes much more sense. We can clearly see the parameters that are
    the most prominent in determining the house sale price. Our dataset has been drastically
    reduced to 11 columns only and was saved to a new `cleaned-data.csv` file. Now,
    this strategy might not be ideal for every use case—you might decide to keep more
    parameters in your dataset to ensure and check if the model would perform better
    with more parameters. But for the purpose of this example, this should be sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have cleaned the data, we also need to make sure that we remove
    any outliers or parameters that are outside of the standard range. We will do
    this in our next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline that removes outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next pipeline will evaluate outliers in our dataset and will remove them
    so that our model performance is not affected by them. We will again use a standard
    Pachyderm pipeline specification to achieve this goal. Here is the pipeline specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline specification takes the cleaned data from the `data-clean` repository
    and applies the `outliers.py` Python script to that data. The pipeline uses the
    same Docker image as the two previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: The `outliers.py` script imports the same list of components as the scripts
    in our previous pipeline steps, including `seaborn`, `matplotlib`, and `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script reads the data from the `cleaned-data.csv` file. Then, it creates
    a histogram that displays outliers in the dataset and saves that histogram to
    the `histogram.png` file. Then, we only leave 50% of the data that is within the
    middle range and remove the rest. We create another histogram that shows this
    new data. We drop the data from the dataset and save it in a new CSV file called
    `removed-outliers-data.csv`. The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create this pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Pachyderm is up and running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Your version might be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `data-clean` pipeline, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command does not return any output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the pipeline was created by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the system response that you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `remove-outliers` pipeline is starting. You can run the `pachctl list pipeline`
    command several times until the pipeline succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: 'List the repositories by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `remove-outliers` repository should have `413.7KiB` of data uploaded to
    it, as indicated in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'List the files in the repository by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first open the `histogram.png` file by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is what you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Outliers in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – Outliers in the dataset
  prefs: []
  type: TYPE_NORMAL
- en: This boxplot shows us that the majority of houses have a sale price in the range
    of $50,000 to $350,000, with most of these being in the range of $110,000 to $220,000\.
    A few others are way outside of this range and might be even considered a separate
    category. Our pipeline removes outliers outside the main box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the histogram after the outliers were removed. We can do
    this by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the new histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Histogram with removed outliers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 – Histogram with removed outliers
  prefs: []
  type: TYPE_NORMAL
- en: We have removed some of the rows from our dataset and we now have 1,400 rows
    instead of the 1,481 rows that we had before.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished cleaning our data, we can next train our model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our next pipeline will train our model on a training part of our dataset. Here
    is the pipeline specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is another standard Pachyderm pipeline. It takes the data
    from the `remove-outliers` repository and applies the `train.py` script to it.
    It uses the same Docker image as other pipelines in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of components that the `train.py` script imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We import `train_test_split`, `metrics`, `r2_score`, `mean_squared_error`, and
    `make_scorer` modules from the `sklearn` library to split the data into train
    and test data and calculate the R2 and MSE metrics for our model. We import the
    `Ridge` regression model from `sklearn.linear_model` to train our model using
    Ridge regression. Ridge regression is a variation of linear regression, one of
    the algorithms that you can use for this type of regression problem. We import
    `seaborn` and `matplotlib` to visualize our results, and `pandas` and `numpy`
    to manipulate the data. `redirect_stdout` is used to redirect output to a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of our script reads the `removed-outliers-data.csv` file from
    the `remove-outliers` repository as a `DataFrame`. Then, we use `train_test_split`
    to split our dataset into training and testing parts. The training part is used
    to train the data, and the testing part is used to test the performance of our
    model in the cross-validation stage. The code is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define our estimator, which is the Ridge regression. `alpha` is the
    parameter that we will be tuning to achieve better performance. We initially set
    `alpha` to `1`, make a prediction, and save our R2 and MSE scores in the `r_squared_mse.txt`
    file, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we''ll plot our data and save it in the `prediction.png` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create this pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that your Pachyderm cluster is running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `train` pipeline by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: No output is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'List all pipelines by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The output should list the `train` pipeline. Wait for the pipeline to finish
    running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the repositories. We can do this by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see a new repository called `train` added with `186.3KiB` of data,
    as indicated in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the files that were uploaded to the repository. We can
    do this by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: There should be two files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `r_squared_mse.txt` file to check the MSE and R2 scores. You can do
    this by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Our R2 value is pretty high, meaning that the calculation should be quite precise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s open the `prediction.png` file by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is what you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Predicted versus real price'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Predicted versus real price
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the predicted price looks pretty close to the real one, with
    a few minor exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: In our last pipeline step, we will try to find the best value of `alpha` and
    will perform cross-validation of our parameters with Grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an evaluation pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our evaluation pipeline specification looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Since you've seen quite a few of those, you can probably guess that it's a standard
    Pachyderm pipeline that takes data from the `remove-outliers` repository and applies
    the `grid-search.py` file to that data. The pipeline uses the same Docker image
    as all other pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The `grid-search.py` file imports the components already familiar to us from
    previous sections. In addition, it imports `GridSearchCV` from the `sklearn.model_selection`
    library and `joblib`, which saves the model to a `pickle` file.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the script performs the same data manipulation as in `train.py`—it
    opens the data file and splits it into two sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set the `estimator` property to have `Ridge` regression and specify
    `scoring` values and `alpha` parameters, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of our script uses `GridSearchCV` to train and determine the
    best `alpha` parameter and saves the best score and the best `alpha` parameter
    in the `best_score.txt` file. The model is also saved in the `my_model.pkl` file.
    The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we plot our performance graph and save it to `performance-plot.png`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the next steps to create this pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Pachyderm is running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output. Note that your version of `pachctl` and
    `pachd` might vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `evaluate` pipeline by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command does not return any output.
  prefs: []
  type: TYPE_NORMAL
- en: 'View the active pipelines by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: You should see the `evaluate` pipeline running or finished with a `success`
    status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the repository that the `evaluate` pipeline has created by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see a new repository called `train` with `121KiB` of data in it,
    as indicated in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'List the files in the `evaluate` repository by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: These files are our model, best MSE and R2 scores, and best `alpha` parameter,
    and a graph that shows how the training data compares to testing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our best scores by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Alpha 10 is our best `alpha` parameter. It should be used for predicting house
    prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `performance-plot.png` file should show us how our training data evaluates
    against the testing data. We can view this by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the graph it outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Performance plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17085_09_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – Performance plot
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, `alpha=10` is likely the best parameter in the range that we
    have provided. This pipeline concludes our example. The resulting model can be
    used to predict house prices with the trained `alpha` hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our example. Now, let's clean up our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After you are done experimenting, you might want to clean up your cluster so
    that you start your next experiment with a fresh install. To clean up the environment,
    proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete all pipelines and repositories by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that no repositories and pipelines exist in your cluster by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: You have successfully cleaned up your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to implement an ML pipeline that performs
    hyperparameter tuning on a house price prediction example. We've created five
    steps of this pipeline, each outputting relevant files and information into Pachyderm
    output repositories. In our first pipeline, we performed an exploratory analysis
    to gather a general understanding of the dataset and built a heatmap that helped
    us outline the correlation between various parameters in our dataset. In our second
    pipeline, we cleaned the data of columns with missing information, as well as
    removed parameters that have little influence on the sale price of a house. In
    our third pipeline, we removed outliers—values that were outside of the standard
    range. Our fourth pipeline split our dataset into two parts—one for testing and
    the other for training. And finally, our fifth pipeline performed hyperparameter
    tuning for the `alpha` parameter and found the best alpha for our use case. The
    last pipeline output our model in a `.pkl` file and created a graph where we could
    see the performance of our model against the training and testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about Pachyderm language clients. While you
    can do pure Python, R, or Scala in Pachyderm, you could also leverage one of our
    language clients or even build your own to take Pachyderm even further.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kaggle House Prices dataset: [https://www.kaggle.com/lespin/house-prices-dataset](https://www.kaggle.com/lespin/house-prices-dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn`: [https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
