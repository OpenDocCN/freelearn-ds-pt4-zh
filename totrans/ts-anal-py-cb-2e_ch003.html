<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Time Series Analysis with Python Cookbook, 2E - Second Edition</title>
<link rel="stylesheet" type="text/css" href="../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/><link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content"><section id="reading-time-series-data-from-files" class="level1 pkt" data-number="3">
<h1 data-number="3">2 Reading Time Series Data from Files</h1>
<section id="join-our-book-community-on-discord-1" class="level2" data-number="3.1">
<h2 data-number="3.1">Join our book community on Discord</h2>
<p>
<img style="width:15rem" src="../media/file0.png" width="200" height="200"/>
</p>
<p><a href="https://packt.link/zmkOY">https://packt.link/zmkOY</a></p>
<p>In this chapter, we will use <strong>pandas</strong>, a popular <strong>Python</strong> library with a rich set of I/O tools, data wrangling, and date/time functionality to streamline working with <strong>time series data</strong>. In addition, you will explore several reader functions available in pandas to ingest data from different file types, such as <strong>Comma-Separated Value</strong> (<strong>CSV</strong>), Excel, and SAS. You will explore reading from files, whether stored locally on your drive or remotely on the cloud, such as an <strong>AWS S3 bucket</strong>.</p>
<p>Time series data is complex and can be in different shapes and formats. Conveniently, the pandas reader functions offer a vast number of arguments (parameters) to help handle such variety in the data.</p>
<p>The <strong>pandas</strong> library provides two fundamental data structures, Series and DataFrame, implemented as classes. The DataFrame class is a distinct data structure for working with tabular data (think rows and columns in a spreadsheet). The main difference between the two data structures is that a Series is one-dimensional (single column), and a DataFrame is two-dimensional (multiple columns). The relationship between the two is that you get a Series when you slice out a column from a DataFrame. You can think of a DataFrame as a side-by-side concatenation of two or more Series objects.</p>
<p>A particular feature of the Series and DataFrames data structures is that they both have a labeled axis called an index. A specific type of index that you will often see with time series data is the <code>DatetimeIndex</code> , which you will explore further in this chapter. Generally, the index makes slicing and dicing operations very intuitive. For example, to make a DataFrame ready for time series analysis, you will learn how to create DataFrames with an index of the <code>DatetimeIndex</code> type.</p>
<p>We will cover the following recipes on how to ingest data into a pandas DataFrame:</p>
<ul>
<li>Reading data from CSVs and other delimited files</li>
<li>Reading data from an Excel file</li>
<li>Reading data from URLs</li>
<li>Reading data from Parquet filesWorking with large data files</li>
</ul>
<blockquote>
<p>WHY DATETIMEINDEX?</p>
<blockquote>
<p>A pandas DataFrame with an index of the <code>DatetimeIndex</code> type unlocks a large set of features and useful functions needed when working with time series data. You can think of it as adding a layer of intelligence or awareness to pandas to treat the DataFrame as a time series DataFrame.</p>
</blockquote>
</blockquote>
</section>
<section id="technical-requirements-1" class="level2" data-number="3.2">
<h2 data-number="3.2">Technical requirements</h2>
<p>In this chapter and forward, we will extensively use pandas 2.2.0 (released January 20, 2024).</p>
<p>Throughout our journey, you will be installing additional Python libraries to use in conjunction with pandas. You can download the Jupyter notebooks from the GitHub repository (<a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch2/Chapter%202.ipynb">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch2/Chapter%202.ipynb</a>) to follow along.</p>
<p>You can download the datasets used in this chapter from the GitHub repository using this link: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch2">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch2</a>.</p>
</section>
<section id="reading-data-from-csvs-and-other-delimited-files" class="level2" data-number="3.3">
<h2 data-number="3.3">Reading data from CSVs and other delimited files</h2>
<p>In this recipe, you will use the <code>pandas.read_csv()</code> function, which offers a large set of parameters that you will explore to ensure the data is properly read into a time series DataFrame. In addition, you will learn how to specify an index column, parse the index to be of the type <code>DatetimeIndex</code>, and parse string columns that contain dates into <code>datetime</code> objects.</p>
<p>Generally, using Python, data read from a CSV file will be in string format (text). When using the <code>read_csv</code> method in pandas, it will try to infer the appropriate data types (dtype), and, in most cases, it does a great job at that. However, there are situations where you will need to explicitly indicate which columns to cast to a specific data type. For example, you will specify which column(s) to parse as dates using the <code>parse_dates</code> parameter in this recipe.</p>
<section id="getting-ready-3" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1">Getting ready</h3>
<p>You will read a CSV file containing hypothetical box office numbers for a movie. The file is provided in the GitHub repository for this book. The data file is in <code>datasets/Ch2/movieboxoffice.csv</code>.</p>
</section>
<section id="how-to-do-it-3" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2">How to do it…</h3>
<p>You will ingest our CSV file using pandas and leverage some of the available parameters in <code>read_csv</code>:</p>
<ol>
<li>First, let's load the libraries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path</code></pre>
</div>
<ol>
<li>Create a <code>Path</code> object for the file location:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>filepath =\
 Path('../../datasets/Ch2/movieboxoffice.csv')</code></pre>
</div>
<ol>
<li>Read the CSV file into a DataFrame using the <code>read_csv</code> function and pass the <code>filepath</code> with additional parameters.</li>
</ol>
<p>The first column in the CSV file contains movie release dates, and it needs to be set as an index of the <code>DatetimeIndex</code> (<code>index_col=0</code> and <code>parse_dates=['Date']</code>) types. Specify which columns you want to include by providing a list of column names to <code>usecols</code>. The default behavior is that the first row includes the header (<code>header=0</code>):</p>
<div class="C1-SHCodePACKT">
<pre><code>ts = pd.read_csv(filepath,
                 header=0,
                 parse_dates=['Date'],
                 index_col=0,
                 infer_datetime_format=True,
                 usecols=['Date',
                          'DOW',
                          'Daily',
                          'Forecast',
                          'Percent Diff'])
ts.head(5)</code></pre>
</div>
<p>This will output the following first five rows:</p>
<figure>
<img src="../media/file16.png" alt="Figure 2.1: The first five rows of the ts DataFrame in JupyterLab" width="441" height="206"/><figcaption aria-hidden="true">Figure 2.1: The first five rows of the ts DataFrame in JupyterLab</figcaption>
</figure>
<ol>
<li>Print a summary of the DataFrame to check the index and column data types:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>ts.info()
&gt;&gt; &lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 128 entries, 2021-04-26 to 2021-08-31
Data columns (total 4 columns):
 #   Column        Non-Null Count  Dtype
---  ------        --------------  -----
 0   DOW           128 non-null    object
 1   Daily         128 non-null    object
 2   Forecast      128 non-null    object
 3   Percent Diff  128 non-null    object
dtypes: object(4)
memory usage: 5.0+ KB</code></pre>
</div>
<ol>
<li>Notice that the <code>Date</code> column is now an index (not a column) of the type <code>DatetimeIndex</code>. Additionally, both the <code>Daily</code> and <code>Forecast</code> columns have the wrong dtype inference. You would expect them to be of the <code>float</code> type. The issue is due to the source CSV file containing dollar signs (<code>$</code>) and thousand separators (<code>,</code>) in both columns. The presence of non-numeric characters will cause the columns to be interpreted as strings. A column with the <code>dtype</code> object indicates either a string column or a column with mixed dtypes (not homogeneous).</li>
</ol>
<p>To fix this, you need to remove both the dollar sign (<code>$</code>) and thousand separators (<code>,</code>) or any other non-numeric character. You can accomplish this using <code>str.replace()</code>, which can take a regular expression to remove all non-numeric characters but exclude the period (<code>.</code>) for the decimal place. Removing these characters does not convert the dtype, so you will need to cast those two columns as a float dtype using <code>.astype(float)</code>:</p>
<div class="C1-SHCodePACKT">
<pre><code>clean = lambda x: x.str.replace('[^\\d]','', regex=True)
c_df = ts[['Daily', 'Forecast']].apply(clean, axis=1)
ts[['Daily', 'Forecast']] = c_df.astype(float)</code></pre>
</div>
<p>Print a summary of the updated DataFrame:</p>
<div class="C1-SHCodePACKT">
<pre><code>ts.info()
&gt;&gt; &lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 128 entries, 2021-04-26 to 2021-08-31
Data columns (total 4 columns):
 #   Column        Non-Null Count  Dtype
---  ------        --------------  -----
 0   DOW           128 non-null    object
 1   Daily         128 non-null    float64
 2   Forecast      128 non-null    float64
 3   Percent Diff  128 non-null    object
dtypes: float64(2), object(2)
memory usage: 5.0+ KB</code></pre>
</div>
<p>Now, you have a DataFrame with <code>DatetimeIndex</code> and both <code>Daily</code> and <code>Forecast</code> columns are of the <code>float64</code> dtype (numeric fields).</p>
</section>
<section id="how-it-works-3" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3">How it works…</h3>
<p>Using pandas for data transformation is fast since it loads the data into memory. For example, the <code>read_csv</code> method reads and loads the entire data into a DataFrame in memory. When requesting a DataFrame summary with the <code>info()</code> method, the output will display memory usage for the entire DataFrame in addition to column and index data types. To get the exact memory usage for each column, including the index, you can use the <code>memory_usage()</code> method:</p>
<div class="C0-SHCodePACKT">
<pre><code>ts.memory_usage()
&gt;&gt;
Index           1024
DOW             1024
Daily           1024
Forecast        1024
Percent Diff    1024
dtype: int64</code></pre>
</div>
<p>The total will match what was provided in the DataFrame summary:</p>
<div class="C0-SHCodePACKT">
<pre><code>ts.memory_usage().sum()
&gt;&gt; 5120</code></pre>
</div>
<p>So far, you have used a few of the available parameters when reading a CSV file using <code>read_csv</code>. The more familiar you become with the different options available in any pandas reader functions, the more upfront preprocessing you can do during data ingestion (reading).</p>
<p>You leveraged the built-in <code>parse_dates</code> argument, which takes in a list of columns (either specified by name or position). The combination of <code>index_col=0</code> and <code>parse_dates=[0]</code> produced a DataFrame with an index of the <code>DatetimeIndex</code> type.</p>
<p>Let's inspect the parameters used in this recipe as defined in the official <code>pandas.read_csv()</code> documentation (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html</a>):</p>
<ul>
<li><code>filepath_or_buffer</code>: This is the first positional argument and the only required field needed (at a minimum) to read a CSV file. Here, you passed the Python path object named <code>filepath</code>. This can also be a string that represents a valid file path such as <code>'../../datasets/Ch2/movieboxoffice.csv'</code> or a URL that points to a remote file location, such as an AWS S3 bucket (we will examine this later in the <em>Reading data from URLs</em> recipe in this chapter).</li>
<li><code>sep</code>: This takes a string to specify which delimiter to use. The default is a comma delimiter (<code>,</code>) which assumes a CSV file. If the file is separated by another delimiter, such as a pipe (<code>|</code>) or semicolon (<code>;</code>), then the argument can be updated, such as <code>sep="|" or sep=";"</code>.</li>
<li>Another alias to <code>sep</code> is <code>delimiter</code>, which can be used as well as a parameter name.</li>
<li><code>header</code>: In this case, you specified that the first <code>row</code> (<code>0</code>) value contains the header information. The default value is <code>infer</code>, which usually works as-is in most cases. If the CSV does not contain a header, then you specify <code>header=None</code>. If the CSV has a header but you prefer to supply custom column names, then you need to specify <code>header=0</code> and overwrite it by providing a list of new column names to the <code>names</code> argument.</li>
<li><code>parse_dates</code>: In the recipe, you provided a list of column positions using <code>[0]</code>, which specified only the first column (by position) should be parsed. The <code>parse_dates</code> argument can take a list of column names, such as <code>["Date"]</code>, or a list of column positions, such as <code>[0, 3]</code>, indicating the first and the fourth columns. If you only intend to parse the index column(s) specified in the <code>index_col</code> parameter, you only need to pass <code>True</code> (Boolean).</li>
<li><code>index_col</code>: You specified that the first column by position (<code>index_col=0</code>) will be used as the DataFrame index. Alternatively, you could provide the column name as a string (<code>index_col='Date'</code>). The parameter also takes in a list of integers (positional indices) or strings (column names), which would create a <code>MultiIndex</code> object.</li>
<li><code>usecols</code>: The default value is <code>None</code>, which includes all the columns in the dataset. Limiting the number of columns to only those that are required results in faster parsing and overall lower memory usage, since you only bring in what is needed. The <code>usecols</code> arguments can take a list of <em>column names</em>, such as <code>['Date', 'DOW', 'Daily', 'Percent Diff', 'Forecast']</code> or a list of <em>positional indices</em>, such as <code>[0, 1, 3, 7, 6]</code>, which would produce the same result.</li>
</ul>
<p>Recall that you specified which columns to include by passing a list of column names to the <strong>usecols</strong> parameter. These names are based on the file header (the first row of the CSV file).</p>
<p>If you decide to provide custom header names, you cannot reference the original names in the <strong>usecols</strong> parameter; this will produce the following error:<code> ValueError: Usecols do not match columns.</code></p>
</section>
<section id="theres-more-3" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4">There's more…</h3>
<p>There are situations where <code>parse_dates</code> may not work (it just cannot parse the date). In such cases, the column(s) will be returned unchanged, and no error will be thrown. This is where the <code>date_format</code> parameter can be helpful.</p>
<p>The following code shows how <code>date_format</code> can be used:</p>
<div class="C0-SHCodePACKT">
<pre><code>ts = pd.read_csv(filepath,
                 parse_dates=[0],
                 index_col=0,
                 date_format="%d-%b-%Y",
                 usecols=[0,1,3, 7, 6])
ts.head()</code></pre>
</div>
<p>The preceding code will print out the first five rows of the <code>ts</code> DataFrame, displaying a correctly parsed <code>Date</code> index.</p>
<figure>
<img src="../media/file17.jpg" alt="Figure 2.2: The first five rows of the ts DataFrame using JupyterLab" width="445" height="208"/><figcaption aria-hidden="true">Figure 2.2: The first five rows of the ts DataFrame using JupyterLab</figcaption>
</figure>
<p>Let's break it down. In the preceding code, since the date is stored as a string in the form <em>26-Apr-2021</em>, you passed <code>"%d-%b-%Y"</code> to reflect that:</p>
<ul>
<li><code>%d</code> represents the day of the month, such as <code>01</code> or <code>02</code>.</li>
<li><code>%b</code> represents the abbreviated month name, such as <code>Apr</code> or <code>May</code>.</li>
<li><code>%Y</code> represents the year as a four-digit number, such as <code>2020</code> or <code>2021</code>.</li>
</ul>
<p>Other common string codes include the following:</p>
<ul>
<li><code>%y</code> represents a two-digit year, such as <code>19</code> or <code>20</code>.</li>
<li><code>%B</code> represents the month's full name, such as <code>January</code> or <code>February</code>.</li>
<li><code>%m</code> represents the month as a two-digit number, such as <code>01</code> or <code>02</code>.</li>
</ul>
<p>For more information on Python's string formats for representing dates, visit <a href="https://strftime.org">https://strftime.org</a>.</p>
</section>
<section id="see-also-3" class="level3" data-number="3.3.5">
<h3 data-number="3.3.5">See also</h3>
<p>When dealing with more complex date formats an alternate options is to use the <code>to_datetime()</code> function. The <code>to_datetime()</code> function is used to convert a string, integer, or float into a datetime object.</p>
<p>Initially, you will read the CSV data as is, then apply the <code>to_datetime()</code> function to parse the specific column(s) as desired. This is demonstrated in the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>ts = pd.read_csv(filepath,
                 index_col=0,
                 usecols=[0,1,3, 7, 6])
ts.index = pd.to_datetime(ts.index, format="%d-%b-%Y")</code></pre>
</div>
<p>The last line, <code>ts.index = pd.to_datetime(ts.index, format="%d-%b-%Y"), </code>converts the index of the <code>ts</code> DataFrame into a <code>DatetimeIndex</code> object. Notice how we specified the data string format similar to what we did with the <code>date_format</code> parameter in the <code>read_csv()</code> function in the <em>There’s more…</em> section.</p>
</section>
</section>
<section id="reading-data-from-an-excel-file" class="level2" data-number="3.4">
<h2 data-number="3.4">Reading data from an Excel file</h2>
<p>To read data from an Excel file, you will need to use a different reader function from pandas. Generally, working with Excel files can be challenging since the file can contain formatted multi-line headers, merged header cells, and images. They may also contain multiple worksheets with custom names (labels). Therefore, it is vital that you always inspect the Excel file first. The most common scenario is reading from an Excel file that contains data partitioned into multiple sheets, which is the focus of this recipe.</p>
<p>In this recipe, you will be using the <code>pandas.read_excel()</code> function and will examine the various parameters available to ensure the data is read properly as a DataFrame with a <code>DatetimeIndex</code> for time series analysis. In addition, you will explore different options to read Excel files with multiple sheets.</p>
<section id="getting-ready-4" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1">Getting ready</h3>
<p>To use <code>pandas.read_excel()</code>, you will need to install an additional library for reading and writing Excel files. In the <code>read_excel()</code> function, you will use the engine parameter to specify which library (engine) to use for processing an Excel file. Depending on the Excel file extension you are working with (for example, <code>.xls</code> or <code>.xlsx</code>), you may need to specify a different engine that may require installing an additional library.</p>
<p>The supported libraries (engines) for reading and writing Excel include <code>xlrd</code>, <code>openpyxl</code>, <code>odf</code>, and <code>pyxlsb</code>. When working with Excel files, the two most common libraries are usually <code>xlrd</code> and <code>openpyxl</code>.</p>
<p>The <code>xlrd</code> library only supports <code>.xls</code> files. So, if you are working with an older Excel format, such as <code>.xls</code>, then <code>xlrd</code> will do just fine. For newer Excel formats, such as <code>.xlsx</code>, we will need a different engine, and in this case, <code>openpyxl</code> would be the recommendation to go with.</p>
<p>To install <code>openpyxl</code> using <code>conda</code>, run the following command in the terminal:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; conda install openpyxl</code></pre>
</div>
<p>To install using <code>pip</code>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; pip install openpyxl</code></pre>
</div>
<p>We will be using the <code>sales_trx_data.xlsx</code> file, which you can download from the book's GitHub repository. See the <em>Technical requirements</em> section of this chapter. The file contains sales data split by year into two sheets (<code>2017</code> and <code>2018</code>), respectively.</p>
</section>
<section id="how-to-do-it-4" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2">How to do it…</h3>
<p>You will ingest the Excel file (<code>.xlsx</code>) using pandas and <code>openpyxl</code>, and leverage some of the available parameters in <code>read_excel()</code>:</p>
<ol>
<li>Import the libraries for this recipe:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
filepath = \
Path('../../datasets/Ch2/sales_trx_data.xlsx')</code></pre>
</div>
<ol>
<li>Read the Excel (<code>.xlxs</code>) file using the <code>read_excel()</code>function. By default, pandas will only read from the first sheet. This is specified under the <code>sheet_name</code> parameter, which is set to <code>0</code> as the default value. Before passing a new argument, you can use <code>pandas.ExcelFile</code> first to inspect the file and determine the number of sheets available. The <code>ExcelFile</code> class will provide additional methods and properties, such as <code>sheet_name</code>, which returns a list of sheet names:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>excelfile = pd.ExcelFile(filepath)
excelfile.sheet_names
&gt;&gt; ['2017', '2018']</code></pre>
</div>
<p>If you have multiple sheets, you can specify which sheets you want to ingest by passing a list to the <code>sheet_name</code> parameter in <code>read_excel</code>. The list can either be positional arguments, such as first, second, and fifth sheets with <code>[0, 1, 4]</code>, sheet names with <code>["Sheet1", "Sheet2", "Sheet5"]</code>, or a combination of both, such as first sheet, second sheet, and a sheet named <code>"Revenue"</code> <code>[0, 1, "Revenue"]</code>.</p>
<p>In the following code, you will use sheet positions to read both the first and second sheets (<code>0</code> and <code>1</code> indexes). This will return a Python <code>dictionary</code> object with two DataFrames. Note that the returned dictionary (key-value pair) has numeric keys (<code>0</code> and <code>1</code>) representing the first and second sheets (positional index), respectively:</p>
<div class="C1-CodePACKT">
<pre><code>ts = pd.read_excel(filepath,
                   engine='openpyxl',
                   index_col=1,
                   sheet_name=[0,1],
                   parse_dates=True)
ts.keys()
&gt;&gt; dict_keys([0, 1])</code></pre>
</div>
<ol>
<li>Alternatively, you can pass a list of sheet names. Notice that the returned dictionary keys are now strings and represent the sheet names as shown in the following code:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>ts = pd.read_excel(filepath,
                   engine='openpyxl',
                   index_col=1,
                   sheet_name=['2017','2018'],
                   parse_dates=True)
ts.keys()
&gt;&gt; dict_keys(['2017', '2018'])</code></pre>
</div>
<ol>
<li>If you want to read from all the available sheets, you will pass <code>None</code> instead. The keys for the dictionary, in this case, will represent sheet names:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>ts = pd.read_excel(filepath,
                   engine='openpyxl',
                   index_col=1,
                   sheet_name=None,
                   parse_dates=True)
ts.keys()
&gt;&gt; dict_keys(['2017', '2018'])</code></pre>
</div>
<p>The two DataFrames within the dictionary are identical (homogeneous-typed) in terms of their schema (column names and data types). You can inspect each DataFrame with <code>ts['2017'].info()</code> and <code>ts['2018'].info()</code>.</p>
<p>They both have a <code>DatetimeIndex</code> object, which you specified in the <code>index_col</code> parameter. The 2017 DataFrame consists of 36,764 rows and the 2018 DataFrame consists of 37,360. In this scenario, you want to stack (combine) the two (think <code>UNION</code> in SQL) into a single DataFrame that contains all 74,124 rows and a <code>DatetimeIndex</code> that spans from <code>2017-01-01</code> to <code>2018-12-31</code>.</p>
<p>To combine the two DataFrames along the index axis (stacked one on top of the other), you will use the <code>pandas.concat()</code> function. The default behavior of the <code>concat()</code> function is to concatenate along the index axis (<code>axis=0</code>). In the following code, you will explicitly specify which DataFrames to concatenate:</p>
<div class="C1-CodePACKT">
<pre><code>ts_combined = pd.concat([ts['2017'],ts['2018']])
ts_combined.info()
&gt;&gt; &lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 74124 entries, 2017-01-01 to 2018-12-31
Data columns (total 4 columns):
 #   Column              Non-Null Count  Dtype
---  ------              --------------  -----
 0   Line_Item_ID        74124 non-null  int64
 1   Credit_Card_Number  74124 non-null  int64
 2   Quantity            74124 non-null  int64
 3   Menu_Item           74124 non-null  object
dtypes: int64(3), object(1)
memory usage: 2.8+ MB</code></pre>
</div>
<ol>
<li>When you have multiple DataFrames returned (think multiple sheets), you can use the <code>concat()</code> function on the returned dictionary. In other words, you can combine the <code>concat()</code> and <code>read_excel()</code> functions in one statement. In this case, you will end up with a <code>MultiIndex</code> DataFrame where the first level is the sheet name (or number) and the second level is the <code>DatetimeIndex</code>. For example, using the <code>ts</code> dictionary, you will get a two-level index: <code>MultiIndex([('2017', '2017-01-01'), ..., ('2018', '2018-12-31')], names=[None, 'Date'], length=74124)</code>.</li>
</ol>
<p>To reduce the number of levels, you can use the <code>droplevel(level=0)</code> method to drop the first level after pandas <code>.concat()</code> shown as follows:</p>
<div class="C1-CodePACKT">
<pre><code>ts_combined = pd.concat(ts).droplevel(level=0)</code></pre>
</div>
<ol>
<li>If you are only reading one sheet, the behavior is slightly different. By default, <code>sheet_name</code> is set to <code>0</code>, which means it reads the first sheet. You can modify this and pass a different value (single value), either the sheet name (string) or sheet position (integer). When passing a single value, the returned object will be a pandas DataFrame and not a dictionary:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>ts = pd.read_excel(filepath,
                   index_col=1,
                   sheet_name='2018',
                   parse_dates=True)
type(ts)
&gt;&gt; pandas.core.frame.DataFrame</code></pre>
</div>
<p>Do note though that if you pass a single value inside two brackets (<code>[1]</code>), then pandas will interpret this differently and the returned object will be a dictionary that contains one DataFrame.</p>
<p>Lastly, note that you did not need to specify the engine in the last example. The <code>read_csv</code> function will determine which engine to use based on the file extension. So, for example, suppose the library for that engine is not installed. In that case, it will throw an <code>ImportError</code> message, indicating that the library (dependency) is missing.</p>
</section>
<section id="how-it-works-4" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3">How it works…</h3>
<p>The <code>pandas.read_excel()</code> function has many common parameters with the <code>pandas.read_csv()</code> function that you used earlier. The <code>read_excel</code> function can either return a DataFrame object or a dictionary of DataFrames. The dependency here is whether you are passing a single value (scalar) or a list to <code>sheet_name</code>.</p>
<p>In the <code>sales_trx_data.xlsx</code> file, both sheets had the same schema (homogeneous- typed). The sales data was partitioned (split) by year, where each sheet contained sales for a particular year. In this case, concatenating the two DataFrames was a natural choice. The <code>pandas.concat()</code> function is like the <code>DataFrame.append()</code> function, in which the second DataFrame was added (appended) to the end of the first DataFrame. This should be similar in behavior to the <code>UNION</code> clause for those coming from a SQL background.</p>
</section>
<section id="theres-more-4" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4">There's more…</h3>
<p>An alternative method to reading an Excel file is with the <code>pandas.ExcelFile()</code> class, which returns a pandas <code>ExcelFile</code> object. Earlier in this recipe, you used <code>ExcelFile()</code> to inspect the number of sheets in the Excel file through the <code>sheet_name</code> property.</p>
<p>The <code>ExcelFile</code> class has several useful methods, including the <code>parse()</code> method to parse the Excel file into a DataFrame, similar to the <code>pandas.read_excel()</code> function.</p>
<p>In the following example, you will use the <code>ExcelFile</code> class to parse the first sheet, assign the first column as an index, and print the first five rows:</p>
<div class="C0-CodePACKT">
<pre><code>excelfile = pd.ExcelFile(filepath)
excelfile.parse(sheet_name='2017',
                index_col=1,
                parse_dates=True).head()</code></pre>
</div>
<p>You should see similar results for the first five rows of the DataFrame:</p>
<figure>
<img src="../media/file18.jpg" alt="Figure 2.3: The first five rows of the DataFrame using JupyterLab" width="555" height="208"/><figcaption aria-hidden="true">Figure 2.3: The first five rows of the DataFrame using JupyterLab</figcaption>
</figure>
<p>From <em>Figure 2.3</em>, it should become clear that <code>ExcelFile.parse()</code> is <em>equivalent</em> to <code>pandas.read_excel()</code>.</p>
</section>
<section id="see-also-4" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5">See also</h3>
<p>For more information on <code>pandas.read_excel()</code> and <code>pandas.ExcelFile()</code>, please refer to the official documentation:</p>
<ul>
<li><code>pandas.read_excel</code>: <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html">https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html</a></li>
<li><code>pandas.ExcelFile.parse</code>: <a href="https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html">https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html</a></li>
</ul>
</section>
</section>
<section id="reading-data-from-urls" class="level2" data-number="3.5">
<h2 data-number="3.5">Reading data from URLs</h2>
<p>Files can be downloaded and stored locally on your machine, or stored on a remote server or cloud location. In the earlier two recipes, <em>Reading from CSVs and other delimited files</em>, and <em>Reading data from an Excel file</em>, both files were stored locally.</p>
<p>Many of the pandas reader functions can read data from remote locations by passing a URL path. For example, <code>read_csv()</code> and <code>read_excel()</code> can take a URL to read a file accessible via the internet. In this recipe, you will read a CSV file using <code>pandas.read_csv()</code> and Excel files using <code>pandas.read_excel()</code> from remote locations, such as GitHub and AWS S3 (private and public buckets). You will also read data directly from an HTML page into a pandas DataFrame.</p>
<section id="getting-ready-5" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1">Getting ready</h3>
<p>You will need to install the <strong>AWS SDK for Python</strong> (<strong>Boto3</strong>) for reading files from S3 buckets. Additionally, you will learn how to use the <code>storage_options</code> parameter available in many of the reader functions in pandas to read from S3 without the Boto3 library.</p>
<p>To use an S3 URL (for example, <code>s3://bucket_name/path-to-file</code>) in pandas, you will need to install the <code>s3fs</code> library. You will also need to install an HTML parser for when we use <code>read_html()</code>. For example, for the parsing engine (the HTML parser), you can install either <code>lxml</code> or <code>html5lib</code>; pandas will pick whichever is installed (it will first look for <code>lxml</code>, and if that fails, then for <code>html5lib</code>). If you plan to use <code>html5lib</code> you will need to install Beautiful Soup (<code>beautifulsoup4</code>).</p>
<p>To install using pip, you can use the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; pip install boto3 s3fs lxml html5lib</code></pre>
</div>
<p>To install using Conda, you can use:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; conda install boto3 s3fs lxml html5lib -y</code></pre>
</div>
</section>
<section id="how-to-do-it-5" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2">How to do it…</h3>
<p>This recipe will present you with different scenarios when reading data from online (remote) sources. Let's import pandas upfront since you will be using it throughout this recipe:</p>
<div class="C0-CodePACKT">
<pre><code>import pandas as pd</code></pre>
</div>
<section id="reading-data-from-github" class="level4" data-number="3.5.2.1">
<h4 data-number="3.5.2.1">Reading data from GitHub</h4>
<p>Sometimes, you may find useful public data on GitHub that you want to use and read directly (without downloading). One of the most common file formats on GitHub are CSV files. Let's start with the following steps:</p>
<ol>
<li>To read a CSV file from GitHub, you will need the URL to the raw content. If you copy the file's GitHub URL from the browser and use it as the file path, you will get a URL that looks like this: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv</a>. This URL is a pointer to the web page in GitHub and not the data itself; hence when using <code>pd.read_csv()</code>, it will throw an error:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>url = 'https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv'
pd.read_csv(url)
ParserError: Error tokenizing data. C error: Expected 1 fields in line 62, saw 2</code></pre>
</div>
<ol>
<li><p>Instead, you will need the raw content, which will give you a URL that looks like this: <a href="https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv">https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv</a>:</p>
<figure>
<img src="../media/file19.jpg" alt="Figure 2.4: The GitHub page for the CSV file. Note the View raw button" width="1272" height="982"/><figcaption aria-hidden="true">Figure 2.4: The GitHub page for the CSV file. Note the View raw button</figcaption>
</figure></li>
<li>In <em>Figure 2.4</em>, notice that the values are not comma-separated (not a comma-delimited file); instead, the file uses semicolon (<code>;</code>) to separate the values.</li>
</ol>
<p>The first column in the file is the <code>Date</code> column. You will need to parse (using the <code>parse_date</code> parameter) and convert it to <code>DatetimeIndex</code> (<code>index_col</code> parameter).</p>
<p>Pass the new URL to <code>pandas.read_csv()</code>:</p>
<div class="C1-CodePACKT">
<pre><code>url = 'https://media.githubusercontent.com/media/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv'
df = pd.read_csv(url,
                 delimiter=';',
                 parse_dates=['Date'],
                 index_col='Date')
df.iloc[:3,1:4]
&gt;&gt;
              CO(GT)  PT08.S1(CO)  NMHC(GT)
Date</code></pre>
</div>
<div class="C0-SHCodePACKT">
<pre><code>10/03/2004 2.6 1360.00 150
10/03/2004  2.0 1292.25 112
10/03/2004  2.2 1402.00 88</code></pre>
</div>
<p>We successfully ingested the data from the CSV file in GitHub into a DataFrame and printed the first three rows of select columns.</p>
</section>
<section id="reading-data-from-a-public-s3-bucket" class="level4" data-number="3.5.2.2">
<h4 data-number="3.5.2.2">Reading data from a public S3 bucket</h4>
<p>AWS supports <strong>virtual-hosted-style</strong> URLs such as <code>https://bucket-name.s3.Region.amazonaws.com/keyname</code>, <strong>path-style</strong> URLs such as <code>https://s3.Region.amazonaws.com/bucket-name/keyname</code>, and using <code>S3://bucket/keyname</code>. Here are examples of how these different URLs may look for our file:</p>
<ul>
<li>A virtual hosted-style URL or an object URL: <a href="https://tscookbook.s3.us-east-1.amazonaws.com/AirQualityUCI.xlsx">https://tscookbook.s3.us-east-1.amazonaws.com/AirQualityUCI.xlsx</a></li>
<li>A path-style URL: <a href="https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx">https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx</a></li>
<li>An S3 protocol: <code>s3://tscookbook/AirQualityUCI.csv</code></li>
</ul>
<p>In this example, you will be reading the <code>AirQualityUCI.xlsx</code> file, which has only one sheet. It contains the same data as <code>AirQualityUCI.csv</code>, which we read earlier from GitHub.</p>
<p>Note that in the URL, you do not need to specify the region as <code>us-east-1</code>. The <code>us-east-1</code> region, which represents US East (North Virginia), is an <strong>exception.</strong> This is not the case for other regions:</p>
<div class="C0-SHCodePACKT">
<pre><code>url = 'https://tscookbook.s3.amazonaws.com/AirQualityUCI.xlsx'
df = pd.read_excel(url,
                   index_col='Date',
                   parse_dates=True)</code></pre>
</div>
<p>Read the same file using the <code>S3://</code> URL:</p>
<div class="C0-SHCodePACKT">
<pre><code>s3uri = 's3://tscookbook/AirQualityUCI.xlsx'
df = pd.read_excel(s3uri,
                   index_col='Date',
                   parse_dates=True)</code></pre>
</div>
<p>You may get an error such as the following:</p>
<div class="C0-SHConPACKT">
<pre><code>ImportError: Install s3fs to access S3</code></pre>
</div>
<p>This indicates that either you do not have the <code>s3fs</code> library installed or possibly you are not using the right Python/Conda environment.</p>
</section>
<section id="reading-data-from-a-private-s3-bucket" class="level4" data-number="3.5.2.3">
<h4 data-number="3.5.2.3">Reading data from a private S3 bucket</h4>
<p>When reading files from a private S3 bucket, you will need to pass your credentials to authenticate. A convenient parameter in many of the I/O functions in pandas is <code>storage_options</code>, which allows you to send additional content with the request, such as a custom header or required credentials to a cloud service.</p>
<p>You will need to pass a dictionary (key-value pair) to provide the additional information along with the request, such as username, password, access keys, and secret keys to<code> storage_options</code> as in <code>{"username": username, "password": password}</code>.</p>
<p>Now, you will read the <code>AirQualityUCI.csv</code> file, located in a private S3 bucket:</p>
<ol>
<li>You will start by storing your AWS credentials in a config <code>.cfg</code> file outside your Python script. Then, use <code>configparser</code> to read the values and store them in Python variables. You do not want your credentials exposed or hardcoded in your code:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code># Example aws.cfg file
[AWS]
aws_access_key=your_access_key
aws_secret_key=your_secret_key</code></pre>
</div>
<p>You can load the <code>aws.cfg</code> file using <code>config.read()</code>:</p>
<div class="C1-CodePACKT">
<pre><code>import configparser
config = configparser.ConfigParser()
config.read('aws.cfg')
AWS_ACCESS_KEY = config['AWS']['aws_access_key']
AWS_SECRET_KEY = config['AWS']['aws_secret_key']</code></pre>
</div>
<ol>
<li>The <em>AWS Access Key ID</em> and <em>Secret Access Key</em> are now stored in A<code>WS_ACCESS_KEY</code> and <code>AWS_SECRET_KEY.</code> Use <code>pandas.read_csv()</code> to read the CSV file and update the <code>storage_options</code> parameter by passing your credentials, as shown in the following code:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>s3uri = "s3://tscookbook-private/AirQuality.csv"
df = pd.read_csv(s3uri,
                 index_col='Date',
                 parse_dates=True,
                 storage_options= {
                         'key': AWS_ACCESS_KEY,
                         'secret': AWS_SECRET_KEY
                     })
df.iloc[:3, 1:4]
&gt;&gt;
           CO(GT)  PT08.S1(CO)  NMHC(GT)
Date
2004-10-03      2.6       1360.0     150.0
2004-10-03      2.0       1292.0     112.0
2004-10-03      2.2       1402.0      88.0</code></pre>
</div>
<ol>
<li>Alternatively, you can use the AWS SDK for Python (Boto3) to achieve similar results. The <code>boto3</code> Python library gives you more control and additional capabilities (beyond just reading and writing to S3). You will pass the same credentials stored earlier in <code>AWS_ACCESS_KEY</code> and <code>AWS_SECRET_KEY</code> and pass them to AWS, using <code>boto3</code> to authenticate:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>import boto3
bucket = "tscookbook-private"
client = boto3.client("s3",
                  aws_access_key_id =AWS_ACCESS_KEY,
                  aws_secret_access_key = AWS_SECRET_KEY)</code></pre>
</div>
<p>Now, the <code>client</code> object has access to many methods specific to the AWS S3 service for creating, deleting, and retrieving bucket information, and more. In addition, Boto3 offers two levels of APIs: client and resource. In the preceding example, you used the client API.</p>
<p>The client is a low-level service access interface that gives you more granular control, for example, <code>boto3.client("s3")</code>. The resource is a higher-level object-oriented interface (an abstraction layer), for example, <code>boto3.resource("s3")</code>.</p>
<p>In <em>Chapter 4</em>, <em>Persisting Time Series Data to Files</em>, you will explore the <strong>resource</strong> API interface when writing to S3. For now, you will use the client interface.</p>
<ol>
<li>You will use the <code>get_object</code> method to retrieve the data. Just provide the bucket name and a key. The key here is the actual filename:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>data = client.get_object(Bucket=bucket, Key='AirQuality.csv')
df = pd.read_csv(data['Body'],
                 index_col='Date',
                 parse_dates=True)
    
df.iloc[:3, 1:4]
&gt;&gt;
           CO(GT)  PT08.S1(CO)  NMHC(GT)
Date
2004-10-03    2,6       1360.0     150.0
2004-10-03      2       1292.0     112.0
2004-10-03    2,2       1402.0      88.0</code></pre>
</div>
<p>When calling the <code>client.get_object()</code> method, a dictionary (key-value pair) is returned, as shown in the following example:</p>
<div class="C1-CodePACKT">
<pre><code>{'ResponseMetadata': {
'RequestId':'MM0CR3XX5QFBQTSG',
'HostId':'vq8iRCJfuA4eWPgHBGhdjir1x52Tdp80ADaSxWrL4Xzsr
VpebSZ6SnskPeYNKCOd/RZfIRT4xIM=',
'HTTPStatusCode':200,
'HTTPHeaders': {'x-amz-id-2': 'vq8iRCJfuA4eWPgHBGhdjir1x52
Tdp80ADaSxWrL4XzsrVpebSZ6SnskPeYNKCOd/RZfIRT4xIM=',
   'x-amz-request-id': 'MM0CR3XX5QFBQTSG',
   'date': 'Tue, 06 Jul 2021 01:08:36 GMT',
   'last-modified': 'Mon, 14 Jun 2021 01:13:05 GMT',
   'etag': '"2ce337accfeb2dbbc6b76833bc6f84b8"',
   'accept-ranges': 'bytes',
   'content-type': 'binary/octet-stream',
   'server': 'AmazonS3',
   'content-length': '1012427'},
   'RetryAttempts': 0},
   'AcceptRanges': 'bytes',
 'LastModified': datetime.datetime(2021, 6, 14, 1, 13, 5, tzinfo=tzutc()),
 'ContentLength': 1012427,
 'ETag': '"2ce337accfeb2dbbc6b76833bc6f84b8"',
 'ContentType': 'binary/octet-stream',
 'Metadata': {},
 'Body': &lt;botocore.response.StreamingBody at 0x7fe9c16b55b0&gt;}</code></pre>
</div>
<p>The content you are interested in is in the response body under the <code>Body</code> key. You passed <code>data['Body']</code> to the <code>read_csv()</code> function, which loads the response stream (<code>StreamingBody</code>) into a DataFrame.</p>
</section>
<section id="reading-data-from-html" class="level4" data-number="3.5.2.4">
<h4 data-number="3.5.2.4">Reading data from HTML</h4>
<p>pandas offers an elegant way to read HTML tables and convert the content into a pandas DataFrame using the <code>pandas.read_html()</code> function:</p>
<ol>
<li>In the following recipe, we will extract HTML tables from Wikipedia for COVID-19 pandemic tracking cases by country and by territory (<a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory">https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory</a>):</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>url = "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
results = pd.read_html(url)
print(len(results))
&gt;&gt;</code></pre>
</div>
<ol>
<li><code>pandas.read_html()</code> returned a list of DataFrames, one for each HTML table found in the URL. Keep in mind that the website's content is dynamic and gets updated regularly, and the results may vary. In our case, it returned 69 DataFrames. The DataFrame at index <code>15</code> contains summary on COVID-19 cases and deaths by region. Grab the DataFrame (at index <code>15</code>) and assign it to the <code>df</code> variable, and print the returned columns:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df = results[15]
df.columns
&gt;&gt;
Index(['Region[30]', 'Total cases', 'Total deaths', 'Cases per million',
       'Deaths per million', 'Current weekly cases', 'Current weekly deaths',
       'Population millions', 'Vaccinated %[31]'],
      dtype='object')</code></pre>
</div>
<ol>
<li>Display the first five rows for <code>Total cases</code>, <code>Total deaths</code>, and the <code>Cases per million</code> columns:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df[['Region[30]','Total cases', 'Total deaths', 'Cases per million']].head(3)
&gt;&gt;
Region[30]      Total cases Total deaths    Cases per million
0   European Union  179537758   1185108 401363
1   North America   103783777   1133607 281404
2   Other Europe        57721948    498259      247054</code></pre>
</div>
</section>
</section>
<section id="how-it-works-5" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3">How it works…</h3>
<p>Most of the pandas reader functions accept a URL as a path. Examples include the following:</p>
<ul>
<li><code>pandas.read_csv()</code></li>
<li><code>pandas.read_excel()</code></li>
<li><code>pandas.read_parquet()</code></li>
<li><code>pandas.read_table()</code></li>
<li><code>pandas.read_pickle()</code></li>
<li><code>pandas.read_orc()</code></li>
<li><code>pandas.read_stata()</code></li>
<li><code>pandas.read_sas()</code></li>
<li><code>pandas.read_json()</code></li>
</ul>
<p>The URL needs to be one of the valid URL schemes that pandas supports, which includes <code>http</code> and <code>https</code>, <code>ftp</code>, <code>s3</code>, <code>gs</code>, or the <code>file</code> protocol.</p>
<p>The <code>read_html()</code> function is great for scraping websites that contain data in HTML tables. It inspects the HTML and searches for all the <code>&lt;table&gt;</code> elements within the HTML. In HTML, table rows are defined with the <code>&lt;tr&gt; &lt;/tr&gt;</code> tags and headers with the <code>&lt;th&gt;&lt;/th&gt;</code> tags. The actual data (cell) is contained within the <code>&lt;td&gt; &lt;/td&gt;</code> tags. The <code>read_html()</code> function looks for <code>&lt;table&gt;</code>, <code>&lt;tr&gt;</code>, <code>&lt;th&gt;</code>, and <code>&lt;td&gt;</code> tags and, converts the content into a DataFrame, and assigns the columns and rows as they were defined in the HTML. If an HTML page contains more than one <code>&lt;table&gt;&lt;/table&gt;</code> tag, <code>read_html</code> will return them all and you will get a list of DataFrames.</p>
<p>The following code demonstrates how <code>pandas.read_html()</code>works:</p>
<div class="C0-SHCodePACKT">
<pre><code>from io import StringIO
import pandas as pd
html = """
 &lt;table&gt;
   &lt;tr&gt;
     &lt;th&gt;Ticker&lt;/th&gt;
     &lt;th&gt;Price&lt;/th&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
     &lt;td&gt;MSFT&lt;/td&gt;
     &lt;td&gt;230&lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
     &lt;td&gt;APPL&lt;/td&gt;
     &lt;td&gt;300&lt;/td&gt;
   &lt;/tr&gt;
     &lt;tr&gt;
     &lt;td&gt;MSTR&lt;/td&gt;
     &lt;td&gt;120&lt;/td&gt;
   &lt;/tr&gt;
 &lt;/table&gt;
 &lt;/body&gt;
 &lt;/html&gt;
 """
  
df = pd.read_html(StringIO(html))
df[0]
&gt;&gt;
  Ticker  Price
0   MSFT    230
1   APPL    300
2   MSTR    120</code></pre>
</div>
<blockquote>
<p><strong>Passing HTML literal strings</strong></p>
<blockquote>
<p>As of pandas version 2.1.0. you will need to wrap HTML code in io.StringIO. The <code>StringIO(&lt;HTML CODE&gt;)</code> creates an in-memory file-like object from the HTML string that can be passed directly to <code>read_html()</code> function.</p>
</blockquote>
</blockquote>
<p>In the preceding code, the <code>read_html()</code> function reads the HTML content from the file-like object and converts an HTML table, represented between “<code>&lt;table&gt; … &lt;/table&gt;</code>“ tags, into a pandas DataFrame. The headers between the <code>&lt;th&gt;</code> and <code>&lt;/th&gt;</code> tags represent the column names of the DataFrame, and the content between the <code>&lt;tr&gt;&lt;td&gt;</code> and <code>&lt;/td&gt;&lt;/tr&gt;</code> tags represent the row data of the DataFrame. Note that if you go ahead and delete the <code>&lt;table&gt;</code> and <code>&lt;/table&gt;</code> table tags, you will get the <code>ValueError: No tables found</code> error.</p>
</section>
<section id="theres-more-5" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4">There's more…</h3>
<p>The <code>read_html()</code> function has an optional <code>attr</code> argument, which takes a dictionary of valid HTML <code>&lt;table&gt;</code> attributes, such as <code>id</code> or <code>class</code>. For example, you can use the <code>attr</code> parameter to narrow down the tables returned to those that match the <code>class</code> attribute <code>sortable</code> as in <code>&lt;table class="sortable"&gt;</code>. The <code>read_html</code> function will inspect the entire HTML page to ensure you target the right set of attributes.</p>
<p>In the previous exercise, you used the <code>read_html</code> function on the COVID-19 Wikipedia page, and it returned 71 tables (DataFrames). The number of tables will probably increase as time goes by as Wikipedia gets updated. You can narrow down the result set and guarantee some consistency by using the <code>attr</code> option. First, start by inspecting the HTML code using your browser. You will see that several of the <code>&lt;table&gt;</code> elements have multiple classes listed, such as <code>sortable</code>. You can look for other unique identifiers.</p>
<div class="C0-SHCodePACKT">
<pre><code>&lt;table class="wikitable sortable mw-datatable covid19-countrynames jquery-tablesorter" id="thetable" style="text-align:right;"&gt;</code></pre>
</div>
<p>Note, if you get the error <code>html5lib not found</code>, please install it you will need to install both <code>html5lib</code> and <code>beautifulSoup4</code>.</p>
<p>To install using <code>conda</code>, use the following:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install html5lib beautifulSoup4</code></pre>
</div>
<p>To install using <code>pip</code>, use the following:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install html5lib beautifulSoup4</code></pre>
</div>
<p>Now, let's use the <code>sortable</code> class and request the data again:</p>
<div class="C0-SHCodePACKT">
<pre><code>url = "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
df = pd.read_html(url, attrs={'class': 'sortable'})
len(df)
&gt;&gt;  7
df[3].columns
&gt;&gt;
Index(['Region[28]', 'Total cases', 'Total deaths', 'Cases per million',
       'Deaths per million', 'Current weekly cases', 'Current weekly deaths',
       'Population millions', 'Vaccinated %[29]'],
      dtype='object')</code></pre>
</div>
<p>The list returned a smaller subset of tables (from <code>71</code> down to <code>7</code>).</p>
</section>
<section id="see-also-5" class="level3" data-number="3.5.5">
<h3 data-number="3.5.5">See also</h3>
<p>For more information, please refer to the official <code>pandas.read_html</code> documentation: <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_html.html">https://pandas.pydata.org/docs/reference/api/pandas.read_html.html</a>.</p>
</section>
</section>
<section id="reading-data-from-parquet-files" class="level2" data-number="3.6">
<h2 data-number="3.6">Reading data from Parquet files</h2>
<p><strong>Parquet</strong> files have emerged as a popular choice for storing and processing large datasets efficiently in the world of data engineering and big data analytics. Initially developed by Twitter and Cloudera, Parquet was later contributed to the <strong>Apache Foundation</strong> as an open-source columnar file format. The focus of Parquet is to prioritize fast data retrieval and efficient compression. Its design specifically caters to analytical workloads and serves as an excellent option for partitioning data, which you will explore in this recipe and again in Chapter 4, <em>Persisting Time Series Data to Files</em>. As a result, Parquet has become the de facto standard for modern data architectures and cloud storage solutions.</p>
<p>In this recipe you learn how to read parquet files using pandas and learn how to query a specific partition for efficient data retrieval.</p>
<section id="getting-ready-6" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1">Getting ready</h3>
<p>You will be reading parquet files that contain weather data from National Oceanic And Atmospheric Administration from Los Angeles Airport stations which you can find in <code>datasets/Ch2/ LA_weather.parquet</code> folder. This contains weather readings from 2010-2023 and partitioned by year (14 subfolders).</p>
<p>You will use the <code>pandas.read_parquet()</code> function which requires you to install a Parquet engine to process the files. You can install either <strong>fastparquet</strong> or <strong>PyArrow</strong>, with the latter being the default choice for pandas.</p>
<p>To install PyArrow using <strong>conda</strong> run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge pyarrow</code></pre>
</div>
<p>To install PyArrow using <strong>pip</strong> run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install pyarrow</code></pre>
</div>
</section>
<section id="how-to-do-it-6" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2">How to do it…</h3>
<p>The <strong>PyArrow</strong> library allows you to pass additional arguments <code>(**kwargs</code>) to the <code>pandas.read_parquet()</code> function, thereby providing more options when reading files, as you will explore.</p>
<section id="reading-all-partitions" class="level4" data-number="3.6.2.1">
<h4 data-number="3.6.2.1">Reading all partitions</h4>
<p>The following steps are for reading all the partitions in the <code>LA_weather.parquet</code> folder in one go:</p>
<ol>
<li>Create a path to reference the Parquet folder, which contains the partitions, and pass it to the <code>read_parquet</code> function.</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>file = Path('../../datasets/Ch2/LA_weather.parquet/')
df = pd.read_parquet(file,
                    engine='pyarrow')</code></pre>
</div>
<ol>
<li>You can validate and check the schema using the <code>.info()</code> method</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df.info()</code></pre>
</div>
<p>This should produce the following output:</p>
<div class="C1-CodePACKT">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 4899 entries, 0 to 4898
Data columns (total 17 columns):
 #   Column           Non-Null Count  Dtype        
---  ------           --------------  -----        
 0   STATION          4899 non-null   object       
 1   NAME             4899 non-null   object       
 2   DATE             4899 non-null   object       
 3   PRCP             4899 non-null   float64      
 4   PRCP_ATTRIBUTES  4899 non-null   object       
 5   SNOW             121 non-null    float64      
 6   SNOW_ATTRIBUTES  121 non-null    object       
 7   SNWD             59 non-null     float64      
 8   SNWD_ATTRIBUTES  59 non-null     object       
 9   TAVG             3713 non-null   float64      
 10  TAVG_ATTRIBUTES  3713 non-null   object       
 11  TMAX             4899 non-null   int64        
 12  TMAX_ATTRIBUTES  4899 non-null   object       
 13  TMIN             4899 non-null   int64        
 14  TMIN_ATTRIBUTES  4899 non-null   object       
 15  DT               4899 non-null   datetime64[ns]
 16  year             4899 non-null   category     
dtypes: category(1), datetime64[ns](1), float64(4), int64(2), object(9)
memory usage: 617.8+ KB</code></pre>
</div>
</section>
<section id="reading-specific-partitions" class="level4" data-number="3.6.2.2">
<h4 data-number="3.6.2.2">Reading specific partitions</h4>
<p>The following steps explain how to read a specific partition or set of partitions using the <code>filters</code> argument and specify columns using the <code>columns</code> argument from the PyArrow library:</p>
<ol>
<li>As the data is partitioned by year, you can utilize the <code>filters</code> argument to specify a particular partition. In the following, you will only read the partition for the year 2012:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>filters = [('year', '==', 2012)]
df_2012 = pd.read_parquet(file,
                          engine='pyarrow',
                          filters=filters)</code></pre>
</div>
<ol>
<li>To read a set of partitions, such as for the years 2021, 2022, and 2023, you can utilize any of the following options, which will produce similar results:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>filters = [('year', '&gt;', 2020)]
filters = [('year', '&gt;=', 2021)]
filters = [('year', 'in', [2021, 2022, 2023])]</code></pre>
</div>
<p>After defining the <code>filters</code> object, you can assign it to the <code>filters</code> argument within the <code>read_parquet()</code> function as demonstrated below:</p>
<div class="C1-CodePACKT">
<pre><code>df = pd.read_parquet(file,
                     engine='pyarrow',
                     filters=filters)</code></pre>
</div>
<ol>
<li>Another useful argument is <code>columns</code> which allows you to specify the column names you want to retrieve as a Python list.</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>columns = ['DATE', 'year', 'TMAX']
df = pd.read_parquet(file,
                     engine='pyarrow',
                     filters=filters,
                     columns=columns)</code></pre>
</div>
<p>In the preceding code, the <code>read_parquet()</code> function will only retrieve the specified columns (‘<em>Date’</em>, ‘<em>year’</em>, and ‘<em>TMAX’</em>) from the Parquet file, using the defined filters. You can validate the results by running <code>df.head()</code> and <code>df.info()</code></p>
<div class="C1-CodePACKT">
<pre><code>df.head()
&gt;&gt;
DATE        year    TMAX
0   2021-01-01  2021    67
1   2021-01-02  2021    63
2   2021-01-03  2021    62
3   2021-01-04  2021    59
4   2021-01-05  2021    57
df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 881 entries, 0 to 880
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   DATE    881 non-null    object 
 1   year    881 non-null    category
 2   TMAX    881 non-null    int64  
dtypes: category(1), int64(1), object(1)
memory usage: 15.2+ KB</code></pre>
</div>
<p>Notice how the memory usage has significantly reduced when narrowing your selection to only the necessary data by specifying the partitions and columns for your data analysis.</p>
</section>
</section>
<section id="how-it-works-6" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3">How it works…</h3>
<p>There are several advantages to working with the Parquet file format, especially when dealing with large data files. The columnar-oriented format of Parquet offers faster data retrieval and efficient compression, making it ideal for cloud storage and reducing storage costs. Parquet employs advanced techniques and algorithms for data encoding, leading to improved compression ratios.</p>
<p>Figure 2.5 below shows an example of a folder structure for a dataset stored as a Parquet file partitioned by year. Each year has its own subfolder, and within each subfolder, there are individual files.</p>
<figure>
<img src="../media/file20.png" alt="Figure 2.5: Example of a folder structure for a Parquet dataset partitioned by year" width="674" height="722"/><figcaption aria-hidden="true">Figure 2.5: Example of a folder structure for a Parquet dataset partitioned by year</figcaption>
</figure>
<p>Parquet files are referred to as “<strong>self-described</strong>” since each file contains the encoded data and additional metadata in the footer section. The metadata includes the version of the Parquet format, data schema and structure (such as column types), and other statistical information such as the minimum and maximum values for the columns. Consequently, when writing and reading Parquet datasets using pandas, you will notice that the DataFrame schema is preserved.</p>
<p>There are some key parameters you need to be familiar with based on the official pandas documentation for the <code>read_parquet()</code> reader function which you can find in their official page here (<a href="https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html">https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html</a>). You already used some of these parameters in the previous <em>How to do it…</em> section. The following shows the key parameters you already used:</p>
<div class="C0-CodePACKT">
<pre><code>read_parquet(
    path: 'FilePath | ReadBuffer[bytes]',
    engine: 'str' = 'auto',
    columns: 'list[str] | None' = None,
    **kwargs,
)</code></pre>
</div>
<ul>
<li><code>path</code>: This is the first positional argument and the only required field needed (at a minimum) to read a Parquet file. In our example, you passed the Python path object named <code>file</code> as the argument. You can also pass a valid URL that points to a remote Parquet file location, such as an AWS S3 bucket.</li>
<li><code>engine</code>: The default value is “auto” if you do not pass any arguments to the engine parameter. The other two valid options are “pyarrow” and “fastparquet” depending on which engine you installed. In our example, you installed the <strong>PyArrow</strong> library (see the <em>Getting ready</em> section). The “auto” option will first attempt to load the <strong>PyArrow</strong>, and then fallback to <strong>fastparquet</strong> if it’s not available.</li>
<li><code>columns</code>: Here you can specify the columns you would like to limit when reading. You will pass this as a Python list even if you select just one column. In our example, you defined a columns variable as <code>columns = ['DATE', 'year', 'TMAX']</code> and then passed it as an argument.</li>
<li><code>**kwargs</code>: Indicates that additional arguments can be passed to the engine. In our case, we used the <strong>PyArrow</strong> library; hence, the <code>read_parquet()</code> pandas function will pass these arguments to the <code>pyarrow.parquet.read_table()</code> function from the PyArrow library. In the previous examples, we passed a list with filtering criteria to the <code>filters</code> parameter in the <code>pyarrow.parquet.read_table()</code>. For additional parameters that you can utilize, you can check the official documentation for <code>pyarrow.parquet.read_table()</code> here <a href="https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html">https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html</a></li>
</ul>
</section>
<section id="theres-more-6" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4">There’s more…</h3>
<ol>
<li>Recall in the <em>Getting Ready</em> section that you installed the PyArrow library as the backend engine for working with Parquet files with pandas. When you used the <code>read_parquet()</code> reader function in pandas, the <code>pyarrow</code> engine was the default.</li>
<li>Since you have already installed the library, you can utilize it directly to work with Parquet files in a similar manner as you did using pandas. Instead of the <code>pandas.read_parquet()</code> function, you will use <code>pyarrow.parquet.read_table()</code> function as shown in the following:</li>
</ol>
<div class="C0-SHCodePACKT">
<pre><code>import pyarrow.parquet as pq
from pathlib import Path
file = Path('../../datasets/Ch2/LA_weather.parquet/')
table = pq.read_table(file, filters=filters, columns=columns)</code></pre>
</div>
<p>The <code>table</code> object is an instance of the <code>pyarrow.Table</code> class. You can validate this using the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pyarrow as pa
isinstance(table, pa.Table)
&gt;&gt; True</code></pre>
</div>
<p>The <code>table</code> object contains many useful methods including the <code>.to_pandas()</code> method to convert the object into a pandas DataFrame:</p>
<div class="C0-SHCodePACKT">
<pre><code>df = table.to_pandas()</code></pre>
</div>
<p>The following illustrates this further to show the similarities between the <code>read_table()</code> and <code>read_parquet()</code> functions:</p>
<div class="C0-SHCodePACKT">
<pre><code>columns = ['DATE','year', 'TMAX']
filters = [('year', 'in', [2021, 2022, 2023])]
tb = pq.read_table(file,
                   filters=filters,
                   columns=columns,
                   use_pandas_metadata=True)
df_pa = tb.to_pandas()
df_pd = pd.read_parquet(file,
                        filters=filters,
                        columns=columns,
                        use_pandas_metadata=True)</code></pre>
</div>
<p>Both <code>df_pa</code> and <code>df_pd</code> are equivalent.</p>
<p>The PyArrow library provides a low-level interface, while pandas provides a high-level interface that is built on top of PyArrow. The same applies if you decide to install the <code>fastparquet</code> library instead. Note that PyArrow is the Python implementation of <strong>Apache Arrow</strong>, an open-source project for in-memory data (columnar memory). While Apache Parquet specifies the columnar file format for efficient storage and retrieval, Apache Arrow allows us to process such large columnar datasets in memory efficiently as well.</p>
</section>
<section id="see-also-6" class="level3" data-number="3.6.5">
<h3 data-number="3.6.5">See also</h3>
<div class="C0-CodePACKT">
<pre><code>To learn more about pandas.read_parquet() function, you can read their latest documentation here https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html</code></pre>
</div>
</section>
</section>
<section id="working-with-large-data-files" class="level2" data-number="3.7">
<h2 data-number="3.7">Working with large data files</h2>
<p>One of the advantages of using pandas is that it provides data structures for in-memory analysis, which results in a performance advantage when working with data. However, this advantage can also become a constraint when working with large datasets, as the amount of data you can load is limited by the available memory. When datasets exceed the available memory, it can lead to performance degradation, especially when pandas creates intermediate copies of the data for certain operations.</p>
<p>In real-world scenarios, there are general best practices to mitigate these limitations, including:</p>
<ul>
<li><strong>Sampling or loading a small number of rows for your Exploratory Data Analysis (EDA):</strong> Before applying your data analysis strategy to the entire dataset, it is a good practice to sample or load a small number of rows. This allows you to get a better understanding of your data, gain some intuition, and identify unnecessary columns that can be eliminated, thus reducing the overall dataset size.</li>
<li><strong>Reduce the Number of Columns:</strong> Keeping only the columns necessary for your analysis can significantly reduce the memory footprint of the dataset.</li>
<li><strong>Chunking</strong>: Utilizing the <code>chunksize</code> parameter available in many of the reader functions in pandas allows you to process the data in smaller, manageable chunks. This technique helps in handling large datasets by processing them piece by piece.</li>
<li><strong>User Other Libraries for Large Datasets</strong>: There are alternative libraries specifically designed for working with large datasets that offer a similar API to pandas, such as <strong>Dask</strong> <strong>Polars, and Modin</strong>.</li>
</ul>
<p>In this recipe, you will learn about techniques within pandas to handle large datasets, such as <em>chunking</em>. Afterward, you will explore three new libraries: <strong>Dask</strong>, <strong>Polars</strong>, and <strong>Modin.</strong> These libraries serve as alternatives to pandas and can be particularly useful when dealing with large datasets.</p>
<section id="getting-ready-7" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1">Getting ready</h3>
<p>In this recipe, you will install Dask and Polars libraries.</p>
<p>To install using pip, you can use the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; pip install “dask[complete]” "modin[all]" polars</code></pre>
</div>
<p>To install using Conda, you can use:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; conda install -c conda-forge dask polars modin-all</code></pre>
</div>
<p>In this recipe, you will be working with the New York Taxi data set from (<a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a>) and we will be working with Yellow Taxi Trip Records for 2023 (covering January to May). In the GitHub repository of the book I have provided the <code>run_once()</code> function that you will need to execute once. It will combine all five months of datasets (five parquet files), and produce one large CSV data set (around 1.72 GB).</p>
<p>Below is the script as a reference:</p>
<div class="C0-CodePACKT">
<pre><code># Script to create one large data file
import pandas as pd
import glob
def run_once():
    # Directory path where Parquet files are located
    directory = '../../datasets/Ch2/yellow_tripdata_2023-*.parquet'
   
    # Get a list of all Parquet files in the directory
    parquet_files = glob.glob(directory)
   
    # Read all Parquet files into a single DataFrame
    dfs = []
    for file in parquet_files:
        df = pd.read_parquet(file)
        dfs.append(df)
   
    # Concatenate all DataFrames into a single DataFrame
    combined_df = pd.concat(dfs)
   
    combined_df.to_csv('../../datasets/Ch2/yellow_tripdata_2023.csv', index=False)
run_once()</code></pre>
</div>
</section>
<section id="how-to-do-it-7" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2">How to do it…</h3>
<p>In this recipe, you will explore four different methods for handling large datasets for ingestion purposes. These methods include:</p>
<ul>
<li>Using the <code>chuncksize</code> parameter, which is available in many of the reader functions in pandas.</li>
<li>Using the Dask library</li>
<li>Using the Polars library</li>
</ul>
<p>The <code>memory_profiler</code> library will be utilized for illustration purposes to show memory consumption. You can install the library using <code>pip</code>:</p>
<div class="C0-SHConPACKT">
<pre><code>Pip install -U memory_profiler</code></pre>
</div>
<p>To use memory_profiler in Jupyter Notebook, you will need to run the following once:</p>
<div class="C0-SHCodePACKT">
<pre><code>%load_ext memory_profiler</code></pre>
</div>
<p>Once loaded, you can use it inside any code cell. You just need to start the cell with the <code>%memit</code> or <code>%%memit in a Jupyter code cell. A typical output will show peak memory size and increment size.</code></p>
<ul>
<li><strong>Peak memory</strong> represents the maximum memory usage during the execution of a specific line of code.</li>
<li><strong>Increment</strong> represents the difference in memory usage between the current line and the previous line.</li>
</ul>
<section id="using-chunksize" class="level4" data-number="3.7.2.1">
<h4 data-number="3.7.2.1">Using Chunksize</h4>
<p>Several reader functions in pandas support chunking through the <code>chunksize</code> parameter. This approach is convenient when you have a large dataset that you need to ingest, but it may not be suitable if you need to perform complex logic on each chunk, which requires coordination between the chunks.</p>
<p>Some of the reader functions in pandas that support the <code>chunksize</code> parameter include: <code>pandas.read_csv()</code>, <code>pandas.read_table()</code>, <code>pandas.read_sql()</code>, <code>pandas.read_sql_query()</code>, <code>pandas.read_sql_table()</code>, <code>pandas.read_json()</code>, <code>pandas.read_fwf()</code>, <code>pandas.read_sas()</code>, <code>pandas.read_spss()</code>, and <code>pandas.read_stata()</code>.</p>
<ol>
<li>First, let’s start by reading this large tile using the traditional approach with <code>read_csv</code> without chunking:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
file_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')
%%time
%%memit
df_pd = pd.read_csv(file_path, low_memory=False)</code></pre>
</div>
<p>Given we have two magic commands <code>%%time</code> and <code>%%memit</code> the output will display memory usage and CPU time as shown below:</p>
<div class="C1-CodePACKT">
<pre><code>peak memory: 10085.03 MiB, increment: 9922.66 MiB
CPU times: user 21.9 s, sys: 2.64 s, total: 24.5 s
Wall time: 25 s</code></pre>
</div>
<ol>
<li>Using the same <code>read_csv</code> function, you utilize the chunksize parameter, which represents the number of lines to read per chunk. In this example, you will use <code>chunksize=10000,</code> which will create a chunk every 10000 rows.</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
df_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)</code></pre>
</div>
<p>This will produce the following output</p>
<div class="C1-CodePACKT">
<pre><code>peak memory: 3101.41 MiB, increment: 0.77 MiB
CPU times: user 25.5 ms, sys: 13.4 ms, total: 38.9 ms
Wall time: 516 ms</code></pre>
</div>
<p>The reason the execution occurred so fast is that what has been returned is an <strong>iterator</strong> object of type <code>TextFileReader</code> as shown:</p>
<div class="C1-CodePACKT">
<pre><code>type(df_pd)
pandas.io.parsers.readers.TextFileReader</code></pre>
</div>
<ol>
<li>To retrieve the data in each chunk you can use the <code>get_chunk()</code> method to retrieve one chunk at a time or use a loop to retrieve all chunks or simply use the <code>pandas.concat() </code>function:</li>
<li>Option 1: Using the <code>get_chunk()</code> method or Python <code>next()</code> function. This will retrieve one chunk at a time, at 10000 records per chunk. Every time you run one get_chunk() or next() you will get the next chunk.</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
df_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)
df_pd.get_chunk()
&gt;&gt;
peak memory: 6823.64 MiB, increment: 9.14 MiB
CPU times: user 72.3 ms, sys: 40.8 ms, total: 113 ms
Wall time: 581 ms
# this is equivalent to
df_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)
Next(df_pd)</code></pre>
</div>
<ol>
<li>Option 2: Looping through the chunks. This is useful if you want to perform simple operations on each chunk before combining each chunk:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
df_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)
final_result = pd.DataFrame()
for chunk in df_pd:
    final_result = pd.concat([final_result, chunk])</code></pre>
</div>
<ol>
<li>Option 3: using pd.concat() to retrieve all the chunks at once in one operation. This may not be as useful in terms of overall performance:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
df_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)
final_result = pd.concat(pf_pd)
&gt;&gt;
peak memory: 9145.42 MiB, increment: 697.86 MiB
CPU times: user 14.9 s, sys: 2.81 s, total: 17.7 s
Wall time: 18.8 s</code></pre>
</div>
<p>The memory and CPU time measurements are added for illustration purposes. As you can observe, looping through the chunks and appending each chunk can be a time-consuming process.</p>
<p>Next, you will learn about how to use Polars which provides a very similar API to that of pandas making the transition to learn Polars a simpler task.</p>
</section>
<section id="using-polars" class="level4" data-number="3.7.2.2">
<h4 data-number="3.7.2.2">Using Polars</h4>
<p>Similar to pandas the Polars library is designed to be used on a single machine but offers higher performance than pandas when working with large datasets. Unlike pandas which is single threaded and cannot leverage multiple cores on a single machine, Polars can utilize all available cores on a single machine for efficient parallel processing. In terms of memory usage, pandas provides in-memory data structures hence its popularity for in-memory analytics. This also means that when you load your CSV file, the entire dataset is loaded into memory, hence working with datasets beyond your memory’s capacity can be problematic. Polars on the other hand requires less memory than pandas for similar operations.</p>
<p>Polars is written in Rust, a programming language that offers similar performance to C and C++ and is becoming a very popular programming language in the land of Machine Learning Operations (MLOps) due to its performance compared to Python.</p>
<p>In this recipe you will explore the basics of Polars, primarily reading a large CSV file using the <code>read_csv()</code> reader function. The</p>
<ol>
<li>Start by importing the Polars library</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>import polars as pl
from pathlib import Path
file_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')</code></pre>
</div>
<ol>
<li>You can now read the CSV file using the <code>read_csv</code> function similar to how you have done it using pandas. Notice the use of the Jupyter magic commands <code>%%time</code> and <code>%%memit</code> for illustration purposes:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
df_pl = pl.read_csv(file_path)
&gt;&gt;
peak memory: 8633.58 MiB, increment: 2505.14 MiB
CPU times: user 4.85 s, sys: 3.28 s, total: 8.13 s
Wall time: 2.81 s</code></pre>
</div>
<ol>
<li>You can use the <code>.head()</code> method to print out the first 5 records of the Polars DataFrame similar to pandas.</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df_pl.head()</code></pre>
</div>
<ol>
<li>To get the total number of rows and columns of the Polars DataFrame you can use the <code>.shape</code> property similar to how pandas:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df_pl.shape
&gt;&gt;
(16186386, 20)</code></pre>
</div>
<ol>
<li>If you decided to use Polars for processing large datasets but later decided to output the results back as a pandas DataFrame, you can do so using the <code>.to_pandas()</code> method:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df_pd = df_pl.to_pandas()</code></pre>
</div>
<p>Just from these simple code runs it becomes clear how fast Polars is. This can be even more obvious from the memory and CPU metrics when comparing <code>read_csv</code> in the Polars library to the <code>read_csv</code> in pandas.</p>
</section>
<section id="using-dask" class="level4" data-number="3.7.2.3">
<h4 data-number="3.7.2.3">Using Dask</h4>
<p>Another popular library for working with large datasets is Dask. It has a similar API to pandas but differs in its distributed computing capabilities allowing it to scale beyond a single machine. Dask integrates pretty well with other popular libraries such as pandas, Scikit-Learn, NumPy, and XGBoost.</p>
<p>Additionally, you can install Dask-ML, an add-on library that provides scalable machine learning alongside popular Python ML libraries such as Scikit-Learn, XGBoot, PyTorch, and TensorFlow/Keras.</p>
<p>In this recipe, you will explore the basics of Polars, primarily reading a large CSV file using the <code>read_csv()</code> reader function.</p>
<ol>
<li>Start by importing the <code>dataframe</code> module from the <code>dask</code> library:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>import dask.dataframe as dd
from pathlib import Path
file_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')</code></pre>
</div>
<ol>
<li>You can now read the CSV file using the <code>read_csv</code> function similar to how you have done it using pandas. Notice the use of the Jupyter magic commands <code>%%time</code> and <code>%%memit</code> for illustration purposes:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
df_dk = dd.read_csv(file_path)
&gt;&gt;
peak memory: 153.22 MiB, increment: 3.38 MiB
CPU times: user 44.9 ms, sys: 12.1 ms, total: 57 ms
Wall time: 389 ms</code></pre>
</div>
<p>Interesting output in terms of memory and CPU utilization. One would assume nothing was read. Let’s run a few tests to understand what is happening.</p>
<ol>
<li>You will explore the df_dk DataFrame using familiar techniques you would normally do in pandas such as checking the size of the DataFrame:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df_dk.shape
&gt;&gt;
(Delayed('int-0ab72188-de09-4d02-a76e-4a2c400e918b'), 20)
df_dk.info()
&lt;class 'dask.dataframe.core.DataFrame'&gt;
Columns: 20 entries, VendorID to airport_fee
dtypes: float64(13), int64(4), string(3)</code></pre>
</div>
<p>Notice that we get insights into the number of columns, and their data types, but no information on the total number of records (rows). Additionally, notice the Delayed object in Dask. We will get back to this shortly.</p>
<ol>
<li>Lastly, try to output the DataFrame using the print function:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>print(df_dk)
&gt;&gt; 
Dask DataFrame Structure:
               VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type fare_amount    extra  mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge Airport_fee airport_fee
npartitions=26                                                                                                                                                                                                                                                                                            
                  int64               string                string         float64       float64    float64             string        int64        int64        int64     float64  float64  float64    float64      float64               float64      float64              float64     float64     float64
                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...
...                 ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...
                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...
                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...
Dask Name: to_pyarrow_string, 2 graph layers</code></pre>
</div>
<p>The output is very interesting; pretty much all that is shown is the structure or layout of the DataFrame, but no data. To simplify the explanation, Dask utilizes a strategy called lazy loading or lazy evaluation. In other words, most workloads in Dask are lazy; they do not get executed immediately until you trigger them with a specific action, for example, using the <code>compute()</code> method. This feature enables Dask to handle large datasets and distributed computing by delaying the actual computation until it is explicit. Instead, Dask constructs a task graph or execution logic behind the scenes almost instantaneously, but the task graph or execution logic is not triggered.</p>
<p>When using the <code>read_csv</code>, Dask does not load the entire dataset “yet”. It only reads the data when you perform specific operations or functions. For example, using the <code>head()</code> method will retrieve only the first 5 records, and that’s it. Thus saving memory and improving performance.</p>
<ol>
<li>Print the first 5 records of the Dask DataFrame using the head method:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df_dk.head()</code></pre>
</div>
<p>You will notice that the first five (5) records are printed out similar to how you would expect when using pandas.</p>
<ol>
<li>To get the total number of records in the dataset, you can use the compute method, which will force evaluation:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>%%time
%%memit
print(df_dk.shape[0].compute())
&gt;&gt;
16186386
peak memory: 6346.53 MiB, increment: 1818.44 MiB
CPU times: user 19.3 s, sys: 5.29 s, total: 24.6 s
Wall time: 10.5 s</code></pre>
</div>
<ol>
<li>Lastly, if you were able to reduce the size of the DataFrame to be easier to load in pandas, you can convert from Dask DataFrame to a pandas DataFrame using the <code>compute()</code> method:</li>
</ol>
<div class="C1-CodePACKT">
<pre><code>df_pd = df_dk.compute()
type(df_pd)
&gt;&gt;
pandas.core.frame.DataFrame</code></pre>
</div>
<p>The Dask library offers different APIs. You only explored the DataFrame API, which is similar to the pandas library. Dask offers many optimization capabilities and has a learning curve for those working with very large datasets and needing to scale their current workflows, whether it be from NumPy, Scikit-Learn, or pandas.</p>
</section>
</section>
<section id="how-it-works-7" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3">How it works…</h3>
<p>Due to pandas popularity, many libraries such as Polars and Dasks were inspired by pandas simplicity and API. This is because these libraries are designed to target pandas users to provide a solution to one of pandas most significant limitations: lack of ability to scale and work with very large datasets that cannot fit into memory.</p>
</section>
<section id="theres-more-7" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4">There's more…</h3>
<p>So far, you have been introduced to better options when working with large files than using pandas, especially if you have memory constraints and cannot fit all the data into the memory available. The pandas library is a single-core framework and does not offer parallel computing capabilities. Instead, there are specialized libraries and frameworks for parallel processing designed to work with big data. Such frameworks do not rely on loading everything into memory and instead can utilize multiple CPU cores, disk usage, or expand into multiple worker nodes (think multiple machines). Earlier, you explored <strong>Dask,</strong> which chunks your data, creates a computation graph, and parallelizes the smaller tasks (chunks) behind the scenes, thus speeding the overall processing time and reducing memory overhead.</p>
<p>These frameworks are great but will require you to spend time learning the framework and may necessitate that you rewrite the original code to leverage these capabilities. So, there might be a learning curve initially. Luckily, this is where the <strong>Modin</strong> project comes into play. The Modin library acts as a wrapper or, more specifically, an abstraction on top of <strong>Dask</strong> or <strong>Ray</strong> that uses an API similar to that of pandas. Modin makes optimizing your pandas' code much more straightforward without learning another framework; all it takes is a single line of code.</p>
<p>Start by importing the necessary libraries:</p>
<div class="C0-SHCodePACKT">
<pre><code>from pathlib import Path
from modin.config import Engine
Engine.put("dask")  # Modin will use Dask
import modin.pandas as pd
file_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')</code></pre>
</div>
<p>Notice a few things here, first we specified the engine to be used. In this case we opted to use Dask. Modin supports other engines including Ray and MPI. Second, notice the import modin.pandas as pd statement, this single line is all that is needed to scale your existing pandas code. Keep in mind that the Modin project is in active development, which means that as pandas mature and adds additional features and functionalities, Modin may still be catching up.</p>
<p>Let’s read our CSV file and compare the metrics in terms of CPU and memory utilization:</p>
<div class="C0-SHCodePACKT">
<pre><code>%%time
%%memit
pd.read_csv(file_path)
&gt;&gt;
peak memory: 348.02 MiB, increment: 168.59 MiB
CPU times: user 1.23 s, sys: 335 ms, total: 1.57 s
Wall time: 8.26 s</code></pre>
</div>
<p>Your data is loaded fast and you can run other pandas functions to further inspect your data such as <code>df_pd.head(),</code> <code>df_pd.info()</code>, <code>df_pd.head()</code> and you will notice how fast the results appear:</p>
<div class="C0-SHCodePACKT">
<pre><code>df_pd.info()
&gt;&gt;
&lt;class 'modin.pandas.dataframe.DataFrame'&gt;
RangeIndex: 16186386 entries, 0 to 16186385
Data columns (total 20 columns):
 #   Column                 Dtype 
---  ------                 ----- 
 0   VendorID               int64 
 1   tpep_pickup_datetime   object
 2   tpep_dropoff_datetime  object
 3   passenger_count        float64
 4   trip_distance          float64
 5   RatecodeID             float64
 6   store_and_fwd_flag     object
 7   PULocationID           int64 
 8   DOLocationID           int64 
 9   payment_type           int64 
 10  fare_amount            float64
 11  extra                  float64
 12  mta_tax                float64
 13  tip_amount             float64
 14  tolls_amount           float64
 15  improvement_surcharge  float64
 16  total_amount           float64
 17  congestion_surcharge   float64
 18  Airport_fee            float64
 19  airport_fee            float64
dtypes: float64(13), int64(4), object(3)
memory usage: 2.4+ GB</code></pre>
</div>
<p>Using Moding allows you to utilize your existing pandas code, skillset and experience with the library without having to learn a new framework. This includes access to the pandas I/O functions (reader and writer functions) and all the parameters you would expect from the pandas library.</p>
</section>
<section id="see-also-7" class="level3" data-number="3.7.5">
<h3 data-number="3.7.5">See also</h3>
<p>Other Python projects are dedicated to making working with large datasets more scalable and performant and, in some cases, better options than pandas.</p>
<ul>
<li>Dask: <a href="https://dask.org/">https://dask.org/</a></li>
<li>Modin: <a href="https://modin.readthedocs.io/en/latest/">https://modin.readthedocs.io/en/latest/</a></li>
<li>Polars: <a href="https://pola.rs">https://pola.rs</a></li>
<li>Ray: <a href="ch003.xhtml">https://ray.io/</a></li>
<li>Vaex: <a href="https://vaex.io/">https://vaex.io/</a></li>
</ul>
</section>
</section>
</section>
</div>
</div>
</body>
</html>