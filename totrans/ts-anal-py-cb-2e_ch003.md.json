["```py\nimport pandas as pd\nfrom pathlib import Path\n```", "```py\nfilepath =\\\n Path('../../datasets/Ch2/movieboxoffice.csv')\n```", "```py\nts = pd.read_csv(filepath,\n                 header=0,\n                 parse_dates=['Date'],\n                 index_col=0,\n                 infer_datetime_format=True,\n                 usecols=['Date',\n                          'DOW',\n                          'Daily',\n                          'Forecast',\n                          'Percent Diff'])\nts.head(5)\n```", "```py\nts.info()\n>> <class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 128 entries, 2021-04-26 to 2021-08-31\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   DOW           128 non-null    object\n 1   Daily         128 non-null    object\n 2   Forecast      128 non-null    object\n 3   Percent Diff  128 non-null    object\ndtypes: object(4)\nmemory usage: 5.0+ KB\n```", "```py\nclean = lambda x: x.str.replace('[^\\\\d]','', regex=True)\nc_df = ts[['Daily', 'Forecast']].apply(clean, axis=1)\nts[['Daily', 'Forecast']] = c_df.astype(float)\n```", "```py\nts.info()\n>> <class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 128 entries, 2021-04-26 to 2021-08-31\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   DOW           128 non-null    object\n 1   Daily         128 non-null    float64\n 2   Forecast      128 non-null    float64\n 3   Percent Diff  128 non-null    object\ndtypes: float64(2), object(2)\nmemory usage: 5.0+ KB\n```", "```py\nts.memory_usage()\n>>\nIndex           1024\nDOW             1024\nDaily           1024\nForecast        1024\nPercent Diff    1024\ndtype: int64\n```", "```py\nts.memory_usage().sum()\n>> 5120\n```", "```py\nts = pd.read_csv(filepath,\n                 parse_dates=[0],\n                 index_col=0,\n                 date_format=\"%d-%b-%Y\",\n                 usecols=[0,1,3, 7, 6])\nts.head()\n```", "```py\nts = pd.read_csv(filepath,\n                 index_col=0,\n                 usecols=[0,1,3, 7, 6])\nts.index = pd.to_datetime(ts.index, format=\"%d-%b-%Y\")\n```", "```py\n>>> conda install openpyxl\n```", "```py\n>>> pip install openpyxl\n```", "```py\nimport pandas as pd\nfrom pathlib import Path\nfilepath = \\\nPath('../../datasets/Ch2/sales_trx_data.xlsx')\n```", "```py\nexcelfile = pd.ExcelFile(filepath)\nexcelfile.sheet_names\n>> ['2017', '2018']\n```", "```py\nts = pd.read_excel(filepath,\n                   engine='openpyxl',\n                   index_col=1,\n                   sheet_name=[0,1],\n                   parse_dates=True)\nts.keys()\n>> dict_keys([0, 1])\n```", "```py\nts = pd.read_excel(filepath,\n                   engine='openpyxl',\n                   index_col=1,\n                   sheet_name=['2017','2018'],\n                   parse_dates=True)\nts.keys()\n>> dict_keys(['2017', '2018'])\n```", "```py\nts = pd.read_excel(filepath,\n                   engine='openpyxl',\n                   index_col=1,\n                   sheet_name=None,\n                   parse_dates=True)\nts.keys()\n>> dict_keys(['2017', '2018'])\n```", "```py\nts_combined = pd.concat([ts['2017'],ts['2018']])\nts_combined.info()\n>> <class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 74124 entries, 2017-01-01 to 2018-12-31\nData columns (total 4 columns):\n #   Column              Non-Null Count  Dtype\n---  ------              --------------  -----\n 0   Line_Item_ID        74124 non-null  int64\n 1   Credit_Card_Number  74124 non-null  int64\n 2   Quantity            74124 non-null  int64\n 3   Menu_Item           74124 non-null  object\ndtypes: int64(3), object(1)\nmemory usage: 2.8+ MB\n```", "```py\nts_combined = pd.concat(ts).droplevel(level=0)\n```", "```py\nts = pd.read_excel(filepath,\n                   index_col=1,\n                   sheet_name='2018',\n                   parse_dates=True)\ntype(ts)\n>> pandas.core.frame.DataFrame\n```", "```py\nexcelfile = pd.ExcelFile(filepath)\nexcelfile.parse(sheet_name='2017',\n                index_col=1,\n                parse_dates=True).head()\n```", "```py\n>>> pip install boto3 s3fs lxml html5lib\n```", "```py\n>>> conda install boto3 s3fs lxml html5lib -y\n```", "```py\nimport pandas as pd\n```", "```py\nurl = 'https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv'\npd.read_csv(url)\nParserError: Error tokenizing data. C error: Expected 1 fields in line 62, saw 2\n```", "```py\nurl = 'https://media.githubusercontent.com/media/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./main/datasets/Ch2/AirQualityUCI.csv'\ndf = pd.read_csv(url,\n                 delimiter=';',\n                 parse_dates=['Date'],\n                 index_col='Date')\ndf.iloc[:3,1:4]\n>>\n              CO(GT)  PT08.S1(CO)  NMHC(GT)\nDate\n```", "```py\n10/03/2004 2.6 1360.00 150\n10/03/2004  2.0 1292.25 112\n10/03/2004  2.2 1402.00 88\n```", "```py\nurl = 'https://tscookbook.s3.amazonaws.com/AirQualityUCI.xlsx'\ndf = pd.read_excel(url,\n                   index_col='Date',\n                   parse_dates=True)\n```", "```py\ns3uri = 's3://tscookbook/AirQualityUCI.xlsx'\ndf = pd.read_excel(s3uri,\n                   index_col='Date',\n                   parse_dates=True)\n```", "```py\nImportError: Install s3fs to access S3\n```", "```py\n# Example aws.cfg file\n[AWS]\naws_access_key=your_access_key\naws_secret_key=your_secret_key\n```", "```py\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read('aws.cfg')\nAWS_ACCESS_KEY = config['AWS']['aws_access_key']\nAWS_SECRET_KEY = config['AWS']['aws_secret_key']\n```", "```py\ns3uri = \"s3://tscookbook-private/AirQuality.csv\"\ndf = pd.read_csv(s3uri,\n                 index_col='Date',\n                 parse_dates=True,\n                 storage_options= {\n                         'key': AWS_ACCESS_KEY,\n                         'secret': AWS_SECRET_KEY\n                     })\ndf.iloc[:3, 1:4]\n>>\n           CO(GT)  PT08.S1(CO)  NMHC(GT)\nDate\n2004-10-03      2.6       1360.0     150.0\n2004-10-03      2.0       1292.0     112.0\n2004-10-03      2.2       1402.0      88.0\n```", "```py\nimport boto3\nbucket = \"tscookbook-private\"\nclient = boto3.client(\"s3\",\n                  aws_access_key_id =AWS_ACCESS_KEY,\n                  aws_secret_access_key = AWS_SECRET_KEY)\n```", "```py\ndata = client.get_object(Bucket=bucket, Key='AirQuality.csv')\ndf = pd.read_csv(data['Body'],\n                 index_col='Date',\n                 parse_dates=True)\n\ndf.iloc[:3, 1:4]\n>>\n           CO(GT)  PT08.S1(CO)  NMHC(GT)\nDate\n2004-10-03    2,6       1360.0     150.0\n2004-10-03      2       1292.0     112.0\n2004-10-03    2,2       1402.0      88.0\n```", "```py\n{'ResponseMetadata': {\n'RequestId':'MM0CR3XX5QFBQTSG',\n'HostId':'vq8iRCJfuA4eWPgHBGhdjir1x52Tdp80ADaSxWrL4Xzsr\nVpebSZ6SnskPeYNKCOd/RZfIRT4xIM=',\n'HTTPStatusCode':200,\n'HTTPHeaders': {'x-amz-id-2': 'vq8iRCJfuA4eWPgHBGhdjir1x52\nTdp80ADaSxWrL4XzsrVpebSZ6SnskPeYNKCOd/RZfIRT4xIM=',\n   'x-amz-request-id': 'MM0CR3XX5QFBQTSG',\n   'date': 'Tue, 06 Jul 2021 01:08:36 GMT',\n   'last-modified': 'Mon, 14 Jun 2021 01:13:05 GMT',\n   'etag': '\"2ce337accfeb2dbbc6b76833bc6f84b8\"',\n   'accept-ranges': 'bytes',\n   'content-type': 'binary/octet-stream',\n   'server': 'AmazonS3',\n   'content-length': '1012427'},\n   'RetryAttempts': 0},\n   'AcceptRanges': 'bytes',\n 'LastModified': datetime.datetime(2021, 6, 14, 1, 13, 5, tzinfo=tzutc()),\n 'ContentLength': 1012427,\n 'ETag': '\"2ce337accfeb2dbbc6b76833bc6f84b8\"',\n 'ContentType': 'binary/octet-stream',\n 'Metadata': {},\n 'Body': <botocore.response.StreamingBody at 0x7fe9c16b55b0>}\n```", "```py\nurl = \"https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory\"\nresults = pd.read_html(url)\nprint(len(results))\n>>\n```", "```py\ndf = results[15]\ndf.columns\n>>\nIndex(['Region[30]', 'Total cases', 'Total deaths', 'Cases per million',\n       'Deaths per million', 'Current weekly cases', 'Current weekly deaths',\n       'Population millions', 'Vaccinated %[31]'],\n      dtype='object')\n```", "```py\ndf[['Region[30]','Total cases', 'Total deaths', 'Cases per million']].head(3)\n>>\nRegion[30]      Total cases Total deaths    Cases per million\n0   European Union  179537758   1185108 401363\n1   North America   103783777   1133607 281404\n2   Other Europe        57721948    498259      247054\n```", "```py\nfrom io import StringIO\nimport pandas as pd\nhtml = \"\"\"\n <table>\n   <tr>\n     <th>Ticker</th>\n     <th>Price</th>\n   </tr>\n   <tr>\n     <td>MSFT</td>\n     <td>230</td>\n   </tr>\n   <tr>\n     <td>APPL</td>\n     <td>300</td>\n   </tr>\n     <tr>\n     <td>MSTR</td>\n     <td>120</td>\n   </tr>\n </table>\n </body>\n </html>\n \"\"\"\n\ndf = pd.read_html(StringIO(html))\ndf[0]\n>>\n  Ticker  Price\n0   MSFT    230\n1   APPL    300\n2   MSTR    120\n```", "```py\n<table class=\"wikitable sortable mw-datatable covid19-countrynames jquery-tablesorter\" id=\"thetable\" style=\"text-align:right;\">\n```", "```py\nconda install html5lib beautifulSoup4\n```", "```py\npip install html5lib beautifulSoup4\n```", "```py\nurl = \"https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory\"\ndf = pd.read_html(url, attrs={'class': 'sortable'})\nlen(df)\n>>  7\ndf[3].columns\n>>\nIndex(['Region[28]', 'Total cases', 'Total deaths', 'Cases per million',\n       'Deaths per million', 'Current weekly cases', 'Current weekly deaths',\n       'Population millions', 'Vaccinated %[29]'],\n      dtype='object')\n```", "```py\nconda install -c conda-forge pyarrow\n```", "```py\npip install pyarrow\n```", "```py\nfile = Path('../../datasets/Ch2/LA_weather.parquet/')\ndf = pd.read_parquet(file,\n                    engine='pyarrow')\n```", "```py\ndf.info()\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4899 entries, 0 to 4898\nData columns (total 17 columns):\n #   Column           Non-Null Count  Dtype        \n---  ------           --------------  -----        \n 0   STATION          4899 non-null   object       \n 1   NAME             4899 non-null   object       \n 2   DATE             4899 non-null   object       \n 3   PRCP             4899 non-null   float64      \n 4   PRCP_ATTRIBUTES  4899 non-null   object       \n 5   SNOW             121 non-null    float64      \n 6   SNOW_ATTRIBUTES  121 non-null    object       \n 7   SNWD             59 non-null     float64      \n 8   SNWD_ATTRIBUTES  59 non-null     object       \n 9   TAVG             3713 non-null   float64      \n 10  TAVG_ATTRIBUTES  3713 non-null   object       \n 11  TMAX             4899 non-null   int64        \n 12  TMAX_ATTRIBUTES  4899 non-null   object       \n 13  TMIN             4899 non-null   int64        \n 14  TMIN_ATTRIBUTES  4899 non-null   object       \n 15  DT               4899 non-null   datetime64[ns]\n 16  year             4899 non-null   category     \ndtypes: category(1), datetime64[ns](1), float64(4), int64(2), object(9)\nmemory usage: 617.8+ KB\n```", "```py\nfilters = [('year', '==', 2012)]\ndf_2012 = pd.read_parquet(file,\n                          engine='pyarrow',\n                          filters=filters)\n```", "```py\nfilters = [('year', '>', 2020)]\nfilters = [('year', '>=', 2021)]\nfilters = [('year', 'in', [2021, 2022, 2023])]\n```", "```py\ndf = pd.read_parquet(file,\n                     engine='pyarrow',\n                     filters=filters)\n```", "```py\ncolumns = ['DATE', 'year', 'TMAX']\ndf = pd.read_parquet(file,\n                     engine='pyarrow',\n                     filters=filters,\n                     columns=columns)\n```", "```py\ndf.head()\n>>\nDATE        year    TMAX\n0   2021-01-01  2021    67\n1   2021-01-02  2021    63\n2   2021-01-03  2021    62\n3   2021-01-04  2021    59\n4   2021-01-05  2021    57\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 881 entries, 0 to 880\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   DATE    881 non-null    object \n 1   year    881 non-null    category\n 2   TMAX    881 non-null    int64  \ndtypes: category(1), int64(1), object(1)\nmemory usage: 15.2+ KB\n```", "```py\nread_parquet(\n    path: 'FilePath | ReadBuffer[bytes]',\n    engine: 'str' = 'auto',\n    columns: 'list[str] | None' = None,\n    **kwargs,\n)\n```", "```py\nimport pyarrow.parquet as pq\nfrom pathlib import Path\nfile = Path('../../datasets/Ch2/LA_weather.parquet/')\ntable = pq.read_table(file, filters=filters, columns=columns)\n```", "```py\nimport pyarrow as pa\nisinstance(table, pa.Table)\n>> True\n```", "```py\ndf = table.to_pandas()\n```", "```py\ncolumns = ['DATE','year', 'TMAX']\nfilters = [('year', 'in', [2021, 2022, 2023])]\ntb = pq.read_table(file,\n                   filters=filters,\n                   columns=columns,\n                   use_pandas_metadata=True)\ndf_pa = tb.to_pandas()\ndf_pd = pd.read_parquet(file,\n                        filters=filters,\n                        columns=columns,\n                        use_pandas_metadata=True)\n```", "```py\nTo learn more about pandas.read_parquet() function, you can read their latest documentation here https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\n```", "```py\n>>> pip install “dask[complete]” \"modin[all]\" polars\n```", "```py\n>>> conda install -c conda-forge dask polars modin-all\n```", "```py\n# Script to create one large data file\nimport pandas as pd\nimport glob\ndef run_once():\n    # Directory path where Parquet files are located\n    directory = '../../datasets/Ch2/yellow_tripdata_2023-*.parquet'\n\n    # Get a list of all Parquet files in the directory\n    parquet_files = glob.glob(directory)\n\n    # Read all Parquet files into a single DataFrame\n    dfs = []\n    for file in parquet_files:\n        df = pd.read_parquet(file)\n        dfs.append(df)\n\n    # Concatenate all DataFrames into a single DataFrame\n    combined_df = pd.concat(dfs)\n\n    combined_df.to_csv('../../datasets/Ch2/yellow_tripdata_2023.csv', index=False)\nrun_once()\n```", "```py\nPip install -U memory_profiler\n```", "```py\n%load_ext memory_profiler\n```", "```py\nimport pandas as pd\nfrom pathlib import Path\nfile_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')\n%%time\n%%memit\ndf_pd = pd.read_csv(file_path, low_memory=False)\n```", "```py\npeak memory: 10085.03 MiB, increment: 9922.66 MiB\nCPU times: user 21.9 s, sys: 2.64 s, total: 24.5 s\nWall time: 25 s\n```", "```py\n%%time\n%%memit\ndf_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)\n```", "```py\npeak memory: 3101.41 MiB, increment: 0.77 MiB\nCPU times: user 25.5 ms, sys: 13.4 ms, total: 38.9 ms\nWall time: 516 ms\n```", "```py\ntype(df_pd)\npandas.io.parsers.readers.TextFileReader\n```", "```py\n%%time\n%%memit\ndf_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)\ndf_pd.get_chunk()\n>>\npeak memory: 6823.64 MiB, increment: 9.14 MiB\nCPU times: user 72.3 ms, sys: 40.8 ms, total: 113 ms\nWall time: 581 ms\n# this is equivalent to\ndf_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)\nNext(df_pd)\n```", "```py\n%%time\n%%memit\ndf_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)\nfinal_result = pd.DataFrame()\nfor chunk in df_pd:\n    final_result = pd.concat([final_result, chunk])\n```", "```py\n%%time\n%%memit\ndf_pd = pd.read_csv(file_path, low_memory=False, chunksize=10000)\nfinal_result = pd.concat(pf_pd)\n>>\npeak memory: 9145.42 MiB, increment: 697.86 MiB\nCPU times: user 14.9 s, sys: 2.81 s, total: 17.7 s\nWall time: 18.8 s\n```", "```py\nimport polars as pl\nfrom pathlib import Path\nfile_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')\n```", "```py\n%%time\n%%memit\ndf_pl = pl.read_csv(file_path)\n>>\npeak memory: 8633.58 MiB, increment: 2505.14 MiB\nCPU times: user 4.85 s, sys: 3.28 s, total: 8.13 s\nWall time: 2.81 s\n```", "```py\ndf_pl.head()\n```", "```py\ndf_pl.shape\n>>\n(16186386, 20)\n```", "```py\ndf_pd = df_pl.to_pandas()\n```", "```py\nimport dask.dataframe as dd\nfrom pathlib import Path\nfile_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')\n```", "```py\n%%time\n%%memit\ndf_dk = dd.read_csv(file_path)\n>>\npeak memory: 153.22 MiB, increment: 3.38 MiB\nCPU times: user 44.9 ms, sys: 12.1 ms, total: 57 ms\nWall time: 389 ms\n```", "```py\ndf_dk.shape\n>>\n(Delayed('int-0ab72188-de09-4d02-a76e-4a2c400e918b'), 20)\ndf_dk.info()\n<class 'dask.dataframe.core.DataFrame'>\nColumns: 20 entries, VendorID to airport_fee\ndtypes: float64(13), int64(4), string(3)\n```", "```py\nprint(df_dk)\n>> \nDask DataFrame Structure:\n               VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type fare_amount    extra  mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge Airport_fee airport_fee\nnpartitions=26                                                                                                                                                                                                                                                                                            \n                  int64               string                string         float64       float64    float64             string        int64        int64        int64     float64  float64  float64    float64      float64               float64      float64              float64     float64     float64\n                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...\n...                 ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...\n                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...\n                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...         ...\nDask Name: to_pyarrow_string, 2 graph layers\n```", "```py\ndf_dk.head()\n```", "```py\n%%time\n%%memit\nprint(df_dk.shape[0].compute())\n>>\n16186386\npeak memory: 6346.53 MiB, increment: 1818.44 MiB\nCPU times: user 19.3 s, sys: 5.29 s, total: 24.6 s\nWall time: 10.5 s\n```", "```py\ndf_pd = df_dk.compute()\ntype(df_pd)\n>>\npandas.core.frame.DataFrame\n```", "```py\nfrom pathlib import Path\nfrom modin.config import Engine\nEngine.put(\"dask\")  # Modin will use Dask\nimport modin.pandas as pd\nfile_path = Path('../../datasets/Ch2/yellow_tripdata_2023.csv')\n```", "```py\n%%time\n%%memit\npd.read_csv(file_path)\n>>\npeak memory: 348.02 MiB, increment: 168.59 MiB\nCPU times: user 1.23 s, sys: 335 ms, total: 1.57 s\nWall time: 8.26 s\n```", "```py\ndf_pd.info()\n>>\n<class 'modin.pandas.dataframe.DataFrame'>\nRangeIndex: 16186386 entries, 0 to 16186385\nData columns (total 20 columns):\n #   Column                 Dtype \n---  ------                 ----- \n 0   VendorID               int64 \n 1   tpep_pickup_datetime   object\n 2   tpep_dropoff_datetime  object\n 3   passenger_count        float64\n 4   trip_distance          float64\n 5   RatecodeID             float64\n 6   store_and_fwd_flag     object\n 7   PULocationID           int64 \n 8   DOLocationID           int64 \n 9   payment_type           int64 \n 10  fare_amount            float64\n 11  extra                  float64\n 12  mta_tax                float64\n 13  tip_amount             float64\n 14  tolls_amount           float64\n 15  improvement_surcharge  float64\n 16  total_amount           float64\n 17  congestion_surcharge   float64\n 18  Airport_fee            float64\n 19  airport_fee            float64\ndtypes: float64(13), int64(4), object(3)\nmemory usage: 2.4+ GB\n```"]