<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Time Series Analysis with Python Cookbook, 2E - Second Edition</title>
<link rel="stylesheet" type="text/css" href="../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/><link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content"><section id="reading-time-series-data-from-databases" class="level1 pkt" data-number="4">
<h1 data-number="4">3 Reading Time Series Data from Databases</h1>
<section id="join-our-book-community-on-discord-2" class="level2" data-number="4.1">
<h2 data-number="4.1">Join our book community on Discord</h2>
<p>
<img style="width:15rem" src="../media/file0.png" width="200" height="200"/>
</p>
<p><a href="https://packt.link/zmkOY">https://packt.link/zmkOY</a></p>
<p><strong>Databases</strong> extend what you can store to include text, images, and media files and are designed for efficient read-and-write operations at a massive scale. Databases can store terabytes and petabytes of data with efficient and optimized data retrieval capabilities, such as when performing analytical operations on <strong>data warehouses</strong> and <strong>data lakes</strong>. A data warehouse is a database designed to store large amounts of structured data, mostly integrated from multiple source systems, built specifically to support business intelligence reporting, dashboards, and advanced analytics. A data lake, on the other hand, stores a large amount of structured, semi-structured, or unstructured data in its raw format. In this chapter, we will continue to use the <strong>pandas</strong> library to read data from databases. We will create time series DataFrames by reading data from <strong>relational</strong> (SQL) databases and <strong>non-relational</strong> (NoSQL) databases.</p>
<p>Additionally, you will explore working with third-party data providers to pull financial data from their database systems.</p>
<p>In this chapter, you will create time series DataFrames with a <code>DatetimeIndex</code> data type by covering the following recipes:</p>
<ul>
<li>Reading data from a relational database</li>
<li>Reading data from Snowflake</li>
<li>Reading data from a document database</li>
<li>Reading data from a time series databases</li>
</ul>
</section>
<section id="technical-requirements-2" class="level2" data-number="4.2">
<h2 data-number="4.2">Technical requirements</h2>
<p>In this chapter, we will be using pandas 2.2.2 (released April 10, 2024) extensively.</p>
<p>You will be working with different types of databases, such as PostgreSQL, Amazon Redshift, MongoDB, InfluxDB, and Snowflake. You will need to install additional Python libraries to connect to these databases.</p>
<p>You can also download the Jupyter notebooks from this book's GitHub repository (<a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>) to follow along.</p>
<p>As a good practice, you will store your database credentials in a config <code>database.cfg</code> file outside your Python script. You can use <code>configparser</code> to read and store the values in Python variables. You do not want your credentials exposed or hard coded in your code:</p>
<div class="C0-SHCodePACKT">
<pre><code># Example of configuration file "database.cfg file"
[SNOWFLAKE]
user=username
password=password
account=snowflakeaccount
warehouse=COMPUTE_WH
database=SNOWFLAKE_SAMPLE_DATA
schema=TPCH_SF1
role=somerole
[POSTGRESQL]
host: 127.0.0.1
dbname: postgres
user: postgres
password: password
[AWS]
host=&lt;your_end_point.your_region.redshift.amazonaws.com&gt;
port=5439
database=dev
username=&lt;your_username&gt;
password=&lt;your_password&gt;</code></pre>
</div>
<p>You can load the <code>database.cfg</code> file using <code>config.read()</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code>import configparser
config = configparser.ConfigParser()
config.read(database.cfg')</code></pre>
</div>
</section>
<section id="reading-data-from-a-relational-database" class="level2" data-number="4.3">
<h2 data-number="4.3">Reading data from a relational database</h2>
<p>In this recipe, you will read data from PostgreSQL, a popular open-source relational database.</p>
<p>You will explore two methods for connecting to and interacting with PostgreSQL. First, you will use <code>psycopg</code>, a PostgreSQL Python connector, to connect and query the database, then parse the results into a pandas DataFrame. In the second approach, you will query the same database again, but this time, you will use <strong>SQLAlchemy</strong>, an <strong>object-relational mapper</strong> (<strong>ORM</strong>) that is well-integrated with pandas.</p>
<section id="getting-ready-8" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1">Getting ready</h3>
<p>In this recipe, it is assumed that you have the latest PostgreSQL installed. At the time of writing, version 16 is the latest stable version.</p>
<p>To connect to and query the database in Python, you will need to install <code>psycopg</code>, a popular PostgreSQL database adapter for Python. You will also need to install <code>SQLAlchemy</code>, which provides flexibility regarding how you want to manage the database, whether for writing or reading data.</p>
<p>To install the libraries using <code>conda</code>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; conda install -c conda-forge psycopg sqlalchemy -y</code></pre>
</div>
<p>To install the libraries using <code>pip</code>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; pip install sqlalchemy psycopg</code></pre>
</div>
<p>If you cannot access a PostgreSQL database, the fastest way to get up and running is via Docker (<a href="https://hub.docker.com/_/postgres">https://hub.docker.com/_/postgres</a>). The following is an example command:</p>
<div class="C0-SHConPACKT">
<pre><code>docker run -d \
    --name postgres-ch3 \
        -p 5432:5432 \
    -e POSTGRES_PASSWORD=password \
    -e PGDATA=/var/lib/postgresql/data/pgdata \
    postgres:16.4-alpine</code></pre>
</div>
<p>This will create a container named <code>postgres-ch3</code>. The <code>username</code> is <code>postgres,</code> and the password is <code>password</code>. The default <code>database</code> created is called <code>postgres</code>.</p>
<p>Once the <strong>postgres-ch3</strong> container is up and running you can connect to it using <strong>DBeaver</strong> as shown:</p>
<figure>
<img src="../media/file22.png" alt="Figure 3.1 – DBeaver PostgreSQL connection settings" width="1686" height="1484"/><figcaption aria-hidden="true">Figure 3.1 – DBeaver PostgreSQL connection settings</figcaption>
</figure>
<p>You will be working with MSFT stock dataset provided in the <code>datasets/Ch3/MSFT.csv</code> folder. I have uploaded the dataset into the database using <strong>DBeaver Community Edition,</strong> which you can download here <a href="https://dbeaver.io/download/">https://dbeaver.io/download/</a></p>
<p>You can import the CSV file by right-clicking on <code>tables</code> under the <code>public</code> schema, as shown in the figure below:</p>
<figure>
<img src="../media/file23.png" alt="Figure 3.2 – Importing data to PostgreSQL using DBeaver" width="1064" height="934"/><figcaption aria-hidden="true">Figure 3.2 – Importing data to PostgreSQL using DBeaver</figcaption>
</figure>
<p>You can confirm all records have been written to the <code>msft</code> table in the <code>postgres</code> database as shown</p>
<figure>
<img src="../media/file24.png" alt="Figure 3.3 – Using SQL Editor in DBeaver to run a SQL query against the msft table" width="1422" height="1026"/><figcaption aria-hidden="true">Figure 3.3 – Using SQL Editor in DBeaver to run a SQL query against the msft table</figcaption>
</figure>
</section>
<section id="how-to-do-it-8" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2">How to do it…</h3>
<p>We will start by connecting to the PostgreSQL instance, querying the database, loading the result set into memory, and parsing the data into a time series DataFrame.</p>
<p>In this recipe, I will connect to a PostgreSQL instance that runs locally, so my connection would be to <code>localhost (127.0.0.1)</code>. You will need to adjust this for your own PostgreSQL database setting.</p>
<section id="using-the-psycopg" class="level4" data-number="4.3.2.1">
<h4 data-number="4.3.2.1">Using the psycopg</h4>
<p><strong>Psycopg</strong> is a Python library (and a database driver) that provides additional functionality and features when working with a PostgreSQL database. Follow these steps:</p>
<ol>
<li>Start by importing the necessary libraries. You will import the required connection parameters from the <code>database.cfg</code> highlighted in the <em>Technical Requirements</em> section. You will create a Python dictionary to store all the parameter values required to establish a connection to the database, such as <code>host</code>, <code>database</code> name, <code>user</code> name, and <code>password</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import psycopg
import pandas as pd
import configparser
config = configparser.ConfigParser()
config.read('database.cfg')
params = dict(config['POSTGRESQL'])</code></pre>
</div>
<ol>
<li>You can establish a connection by passing the parameters to the <code>connect()</code> method. Once connected, you can create a <strong>cursor</strong> object that can be used to execute SQL queries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>conn = psycopg.connect(**params)
cursor = conn.cursor()</code></pre>
</div>
<ol>
<li>The cursor object provides several attributes and methods, including <code>execute</code>, <code>executemany</code>, <code>fetchall</code>, <code>fetchmany</code> and <code>fetchone</code>. The following code uses the cursor object to pass a SQL query and then checks the number of records that have been produced by that query using the <code>rowcount</code> attribute:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>cursor.execute("""
SELECT date, close, volume
FROM msft
ORDER BY date;
""")
cursor.rowcount
&gt;&gt; 1259</code></pre>
</div>
<ol>
<li>The returned result set after executing the query will not include a header (no columns names). Alternatively, you can grab the column names from the cursor object using the <code>description</code> attribute, as shown in the following code:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>cursor.description
&gt;&gt;
[&lt;Column 'date', type: varchar(50) (oid: 1043)&gt;,
 &lt;Column 'close', type: float4 (oid: 700)&gt;,
 &lt;Column 'volume', type: int4 (oid: 23)&gt;]</code></pre>
</div>
<ol>
<li>You can use a list comprehension to extract the column names from <code>cursor.description</code> to pass as column headers when creating the DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>columns = [col[0] for col in cursor.description]
columns
&gt;&gt;
['date', 'close', 'volume']</code></pre>
</div>
<ol>
<li>To fetch the results that were produced by the executed query you will use the <code>fetchall</code> method. You will create a DataFrame based on the fetched result set. Make sure that you pass the column names that you just captured:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>data = cursor.fetchall()
df = pd.DataFrame(data, columns=columns)
df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1259 entries, 0 to 1258
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   date    1259 non-null   object
 1   close   1259 non-null   float64
 2   volume  1259 non-null   int64 
dtypes: float64(1), int64(1), object(1)
memory usage: 29.6+ KB</code></pre>
</div>
<p>Notice the <code>date</code> column is returned as an <code>object</code> type, not a <code>datetime</code> type.</p>
<ol>
<li>Parse the <code>date</code> column using <code>pd.to_datetime()</code> and set it as the index for the DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date')
print(df.tail(3))
&gt;&gt;
             close    volume
date                       
2024-08-30  417.14  24308300
2024-09-03  409.44  20285900
2024-09-04  408.84   9167942</code></pre>
</div>
<p>In the preceding code, the cursor returned a list of tuples <strong>without a header</strong>. You can confirm this by running the following code:</p>
<div class="C1-SHCodePACKT">
<pre><code>data = cursor.fetchall()
data[0:5]
&gt;&gt;
[('2019-09-04', 131.45726, 17995900),
 ('2019-09-05', 133.7687, 26101800),
 ('2019-09-06', 132.86136, 20824500),
 ('2019-09-09', 131.35222, 25773900),
 ('2019-09-10', 129.97684, 28903400)]</code></pre>
</div>
<p>You can instruct the cursor to return a <code>dict_row</code> type, which will include the column name information (the header). This is more convenient when converting into a DataFrame. This can be done by passing the <code>dict_row</code> class to the <code>row_factory</code> parameter:</p>
<div class="C1-SHCodePACKT">
<pre><code>from psycopg.rows import dict_row
conn = psycopg.connect(**params, row_factory=dict_row)
cursor = conn.cursor()
cursor.execute("SELECT * FROM msft;")
data = cursor.fetchall()
data[0:2]
&gt;&gt;
[{'date': '2019-09-04',
  'open': 131.14206,
  'high': 131.51457,
  'low': 130.35883,
  'close': 131.45726,
  'volume': 17995900},
 {'date': '2019-09-05',
  'open': 132.87086,
  'high': 134.08391,
  'low': 132.53656,
  'close': 133.7687,
  'volume': 26101800}]</code></pre>
</div>
<p>Notice the column names being available. You can now create a DataFrame as in the following code:</p>
<div class="C1-SHCodePACKT">
<pre><code>df = pd.DataFrame(data)
print(df.head())
&gt;&gt;
        date       open       high        low      close    volume
0  2019-09-04  131.14206  131.51457  130.35883  131.45726  17995900
1  2019-09-05  132.87086  134.08391  132.53656  133.76870  26101800
2  2019-09-06  133.74963  133.89291  132.00171  132.86136  20824500
3  2019-09-09  133.32938  133.48220  130.33977  131.35222  25773900
4  2019-09-10  130.66455  130.75050  128.47725  129.97684  28903400</code></pre>
</div>
<ol>
<li>Close the cursor and the connection to the database:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>cursor.close()
conn.close()</code></pre>
</div>
<p>Note that <code>psycopg</code> connections and cursors can be used in Python's <code>with</code> statement for exception handling when committing a transaction. The cursor object provides three different fetching functions; that is, <code>fetchall</code>, <code>fetchmany</code>, and <code>fetchone</code>. The <code>fetchone</code> method returns a single tuple. The following example shows this concept:</p>
<div class="C1-SHCodePACKT">
<pre><code>with psycopg.connect(**params) as conn:
     with conn.cursor() as cursor:
            cursor.execute('SELECT * FROM msft')
            data = cursor.fetchone()
print(data)
&gt;&gt;
('2019-09-04', 131.14206, 131.51457, 130.35883, 131.45726, 17995900)</code></pre>
</div>
</section>
<section id="using-pandas-and-sqlalchemy" class="level4" data-number="4.3.2.2">
<h4 data-number="4.3.2.2">Using pandas and SQLAlchemy</h4>
<p>SQLAlchemy is a very popular open-source library for working with relational databases in Python. SQLAlchemy can be referred to as an <strong>Object-Relational Mapper (ORM)</strong>, which provides an abstraction layer (think of it as an interface) so that you can use object-oriented programming to interact with a relational database.</p>
<p>You will be using SQLAlchemy because it integrates very well with pandas, and several of the pandas SQL reader and writer functions depend on SQLAlchemy as the abstraction layer. SQLAlchemy does the behind-the-scenes translation for any pandas SQL read or write requests. This translation ensures that the SQL statement from pandas is represented in the correct syntax/format for the underlying database type (MySQL, Oracle, SQL Server, or PostgreSQL, to name a few).</p>
<p>Some of the pandas SQL reader functions that rely on SQLAlchemy include <code>pandas.read_sql</code>, <code>pandas.read_sql_query</code>, and <code>pandas.read_sql_table</code>. Let's perform the following steps:</p>
<ol>
<li>Start by importing the necessary libraries. Note that, behind the scenes, SQLAlchemy will use <strong>psycopg</strong> (or any other database driver that is installed and supported by SQLAlchemy):</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from sqlalchemy import create_engine
engine =\
    create_engine("postgresql+psycopg://postgres:password@localhost:5432/postgres")
query = "SELECT * FROM msft"
df = pd.read_sql(query,
                 engine,
                 index_col='date',
                 parse_dates={'date': '%Y-%m-%d'})
print(df.tail(3))
&gt;&gt;
              open    high     low   close    volume
date                                               
2024-08-30  415.60  417.49  412.13  417.14  24308300
2024-09-03  417.91  419.88  407.03  409.44  20285900
2024-09-04  405.63  411.24  404.37  408.84   9167942</code></pre>
</div>
<p>In the preceding example, for <code>parse_dates</code>, you passed a dictionary in the format of <code>{key: value}</code>, where <code>key</code> is the column name and the <code>value</code> is a string representation of the date format. Unlike the previous <strong>psycopg</strong> approach, <code>pandas.read_sql</code> did a better job in getting the data types correct. Notice that our index is of the <code>DatetimeIndex</code> type:</p>
<div class="C1-SHCodePACKT">
<pre><code>df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 1259 entries, 2019-09-04 to 2024-09-04
Data columns (total 5 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   open    1259 non-null   float64
 1   high    1259 non-null   float64
 2   low     1259 non-null   float64
 3   close   1259 non-null   float64
 4   volume  1259 non-null   int64 
dtypes: float64(4), int64(1)
memory usage: 59.0 KB</code></pre>
</div>
<ol>
<li>You could also accomplish the same results using the <code>pandas.read_sql_query</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df = pd.read_sql_query(query,
                       engine,
                       index_col='date',
                       parse_dates={'date':'%Y-%m-%d'})</code></pre>
</div>
<ol>
<li>The <code>pandas.read_sql_table</code> is another SQL reader function provided by pandas that takes in a table name instead of a SQL query. Think of this as a <code>SELECT * FROM tablename</code> query:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df = pd.read_sql_table('msft',
                        engine,
                        index_col='date',
                        parse_dates={'date':'%Y-%m-%d'})</code></pre>
</div>
<blockquote>
<p>The <code>read_sql</code> reader function is more versatile as it is a wrapper to <code>read_sql_query</code> and <code>read_sql_table</code>. The <code>read_sql</code> function can take either a SQL query or a table name.</p>
</blockquote>
</section>
</section>
<section id="how-it-works-8" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3">How it works…</h3>
<p>You explored two methods to connect to a PostgreSQL database in this recipe: using the psycopg driver directly or utilizing pandas and SQLAlchemy.</p>
<p>When using <strong>psycopg</strong> to connect to a PostgreSQL, you first need to create a connection object followed by a cursor object. This concept of a connection objects and cursors is consistent across different database drivers in Python. Once you create a cursor object, you have access to several methods, including:</p>
<ul>
<li><code>Execute()</code> – executes a SQL query (CRUD) or command to the database</li>
<li><code>Executemany()</code> – executes the same database operation with a sequence of input data, for example, this can be useful with INSERT INTO for bulk insert.</li>
<li><code>Fetchall()</code> – returns all remaining records from the current query result set</li>
<li><code>Fetchone() </code>- returns the next record (one record) from the current query result set</li>
<li><code>fetchmany(n)</code> – returns <code>n</code> number of records from the current query result set</li>
<li><code>close() </code>- close the current cursor and free associated resources</li>
</ul>
<p>On the other hand, creating an <strong>engine</strong> object is the first step when working with SQLAlchemy, as it provides instructions on the database that is being considered. This is known as a <strong>dialect</strong>.</p>
<p>When you created the engine with <code>create_engine,</code> you passed a URL as the connection string. Let's examine the engine connection string for SQLAlchemy:</p>
<div class="C0-SHCodePACKT">
<pre><code>create_engine("dialect+driver://username:password@host:port/database")</code></pre>
</div>
<ul>
<li><code>dialect</code> –the name of the SQLAlchemy dialect (database type) such as postgresql, mysql, sqlite, oracle, or mssql.</li>
<li><code>driver</code> –the name of the installed driver (DBAPI) to connect to the specified dialect, such as <code>psycopg</code> or <code>pg8000</code> for PostgreSQL database.</li>
<li><code>username</code> -the login username for database authentication</li>
<li><code>password</code> -the password for the username specified</li>
<li><code>host</code> -the server where the database is hosted</li>
<li><code>port</code> -the specific port for the database</li>
<li><code>database</code> -the name of the specific database you want to connect to</li>
</ul>
<p>Earlier, you used <code>psycopg</code> as the database driver for PostgreSQL. The <code>psycopg</code> driver is referred to as a <strong>database API (DBAPI)</strong> and SQLAlchemy supports many DBAPI wrappers based on Python's DBAPI specifications to connect to and interface with various types of relational databases. SQLAlchemy already comes with built-in dialects to work with different flavors of RDBMS, such as the following:</p>
<ul>
<li>SQL Server</li>
<li>SQLite</li>
<li>PostgreSQL</li>
<li>MySQL</li>
<li>Oracle</li>
<li>Snowflake</li>
</ul>
<p>When connecting to a database using SQLAlchemy, you need to specify the <strong>dialect</strong> and the <strong>driver</strong> (DBAPI) to be used. This is what the URL string looks like for PostgreSQL:</p>
<div class="C0-SHCodePACKT">
<pre><code>create_engine("postgresql+psycopg2://username:password@localhost:5432/dbname")</code></pre>
</div>
<p>This is what it would look like if you are using <strong>MySQL</strong> database with <code>PyMySQL</code> driver:</p>
<div class="C0-SHCodePACKT">
<pre><code>create_engine("mysql+pymysql://username:password@localhost:3306/dbname")</code></pre>
</div>
<p>In the previous code examples in the <em>How to do it…</em> section, you did not need to specify the <code>psycopg</code> driver since it is the default DBAPI that SQLAlchemy uses. This example would work just fine, assuming that <code>psycopg</code> is installed:</p>
<div class="C0-SHCodePACKT">
<pre><code>create_engine("postgresql://username:password@localhost:5432/dbname")</code></pre>
</div>
<p>There are other PostgreSQL drivers (DBAPI) that are supported by SQLAlchemy, including the following:</p>
<ul>
<li><code>psycopg</code></li>
<li><code>pg8000</code></li>
<li><code>asyncpg</code></li>
</ul>
<p>For a more comprehensive list of supported dialects and drivers, you can visit the official documentation page at <a href="https://docs.sqlalchemy.org/en/20/dialects/index.html">https://docs.sqlalchemy.org/en/20/dialects/index.html</a>.</p>
<p>The advantage of using SQLAlchemy is that it is well-integrated with pandas. If you read the official pandas documentation for <code>read_sql</code>, <code>read_sql_query</code>, <code>read_sql_table</code>, and <code>to_sql</code> you will notice that the <code>con</code> argument is expecting a SQLAlchemy connection object (engine).</p>
<p>Another advantage is that you can easily change the backend engine (database), for example from PostgreSQL to MySQL, without needing to change the rest of the code.</p>
</section>
<section id="theres-more-8" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4">There's more…</h3>
<p>In this section we will explore some additional concepts to help you better grasp the versatility of SQLAclhemy and bring some of the previous concepts discussed in the recipe <em>Working with large data files</em> in Chapter 2, <em>Reading Time Series Data from Files</em>.</p>
<p>Specifically, we will discuss:</p>
<ul>
<li>Generating the connecitoin URL in SQLAlchemy</li>
<li>Extending to Amazon Redshift database</li>
<li>Chunking in pandas</li>
</ul>
<section id="generating-the-connection-url-in-sqlalchemy" class="level4" data-number="4.3.4.1">
<h4 data-number="4.3.4.1">Generating the connection URL in SQLAlchemy</h4>
<p>In this recipe you were introduced to the <code>create_engine</code> function from SQLAlchemy library which takes a URL string to establish a connection to a database. So far, you have been creating the URL string manually but there is a more convenient way to generate the URL for you. This can be accomplished with the <code>create</code> method form the <code>URL</code> class in SQLAlchemy.</p>
<p>The following code demonstrates this:</p>
<div class="C0-SHCodePACKT">
<pre><code>from sqlalchemy import URL, create_engine
url = URL.create(
    drivername='postgresql+psycopg',
    host= '127.0.0.1',
    username='postgres',
    password='password',
    database='postgres',
    port= 5432
)
&gt;&gt;
postgresql+psycopg://postgres:***@127.0.0.1:5432/postgres</code></pre>
</div>
<p>Notice the <code>drivername</code> consists of the <em>dialect</em> and the <em>driver</em> in format <code>dialct+driver</code>.</p>
<p>You can now pass the <code>url</code> to <code>create_engine</code> as you have done before.</p>
<div class="C0-SHCodePACKT">
<pre><code>engine = create_engine(url)
df = pd.read_sql('select * from msft;', engine)</code></pre>
</div>
</section>
<section id="extending-to-amazon-redshift-database" class="level4" data-number="4.3.4.2">
<h4 data-number="4.3.4.2">Extending to Amazon Redshift database</h4>
<p>We discussed the versatility of SQLAlchmey, which allows you to change the engine (database backend) and keep the remaining code as is. For example, using PostgreSQL or MySQL, or any other supported dialect by SQLAlchemy. We will also explore connecting to a cloud data warehouse like Amazon Redshift.</p>
<p>It is worth mentioning that <strong>Amazon Redshift</strong>, a cloud data warehouse, is based on PostgreSQL at its core. You will install the Amazon Redshift driver for SQLAlchemy (it uses the psycopg2 DBAPI).</p>
<p>You can install using <strong>conda</strong>:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install conda-forge::sqlalchemy-redshift</code></pre>
</div>
<p>You can also install using <strong>pip</strong>:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install sqlalchemy-redshift</code></pre>
</div>
<p>Because we do not want to expose your AWS credentials in your code, you will update the <code>database.cfg</code> file discussed in the <em>Technical Requirements</em> section to include your AWS Redshift information:</p>
<div class="C0-SHConPACKT">
<pre><code>[AWS]
host=&lt;your_end_point.your_region.redshift.amazonaws.com&gt;
port=5439
database=dev
username=&lt;your_username&gt;
password=&lt;your_password&gt;</code></pre>
</div>
<p>You will use <code>configparser</code> to load your values:</p>
<div class="C0-SHCodePACKT">
<pre><code>from configparser import ConfigParser
config = ConfigParser()
config.read('database.cfg')
config.sections()
params = dict(config['AWS'])</code></pre>
</div>
<p>You will use the URL.create method to generate your URL:</p>
<div class="C0-SHCodePACKT">
<pre><code>url = URL.create('redshift+psycopg2', **params)
aws_engine = create_engine(url)</code></pre>
</div>
<p>Now, you can switch the engine from our previous code, which originally pointing to our local instance of PostgreSQL, to run the same query on Amazon Redshift. This assumes you have a <code>msft</code> table in Amazon Redshift.</p>
<div class="C0-SHCodePACKT">
<pre><code>df = pd.read_sql(query,
                 aws_engine,
                 index_col='date',
                 parse_dates=True)</code></pre>
</div>
<p>To learn more about <code>sqlalchemy-redshift</code>, you can refer to the project's repository here: <a href="https://github.com/sqlalchemy-redshift/sqlalchemy-redshift">https://github.com/sqlalchemy-redshift/sqlalchemy-redshift</a>.</p>
<blockquote>
<p>The Amazon Redshift example can be extended to other databases such as Google BigQuery, Teradata, or Microsoft SQL Server as long there is a supported SQLAlchemy dialect for that database. For a complete list visit the official page here <a href="https://docs.sqlalchemy.org/en/20/dialects/index.html">https://docs.sqlalchemy.org/en/20/dialects/index.html</a></p>
</blockquote>
</section>
<section id="chunking-with-pandas" class="level4" data-number="4.3.4.3">
<h4 data-number="4.3.4.3">Chunking with pandas</h4>
<p>When you executed the query against <code>msft</code> table, it returned 1259 records. Imagine working with a much larger database that returns millions of records, if not more. This is where the <code>chunking</code> parameter helps.</p>
<p>The <code>chunksize</code> parameter allows you to break down a large dataset into smaller and more manageable chunks of data that can fit into your local memory. When executing the <code>read_sql</code> function, just pass the number of rows to be retrieved (per chunk) to the <code>chunksize</code> parameter, which then returns a <code>generator</code> object. You can then loop through the generator object, or use <code>next()</code> to capture one chunk at a time, and perform whatever calculations or processing needed. Let's look at an example of how chunking works. You will request <code>500</code> records (rows) at a time:</p>
<div class="C0-SHCodePACKT">
<pre><code>df_gen = pd.read_sql(query,
                 engine,
                 index_col='date',
                 parse_dates=True,
                 chunksize=500)</code></pre>
</div>
<p>The preceding code will generate three (3) chunks. You can iterate through the <code>df_gen</code> generator object as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>for idx, data in enumerate(df_gen):
     print(idx, data.shape)
&gt;&gt;
0 (500, 5)
1 (500, 5)
2 (259, 5)</code></pre>
</div>
<p>The preceding code demonstrated how chunking works. Using the <code>chunksize</code> parameter should reduce memory usage since the code loads a smaller number of rows per chunk at a time.</p>
</section>
</section>
<section id="see-also-8" class="level3" data-number="4.3.5">
<h3 data-number="4.3.5">See also</h3>
<p>For additional information regarding these topics, take a look at the following links:</p>
<ul>
<li>For <strong>SQLAlchemy</strong>, you can visit <a href="https://www.sqlalchemy.org/">https://www.sqlalchemy.org/</a></li>
<li>For the <code>pandas.read_sql</code> function visit <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html">https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html</a></li>
<li>For the <code>pandas.read_sql_query</code> function visit <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html">https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html</a></li>
<li>For the <code>pandas.read_sql_table</code> function visit <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html">https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html</a></li>
</ul>
</section>
</section>
<section id="reading-data-from-snowflake" class="level2" data-number="4.4">
<h2 data-number="4.4">Reading data from Snowflake</h2>
<p>A very common place to extract data for analytics is usually a company's <em>data warehouse</em>. Data warehouses host a massive amount of data that, in most cases, contains integrated data to support various reporting and analytics needs, in addition to historical data from various source systems.</p>
<p>The evolution of the cloud brought us cloud data warehouses such as <strong>Amazon Redshift</strong>, <strong>Google BigQuery</strong>, <strong>Azure SQL Data Warehouse</strong>, and <strong>Snowflake</strong>.</p>
<p>In this recipe, you will work with <em>Snowflake</em>, a powerful <strong>Software as a Service</strong> (<strong>SaaS</strong>) cloud-based data warehousing platform that can be hosted on different cloud platforms, such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>), <strong>Google Cloud Platform</strong> (<strong>GCP</strong>), and <strong>Microsoft Azure</strong>. You will learn how to connect to Snowflake using Python to extract time series data and load it into a pandas DataFrame.</p>
<section id="getting-ready-9" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1">Getting ready</h3>
<p>This recipe assumes you have access to Snowflake. You will explore three (3) different methods to connect to Snowflake so you will need to install three (3) different libraries .</p>
<p>The recommended approach for the <code>snowflake-connector-python</code> library is to install it using <strong>pip</strong> allowing you to install <em>extras</em> such as <code>pandas</code> as shown:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install snowflake-sqlalchemy snowflake-snowpark-python
pip install "snowflake-connector-python[pandas]"</code></pre>
</div>
<p>You can also install with <strong>conda</strong>, but if you want to use <code>snowflake-connector-python</code> with pandas you will need to use the pip install.</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge snowflake-sqlalchemy snowflake-snowpark-python
conda install -c conda-froge snowflake-connector-python</code></pre>
</div>
<p>Make sure you the configuration file <code>database.cfg</code> that you created in the <em>Technical Requirements</em> section contains your <strong>Snowflake</strong> connection information:</p>
<div class="C0-SHCodePACKT">
<pre><code>[SNOWFLAKE]
user=username
password=password
account=snowflakeaccount
warehouse=COMPUTE_WH
database=SNOWFLAKE_SAMPLE_DATA
schema=TPCH_SF1
role=somerole</code></pre>
</div>
<p>In this recipe you will be working with the <code>SNOWFLAKE_SAMPLE_DATA </code>database and the <code>TPCH_SF1 schema provided by Snowflake</code>.</p>
<blockquote>
<p>Capturing the proper <code>account</code> value can cause confusion for many. To ensure you have the right format use the <em>Copy account URL</em> option from Snowflake, which may look like this <code>https://abc1234.us-east-1.snowflakecomputing.com</code> the <code>abc1234.us-east-1</code> part is what you will use as the <code>account</code> value.</p>
</blockquote>
</section>
<section id="how-to-do-it..." class="level3" data-number="4.4.2">
<h3 data-number="4.4.2">How to do it...</h3>
<p>We will explore three (3) methods and libraries to connect to the Snowflake database. In the first method, you will use the Snowflake Python connector to establish a connection and create a cursor to query and fetch the data. In the second method, you will use the Snowflake <strong>SQLAlchemy</strong>. In the third method, you will explore the <strong>Snowpark</strong> Python API. Let's get started:</p>
<section id="using-snowflake-connector-python" class="level4" data-number="4.4.2.1">
<h4 data-number="4.4.2.1">Using snowflake-connector-python</h4>
<ol>
<li>We will start by importing the necessary libraries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from snowflake import connector
from configparser import ConfigParser</code></pre>
</div>
<ol>
<li>Using <code>ConfigParser</code>, you will extract the content under the <code>[SNOWFLAKE]</code> section to avoid exposing or hardcoding your credentials. You can read the entire content of the <code>[SNOWFLAKE]</code> section and convert it into a dictionary object, as shown here:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>config = ConfigParser()
config.read(database.cfg')
params = dict(config['SNOWFLAKE'])</code></pre>
</div>
<ol>
<li>You will need to pass the parameters to the <code>connector.connect()</code> to establish a connection with Snowflake. We can easily <em>unpack</em> the dictionary's content since the dictionary keys match the parameter names. Once the connection has been established, we can create our <em>cursor</em>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>con = connector.connect(**params)
cursor = con.cursor()</code></pre>
</div>
<ol>
<li>The cursor object has many methods, such as <code>execute</code>, <code>fetchall</code>, <code>fetchmany</code>, <code>fetchone</code>, fetch_pandas_all, and <code>fetch_pandas_batches</code>.</li>
</ol>
<p>You will start with the <code>execute</code> method to pass a SQL query to the database, then use any of the available fetch methods to retrieve the data. In the following example, you will query the <code>ORDERS</code> table and then leverage the <code>fetch_pandas_all</code> method to retrieve the entire result set as a pandas DataFrame:</p>
<div class="C1-SHCodePACKT">
<pre><code>query = "SELECT * FROM ORDERS;"
cursor.execute(query)
df = cursor.fetch_pandas_all()</code></pre>
</div>
<p>The previsou code could have been written as:</p>
<div class="C1-SHCodePACKT">
<pre><code>df = cursor.execute(query).fetch_pandas_all()</code></pre>
</div>
<ol>
<li>Inspect the DataFrame using <code>df.info()</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1500000 entries, 0 to 1499999
Data columns (total 9 columns):
 #   Column           Non-Null Count    Dtype 
---  ------           --------------    ----- 
 0   O_ORDERKEY       1500000 non-null  int32 
 1   O_CUSTKEY        1500000 non-null  int32 
 2   O_ORDERSTATUS    1500000 non-null  object
 3   O_TOTALPRICE     1500000 non-null  float64
 4   O_ORDERDATE      1500000 non-null  object
 5   O_ORDERPRIORITY  1500000 non-null  object
 6   O_CLERK          1500000 non-null  object
 7   O_SHIPPRIORITY   1500000 non-null  int8  
 8   O_COMMENT        1500000 non-null  object
dtypes: float64(1), int32(2), int8(1), object(5)
memory usage: 81.5+ MB</code></pre>
</div>
<ol>
<li>From the preceding output, you can see that the DataFrame's Index is just a sequence of numbers and that the <code>O_ORDERDATE</code> column is not a <code>Date</code> field. You can parse the <code>O_ORDERDATE</code> column to a datetime type using <code>pandas.to_datetime()</code> function and then setting the column as the DataFrame’s index with the <code>DataFrame.set_index()</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df_ts = (
    df.set_index(
                pd.to_datetime(df['O_ORDERDATE'])
                )
                .drop(columns='O_ORDERDATE'))</code></pre>
</div>
<p>Let’s display the first four (4) columns and first five (5) rows of the <code>df_ts</code> DataFrame:</p>
<div class="C1-SHCodePACKT">
<pre><code>print(df_ts.iloc[0:3, 1:5])
&gt;&gt;
             O_CUSTKEY O_ORDERSTATUS  O_TOTALPRICE  O_ORDERPRIORITY
O_ORDERDATE                                                       
1994-02-21       13726             F      99406.41         3-MEDIUM
1997-04-14      129376             O     256838.41  4-NOT SPECIFIED
1997-11-24      141613             O     150849.49  4-NOT SPECIFIED</code></pre>
</div>
<ol>
<li>Inspect the index of the DataFrame. Print the first two indexes:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df_ts.index[0:2]
&gt;&gt;
DatetimeIndex(['1994-02-21', '1997-04-14'], dtype='datetime64[ns]', name='O_ORDERDATE', freq=None)</code></pre>
</div>
<ol>
<li>Finally, you can close the cursor and current connection</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>Cursor.close()
con.close()</code></pre>
</div>
<p>You now have a time series DataFrame with a <code>DatetimeIndex</code>.</p>
</section>
<section id="using-sqlalchmey" class="level4" data-number="4.4.2.2">
<h4 data-number="4.4.2.2">Using SQLAlchmey</h4>
<p>In the previous recipe, <code>Reading data from a relational database</code>, you explored the pandas read_sql, read_sql_query, and read_sql_table functions. This was accomplished by utilizing SQLAlchemy and installing one of the supported dialicts. Here, we will use the Snowflake dialect after installing the snowflake-sqlalchemy driver.</p>
<p>SQLAclhemy is better integrated with pandas as you will experience in this section.</p>
<ol>
<li>Start by importing the necessary libraries and reading the Snowflake connection parameters from the <code>[SNOWFLAKE]</code> section in the <code>database.cfg</code> file.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from sqlalchemy import create_engine
from snowflake.sqlalchemy import URL
import configparser
config = ConfigParser()
config.read('database.cfg')
params = dict(config['SNOWFLAKE'])</code></pre>
</div>
<ol>
<li>You will use the URL class to generate the URL connection string. We will create our engine object and then open a connection with the engine.connect():</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>url = URL(**params)
engine = create_engine(url)
connection = engine.connect()</code></pre>
</div>
<ol>
<li>Now, you can use either the read_sql or read_sql_query to execute our query against the ORDERS table in the SNOWFLAKE_SAMPLE_DATA database:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = "SELECT * FROM ORDERS;"
df = pd.read_sql(query,
                 connection,
                 index_col='o_orderdate',
                 parse_dates='o_orderdate')
df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 1500000 entries, 1992-04-22 to 1994-03-19
Data columns (total 8 columns):
 #   Column           Non-Null Count    Dtype 
---  ------           --------------    ----- 
 0   o_orderkey       1500000 non-null  int64 
 1   o_custkey        1500000 non-null  int64 
 2   o_orderstatus    1500000 non-null  object
 3   o_totalprice     1500000 non-null  float64
 4   o_orderpriority  1500000 non-null  object
 5   o_clerk          1500000 non-null  object
 6   o_shippriority   1500000 non-null  int64 
 7   o_comment        1500000 non-null  object
dtypes: float64(1), int64(3), object(4)
memory usage: 103.0+ MB</code></pre>
</div>
<p>Notice how we were able to parse the o_orderdate column and set it as index all in one step when compared to the Snowflake Python connector method performed earlier.</p>
<ol>
<li>Finally, close the connection to the database</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>connection.close()
engine.dispose()</code></pre>
</div>
<p>The code could be further simplified by using a <strong>context manager</strong> to automatically allocate and release resources. The following example uses the <code>with engine.connect()</code>:</p>
<div class="C1-SHCodePACKT">
<pre><code>query = "SELECT * FROM ORDERS;"
url = URL(**params)
engine = create_engine(url)
with engine.connect() as connection:
    df = pd.read_sql(query,
                      connection,
                      index_col='o_orderdate',
                      parse_dates=['o_orderdate'])
df.info()</code></pre>
</div>
<p>This should produce the same results without needing to close connection and dispose the engine.</p>
</section>
<section id="using-snowflake-snowpark-python" class="level4" data-number="4.4.2.3">
<h4 data-number="4.4.2.3">Using snowflake-snowpark-python</h4>
<p>The Snowpark API supports Java, Python, and Scala. You have already installed the <code>snowflake-snowpark-python</code> as described in the <em>Getting Ready</em> section of this recipe.</p>
<ol>
<li>Start by importing the necessary libraries and reading the Snowflake connection parameters from the <code>[SNOWFLAKE]</code> section in the <code>database.cfg</code> file</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from snowflake.snowpark import Session
from configparser import ConfigParser
config = ConfigParser()
config.read('database.cfg')
params = dict(config['SNOWFLAKE'])</code></pre>
</div>
<ol>
<li>Create a session by establishing a connection with the Snowflake database</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>session = Session.builder.configs(params).create()</code></pre>
</div>
<ol>
<li>The session has several <code>DataFrameReader</code> methods such as <code>read</code>, <code>table</code>, and <code>sql. </code>Any of these methods would return a Snowpark DataFrame object. The returned Snowpark DataFrame object has access to the <code>to_pandas</code> method to convert into a more familiar pandas DataFrame. You will explore the <code>read</code>, <code>table</code>, and <code>sql</code> methods to return the same result set.</li>
</ol>
<p>Start with the <code>read</code> method. More specifically, you will be using the <code>read.table</code> and pass it a table name. This will return the content of the table and covert into a pandas DataFrame with the <code>to_pandas</code> method. Think of this as equivalent to <code>SELECT * FROM TABLE</code>.</p>
<div class="C1-SHCodePACKT">
<pre><code>orders = session.read.table("ORDERS").to_pandas()</code></pre>
</div>
<p>Similarly, the <code>table</code> method take a table name, and the returned object (Snowpark DataFrame) as access to the <code>to_pandas</code> method:</p>
<div class="C1-SHCodePACKT">
<pre><code>orders = session.table("ORDERS").to_pandas()</code></pre>
</div>
<p>Lastly, the <code>sql</code> method which takes a SQL query:</p>
<div class="C1-SHCodePACKT">
<pre><code>query = 'SELECT * FROM ORDERS'
orders = session.sql(query).to_pandas()
orders.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1500000 entries, 0 to 1499999
Data columns (total 9 columns):
 #   Column           Non-Null Count    Dtype 
---  ------           --------------    ----- 
 0   O_ORDERKEY       1500000 non-null  int32 
 1   O_CUSTKEY        1500000 non-null  int32 
 2   O_ORDERSTATUS    1500000 non-null  object
 3   O_TOTALPRICE     1500000 non-null  float64
 4   O_ORDERDATE      1500000 non-null  object
 5   O_ORDERPRIORITY  1500000 non-null  object
 6   O_CLERK          1500000 non-null  object
 7   O_SHIPPRIORITY   1500000 non-null  int8  
 8   O_COMMENT        1500000 non-null  object
dtypes: float64(1), int32(2), int8(1), object(5)
memory usage: 81.5+ MB</code></pre>
</div>
<p>All three approaches should produce the same pandas DataFrame.</p>
</section>
</section>
<section id="how-it-works..." class="level3" data-number="4.4.3">
<h3 data-number="4.4.3">How it works...</h3>
<p>The Snowflake Python connector, Snowflake SQLAlchmey driver, and Snowpark Python require the same input variables to establish a connection to the Snowflake database. These include the following:</p>
<figure>
<img src="../media/file25.jpg" alt="Table 3.1 – Input variables for the Snowflake Python connector" width="1465" height="581"/><figcaption aria-hidden="true">Table 3.1 – Input variables for the Snowflake Python connector</figcaption>
</figure>
<p>Recall that in the previous activity, you used the same configuration for all three methods in the <code>database.cfg</code> file under the <code>[SNOWFLAKE]</code> section.</p>
<section id="snowflake-python-connector" class="level4" data-number="4.4.3.1">
<h4 data-number="4.4.3.1">Snowflake Python Connector</h4>
<p>When using the Python connector, you first establish a connection to the database with <code>con = connector.connect(**params).</code> Once the connection is accepted, you create a <strong>cursor</strong> object with <code>cursor = con.cursor()</code>.</p>
<p>The cursor provides methods for performing execute and fetch operations such as <code>describe()</code>, <code>execute()</code>, <code>execute_async()</code>, <code>executemany()</code>, <code>fetchone()</code>, <code>fetchall()</code>, <code>fetchmany()</code>, <code>fetch_pandas_all()</code>, and <code>fetch_pandas_batches()</code>, and each cursor has several attributes including <code>description</code>, <code>rowcount</code>, <code>rownumber</code>, to name few. Note there are familiar methods and attributes discussed in the previous recipe, <em>Reading data from a relational database</em>, when using the Python connector <strong>psycopg.</strong></p>
<ul>
<li><code>Execute()</code> – executes a SQL query (CRUD) or command to the database</li>
<li><code>executemany()</code> – executes the same database operation with a sequence of input data, for example, this can be useful with INSERT INTO for bulk insert.</li>
<li><code>Fetchall()</code> – returns all remaining records from the current query result set</li>
<li><code>fetchone() </code>- returns the next record (one record) from the current query result set</li>
<li><code>fetchmany(n)</code> – returns <code>n</code> number of records from the current query result set</li>
<li><code>fetch_pandas_all() - </code>returns all remaining records from the current query result set and loads them into a pandas DataFrame</li>
<li><code>fetch_pandas_batches()</code> - returns a subset of the remaining records form the current query result set and loads them into a pandas DataFrame</li>
<li><code>close() </code>- close the current cursor and free associated resources</li>
<li><code>describe() </code>– returns metadata about the result set without executing the query. Alternatively, you can use <code>execute()</code> followed by <code>description</code> attribute to obtain the same metadata information.</li>
</ul>
<p>For a complete list of attributes and methods, you can refer to the official documentation at <a href="https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor">https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor</a>.</p>
</section>
<section id="sqlalchmey-api" class="level4" data-number="4.4.3.2">
<h4 data-number="4.4.3.2">SQLAlchmey API</h4>
<p>When SQLAlchemy, you are able to leverage the <code>pandas.read_sql, pandas.read_sql_query, </code>and<code> pandas.read_sql_query</code> reader functions and leverage many of the available parameters to transform and process the data at read time such as <code>index_col</code> and <code>parse_dates</code>. On the other hand, when using the Snowflake Python connector, the <code>fetch_pandas_all()</code> function does not take in any parameters, and you will need to parse and adjust the DataFrame afterward.</p>
<p>The Snowflake SQLAlchemy library provides a convenience method, <code>URL</code>, to help construct the connection string to connect to the Snowflake database. Typically, SQLAlchemy expects a URL to be provided in the following format:</p>
<div class="C0-SHCodePACKT">
<pre><code>'snowflake://&lt;user&gt;:&lt;password&gt;@&lt;account&gt;/&lt;database&gt;/&lt;schema&gt;
?warehouse=&lt;warehouse&gt;&amp;role=&lt;role&gt;'</code></pre>
</div>
<p>Using the <code>URL</code> method, we passed our parameters, and the method took care of constructing the connection string that is expected:</p>
<div class="C0-SHCodePACKT">
<pre><code>engine = create_engine(URL(
    account = '&lt;your_account&gt;',
    user = '&lt;your_username&gt;',
    password = '&lt;your_password&gt;',
    database = '&lt;your_database&gt;',
    schema = '&lt;your_schema&gt;',
    warehouse = '&lt;your_warehouse&gt;',
    role='&lt;your_role&gt;',
))</code></pre>
</div>
<p>Alternatively, we stored our Snowflake parameters in the configuration file database.cfg and stored as Python dictionary. This way, you will not be exposing your credentials within the code.</p>
<div class="C0-SHCodePACKT">
<pre><code>params = dict(config['SNOWFLAKE'])
url = create_engine(URL(**params))</code></pre>
</div>
<blockquote>
<p>If you compare the process in this recipe, using <strong>SQLAlchemy</strong>, for Snowflake, and that of the previous recipe, <em>Reading data from a relational database</em>, you will observe similarities in the process and code. This is one of the advantages of using SQLAlchemy, it creates a standard process across a variety of databases as long as SQLAlchmey supports them. <em>SQLAlchemy is well integrated with pandas and makes it easy to switch dialects (backend databases) without much change to your code</em>.</p>
</blockquote>
</section>
<section id="snowpark-api" class="level4" data-number="4.4.3.3">
<h4 data-number="4.4.3.3">Snowpark API</h4>
<p>In the previous methods, you simply used the libraries <code>snowflake-connector-python</code> and <code>snowflake-connector-python</code> as connectors to your Snowflake database, and then fetch the data to process the data locally.</p>
<p>Snowpark offers more than just a mechanism to connect to your database. It allows you to process the data directly within the Snowflake environment on the cloud without the need to move the data outside or process locally. In addition, Snowpark is well suited for more complex tasks such as building complex data pipelines or Snowpark ML for working with machine learning models all on the Snowflake cloud.</p>
<p>In our recipe, and similar to the other methods, we need to establish our connection with Snowflake. This was accomplished using the <code>Session</code> class.</p>
<div class="C0-SHCodePACKT">
<pre><code>params = dict(config['SNOWFLAKE'])
session = Session.builder.configs(params).create()</code></pre>
</div>
<p>There are similarity in the API and conecpts between Snowpark and PySpark (Spark). More specifically, Snowpark DataFrame are considered <em>lazily-evaluated relational dataset</em>. The <code>to_pandas</code> method does two things: it executes the query and loads the results into a pandas DataFrame (data is being fetched outside of Snowflake). To convert the pandas DataFrame back to a Snowpark DataFrame (inside Snowflake) you can use the <code>create_dataframe</code> method as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>df = session.create_dataframe(orders)</code></pre>
</div>
<p>In order successfully execute the preceding code, you need have <em>write permission</em> since Snowflake will create a <strong>temporary</strong> table to store the pandas DataFrame (inside Snowflake) and then returns a Snowpark DataFrame that points to that temporary table. Alternatively, if you want to persist the pandas DataFrame into a table you can use the <code>write_pandas</code> method as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>df = session.write_pandas(orders, table_name='temp_table')</code></pre>
</div>
<p>In the preceding code, you passed the pandas DataFrame and table name.</p>
</section>
</section>
<section id="theres-more..." class="level3" data-number="4.4.4">
<h3 data-number="4.4.4">There's more...</h3>
<p>You may have noticed that the columns in the returned DataFrame, when using the Snowflake Python connector and Snowpark, all came back in uppercase, while they were lowercased when using Snowflake SQLAlchemy.</p>
<p>The reason for this is because Snowflake, by default, stores unquoted object names in uppercase when these objects are created. In the previous code, for example, our <code>Order Date</code> column was returned as <code>O_ORDERDATE</code>.</p>
<p>To explicitly indicate the name is case-sensitive, you will need to use quotes when creating the object in Snowflake (for example, <code>'o_orderdate'</code> or <code>'OrderDate'</code>). In contrast, using Snowflake SQLAlchemy converts the names into lowercase by default.</p>
</section>
<section id="see-also-9" class="level3" data-number="4.4.5">
<h3 data-number="4.4.5">See also</h3>
<ul>
<li>For more information on the <strong>Snowflake Connector for Python</strong>, you can visit the official documentation at <a href="https://docs.snowflake.com/en/user-guide/python-connector.html">https://docs.snowflake.com/en/user-guide/python-connector.html</a></li>
<li>For more information regarding <strong>Snowflake SQLAlchemy</strong>, you can visit the official documentation at <a href="https://docs.snowflake.com/en/user-guide/sqlalchemy.html">https://docs.snowflake.com/en/user-guide/sqlalchemy.html</a></li>
<li>For more information regarding <strong>Snowpark API</strong>, you can visit the official documentation at <a href="https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index">https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index</a></li>
</ul>
</section>
</section>
<section id="reading-data-from-a-document-database" class="level2" data-number="4.5">
<h2 data-number="4.5">Reading data from a document database</h2>
<p><strong>MongoDB</strong>, a <strong>NoSQL</strong> database, stores data in documents and uses BSON (a JSON-like structure) to store schema-less data. Unlike relational databases, where data is stored in tables that consist of rows and columns, document-oriented databases store data in collections and documents.</p>
<p>A document represents the lowest granular level of data being stored, as rows do in relational databases. A collection, like a table in relational databases, stores documents. Unlike relational databases, a collection can store documents of different schemas and structures.</p>
<section id="getting-ready-10" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1">Getting ready</h3>
<p>In this recipe, it is assumed that you have a running instance of MongoDB. To get ready for this recipe, you will need to install the <code>PyMongo</code> Python library to connect to MongoDB.</p>
<p>To install MongoDB using <code>conda</code>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge pymongo -y</code></pre>
</div>
<p>To install MongoDB using <code>pip</code>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>python -m pip install pymongo</code></pre>
</div>
<p>If you do not have access to a PostgreSQL database, then the fastest way to get up and running is via Docker (<a href="https://hub.docker.com/_/mongo">https://hub.docker.com/_/mongo</a>). The following is an example command:</p>
<div class="C0-SHConPACKT">
<pre><code>docker run -d \
    --name mongo-ch3 \
    -p 27017:27017 \
    --env MARIADB_ROOT_PASSWORD=password \
    mongo:8.0-rc</code></pre>
</div>
<p>Alternatively, you can try <strong>MongoDB Atlas</strong> for free here <a href="https://www.mongodb.com/products/platform/atlas-database">https://www.mongodb.com/products/platform/atlas-database</a>. <strong>MongoDB Atlas</strong> is a fully managed cloud database that can be deployed on your favorite cloud providers such as AWS, Azure, and GCP.</p>
<blockquote>
<p>NOTE ABOUT USING MONGODB ATLAS</p>
<blockquote>
<p>If you are connecting to MongoDB Atlas (Cloud) Free Tier or their M2/M5 shared tier cluster, then you will be using the <code>mongodb+srv</code> protocol. In this case, you can either specify this during the pip install with <code>python -m pip install "pymongo[srv]"</code></p>
</blockquote>
</blockquote>
<p>Optionally, for a GUI interface to your MongoDB you can install <strong>MongoDB Compass</strong> form here <a href="https://www.mongodb.com/products/tools/compass">https://www.mongodb.com/products/tools/compass</a></p>
<p>I am using MongoDB Compass to create the database, collection, and load the data. In Chapter 5, <em>Persisting Time Series Data to Databases</em>, you learn how to create your database, collection, and load data using Python.</p>
<p>Using <strong>Compass</strong> select the option <strong>Create Database</strong>. For the Database Name you can enter <code>stock_data</code> and <code>microsoft</code> for the <strong>Collection</strong> Name. Click the <strong>Time-Series</strong> checkbox and specify the <code>date</code> as the <strong>timeField</strong>.</p>
<figure>
<img src="../media/file26.png" alt="Figure 3.4 – MongoDB Compass Create Database screen" width="1130" height="1138"/><figcaption aria-hidden="true">Figure 3.4 – MongoDB Compass Create Database screen</figcaption>
</figure>
<p>Once the <strong>Database</strong> and <strong>Collection</strong> are created, click <strong>Import Data</strong> and select the MSFT stock dataset provided in the <code>datasets/Ch3/MSFT.csv</code> folder.</p>
<figure>
<img src="../media/file27.png" alt="Figure 3.5 – MongoDB Compass Import Data screen" width="2858" height="1174"/><figcaption aria-hidden="true">Figure 3.5 – MongoDB Compass Import Data screen</figcaption>
</figure>
<figure>
<img src="../media/file28.png" alt="Figure 3.6- MonogDB Compass review data types screen before import" width="1854" height="1182"/><figcaption aria-hidden="true">Figure 3.6- MonogDB Compass review data types screen before import</figcaption>
</figure>
<p>The final page confirms the data types. Finally, click <strong>Import</strong>.</p>
</section>
<section id="how-to-do-it-9" class="level3" data-number="4.5.2">
<h3 data-number="4.5.2">How to do it…</h3>
<p>In this recipe, you will connect to the MongoDB instance you have set up. If you are using an on-premises install (local install or Docker container), then your connection string will be something like <code>mongodb://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;DatabaseName&gt;.</code> If you are using Atlas, then your connection may look more like <code>mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;clusterName&gt;.mongodb.net/&lt;DatabaseName&gt;?retryWrites=true&amp;w=majority</code>.</p>
<p>Perform the following steps:</p>
<ol>
<li>First, let's import the necessary libraries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from pymongo import MongoClient, uri_parser</code></pre>
</div>
<p>Establish a connection to MongoDB. For a self-hosted instance, such as a local install, this would look something like this:</p>
<div class="C1-SHCodePACKT">
<pre><code># connecting to a self-hosted instance
url = "mongodb://127.0.0.1:27017"
client = MongoClient(url)
&gt;&gt;
MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)</code></pre>
</div>
<p>This is equivalent to the following:</p>
<div class="C1-SHCodePACKT">
<pre><code>client = MongoClient(host=['127.0.0.1:27017'],
                     password=None,
                     username=None,
                     document_class=dict,
                     tz_aware=False,
                     connect=True)</code></pre>
</div>
<p>If your self-hosted MongoDB instance has a username and password, you must supply those.</p>
<p>The <code>uri_parser</code> is a useful utility function that allows you to <em>validate</em> a MongoDB URL as shown:</p>
<div class="C1-SHCodePACKT">
<pre><code>uri_parser.parse_uri("mongodb://127.0.0.1:27107")
&gt;&gt;
{'nodelist': [('127.0.0.1', 27107)],
 'username': None,
 'password': None,
 'database': None,
 'collection': None,
 'options': {},
 'fqdn': None}</code></pre>
</div>
<p>If you are connecting to <strong>MongoDB</strong> <strong>Atlas</strong> then your connection string will look more like this:</p>
<div class="C1-SHCodePACKT">
<pre><code># connecting to Atlas cloud Cluster
cluster = 'cluster0'
username = 'user'
password = 'password'
database = 'stock_data'
url = \
f"mongodb+srv://{username}:{password}@{cluster}.3rncb.mongodb.net/{database}"
client =  MongoClient(url)
client
&gt;&gt;
MongoClient(host=['cluster0-shard-00-00.3rncb.mongodb.net:27017', 'cluster0-shard-00-01.3rncb.mongodb.net:27017', 'cluster0-shard-00-02.3rncb.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='somesource', replicaset='Cluster0-shard-0', ssl=True)</code></pre>
</div>
<p>In the previous recipes of this chapter, we leveraged a configuration file, for example a <code>database.cfg</code> file to store our connection information and hide our credentials. You should follow that recommendation as well.</p>
<blockquote>
<p>If your username or password contain special characters including the space chatacter (<code>: / ? # [ ] @ ! $ &amp; ' ( ) * , ; = %</code>),<code> </code>then you will need to encode them. You can perform percent-encoding (percent-escape) using the <code>quote_plus()</code> function from the <code>urlib</code> Python library.</p>
<blockquote>
<p>Here is an example:</p>
</blockquote>
<blockquote>
<p><code>username = urllib.parse.quote_plus('user!*@')</code></p>
</blockquote>
<blockquote>
<p><code>password = urllib.parse.quote_plus('pass/w@rd')</code></p>
</blockquote>
<blockquote>
<p>For more information, you can read the documentation here <a href="https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters">https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters</a></p>
</blockquote>
</blockquote>
<ol>
<li>Once connected, you can list all the databases available. In this example, I named the database <code>stock_data</code> and the collection <code>microsoft</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>client.list_database_names()
&gt;&gt;
['admin', 'config', 'local', 'stock_data']</code></pre>
</div>
<ol>
<li>You can list the collections that are available under the <code>stock_data</code> database using <code>list_collection_names</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>db = client['stock_data']
db.list_collection_names()
&gt;&gt;
['microsoft', 'system.buckets.microsoft', 'system.views']</code></pre>
</div>
<ol>
<li>Now, you can specify which collection to query. In this case, we are interested in the one called <code>microsoft</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>collection = db['microsoft']</code></pre>
</div>
<ol>
<li>Now, query the database into a pandas DataFrame using the <code>find</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>results = collection.find({})
msft_df = (pd.DataFrame(results)
             .set_index('date')
             .drop(columns='_id'))
msft_df.head()
&gt;&gt;
               close         low    volume        high        open
date                                                               
2019-09-04  131.457260  130.358829  17995900  131.514567  131.142059
2019-09-05  133.768707  132.536556  26101800  134.083908  132.870864
2019-09-06  132.861359  132.001715  20824500  133.892908  133.749641
2019-09-09  131.352219  130.339762  25773900  133.482199  133.329371
2019-09-10  129.976837  128.477244  28903400  130.750506  130.664546</code></pre>
</div>
</section>
<section id="how-it-works-9" class="level3" data-number="4.5.3">
<h3 data-number="4.5.3">How it works…</h3>
<p>The first step is to connect to the database, which we did by creating a client object with <code>MongoClient</code> for MongoDB instance. This will give you access to a set of methods, such as <code>list_databases_names(), list_databases()</code>, and additional attributes, such as <code>address</code> and<code> HOST</code>.</p>
<p><code>MongoClient()</code> accepts a connection string that should follow MongoDB's URI format, as follows:</p>
<div class="C0-SHCodePACKT">
<pre><code>client = MongoClient("mongodb://localhost:27017")</code></pre>
</div>
<p>Alternatively, the same can be accomplished by explicitly providing <em>host</em> (string) and <em>port</em> (numeric) positional arguments, as follows:</p>
<div class="C0-SHCodePACKT">
<pre><code>client = MongoClient('localhost', 27017)</code></pre>
</div>
<p>The host string can either be the hostname or the IP address, as shown here:</p>
<div class="C0-SHCodePACKT">
<pre><code>client = MongoClient('127.0.0.1', 27017)</code></pre>
</div>
<p>Note that to connect to your <strong>localhost</strong> that uses the default port (<code>27017</code>), you can establish a connection without providing any arguments, as shown in the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code># using default values for host and port
client = MongoClient()</code></pre>
</div>
<p>Further, you can explicitly supply the named parameters as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>client = MongoClient(host='127.0.0.1',
                     port=27017,
                     password=password,
                     username=username,
                     document_class=dict,
                     tz_aware=False,
                     connect=True)</code></pre>
</div>
<p>Let’s explore these parameters:</p>
<ul>
<li><code>host</code> – this can be a hostname or IP address or a MongoDB URI. It can also take a Python list of hostnames.</li>
<li><code>password</code> – your assigned password. Read the note regarding special characters in the <em>Getting Ready</em> section.</li>
<li><code>username</code> - your assigned username. Read the note regarding special characters in the <em>Getting Ready</em> section.</li>
<li><code>document_class</code> – specify the class to use for the documents returned from your query. The default value is <code>dict</code>.</li>
<li><code>tz_aware</code> – specify if datetime instances are time zone aware. The default is <code>False</code>, meaning they are naive (not time zone aware).</li>
<li><code>connect</code> – Whether to immediately connect to the MongoDB instance. The default value is <code>True</code>.</li>
</ul>
<p>For additional parameters, you can reference the official documentation page here <a href="https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html">https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html</a>.</p>
<p>Once the connection to your MongoDB instance has been established, you can specify which database to use, list its collections, and query any available collections. The overall flow in terms of navigation before you can query and retrieve the documents is to specify the <strong>database</strong>, select the <strong>collection</strong> you are interested in, and then submit the <strong>query</strong>.</p>
<p>In the preceding example, our database was called <code>stock_data</code>, which contained a collection called <code>microsoft</code>. You can have multiple collections in a database and multiple documents in a collection. To think of this in terms of relational databases, recall that a collection is like a table and that documents represent rows in that table.</p>
<p>In PyMongo, you can specify a database using different syntax, as shown in the following code. Keep in mind that all these statements will produce a <code>pymongo.database.Database</code> object:</p>
<div class="C0-SHCodePACKT">
<pre><code># Specifying the database
db = client['stock_data']
db = client.stock_data
db = client.get_database('stock_data')</code></pre>
</div>
<p>In the preceding code, <code>get_database()</code> can take in additional arguments for the <code>codec_options</code>, <code>read_preference, write_concern</code>, and <code>read_concern</code> parameters, where the latter two are focused more on operations across nodes and how to determine if the operation was successful or not.</p>
<p>Similarly, once you have the <code>PyMongo</code> database object, you can specify a collection using different syntax, as shown in the following example:</p>
<div class="C0-SHCodePACKT">
<pre><code># Specifying the collection
collection = db.microsoft
collection = db['microsoft']
collection = db.get_collection('microsoft')</code></pre>
</div>
<p>The <code>get_collection()</code> method provides additional parameters, similar to <code>get_database()</code>.</p>
<p>The three syntax variations in the preceding example return a <code>pymongo.database.Collection</code> object, which comes with additional built-in methods and attributes such as <code>find</code>, <code>find_one</code>, <code>find_one_and_delete</code>, <code>find_one_and_reaplce</code>, <code>find_one_and_update</code>, <code>update</code>, <code>update_one</code>, <code>update_many</code>, <code>delete_one</code>, and <code>delete_many</code>, to name a few.</p>
<p>Let’s explore the different <em>retrieval</em> collection methods:</p>
<ul>
<li><code>find()</code> – retrieves multiple documents from a collection based on the submitted query</li>
<li><code>find_one()</code> – retrieves a single document form a collection based on the submitted query. If multiple documents match, then the first instance is returned.</li>
<li><code>find_one_and_delete() </code>– finds the single document like <code>find_one</code>, but it deletes it from the collection, and returns it (the deleted document).</li>
<li><code>find_one_and_replace()</code> - finds a single document and replaces it with a new document, returning either the original or the replaced document.</li>
<li><code>find_one_and_update()</code> - finds a single document and updates it, returning either the original or the updated document. This is different than the <code>find_one_and_replace</code>, since it updates the existing document instead of replacing the entire document.</li>
</ul>
<p>Once you are at the collection level, you can start querying the data. In the recipe, you used <code>find()</code>, which you can think of as doing something similar to a <code>SELECT</code> statement in SQL.</p>
<p>In the <code>How to do it…</code> section, in <code>step 5</code>, you queried the entire collection to retrieve all the documents using this line of code:</p>
<div class="C0-SHCodePACKT">
<pre><code>collection.find({})</code></pre>
</div>
<p>The empty dictionary, <code>{}</code>, in <code>find()</code> represents our filtering criteria. When you pass an empty filter criterion with <code>{}</code>, you are retrieving everything. This resembles <code>SELECT *</code> in a SQL database. Alternatively, one could also use <code>collection.find()</code> to retrieve all documents.</p>
<p>To query documents in MongoDB you will need to be familiar with the MongoDB Query Language (MQL). You would normally write your query and pass it to the <code>find</code> method which acts like a filter.</p>
<p>A query or a filter takes a <strong>key-value</strong> pair to return a select number of documents where the keys match the values specified. The following is an example of a query to find stocks with closing price great than 130:<br/>
</p>
<div class="C0-SHCodePACKT">
<pre><code>query = {"close": {"$gt": 130}}
results = collection.find(query)</code></pre>
</div>
<p>The results objects is actually a <strong>cursor</strong> and does not contain the result set yet. You can loop through the cursor or convert into a DataFrame. Generally, when <code>collection.find()</code> is executed, it returns a <strong>cursor</strong> (more specifically, a <code>pymongo.cursor.Cursor</code> object). This cursor object is just a pointer to the result set of the query, which allows you to iterate over the results. You can then use a <code>for</code> loop or <code>next()</code> method (think of a Python iterator). However, in this recipe, instead of looping through our cursor object, we conveniently converted the entire result set into a pandas DataFrame.</p>
<p>Here is an example of retrieving the result set into a pandas DataFrame</p>
<div class="C0-SHCodePACKT">
<pre><code>df = pd.DataFrame(results)
df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1256 entries, 0 to 1255
Data columns (total 7 columns):
 #   Column  Non-Null Count  Dtype        
---  ------  --------------  -----        
 0   date    1256 non-null   datetime64[ns]
 1   close   1256 non-null   float64      
 2   _id     1256 non-null   object       
 3   low     1256 non-null   float64      
 4   volume  1256 non-null   int64        
 5   high    1256 non-null   float64      
 6   open    1256 non-null   float64      
dtypes: datetime64[ns](1), float64(4), int64(1), object(1)
memory usage: 68.8+ KB</code></pre>
</div>
<p>Notice from the output the <code>_id</code> column is added which was not part of the original <code>MSFT.csv</code> file. This was automatically added by MongoDB as a <strong>unique identifier</strong> for each document in a collection.</p>
<p>In the preceding code, the query, which acts as a filter to only retrieve data where <code>close</code> values are greater than <code>130.</code>PyMongo allows you to pass a dictionary (key-value pair) that specifies which fields to be retrieved. Here is an example:</p>
<div class="C1-SHCodePACKT">
<pre><code>query = {"close": {"$gt": 130}}
projection = {
   "_id": 0,
   "date":1,
   "close": 1,
   "volume": 1
}
results = collection.find(query, projection)
df = pd.DataFrame(results).set_index(keys='date')
print(df.head())
&gt;&gt;
               close    volume
date                           
2019-09-04  131.457260  17995900
2019-09-05  133.768707  26101800
2019-09-06  132.861359  20824500
2019-09-09  131.352219  25773900
2019-09-11  130.014984  24726100</code></pre>
</div>
<p>In the preceding code we specified that <code>_id</code> should not be returned, and only <code>date</code>, <code>close</code>, and <code>volume</code> fields to be returned.</p>
<p>Lastly, in our previous example, notice the <code>$gt </code>used in the query. This represents greater than, and more specifically it translates to <em>“greater than 130”</em>. In MQL, operators start with the dollar sign <code>$</code>. Here is a sample list of commonly used operators in MQL:</p>
<ul>
<li><code>$eq</code> - matches values that are equal to a specified value
<ul>
<li>Example:<code> </code>The query<code> {"close": {"$eq": 130}} </code>finds documents where the <code>close</code> field is exactly 130.</li>
</ul></li>
<li><code>$gt </code>- matches values that are greater than a specified value
<ul>
<li>Example: The query <code>{"close": {"$gt": 130}} </code>finds documents where the close field is greater than 130.</li>
</ul></li>
<li><code>$gte </code>- matches values that are greater than or equal to a specified value
<ul>
<li>Example: The query <code>{"close": {"$gte": 130}} </code>finds documents where the close field is greater than or equal to 130.</li>
</ul></li>
<li><code>$lt </code>- matches values that are less than a specified value
<ul>
<li>Example: The query <code>{"close": {"$lt": 130}} </code>finds documents where the close field is less than 130.</li>
</ul></li>
<li><code>$lte </code>- matches values that are less than or equal to a specified value
<ul>
<li>Example: The query <code>{"close": {"$lt3": 130}} </code>finds documents where the close field is less than or equal to 130.</li>
</ul></li>
<li><code>$and </code>- joins query clauses with a logical <strong>AND</strong> operator. All conditions must be true.
<ul>
<li>Example: The query <code>{"$and": [{"close": {"$gt": 130}}, {"volume": {"$lt": 20000000}}]} </code>finds documents where the close field is greater than 130 <strong>AND</strong> the volume is less than 20,000,000.</li>
</ul></li>
<li><code>$or</code>- joins query clauses with a logical <strong>OR</strong> operator. At least one condition must be true.
<ul>
<li><code>Example: The query {"$or": [{"close": {"$gt": 135}}, {"volume": {"$gt": 30000000}}]}</code> finds documents where the close field is greater than 135 <strong>OR</strong> the volume is greater than 30,000,000.</li>
</ul></li>
<li><code>$in</code>- matches values specified in an array (list)
<ul>
<li>Example:<code> </code>The query <code>{"date": {"$in": [datetime.datetime(2019, 9, 4), datetime.datetime(2019, 9, 5), datetime.datetime(2019, 9, 6)]}}</code> finds documents where the date field matches any of the specified dates: September 4, 2019; September 5, 2019; September 6, 2019.</li>
</ul></li>
</ul>
<p>For a more comprehensive list of operators in MQL you can visit the official documentation here <a href="https://www.mongodb.com/docs/manual/reference/operator/query/">https://www.mongodb.com/docs/manual/reference/operator/query/</a></p>
</section>
<section id="theres-more-9" class="level3" data-number="4.5.4">
<h3 data-number="4.5.4">There's more…</h3>
<p>There are different ways to retrieve data from MongoDB using <code>PyMongo</code>. In the previous section, we used <code>db.collection.find()</code>, which always returns a cursor. As we discussed earlier, <code>find()</code> returns all the matching documents that are available in the specified collection. If you want to return the first occurrence of matching documents, then <code>db.collection.find_one()</code> would be the best choice and would return a <strong>dictionary</strong> object, not a cursor. Keep in mind that this only returns one document, as shown in the following example:</p>
<div class="C0-SHCodePACKT">
<pre><code>db.microsoft.find_one()
&gt;&gt;&gt;
{'date': datetime.datetime(2019, 9, 4, 0, 0),
 'close': 131.45726013183594,
 '_id': ObjectId('66e30c09a07d56b6db2f446e'),
 'low': 130.35882921332006,
 'volume': 17995900,
 'high': 131.5145667829114,
 'open': 131.14205897649285}</code></pre>
</div>
<p>When it comes to working with cursors, there are several ways you can traverse through the data:</p>
<ul>
<li>Converting into a pandas DataFrame using <code>pd.DataFrame(cursor)</code>, as shown in the following code:</li>
</ul>
<div class="C1-SHCodePACKT">
<pre><code>cursor = db.microsoft.find()
df = pd.DataFrame(cursor)</code></pre>
</div>
<ul>
<li>Converting into a Python <strong>list</strong> or <strong>tuple</strong>:</li>
</ul>
<div class="C1-SHCodePACKT">
<pre><code>data = list(db.microsoft.find())</code></pre>
</div>
<p>You can also convert the <strong>Cursor</strong> object into a Python list and then convert that into a pandas DataFrame, like this:</p>
<div class="C0-SHCodePACKT">
<pre><code>data = list(db.microsoft.find())
df = pd.DataFrame(data)</code></pre>
</div>
<ul>
<li>Using <code>next()</code> to get move the pointer to the next item in the result set:</li>
</ul>
<div class="C1-SHCodePACKT">
<pre><code>cursor = db.microsoft.find()
cursor.next()</code></pre>
</div>
<ul>
<li><strong>Looping</strong> through the object, for example, with a <code>for</code> loop:</li>
</ul>
<div class="C1-SHCodePACKT">
<pre><code>cursor = db.microsoft.find()
for doc in cursor:
    print(doc)</code></pre>
</div>
<p>The previous code will loop through the entire result set. If for example you want to loop thorugh the first 5 records you can use the following:</p>
<div class="C1-SHCodePACKT">
<pre><code>cursor = db.microsoft.find()
for doc in cursor[0:5]:
    print(doc)</code></pre>
</div>
<ul>
<li>Specifying an <strong>index</strong>. Here, we are printing the first value:</li>
</ul>
<div class="C1-SHCodePACKT">
<pre><code>cursor = db.microsoft.find()
cursor[0]</code></pre>
</div>
<p>Note that if you provided a slice, such as <code>cursor[0:1]</code>, which is a range, then it will return a cursor object (not a document).</p>
</section>
<section id="see-also-10" class="level3" data-number="4.5.5">
<h3 data-number="4.5.5">See also</h3>
<p>For more information on the <strong>PyMongo</strong> API, please refer to the official documentation, which you can find here: <a href="ch004.xhtml">https://pymongo.readthedocs.io/en/stable/index.html</a>.</p>
</section>
</section>
<section id="reading-data-from-a-time-series-databases" class="level2" data-number="4.6">
<h2 data-number="4.6">Reading data from a time series databases</h2>
<p>A time series database, a type of <strong>NoSQL</strong> database, is optimized for time-stamped or time series data and provides improved performance, especially when working with large datasets containing IoT data or sensor data. In the past, common use cases for time series databases were mostly associated with financial stock data, but their use cases have expanded into other disciplines and domains. In this recipe you will explore three popular time series databases: <strong>InfluxDB</strong>, <strong>TimescaleDB,</strong> and <strong>TDEngine</strong>.</p>
<p><strong>InfluxDB</strong> is a popular open source time series database with a large community base. In this recipe, we will be using InfluxDB's latest release as of this writing; that is, version 2.7.10. The most recent InfluxDB releases introduced the Flux data scripting language, which you will use with the Python API to query our time series data.</p>
<p><strong>TimescaleDB</strong>, on the other hand, is an extension of PostgreSQL specifically optimized for time series data. It leverages the power and flexibility of PostgreSQL while providing additional features tailored for handling time-stamped information efficiently. One advantage with TimescaleDB is that you can leverage SQL for querying the data. TimescaleDB is an open-source time series database and in this recipe, we will be using TimescaleDB's latest release as of this writing, version 2.16.1</p>
<p><strong>TDEngine</strong> is an open-source time series database designed for Internet of Things (IoT), big data, and real-time analytics. Like TimescaleDB, TDEngine utilizes SQL for querying the data. In this recipe, we will be working with TDEngine version 3.3.2.0, the latest as of this writing.</p>
<section id="getting-ready-11" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1">Getting ready</h3>
<p>This recipe assumes that you have access to a running instance of InfluxDB, TimeseriesDB, or TDEngine. You will install the appropriate libraries to connect and interact with these databases using Python. For <strong>InfluxDB</strong> V2, you will need to install <code>influxdb-client</code>; for <strong>TimescaleDB,</strong> you will need to install the PostgreSQL Python library <code>psycopg2 </code>(recall in the <em>Reading from a relational database</em> recipe of this chapter, we used <code>psycopg3</code>); and finally, for <strong>TDEngine,</strong> you will need to install <code>taospy</code>.</p>
<p>You can install these libraries using <code>pip</code>, as follows:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install 'influxdb-client[ciso]'
pip install 'taospy[ws]'
pip install psycopg2</code></pre>
</div>
<p>To install using <strong>conda</strong> use the following:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge influxdb-client
conda install -c conda-forge taospy taospyws
conda install -c conda-forge psycopg2</code></pre>
</div>
<p>If you do not have access to these databases, then the fastest way to get up and running is via Docker. The following are example commands for InlfuxDB, TimescaleDB, and TDEngine:</p>
<section id="influxdb-docker-container" class="level4" data-number="4.6.1.1">
<h4 data-number="4.6.1.1">InfluxDB Docker Container</h4>
<p>To create a InfluxDB container you will need to run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>docker run -d\
    --name influxdb-ch3 \
    -p 8086:8086 \
    influxdb:2.7.9-alpine</code></pre>
</div>
<p>For more information you can access the official docker hub page <a href="https://hub.docker.com/_/influxdb">https://hub.docker.com/_/influxdb</a></p>
<p>Once the <strong>influxdb-ch3</strong> container is up and running you can navigate to <code>http://localhost:8086</code> using your favorite browser and continue the setup, such as username, password, initial organization name, and initial bucket name.</p>
<p>For this recipe, we will use the <strong>National Oceanic and Atmospheric Administration</strong> (<strong>NOAA</strong>) water level sample data from August 17, 2019, to September 17, 2019, for Santa Monica and Coyote Creek.</p>
<p>In the Data Explorer UI, you can run the following <strong>Flux</strong> query to load the sample dataset:</p>
<div class="C0-SHCodePACKT">
<pre><code>import "influxdata/influxdb/sample"
sample.data(set: "noaaWater")
    |&gt; to(bucket: "tscookbook")</code></pre>
</div>
<p>In the previous snippet, the NOAA dataset was loaded into the <code>tscookbook</code> bucket created during the initial setup.</p>
<p>For instructions on how to load the sample data, or other provided <em>sample datasets</em>, please refer to the InfluxDB official documentation at <a href="https://docs.influxdata.com/influxdb/v2/reference/sample-data/">https://docs.influxdata.com/influxdb/v2/reference/sample-data/</a></p>
</section>
<section id="timescaledb-docker-container" class="level4" data-number="4.6.1.2">
<h4 data-number="4.6.1.2">TimescaleDB Docker Container</h4>
<p>To create a TimescaleDB container you will need to run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>docker run -d \
    --name timescaledb-ch3 \
    -p 5432:5432 \
    -e POSTGRES_PASSWORD=password \
    timescale/timescaledb:latest-pg16</code></pre>
</div>
<p>For more information you can access the official docker hub page <a href="https://hub.docker.com/r/timescale/timescaledb">https://hub.docker.com/r/timescale/timescaledb</a></p>
<p>Once the <strong>timescaledb-ch3</strong> container is up and running, you can load the <code>MSFT.csv</code> file using the same instructions as in the <code>Getting Ready</code> section of the <em>Reading data from a relational database</em> recipe.</p>
<p>Note, the default username is <code>postgres</code> and the password is whatever password you setup in the docker command.</p>
<figure>
<img src="../media/file29.png" alt="Figure 3. – DBeaver TimescaleDB/Postgres connection settings (should look similar to Figure 3.1)" width="1049" height="678"/><figcaption aria-hidden="true">Figure 3. – DBeaver TimescaleDB/Postgres connection settings (should look similar to Figure 3.1)</figcaption>
</figure>
<blockquote>
<p>Since TimescaleDB is based on PostgreSQL, it also defaults to port 5432. So, if you are already running a local instance of PostgreSQL database which defaults to port 5432 you may run into an issue with TimescaleDB. In such case, you may opt to change the docker run configuration and change the port.</p>
</blockquote>
</section>
<section id="tdengine-docker-container-to-be-deleted-section" class="level4" data-number="4.6.1.3">
<h4 data-number="4.6.1.3">TDEngine Docker Container [TO BE DELETED SECTION]</h4>
<p>To create a TDEngine container you will need to run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>docker run -d \
   --name tdengine-ch3 \
   -p 6030-6060:6030-6060 \
   -p 6030-6060:6030-6060/udp \
   tdengine/tdengine:3.3.2.0</code></pre>
</div>
<p>For more information you can access the official docker hub page <a href="https://hub.docker.com/r/tdengine/tdengine">https://hub.docker.com/r/tdengine/tdengine</a></p>
<p>Once the <strong>tdengine-ch3</strong> container is up and running, you can create a demo dataset by running the <code>taosBenchmark</code> command from inside the container shell. Here are the steps to access the shell from inside the running container to run the needed command to install and setup the demo data set:</p>
<div class="C0-SHConPACKT">
<pre><code>docker exec -it tdengine-ch3 /bin/bash
&gt;&gt;
root@9999897cbeb4:~# taosBenchmark</code></pre>
</div>
<p>Once the demo set is created you can exit out of the terminal. You can now leverage DBeaver to verify the data is created. You can use the same instructions as in the <code>Getting Ready</code> section of the <em>Reading data from a relational database</em> recipe.</p>
<p>Note, the default username is <code>root</code> and the default password is <code>taosdata</code></p>
<figure>
<img src="../media/file30.png" alt="Figure 3. – DBeaver TDEngine connection settings" width="841" height="737"/><figcaption aria-hidden="true">Figure 3. – DBeaver TDEngine connection settings</figcaption>
</figure>
<p>You should now see a <code>test</code> <strong>database</strong> created, and a <code>meters</code> <strong>supertable</strong> with 10,000 <strong>subtables</strong> named <code>d0 </code>to <code>d9999 </code>and each table contains around 10,000 rows and four columns (<code>ts</code>, <code>current</code>, <code>voltage</code>, and <code>phase</code>). You may not be able to see the <code>meters</code> supertable in the DBeaver navigator pane, but if you run the following SQL query “<code>SELECT COUNT(*) FROM test.meters;”</code> which should output 100,000,000 rows in the meters supertable (10,000 subtables multiplied by 10,000 rows in each).</p>
</section>
</section>
<section id="how-to-do-it-10" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2">How to do it…</h3>
<p>This recipe will demonstrate how you can connect and interact with three popular time series database systems.</p>
<section id="influxdb" class="level4" data-number="4.6.2.1">
<h4 data-number="4.6.2.1">InfluxDB</h4>
<p>We will be leveraging the <code>Influxdb_client</code> Python SDK for InfluxDB 2.x, which provides support for pandas DataFrames in terms of both read and write functionality. Let's get started:</p>
<ol>
<li>First, let's import the necessary libraries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from influxdb_client import InfluxDBClient
import pandas as pd</code></pre>
</div>
<ol>
<li>To establish your connection using <code>InfluxDBClient(url="http://localhost:8086", token=token)</code>, you will need to define the <code>token</code>, <code>org</code>, and <code>bucket</code> variables:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>token = "c5c0JUoz-\
joisPCttI6hy8aLccEyaflyfNj1S_Kff34N_4moiCQacH8BLbLzFu4qWTP8ibSk3JNYtv9zlUwxeA=="
org = "ts"
bucket = "tscookbook"</code></pre>
</div>
<p>Think of a bucket as database in relational databases.</p>
<ol>
<li>Now, you are ready to establish your connection by passing the <code>url</code>, <code>token</code>, and <code>org</code> parameters to <code>InlfuxDBClient()</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>client = InfluxDBClient(url="http://localhost:8086",
                        token=token,
                        org=org)</code></pre>
</div>
<ol>
<li>Next, you will instantiate <code>query_api</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query_api = client.query_api()</code></pre>
</div>
<ol>
<li>Pass your Flux query and request the results to be in pandas DataFrame format using the <code>query_data_frame</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = f'''
from(bucket: "tscookbook")
  |&gt; range(start: 2019-09-01T00:00:00Z)
  |&gt; filter(fn: (r) =&gt;
        r._measurement == "h2o_temperature" and
        r.location == "coyote_creek" and
        r._field == "degrees"
  )
  |&gt; movingAverage(n: 120)
  |&gt; pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
'''
result = client.query_api().query_data_frame(org=org, query=query)</code></pre>
</div>
<ol>
<li>In the preceding Flux script, selected the measurement <code>h2o_temparature</code> and where the location is <code>coyote_creek</code>. Let's inspect the DataFrame. Pay attention to the data types in the following output:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>result.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 3885 entries, 0 to 3884
Data columns (total 8 columns):
 #   Column        Non-Null Count  Dtype             
---  ------        --------------  -----             
 0   result        3885 non-null   object            
 1   table         3885 non-null   int64             
 2   _start        3885 non-null   datetime64[ns, UTC]
 3   _stop         3885 non-null   datetime64[ns, UTC]
 4   _time         3885 non-null   datetime64[ns, UTC]
 5   _measurement  3885 non-null   object            
 6   location      3885 non-null   object            
 7   degrees       3885 non-null   float64           
dtypes: datetime64[ns, UTC](3), float64(1), int64(1), object(3)
memory usage: 242.9+ KB</code></pre>
</div>
<ol>
<li>If you want to retrieve only the time and degrees columns, you can update the Flux query as shown:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = f'''
from(bucket: "tscookbook")
  |&gt; range(start: 2019-09-01T00:00:00Z)
  |&gt; filter(fn: (r) =&gt;
        r._measurement == "h2o_temperature" and
        r.location == "coyote_creek" and
        r._field == "degrees"
  )
  |&gt; movingAverage(n: 120)
  |&gt; pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
  |&gt; keep(columns: ["_time", "degrees"])
'''
result = client.query_api().query_data_frame( query=query)
result.head()
&gt;&gt;
result  table                     _time    degrees
0  _result      0 2019-09-01 11:54:00+00:00  64.891667
1  _result      0 2019-09-01 12:00:00+00:00  64.891667
2  _result      0 2019-09-01 12:06:00+00:00  64.816667
3  _result      0 2019-09-01 12:12:00+00:00  64.841667
4  _result      0 2019-09-01 12:18:00+00:00  64.850000</code></pre>
</div>
<p>The original dataset contains 15,258 observations collected every six (6) minutes between the two stations (locations). The moving average is calculated over 120 data points. It is important to understand the graduality of your dataset. The final DataFrame contains 3885 records.</p>
</section>
<section id="timescaledb" class="level4" data-number="4.6.2.2">
<h4 data-number="4.6.2.2">TimescaleDB</h4>
<p>Since TimescaleDB is based on PostgreSQL and we have already installed <strong>psycopg2</strong> , retrieving and querying your data should be similar to the approach used in the recipe <em>Reading data from a relational database</em>.</p>
<p>Here is a brief on how this can be done using pandas from_sql:</p>
<ol>
<li>Import SQLAlchemy and pandas</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from sqlalchemy import create_engine</code></pre>
</div>
<ol>
<li>Create the engine object with the proper connection string to the PostgreSQL backend</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>engine =\
    create_engine("postgresql+psycopg2://postgres:password@localhost:5432/postgres")</code></pre>
</div>
<ol>
<li>Finally, use the <code>read_sql</code> method to retrieve the result set of your query into a pandas DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = "SELECT * FROM msft"
df = pd.read_sql(query,
                 engine,
                 index_col='date',
                 parse_dates={'date': '%Y-%m-%d'})
print(df.head())</code></pre>
</div>
<p>TimescaleDB offers many advantages over PostgreSQL and you will explore some of these in <em>Chapter 5</em>, <em>Persisting Time Series Data to Databases</em>. Still, querying TimescaleDB brings a similar experience to those familiar with SQL and PostgreSQL.</p>
</section>
<section id="tdengine" class="level4" data-number="4.6.2.3">
<h4 data-number="4.6.2.3">TDEngine</h4>
<p>For this recipe let’s update our configuration file <code>database.cfg</code> from the <em>Technical Requirements</em> to include a [TDENGINE] section as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>[TDENGINE]
user=root
password=taosdata
url=http://localhost:6041</code></pre>
</div>
<p>You will start by establishing a connection to the TDEngine server, and then run a query against the demo dataset from the <strong>taosBenchmark</strong> described in the <em>Getting Read</em> section.</p>
<ol>
<li>Start by importing the required libraries.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import taosrest
import pandas as pd</code></pre>
</div>
<ol>
<li>You will create a Python dictionary to store all the parameter values required to establish a connection to the database, such as <code>url</code>, <code>user</code> and <code>password</code></li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import configparser
config = configparser.ConfigParser()
config.read('database.cfg')
params = dict(config['TDENGINE'])</code></pre>
</div>
<ol>
<li>Establish a connection to the server</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>conn = taosrest.connect(**params)</code></pre>
</div>
<ol>
<li>Run the following query and execute the query using the <code>query</code> method from the connection object conn</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = """
SELECT *
FROM test.meters
WHERE location = 'California.LosAngles'
LIMIT 100000;
"""
results = conn.query(query)</code></pre>
</div>
<ol>
<li>You can verify the number of rows and column names in the result set</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>results.rows
&gt;&gt;
100000
results.fields
&gt;&gt;
[{'name': 'ts', 'type': 'TIMESTAMP', 'bytes': 8},
 {'name': 'current', 'type': 'FLOAT', 'bytes': 4},
 {'name': 'voltage', 'type': 'INT', 'bytes': 4},
 {'name': 'phase', 'type': 'FLOAT', 'bytes': 4},
 {'name': 'groupid', 'type': 'INT', 'bytes': 4},
 {'name': 'location', 'type': 'VARCHAR', 'bytes': 24}]</code></pre>
</div>
<ol>
<li>The <code>results.data</code> contains the values from the result set but without column headers. Before we write our result set into a pandas DataFrame we need to capture the column names in a list from <code>results.fields:</code></li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>cols = [col['name'] for col in results.fields ]
df = pd.DataFrame(results.data, columns=cols)
df = df.set_index('ts')
df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 100000 entries, 2017-07-14 05:40:00 to 2017-07-14 05:40:05.903000
Data columns (total 5 columns):
 #   Column    Non-Null Count   Dtype 
---  ------    --------------   ----- 
 0   current   100000 non-null  float64
 1   voltage   100000 non-null  int64 
 2   phase     100000 non-null  float64
 3   groupid   100000 non-null  int64 
 4   location  100000 non-null  object
dtypes: float64(2), int64(2), object(1)
memory usage: 4.6+ MB</code></pre>
</div>
</section>
</section>
<section id="how-it-works-10" class="level3" data-number="4.6.3">
<h3 data-number="4.6.3">How it works…</h3>
<p>Both TimescaleDB and TDEngine use SQL to query the data, while InfluxDB utilizes their proprietary query language, Flux.</p>
<p>InfluxDB 1.8x introduced the <strong>Flux</strong> query language as an alternative query language to <strong>InfluxQL</strong>, with the latter having a closer resemblance to SQL. InfluxDB 2.0 introduced the concept of <strong>buckets</strong>, which is where data is stored, whereas InfluxDB 1.x stored data in databases.</p>
<p>In this recipe, we started by creating an instance of <code>InfluxDbClient</code>, which later gave us access to the <code>query_api </code>method, which gives additional methods, including:</p>
<ul>
<li><code>query()</code> returns the result as a <strong>FluxTable</strong>.</li>
<li><code>query_csv()</code> returns the result as a CSV iterator (CSV reader).</li>
<li><code>query_data_frame()</code> returns the result as a pandas DataFrame.</li>
<li><code>query_data_frame_stream()</code> returns a stream of pandas DataFrames as a generator.</li>
<li><code>query_raw()</code> returns the result as raw unprocessed data in <code>s</code> string format.</li>
<li><code>query_stream()</code> is similar to <code>query_data_frame_stream</code> but returns a stream of <code>FluxRecord</code> as a generator.</li>
</ul>
<p>In the recipe, you used <code>client.query_api()</code> to fetch the data, as shown here:</p>
<div class="C0-SHCodePACKT">
<pre><code>result = client.query_api().query_data_frame(org=org, query=query)</code></pre>
</div>
<p>You used <code>query_data_frame</code>, which executes a synchronous Flux query and returns a pandas DataFrame with which you are familiar.</p>
<p>Notice we had to use the <code>pivot</code> function in the Flux query to transform the results into a tabular format suitable for pandas DataFrames.</p>
<div class="C0-SHCodePACKT">
<pre><code>pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")</code></pre>
</div>
<p>Let’s break the preceding line of code:</p>
<p>The <code>pivot()</code> is used to reshape the data and transform it from a long to a wide format.</p>
<p>The <code>rowKey </code>parameter specifies which column to use as the unique identifier for each row. In our example, we specified <code>["_time"]</code> so each row will have a unique timestamp</p>
<p>The <code>columnKey</code> parameter specifies which column’s values will be used to create new columns in the output. In our example, we specified <code>["_field"] </code>to create columns from field names</p>
<p>The <code>valueColumn</code> parameter specifies which column contains the values, we specified <code>"_value"</code> to fill the new columns with corresponding values.</p>
</section>
<section id="theres-more-10" class="level3" data-number="4.6.4">
<h3 data-number="4.6.4">There's more…</h3>
<p>When working with <strong>InfluxDB</strong> and the <code>influxdb-client,</code> there is an additional argument that you can use to create the DataFrame index. In <code>query_data_frame(),</code> you can pass a list as an argument to the <code>data_frame_index</code> parameter, as shown in the following example:</p>
<div class="C0-SHCodePACKT">
<pre><code>result =\
query_api.query_data_frame(query=query,                  
                                    data_frame_index=['_time'])
result['_value'].head()
&gt;&gt;
_time
2021-04-01 01:45:02.350669+00:00    64.983333
2021-04-01 01:51:02.350669+00:00    64.975000
2021-04-01 01:57:02.350669+00:00    64.916667
2021-04-01 02:03:02.350669+00:00    64.933333
2021-04-01 02:09:02.350669+00:00    64.958333
Name: _value, dtype: float64</code></pre>
</div>
<p>This returns a time series DataFrame with a <code>DatetimeIndex</code> (<code>_time</code>).</p>
</section>
<section id="see-also-11" class="level3" data-number="4.6.5">
<h3 data-number="4.6.5">See also</h3>
<ul>
<li>If you are new to InfluxDB Flux query language, check out the <em>Get Started with Flux</em> official documentation at <a href="https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/">https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/</a>.</li>
<li>Please refer to the official <strong>InfluxDB-Client</strong> Python library documentation on GitHub at <a href="https://github.com/influxdata/influxdb-client-python">https://github.com/influxdata/influxdb-client-python</a>.</li>
<li>To learn more about the <strong>TDEngine</strong> python library, you refer to the official documentation at <a href="https://docs.tdengine.com/cloud/programming/client-libraries/python/">https://docs.tdengine.com/cloud/programming/client-libraries/python/</a></li>
</ul>
</section>
</section>
</section>
</div>
</div>
</body>
</html>