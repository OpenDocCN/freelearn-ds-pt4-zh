["```py\n# Example of configuration file \"database.cfg file\"\n[SNOWFLAKE]\nuser=username\npassword=password\naccount=snowflakeaccount\nwarehouse=COMPUTE_WH\ndatabase=SNOWFLAKE_SAMPLE_DATA\nschema=TPCH_SF1\nrole=somerole\n[POSTGRESQL]\nhost: 127.0.0.1\ndbname: postgres\nuser: postgres\npassword: password\n[AWS]\nhost=<your_end_point.your_region.redshift.amazonaws.com>\nport=5439\ndatabase=dev\nusername=<your_username>\npassword=<your_password>\n```", "```py\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(database.cfg')\n```", "```py\n>>> conda install -c conda-forge psycopg sqlalchemy -y\n```", "```py\n>>> pip install sqlalchemy psycopg\n```", "```py\ndocker run -d \\\n    --name postgres-ch3 \\\n        -p 5432:5432 \\\n    -e POSTGRES_PASSWORD=password \\\n    -e PGDATA=/var/lib/postgresql/data/pgdata \\\n    postgres:16.4-alpine\n```", "```py\nimport psycopg\nimport pandas as pd\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read('database.cfg')\nparams = dict(config['POSTGRESQL'])\n```", "```py\nconn = psycopg.connect(**params)\ncursor = conn.cursor()\n```", "```py\ncursor.execute(\"\"\"\nSELECT date, close, volume\nFROM msft\nORDER BY date;\n\"\"\")\ncursor.rowcount\n>> 1259\n```", "```py\ncursor.description\n>>\n[<Column 'date', type: varchar(50) (oid: 1043)>,\n <Column 'close', type: float4 (oid: 700)>,\n <Column 'volume', type: int4 (oid: 23)>]\n```", "```py\ncolumns = [col[0] for col in cursor.description]\ncolumns\n>>\n['date', 'close', 'volume']\n```", "```py\ndata = cursor.fetchall()\ndf = pd.DataFrame(data, columns=columns)\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1259 entries, 0 to 1258\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   date    1259 non-null   object\n 1   close   1259 non-null   float64\n 2   volume  1259 non-null   int64 \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 29.6+ KB\n```", "```py\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date')\nprint(df.tail(3))\n>>\n             close    volume\ndate                       \n2024-08-30  417.14  24308300\n2024-09-03  409.44  20285900\n2024-09-04  408.84   9167942\n```", "```py\ndata = cursor.fetchall()\ndata[0:5]\n>>\n[('2019-09-04', 131.45726, 17995900),\n ('2019-09-05', 133.7687, 26101800),\n ('2019-09-06', 132.86136, 20824500),\n ('2019-09-09', 131.35222, 25773900),\n ('2019-09-10', 129.97684, 28903400)]\n```", "```py\nfrom psycopg.rows import dict_row\nconn = psycopg.connect(**params, row_factory=dict_row)\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM msft;\")\ndata = cursor.fetchall()\ndata[0:2]\n>>\n[{'date': '2019-09-04',\n  'open': 131.14206,\n  'high': 131.51457,\n  'low': 130.35883,\n  'close': 131.45726,\n  'volume': 17995900},\n {'date': '2019-09-05',\n  'open': 132.87086,\n  'high': 134.08391,\n  'low': 132.53656,\n  'close': 133.7687,\n  'volume': 26101800}]\n```", "```py\ndf = pd.DataFrame(data)\nprint(df.head())\n>>\n        date       open       high        low      close    volume\n0  2019-09-04  131.14206  131.51457  130.35883  131.45726  17995900\n1  2019-09-05  132.87086  134.08391  132.53656  133.76870  26101800\n2  2019-09-06  133.74963  133.89291  132.00171  132.86136  20824500\n3  2019-09-09  133.32938  133.48220  130.33977  131.35222  25773900\n4  2019-09-10  130.66455  130.75050  128.47725  129.97684  28903400\n```", "```py\ncursor.close()\nconn.close()\n```", "```py\nwith psycopg.connect(**params) as conn:\n     with conn.cursor() as cursor:\n            cursor.execute('SELECT * FROM msft')\n            data = cursor.fetchone()\nprint(data)\n>>\n('2019-09-04', 131.14206, 131.51457, 130.35883, 131.45726, 17995900)\n```", "```py\nimport pandas as pd\nfrom sqlalchemy import create_engine\nengine =\\\n    create_engine(\"postgresql+psycopg://postgres:password@localhost:5432/postgres\")\nquery = \"SELECT * FROM msft\"\ndf = pd.read_sql(query,\n                 engine,\n                 index_col='date',\n                 parse_dates={'date': '%Y-%m-%d'})\nprint(df.tail(3))\n>>\n              open    high     low   close    volume\ndate                                               \n2024-08-30  415.60  417.49  412.13  417.14  24308300\n2024-09-03  417.91  419.88  407.03  409.44  20285900\n2024-09-04  405.63  411.24  404.37  408.84   9167942\n```", "```py\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1259 entries, 2019-09-04 to 2024-09-04\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   open    1259 non-null   float64\n 1   high    1259 non-null   float64\n 2   low     1259 non-null   float64\n 3   close   1259 non-null   float64\n 4   volume  1259 non-null   int64 \ndtypes: float64(4), int64(1)\nmemory usage: 59.0 KB\n```", "```py\ndf = pd.read_sql_query(query,\n                       engine,\n                       index_col='date',\n                       parse_dates={'date':'%Y-%m-%d'})\n```", "```py\ndf = pd.read_sql_table('msft',\n                        engine,\n                        index_col='date',\n                        parse_dates={'date':'%Y-%m-%d'})\n```", "```py\ncreate_engine(\"dialect+driver://username:password@host:port/database\")\n```", "```py\ncreate_engine(\"postgresql+psycopg2://username:password@localhost:5432/dbname\")\n```", "```py\ncreate_engine(\"mysql+pymysql://username:password@localhost:3306/dbname\")\n```", "```py\ncreate_engine(\"postgresql://username:password@localhost:5432/dbname\")\n```", "```py\nfrom sqlalchemy import URL, create_engine\nurl = URL.create(\n    drivername='postgresql+psycopg',\n    host= '127.0.0.1',\n    username='postgres',\n    password='password',\n    database='postgres',\n    port= 5432\n)\n>>\npostgresql+psycopg://postgres:***@127.0.0.1:5432/postgres\n```", "```py\nengine = create_engine(url)\ndf = pd.read_sql('select * from msft;', engine)\n```", "```py\nconda install conda-forge::sqlalchemy-redshift\n```", "```py\npip install sqlalchemy-redshift\n```", "```py\n[AWS]\nhost=<your_end_point.your_region.redshift.amazonaws.com>\nport=5439\ndatabase=dev\nusername=<your_username>\npassword=<your_password>\n```", "```py\nfrom configparser import ConfigParser\nconfig = ConfigParser()\nconfig.read('database.cfg')\nconfig.sections()\nparams = dict(config['AWS'])\n```", "```py\nurl = URL.create('redshift+psycopg2', **params)\naws_engine = create_engine(url)\n```", "```py\ndf = pd.read_sql(query,\n                 aws_engine,\n                 index_col='date',\n                 parse_dates=True)\n```", "```py\ndf_gen = pd.read_sql(query,\n                 engine,\n                 index_col='date',\n                 parse_dates=True,\n                 chunksize=500)\n```", "```py\nfor idx, data in enumerate(df_gen):\n     print(idx, data.shape)\n>>\n0 (500, 5)\n1 (500, 5)\n2 (259, 5)\n```", "```py\npip install snowflake-sqlalchemy snowflake-snowpark-python\npip install \"snowflake-connector-python[pandas]\"\n```", "```py\nconda install -c conda-forge snowflake-sqlalchemy snowflake-snowpark-python\nconda install -c conda-froge snowflake-connector-python\n```", "```py\n[SNOWFLAKE]\nuser=username\npassword=password\naccount=snowflakeaccount\nwarehouse=COMPUTE_WH\ndatabase=SNOWFLAKE_SAMPLE_DATA\nschema=TPCH_SF1\nrole=somerole\n```", "```py\nimport pandas as pd\nfrom snowflake import connector\nfrom configparser import ConfigParser\n```", "```py\nconfig = ConfigParser()\nconfig.read(database.cfg')\nparams = dict(config['SNOWFLAKE'])\n```", "```py\ncon = connector.connect(**params)\ncursor = con.cursor()\n```", "```py\nquery = \"SELECT * FROM ORDERS;\"\ncursor.execute(query)\ndf = cursor.fetch_pandas_all()\n```", "```py\ndf = cursor.execute(query).fetch_pandas_all()\n```", "```py\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1500000 entries, 0 to 1499999\nData columns (total 9 columns):\n #   Column           Non-Null Count    Dtype \n---  ------           --------------    ----- \n 0   O_ORDERKEY       1500000 non-null  int32 \n 1   O_CUSTKEY        1500000 non-null  int32 \n 2   O_ORDERSTATUS    1500000 non-null  object\n 3   O_TOTALPRICE     1500000 non-null  float64\n 4   O_ORDERDATE      1500000 non-null  object\n 5   O_ORDERPRIORITY  1500000 non-null  object\n 6   O_CLERK          1500000 non-null  object\n 7   O_SHIPPRIORITY   1500000 non-null  int8  \n 8   O_COMMENT        1500000 non-null  object\ndtypes: float64(1), int32(2), int8(1), object(5)\nmemory usage: 81.5+ MB\n```", "```py\ndf_ts = (\n    df.set_index(\n                pd.to_datetime(df['O_ORDERDATE'])\n                )\n                .drop(columns='O_ORDERDATE'))\n```", "```py\nprint(df_ts.iloc[0:3, 1:5])\n>>\n             O_CUSTKEY O_ORDERSTATUS  O_TOTALPRICE  O_ORDERPRIORITY\nO_ORDERDATE                                                       \n1994-02-21       13726             F      99406.41         3-MEDIUM\n1997-04-14      129376             O     256838.41  4-NOT SPECIFIED\n1997-11-24      141613             O     150849.49  4-NOT SPECIFIED\n```", "```py\ndf_ts.index[0:2]\n>>\nDatetimeIndex(['1994-02-21', '1997-04-14'], dtype='datetime64[ns]', name='O_ORDERDATE', freq=None)\n```", "```py\nCursor.close()\ncon.close()\n```", "```py\nfrom sqlalchemy import create_engine\nfrom snowflake.sqlalchemy import URL\nimport configparser\nconfig = ConfigParser()\nconfig.read('database.cfg')\nparams = dict(config['SNOWFLAKE'])\n```", "```py\nurl = URL(**params)\nengine = create_engine(url)\nconnection = engine.connect()\n```", "```py\nquery = \"SELECT * FROM ORDERS;\"\ndf = pd.read_sql(query,\n                 connection,\n                 index_col='o_orderdate',\n                 parse_dates='o_orderdate')\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1500000 entries, 1992-04-22 to 1994-03-19\nData columns (total 8 columns):\n #   Column           Non-Null Count    Dtype \n---  ------           --------------    ----- \n 0   o_orderkey       1500000 non-null  int64 \n 1   o_custkey        1500000 non-null  int64 \n 2   o_orderstatus    1500000 non-null  object\n 3   o_totalprice     1500000 non-null  float64\n 4   o_orderpriority  1500000 non-null  object\n 5   o_clerk          1500000 non-null  object\n 6   o_shippriority   1500000 non-null  int64 \n 7   o_comment        1500000 non-null  object\ndtypes: float64(1), int64(3), object(4)\nmemory usage: 103.0+ MB\n```", "```py\nconnection.close()\nengine.dispose()\n```", "```py\nquery = \"SELECT * FROM ORDERS;\"\nurl = URL(**params)\nengine = create_engine(url)\nwith engine.connect() as connection:\n    df = pd.read_sql(query,\n                      connection,\n                      index_col='o_orderdate',\n                      parse_dates=['o_orderdate'])\ndf.info()\n```", "```py\nfrom snowflake.snowpark import Session\nfrom configparser import ConfigParser\nconfig = ConfigParser()\nconfig.read('database.cfg')\nparams = dict(config['SNOWFLAKE'])\n```", "```py\nsession = Session.builder.configs(params).create()\n```", "```py\norders = session.read.table(\"ORDERS\").to_pandas()\n```", "```py\norders = session.table(\"ORDERS\").to_pandas()\n```", "```py\nquery = 'SELECT * FROM ORDERS'\norders = session.sql(query).to_pandas()\norders.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1500000 entries, 0 to 1499999\nData columns (total 9 columns):\n #   Column           Non-Null Count    Dtype \n---  ------           --------------    ----- \n 0   O_ORDERKEY       1500000 non-null  int32 \n 1   O_CUSTKEY        1500000 non-null  int32 \n 2   O_ORDERSTATUS    1500000 non-null  object\n 3   O_TOTALPRICE     1500000 non-null  float64\n 4   O_ORDERDATE      1500000 non-null  object\n 5   O_ORDERPRIORITY  1500000 non-null  object\n 6   O_CLERK          1500000 non-null  object\n 7   O_SHIPPRIORITY   1500000 non-null  int8  \n 8   O_COMMENT        1500000 non-null  object\ndtypes: float64(1), int32(2), int8(1), object(5)\nmemory usage: 81.5+ MB\n```", "```py\n'snowflake://<user>:<password>@<account>/<database>/<schema>\n?warehouse=<warehouse>&role=<role>'\n```", "```py\nengine = create_engine(URL(\n    account = '<your_account>',\n    user = '<your_username>',\n    password = '<your_password>',\n    database = '<your_database>',\n    schema = '<your_schema>',\n    warehouse = '<your_warehouse>',\n    role='<your_role>',\n))\n```", "```py\nparams = dict(config['SNOWFLAKE'])\nurl = create_engine(URL(**params))\n```", "```py\nparams = dict(config['SNOWFLAKE'])\nsession = Session.builder.configs(params).create()\n```", "```py\ndf = session.create_dataframe(orders)\n```", "```py\ndf = session.write_pandas(orders, table_name='temp_table')\n```", "```py\nconda install -c conda-forge pymongo -y\n```", "```py\npython -m pip install pymongo\n```", "```py\ndocker run -d \\\n    --name mongo-ch3 \\\n    -p 27017:27017 \\\n    --env MARIADB_ROOT_PASSWORD=password \\\n    mongo:8.0-rc\n```", "```py\nimport pandas as pd\nfrom pymongo import MongoClient, uri_parser\n```", "```py\n# connecting to a self-hosted instance\nurl = \"mongodb://127.0.0.1:27017\"\nclient = MongoClient(url)\n>>\nMongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)\n```", "```py\nclient = MongoClient(host=['127.0.0.1:27017'],\n                     password=None,\n                     username=None,\n                     document_class=dict,\n                     tz_aware=False,\n                     connect=True)\n```", "```py\nuri_parser.parse_uri(\"mongodb://127.0.0.1:27107\")\n>>\n{'nodelist': [('127.0.0.1', 27107)],\n 'username': None,\n 'password': None,\n 'database': None,\n 'collection': None,\n 'options': {},\n 'fqdn': None}\n```", "```py\n# connecting to Atlas cloud Cluster\ncluster = 'cluster0'\nusername = 'user'\npassword = 'password'\ndatabase = 'stock_data'\nurl = \\\nf\"mongodb+srv://{username}:{password}@{cluster}.3rncb.mongodb.net/{database}\"\nclient =  MongoClient(url)\nclient\n>>\nMongoClient(host=['cluster0-shard-00-00.3rncb.mongodb.net:27017', 'cluster0-shard-00-01.3rncb.mongodb.net:27017', 'cluster0-shard-00-02.3rncb.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='somesource', replicaset='Cluster0-shard-0', ssl=True)\n```", "```py\nclient.list_database_names()\n>>\n['admin', 'config', 'local', 'stock_data']\n```", "```py\ndb = client['stock_data']\ndb.list_collection_names()\n>>\n['microsoft', 'system.buckets.microsoft', 'system.views']\n```", "```py\ncollection = db['microsoft']\n```", "```py\nresults = collection.find({})\nmsft_df = (pd.DataFrame(results)\n             .set_index('date')\n             .drop(columns='_id'))\nmsft_df.head()\n>>\n               close         low    volume        high        open\ndate                                                               \n2019-09-04  131.457260  130.358829  17995900  131.514567  131.142059\n2019-09-05  133.768707  132.536556  26101800  134.083908  132.870864\n2019-09-06  132.861359  132.001715  20824500  133.892908  133.749641\n2019-09-09  131.352219  130.339762  25773900  133.482199  133.329371\n2019-09-10  129.976837  128.477244  28903400  130.750506  130.664546\n```", "```py\nclient = MongoClient(\"mongodb://localhost:27017\")\n```", "```py\nclient = MongoClient('localhost', 27017)\n```", "```py\nclient = MongoClient('127.0.0.1', 27017)\n```", "```py\n# using default values for host and port\nclient = MongoClient()\n```", "```py\nclient = MongoClient(host='127.0.0.1',\n                     port=27017,\n                     password=password,\n                     username=username,\n                     document_class=dict,\n                     tz_aware=False,\n                     connect=True)\n```", "```py\n# Specifying the database\ndb = client['stock_data']\ndb = client.stock_data\ndb = client.get_database('stock_data')\n```", "```py\n# Specifying the collection\ncollection = db.microsoft\ncollection = db['microsoft']\ncollection = db.get_collection('microsoft')\n```", "```py\ncollection.find({})\n```", "```py\nquery = {\"close\": {\"$gt\": 130}}\nresults = collection.find(query)\n```", "```py\ndf = pd.DataFrame(results)\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1256 entries, 0 to 1255\nData columns (total 7 columns):\n #   Column  Non-Null Count  Dtype        \n---  ------  --------------  -----        \n 0   date    1256 non-null   datetime64[ns]\n 1   close   1256 non-null   float64      \n 2   _id     1256 non-null   object       \n 3   low     1256 non-null   float64      \n 4   volume  1256 non-null   int64        \n 5   high    1256 non-null   float64      \n 6   open    1256 non-null   float64      \ndtypes: datetime64[ns](1), float64(4), int64(1), object(1)\nmemory usage: 68.8+ KB\n```", "```py\nquery = {\"close\": {\"$gt\": 130}}\nprojection = {\n   \"_id\": 0,\n   \"date\":1,\n   \"close\": 1,\n   \"volume\": 1\n}\nresults = collection.find(query, projection)\ndf = pd.DataFrame(results).set_index(keys='date')\nprint(df.head())\n>>\n               close    volume\ndate                           \n2019-09-04  131.457260  17995900\n2019-09-05  133.768707  26101800\n2019-09-06  132.861359  20824500\n2019-09-09  131.352219  25773900\n2019-09-11  130.014984  24726100\n```", "```py\ndb.microsoft.find_one()\n>>>\n{'date': datetime.datetime(2019, 9, 4, 0, 0),\n 'close': 131.45726013183594,\n '_id': ObjectId('66e30c09a07d56b6db2f446e'),\n 'low': 130.35882921332006,\n 'volume': 17995900,\n 'high': 131.5145667829114,\n 'open': 131.14205897649285}\n```", "```py\ncursor = db.microsoft.find()\ndf = pd.DataFrame(cursor)\n```", "```py\ndata = list(db.microsoft.find())\n```", "```py\ndata = list(db.microsoft.find())\ndf = pd.DataFrame(data)\n```", "```py\ncursor = db.microsoft.find()\ncursor.next()\n```", "```py\ncursor = db.microsoft.find()\nfor doc in cursor:\n    print(doc)\n```", "```py\ncursor = db.microsoft.find()\nfor doc in cursor[0:5]:\n    print(doc)\n```", "```py\ncursor = db.microsoft.find()\ncursor[0]\n```", "```py\npip install 'influxdb-client[ciso]'\npip install 'taospy[ws]'\npip install psycopg2\n```", "```py\nconda install -c conda-forge influxdb-client\nconda install -c conda-forge taospy taospyws\nconda install -c conda-forge psycopg2\n```", "```py\ndocker run -d\\\n    --name influxdb-ch3 \\\n    -p 8086:8086 \\\n    influxdb:2.7.9-alpine\n```", "```py\nimport \"influxdata/influxdb/sample\"\nsample.data(set: \"noaaWater\")\n    |> to(bucket: \"tscookbook\")\n```", "```py\ndocker run -d \\\n    --name timescaledb-ch3 \\\n    -p 5432:5432 \\\n    -e POSTGRES_PASSWORD=password \\\n    timescale/timescaledb:latest-pg16\n```", "```py\ndocker run -d \\\n   --name tdengine-ch3 \\\n   -p 6030-6060:6030-6060 \\\n   -p 6030-6060:6030-6060/udp \\\n   tdengine/tdengine:3.3.2.0\n```", "```py\ndocker exec -it tdengine-ch3 /bin/bash\n>>\nroot@9999897cbeb4:~# taosBenchmark\n```", "```py\nfrom influxdb_client import InfluxDBClient\nimport pandas as pd\n```", "```py\ntoken = \"c5c0JUoz-\\\njoisPCttI6hy8aLccEyaflyfNj1S_Kff34N_4moiCQacH8BLbLzFu4qWTP8ibSk3JNYtv9zlUwxeA==\"\norg = \"ts\"\nbucket = \"tscookbook\"\n```", "```py\nclient = InfluxDBClient(url=\"http://localhost:8086\",\n                        token=token,\n                        org=org)\n```", "```py\nquery_api = client.query_api()\n```", "```py\nquery = f'''\nfrom(bucket: \"tscookbook\")\n  |> range(start: 2019-09-01T00:00:00Z)\n  |> filter(fn: (r) =>\n        r._measurement == \"h2o_temperature\" and\n        r.location == \"coyote_creek\" and\n        r._field == \"degrees\"\n  )\n  |> movingAverage(n: 120)\n  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n'''\nresult = client.query_api().query_data_frame(org=org, query=query)\n```", "```py\nresult.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3885 entries, 0 to 3884\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype             \n---  ------        --------------  -----             \n 0   result        3885 non-null   object            \n 1   table         3885 non-null   int64             \n 2   _start        3885 non-null   datetime64[ns, UTC]\n 3   _stop         3885 non-null   datetime64[ns, UTC]\n 4   _time         3885 non-null   datetime64[ns, UTC]\n 5   _measurement  3885 non-null   object            \n 6   location      3885 non-null   object            \n 7   degrees       3885 non-null   float64           \ndtypes: datetime64[ns, UTC](3), float64(1), int64(1), object(3)\nmemory usage: 242.9+ KB\n```", "```py\nquery = f'''\nfrom(bucket: \"tscookbook\")\n  |> range(start: 2019-09-01T00:00:00Z)\n  |> filter(fn: (r) =>\n        r._measurement == \"h2o_temperature\" and\n        r.location == \"coyote_creek\" and\n        r._field == \"degrees\"\n  )\n  |> movingAverage(n: 120)\n  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n  |> keep(columns: [\"_time\", \"degrees\"])\n'''\nresult = client.query_api().query_data_frame( query=query)\nresult.head()\n>>\nresult  table                     _time    degrees\n0  _result      0 2019-09-01 11:54:00+00:00  64.891667\n1  _result      0 2019-09-01 12:00:00+00:00  64.891667\n2  _result      0 2019-09-01 12:06:00+00:00  64.816667\n3  _result      0 2019-09-01 12:12:00+00:00  64.841667\n4  _result      0 2019-09-01 12:18:00+00:00  64.850000\n```", "```py\nimport pandas as pd\nfrom sqlalchemy import create_engine\n```", "```py\nengine =\\\n    create_engine(\"postgresql+psycopg2://postgres:password@localhost:5432/postgres\")\n```", "```py\nquery = \"SELECT * FROM msft\"\ndf = pd.read_sql(query,\n                 engine,\n                 index_col='date',\n                 parse_dates={'date': '%Y-%m-%d'})\nprint(df.head())\n```", "```py\n[TDENGINE]\nuser=root\npassword=taosdata\nurl=http://localhost:6041\n```", "```py\nimport taosrest\nimport pandas as pd\n```", "```py\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read('database.cfg')\nparams = dict(config['TDENGINE'])\n```", "```py\nconn = taosrest.connect(**params)\n```", "```py\nquery = \"\"\"\nSELECT *\nFROM test.meters\nWHERE location = 'California.LosAngles'\nLIMIT 100000;\n\"\"\"\nresults = conn.query(query)\n```", "```py\nresults.rows\n>>\n100000\nresults.fields\n>>\n[{'name': 'ts', 'type': 'TIMESTAMP', 'bytes': 8},\n {'name': 'current', 'type': 'FLOAT', 'bytes': 4},\n {'name': 'voltage', 'type': 'INT', 'bytes': 4},\n {'name': 'phase', 'type': 'FLOAT', 'bytes': 4},\n {'name': 'groupid', 'type': 'INT', 'bytes': 4},\n {'name': 'location', 'type': 'VARCHAR', 'bytes': 24}]\n```", "```py\ncols = [col['name'] for col in results.fields ]\ndf = pd.DataFrame(results.data, columns=cols)\ndf = df.set_index('ts')\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 100000 entries, 2017-07-14 05:40:00 to 2017-07-14 05:40:05.903000\nData columns (total 5 columns):\n #   Column    Non-Null Count   Dtype \n---  ------    --------------   ----- \n 0   current   100000 non-null  float64\n 1   voltage   100000 non-null  int64 \n 2   phase     100000 non-null  float64\n 3   groupid   100000 non-null  int64 \n 4   location  100000 non-null  object\ndtypes: float64(2), int64(2), object(1)\nmemory usage: 4.6+ MB\n```", "```py\nresult = client.query_api().query_data_frame(org=org, query=query)\n```", "```py\npivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n```", "```py\nresult =\\\nquery_api.query_data_frame(query=query,                  \n                                    data_frame_index=['_time'])\nresult['_value'].head()\n>>\n_time\n2021-04-01 01:45:02.350669+00:00    64.983333\n2021-04-01 01:51:02.350669+00:00    64.975000\n2021-04-01 01:57:02.350669+00:00    64.916667\n2021-04-01 02:03:02.350669+00:00    64.933333\n2021-04-01 02:09:02.350669+00:00    64.958333\nName: _value, dtype: float64\n```"]