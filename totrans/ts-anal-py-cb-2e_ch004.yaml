- en: 3 Reading Time Series Data from Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/file0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Databases** extend what you can store to include text, images, and media
    files and are designed for efficient read-and-write operations at a massive scale.
    Databases can store terabytes and petabytes of data with efficient and optimized
    data retrieval capabilities, such as when performing analytical operations on
    **data warehouses** and **data lakes**. A data warehouse is a database designed
    to store large amounts of structured data, mostly integrated from multiple source
    systems, built specifically to support business intelligence reporting, dashboards,
    and advanced analytics. A data lake, on the other hand, stores a large amount
    of structured, semi-structured, or unstructured data in its raw format. In this
    chapter, we will continue to use the **pandas** library to read data from databases.
    We will create time series DataFrames by reading data from **relational** (SQL)
    databases and **non-relational** (NoSQL) databases.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you will explore working with third-party data providers to pull
    financial data from their database systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will create time series DataFrames with a `DatetimeIndex`
    data type by covering the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from a relational database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from Snowflake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from a document database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from a time series databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will be using pandas 2.2.2 (released April 10, 2024) extensively.
  prefs: []
  type: TYPE_NORMAL
- en: You will be working with different types of databases, such as PostgreSQL, Amazon
    Redshift, MongoDB, InfluxDB, and Snowflake. You will need to install additional
    Python libraries to connect to these databases.
  prefs: []
  type: TYPE_NORMAL
- en: You can also download the Jupyter notebooks from this book's GitHub repository
    ([https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook))
    to follow along.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a good practice, you will store your database credentials in a config `database.cfg`
    file outside your Python script. You can use `configparser` to read and store
    the values in Python variables. You do not want your credentials exposed or hard
    coded in your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can load the `database.cfg` file using `config.read()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Reading data from a relational database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will read data from PostgreSQL, a popular open-source relational
    database.
  prefs: []
  type: TYPE_NORMAL
- en: You will explore two methods for connecting to and interacting with PostgreSQL.
    First, you will use `psycopg`, a PostgreSQL Python connector, to connect and query
    the database, then parse the results into a pandas DataFrame. In the second approach,
    you will query the same database again, but this time, you will use **SQLAlchemy**,
    an **object-relational mapper** (**ORM**) that is well-integrated with pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, it is assumed that you have the latest PostgreSQL installed.
    At the time of writing, version 16 is the latest stable version.
  prefs: []
  type: TYPE_NORMAL
- en: To connect to and query the database in Python, you will need to install `psycopg`,
    a popular PostgreSQL database adapter for Python. You will also need to install
    `SQLAlchemy`, which provides flexibility regarding how you want to manage the
    database, whether for writing or reading data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the libraries using `conda`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the libraries using `pip`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you cannot access a PostgreSQL database, the fastest way to get up and running
    is via Docker ([https://hub.docker.com/_/postgres](https://hub.docker.com/_/postgres)).
    The following is an example command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will create a container named `postgres-ch3`. The `username` is `postgres,`
    and the password is `password`. The default `database` created is called `postgres`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the **postgres-ch3** container is up and running you can connect to it
    using **DBeaver** as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – DBeaver PostgreSQL connection settings](img/file22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – DBeaver PostgreSQL connection settings
  prefs: []
  type: TYPE_NORMAL
- en: You will be working with MSFT stock dataset provided in the `datasets/Ch3/MSFT.csv`
    folder. I have uploaded the dataset into the database using **DBeaver Community
    Edition,** which you can download here [https://dbeaver.io/download/](https://dbeaver.io/download/)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can import the CSV file by right-clicking on `tables` under the `public`
    schema, as shown in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Importing data to PostgreSQL using DBeaver](img/file23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Importing data to PostgreSQL using DBeaver
  prefs: []
  type: TYPE_NORMAL
- en: You can confirm all records have been written to the `msft` table in the `postgres`
    database as shown
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Using SQL Editor in DBeaver to run a SQL query against the msft
    table](img/file24.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Using SQL Editor in DBeaver to run a SQL query against the msft
    table
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will start by connecting to the PostgreSQL instance, querying the database,
    loading the result set into memory, and parsing the data into a time series DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, I will connect to a PostgreSQL instance that runs locally, so
    my connection would be to `localhost (127.0.0.1)`. You will need to adjust this
    for your own PostgreSQL database setting.
  prefs: []
  type: TYPE_NORMAL
- en: Using the psycopg
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Psycopg** is a Python library (and a database driver) that provides additional
    functionality and features when working with a PostgreSQL database. Follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the necessary libraries. You will import the required connection
    parameters from the `database.cfg` highlighted in the *Technical Requirements*
    section. You will create a Python dictionary to store all the parameter values
    required to establish a connection to the database, such as `host`, `database`
    name, `user` name, and `password`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can establish a connection by passing the parameters to the `connect()`
    method. Once connected, you can create a **cursor** object that can be used to
    execute SQL queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The cursor object provides several attributes and methods, including `execute`,
    `executemany`, `fetchall`, `fetchmany` and `fetchone`. The following code uses
    the cursor object to pass a SQL query and then checks the number of records that
    have been produced by that query using the `rowcount` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned result set after executing the query will not include a header
    (no columns names). Alternatively, you can grab the column names from the cursor
    object using the `description` attribute, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use a list comprehension to extract the column names from `cursor.description`
    to pass as column headers when creating the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To fetch the results that were produced by the executed query you will use
    the `fetchall` method. You will create a DataFrame based on the fetched result
    set. Make sure that you pass the column names that you just captured:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `date` column is returned as an `object` type, not a `datetime` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parse the `date` column using `pd.to_datetime()` and set it as the index for
    the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the cursor returned a list of tuples **without a header**.
    You can confirm this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can instruct the cursor to return a `dict_row` type, which will include
    the column name information (the header). This is more convenient when converting
    into a DataFrame. This can be done by passing the `dict_row` class to the `row_factory`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the column names being available. You can now create a DataFrame as
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the cursor and the connection to the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `psycopg` connections and cursors can be used in Python''s `with`
    statement for exception handling when committing a transaction. The cursor object
    provides three different fetching functions; that is, `fetchall`, `fetchmany`,
    and `fetchone`. The `fetchone` method returns a single tuple. The following example
    shows this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using pandas and SQLAlchemy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SQLAlchemy is a very popular open-source library for working with relational
    databases in Python. SQLAlchemy can be referred to as an **Object-Relational Mapper
    (ORM)**, which provides an abstraction layer (think of it as an interface) so
    that you can use object-oriented programming to interact with a relational database.
  prefs: []
  type: TYPE_NORMAL
- en: You will be using SQLAlchemy because it integrates very well with pandas, and
    several of the pandas SQL reader and writer functions depend on SQLAlchemy as
    the abstraction layer. SQLAlchemy does the behind-the-scenes translation for any
    pandas SQL read or write requests. This translation ensures that the SQL statement
    from pandas is represented in the correct syntax/format for the underlying database
    type (MySQL, Oracle, SQL Server, or PostgreSQL, to name a few).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the pandas SQL reader functions that rely on SQLAlchemy include `pandas.read_sql`,
    `pandas.read_sql_query`, and `pandas.read_sql_table`. Let''s perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the necessary libraries. Note that, behind the scenes, SQLAlchemy
    will use **psycopg** (or any other database driver that is installed and supported
    by SQLAlchemy):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, for `parse_dates`, you passed a dictionary in the
    format of `{key: value}`, where `key` is the column name and the `value` is a
    string representation of the date format. Unlike the previous **psycopg** approach,
    `pandas.read_sql` did a better job in getting the data types correct. Notice that
    our index is of the `DatetimeIndex` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You could also accomplish the same results using the `pandas.read_sql_query`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pandas.read_sql_table` is another SQL reader function provided by pandas
    that takes in a table name instead of a SQL query. Think of this as a `SELECT
    * FROM tablename` query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `read_sql` reader function is more versatile as it is a wrapper to `read_sql_query`
    and `read_sql_table`. The `read_sql` function can take either a SQL query or a
    table name.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You explored two methods to connect to a PostgreSQL database in this recipe:
    using the psycopg driver directly or utilizing pandas and SQLAlchemy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When using **psycopg** to connect to a PostgreSQL, you first need to create
    a connection object followed by a cursor object. This concept of a connection
    objects and cursors is consistent across different database drivers in Python.
    Once you create a cursor object, you have access to several methods, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Execute()` – executes a SQL query (CRUD) or command to the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Executemany()` – executes the same database operation with a sequence of input
    data, for example, this can be useful with INSERT INTO for bulk insert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fetchall()` – returns all remaining records from the current query result
    set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fetchone()` - returns the next record (one record) from the current query
    result set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetchmany(n)` – returns `n` number of records from the current query result
    set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`close()` - close the current cursor and free associated resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, creating an **engine** object is the first step when working
    with SQLAlchemy, as it provides instructions on the database that is being considered.
    This is known as a **dialect**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you created the engine with `create_engine,` you passed a URL as the connection
    string. Let''s examine the engine connection string for SQLAlchemy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`dialect` –the name of the SQLAlchemy dialect (database type) such as postgresql,
    mysql, sqlite, oracle, or mssql.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`driver` –the name of the installed driver (DBAPI) to connect to the specified
    dialect, such as `psycopg` or `pg8000` for PostgreSQL database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`username` -the login username for database authentication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`password` -the password for the username specified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` -the server where the database is hosted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port` -the specific port for the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`database` -the name of the specific database you want to connect to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Earlier, you used `psycopg` as the database driver for PostgreSQL. The `psycopg`
    driver is referred to as a **database API (DBAPI)** and SQLAlchemy supports many
    DBAPI wrappers based on Python''s DBAPI specifications to connect to and interface
    with various types of relational databases. SQLAlchemy already comes with built-in
    dialects to work with different flavors of RDBMS, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: SQL Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQLite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowflake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When connecting to a database using SQLAlchemy, you need to specify the **dialect**
    and the **driver** (DBAPI) to be used. This is what the URL string looks like
    for PostgreSQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what it would look like if you are using **MySQL** database with `PyMySQL`
    driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code examples in the *How to do it…* section, you did not need
    to specify the `psycopg` driver since it is the default DBAPI that SQLAlchemy
    uses. This example would work just fine, assuming that `psycopg` is installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'There are other PostgreSQL drivers (DBAPI) that are supported by SQLAlchemy,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`psycopg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pg8000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`asyncpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a more comprehensive list of supported dialects and drivers, you can visit
    the official documentation page at [https://docs.sqlalchemy.org/en/20/dialects/index.html](https://docs.sqlalchemy.org/en/20/dialects/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using SQLAlchemy is that it is well-integrated with pandas.
    If you read the official pandas documentation for `read_sql`, `read_sql_query`,
    `read_sql_table`, and `to_sql` you will notice that the `con` argument is expecting
    a SQLAlchemy connection object (engine).
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is that you can easily change the backend engine (database),
    for example from PostgreSQL to MySQL, without needing to change the rest of the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we will explore some additional concepts to help you better
    grasp the versatility of SQLAclhemy and bring some of the previous concepts discussed
    in the recipe *Working with large data files* in Chapter 2, *Reading Time Series
    Data from Files*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating the connecitoin URL in SQLAlchemy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending to Amazon Redshift database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking in pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the connection URL in SQLAlchemy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this recipe you were introduced to the `create_engine` function from SQLAlchemy
    library which takes a URL string to establish a connection to a database. So far,
    you have been creating the URL string manually but there is a more convenient
    way to generate the URL for you. This can be accomplished with the `create` method
    form the `URL` class in SQLAlchemy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `drivername` consists of the *dialect* and the *driver* in format
    `dialct+driver`.
  prefs: []
  type: TYPE_NORMAL
- en: You can now pass the `url` to `create_engine` as you have done before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Extending to Amazon Redshift database
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We discussed the versatility of SQLAlchmey, which allows you to change the engine
    (database backend) and keep the remaining code as is. For example, using PostgreSQL
    or MySQL, or any other supported dialect by SQLAlchemy. We will also explore connecting
    to a cloud data warehouse like Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that **Amazon Redshift**, a cloud data warehouse, is
    based on PostgreSQL at its core. You will install the Amazon Redshift driver for
    SQLAlchemy (it uses the psycopg2 DBAPI).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install using **conda**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also install using **pip**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we do not want to expose your AWS credentials in your code, you will
    update the `database.cfg` file discussed in the *Technical Requirements* section
    to include your AWS Redshift information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You will use `configparser` to load your values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You will use the URL.create method to generate your URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can switch the engine from our previous code, which originally pointing
    to our local instance of PostgreSQL, to run the same query on Amazon Redshift.
    This assumes you have a `msft` table in Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To learn more about `sqlalchemy-redshift`, you can refer to the project''s
    repository here: [https://github.com/sqlalchemy-redshift/sqlalchemy-redshift](https://github.com/sqlalchemy-redshift/sqlalchemy-redshift).'
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon Redshift example can be extended to other databases such as Google
    BigQuery, Teradata, or Microsoft SQL Server as long there is a supported SQLAlchemy
    dialect for that database. For a complete list visit the official page here [https://docs.sqlalchemy.org/en/20/dialects/index.html](https://docs.sqlalchemy.org/en/20/dialects/index.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chunking with pandas
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you executed the query against `msft` table, it returned 1259 records.
    Imagine working with a much larger database that returns millions of records,
    if not more. This is where the `chunking` parameter helps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `chunksize` parameter allows you to break down a large dataset into smaller
    and more manageable chunks of data that can fit into your local memory. When executing
    the `read_sql` function, just pass the number of rows to be retrieved (per chunk)
    to the `chunksize` parameter, which then returns a `generator` object. You can
    then loop through the generator object, or use `next()` to capture one chunk at
    a time, and perform whatever calculations or processing needed. Let''s look at
    an example of how chunking works. You will request `500` records (rows) at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will generate three (3) chunks. You can iterate through
    the `df_gen` generator object as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code demonstrated how chunking works. Using the `chunksize` parameter
    should reduce memory usage since the code loads a smaller number of rows per chunk
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For additional information regarding these topics, take a look at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: For **SQLAlchemy**, you can visit [https://www.sqlalchemy.org/](https://www.sqlalchemy.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the `pandas.read_sql` function visit [https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the `pandas.read_sql_query` function visit [https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the `pandas.read_sql_table` function visit [https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from Snowflake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very common place to extract data for analytics is usually a company's *data
    warehouse*. Data warehouses host a massive amount of data that, in most cases,
    contains integrated data to support various reporting and analytics needs, in
    addition to historical data from various source systems.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of the cloud brought us cloud data warehouses such as **Amazon
    Redshift**, **Google BigQuery**, **Azure SQL Data Warehouse**, and **Snowflake**.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will work with *Snowflake*, a powerful **Software as a Service**
    (**SaaS**) cloud-based data warehousing platform that can be hosted on different
    cloud platforms, such as **Amazon Web Services** (**AWS**), **Google Cloud Platform**
    (**GCP**), and **Microsoft Azure**. You will learn how to connect to Snowflake
    using Python to extract time series data and load it into a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This recipe assumes you have access to Snowflake. You will explore three (3)
    different methods to connect to Snowflake so you will need to install three (3)
    different libraries .
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommended approach for the `snowflake-connector-python` library is to
    install it using **pip** allowing you to install *extras* such as `pandas` as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You can also install with **conda**, but if you want to use `snowflake-connector-python`
    with pandas you will need to use the pip install.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure you the configuration file `database.cfg` that you created in the
    *Technical Requirements* section contains your **Snowflake** connection information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe you will be working with the `SNOWFLAKE_SAMPLE_DATA` database
    and the `TPCH_SF1 schema provided by Snowflake`.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing the proper `account` value can cause confusion for many. To ensure
    you have the right format use the *Copy account URL* option from Snowflake, which
    may look like this `https://abc1234.us-east-1.snowflakecomputing.com` the `abc1234.us-east-1`
    part is what you will use as the `account` value.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will explore three (3) methods and libraries to connect to the Snowflake
    database. In the first method, you will use the Snowflake Python connector to
    establish a connection and create a cursor to query and fetch the data. In the
    second method, you will use the Snowflake **SQLAlchemy**. In the third method,
    you will explore the **Snowpark** Python API. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Using snowflake-connector-python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will start by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `ConfigParser`, you will extract the content under the `[SNOWFLAKE]`
    section to avoid exposing or hardcoding your credentials. You can read the entire
    content of the `[SNOWFLAKE]` section and convert it into a dictionary object,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to pass the parameters to the `connector.connect()` to establish
    a connection with Snowflake. We can easily *unpack* the dictionary''s content
    since the dictionary keys match the parameter names. Once the connection has been
    established, we can create our *cursor*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The cursor object has many methods, such as `execute`, `fetchall`, `fetchmany`,
    `fetchone`, fetch_pandas_all, and `fetch_pandas_batches`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will start with the `execute` method to pass a SQL query to the database,
    then use any of the available fetch methods to retrieve the data. In the following
    example, you will query the `ORDERS` table and then leverage the `fetch_pandas_all`
    method to retrieve the entire result set as a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The previsou code could have been written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the DataFrame using `df.info()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, you can see that the DataFrame''s Index is just
    a sequence of numbers and that the `O_ORDERDATE` column is not a `Date` field.
    You can parse the `O_ORDERDATE` column to a datetime type using `pandas.to_datetime()`
    function and then setting the column as the DataFrame’s index with the `DataFrame.set_index()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s display the first four (4) columns and first five (5) rows of the `df_ts`
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the index of the DataFrame. Print the first two indexes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you can close the cursor and current connection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: You now have a time series DataFrame with a `DatetimeIndex`.
  prefs: []
  type: TYPE_NORMAL
- en: Using SQLAlchmey
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous recipe, `Reading data from a relational database`, you explored
    the pandas read_sql, read_sql_query, and read_sql_table functions. This was accomplished
    by utilizing SQLAlchemy and installing one of the supported dialicts. Here, we
    will use the Snowflake dialect after installing the snowflake-sqlalchemy driver.
  prefs: []
  type: TYPE_NORMAL
- en: SQLAclhemy is better integrated with pandas as you will experience in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Start by importing the necessary libraries and reading the Snowflake connection
    parameters from the `[SNOWFLAKE]` section in the `database.cfg` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'You will use the URL class to generate the URL connection string. We will create
    our engine object and then open a connection with the engine.connect():'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can use either the read_sql or read_sql_query to execute our query
    against the ORDERS table in the SNOWFLAKE_SAMPLE_DATA database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we were able to parse the o_orderdate column and set it as index
    all in one step when compared to the Snowflake Python connector method performed
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, close the connection to the database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The code could be further simplified by using a **context manager** to automatically
    allocate and release resources. The following example uses the `with engine.connect()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This should produce the same results without needing to close connection and
    dispose the engine.
  prefs: []
  type: TYPE_NORMAL
- en: Using snowflake-snowpark-python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Snowpark API supports Java, Python, and Scala. You have already installed
    the `snowflake-snowpark-python` as described in the *Getting Ready* section of
    this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Start by importing the necessary libraries and reading the Snowflake connection
    parameters from the `[SNOWFLAKE]` section in the `database.cfg` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Create a session by establishing a connection with the Snowflake database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The session has several `DataFrameReader` methods such as `read`, `table`, and
    `sql.` Any of these methods would return a Snowpark DataFrame object. The returned
    Snowpark DataFrame object has access to the `to_pandas` method to convert into
    a more familiar pandas DataFrame. You will explore the `read`, `table`, and `sql`
    methods to return the same result set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with the `read` method. More specifically, you will be using the `read.table`
    and pass it a table name. This will return the content of the table and covert
    into a pandas DataFrame with the `to_pandas` method. Think of this as equivalent
    to `SELECT * FROM TABLE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the `table` method take a table name, and the returned object (Snowpark
    DataFrame) as access to the `to_pandas` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, the `sql` method which takes a SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: All three approaches should produce the same pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Snowflake Python connector, Snowflake SQLAlchmey driver, and Snowpark Python
    require the same input variables to establish a connection to the Snowflake database.
    These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 3.1 – Input variables for the Snowflake Python connector](img/file25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 3.1 – Input variables for the Snowflake Python connector
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in the previous activity, you used the same configuration for all
    three methods in the `database.cfg` file under the `[SNOWFLAKE]` section.
  prefs: []
  type: TYPE_NORMAL
- en: Snowflake Python Connector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When using the Python connector, you first establish a connection to the database
    with `con = connector.connect(**params).` Once the connection is accepted, you
    create a **cursor** object with `cursor = con.cursor()`.
  prefs: []
  type: TYPE_NORMAL
- en: The cursor provides methods for performing execute and fetch operations such
    as `describe()`, `execute()`, `execute_async()`, `executemany()`, `fetchone()`,
    `fetchall()`, `fetchmany()`, `fetch_pandas_all()`, and `fetch_pandas_batches()`,
    and each cursor has several attributes including `description`, `rowcount`, `rownumber`,
    to name few. Note there are familiar methods and attributes discussed in the previous
    recipe, *Reading data from a relational database*, when using the Python connector
    **psycopg.**
  prefs: []
  type: TYPE_NORMAL
- en: '`Execute()` – executes a SQL query (CRUD) or command to the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`executemany()` – executes the same database operation with a sequence of input
    data, for example, this can be useful with INSERT INTO for bulk insert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fetchall()` – returns all remaining records from the current query result
    set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetchone()` - returns the next record (one record) from the current query
    result set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetchmany(n)` – returns `n` number of records from the current query result
    set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_pandas_all() -` returns all remaining records from the current query
    result set and loads them into a pandas DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_pandas_batches()` - returns a subset of the remaining records form the
    current query result set and loads them into a pandas DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`close()` - close the current cursor and free associated resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe()` – returns metadata about the result set without executing the
    query. Alternatively, you can use `execute()` followed by `description` attribute
    to obtain the same metadata information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete list of attributes and methods, you can refer to the official
    documentation at [https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor](https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor).
  prefs: []
  type: TYPE_NORMAL
- en: SQLAlchmey API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When SQLAlchemy, you are able to leverage the `pandas.read_sql, pandas.read_sql_query,`
    and `pandas.read_sql_query` reader functions and leverage many of the available
    parameters to transform and process the data at read time such as `index_col`
    and `parse_dates`. On the other hand, when using the Snowflake Python connector,
    the `fetch_pandas_all()` function does not take in any parameters, and you will
    need to parse and adjust the DataFrame afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Snowflake SQLAlchemy library provides a convenience method, `URL`, to help
    construct the connection string to connect to the Snowflake database. Typically,
    SQLAlchemy expects a URL to be provided in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `URL` method, we passed our parameters, and the method took care
    of constructing the connection string that is expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we stored our Snowflake parameters in the configuration file
    database.cfg and stored as Python dictionary. This way, you will not be exposing
    your credentials within the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: If you compare the process in this recipe, using **SQLAlchemy**, for Snowflake,
    and that of the previous recipe, *Reading data from a relational database*, you
    will observe similarities in the process and code. This is one of the advantages
    of using SQLAlchemy, it creates a standard process across a variety of databases
    as long as SQLAlchmey supports them. *SQLAlchemy is well integrated with pandas
    and makes it easy to switch dialects (backend databases) without much change to
    your code*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Snowpark API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous methods, you simply used the libraries `snowflake-connector-python`
    and `snowflake-connector-python` as connectors to your Snowflake database, and
    then fetch the data to process the data locally.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark offers more than just a mechanism to connect to your database. It allows
    you to process the data directly within the Snowflake environment on the cloud
    without the need to move the data outside or process locally. In addition, Snowpark
    is well suited for more complex tasks such as building complex data pipelines
    or Snowpark ML for working with machine learning models all on the Snowflake cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In our recipe, and similar to the other methods, we need to establish our connection
    with Snowflake. This was accomplished using the `Session` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'There are similarity in the API and conecpts between Snowpark and PySpark (Spark).
    More specifically, Snowpark DataFrame are considered *lazily-evaluated relational
    dataset*. The `to_pandas` method does two things: it executes the query and loads
    the results into a pandas DataFrame (data is being fetched outside of Snowflake).
    To convert the pandas DataFrame back to a Snowpark DataFrame (inside Snowflake)
    you can use the `create_dataframe` method as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In order successfully execute the preceding code, you need have *write permission*
    since Snowflake will create a **temporary** table to store the pandas DataFrame
    (inside Snowflake) and then returns a Snowpark DataFrame that points to that temporary
    table. Alternatively, if you want to persist the pandas DataFrame into a table
    you can use the `write_pandas` method as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, you passed the pandas DataFrame and table name.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that the columns in the returned DataFrame, when using
    the Snowflake Python connector and Snowpark, all came back in uppercase, while
    they were lowercased when using Snowflake SQLAlchemy.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is because Snowflake, by default, stores unquoted object
    names in uppercase when these objects are created. In the previous code, for example,
    our `Order Date` column was returned as `O_ORDERDATE`.
  prefs: []
  type: TYPE_NORMAL
- en: To explicitly indicate the name is case-sensitive, you will need to use quotes
    when creating the object in Snowflake (for example, `'o_orderdate'` or `'OrderDate'`).
    In contrast, using Snowflake SQLAlchemy converts the names into lowercase by default.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the **Snowflake Connector for Python**, you can visit
    the official documentation at [https://docs.snowflake.com/en/user-guide/python-connector.html](https://docs.snowflake.com/en/user-guide/python-connector.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information regarding **Snowflake SQLAlchemy**, you can visit the official
    documentation at [https://docs.snowflake.com/en/user-guide/sqlalchemy.html](https://docs.snowflake.com/en/user-guide/sqlalchemy.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information regarding **Snowpark API**, you can visit the official
    documentation at [https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/index)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from a document database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MongoDB**, a **NoSQL** database, stores data in documents and uses BSON (a
    JSON-like structure) to store schema-less data. Unlike relational databases, where
    data is stored in tables that consist of rows and columns, document-oriented databases
    store data in collections and documents.'
  prefs: []
  type: TYPE_NORMAL
- en: A document represents the lowest granular level of data being stored, as rows
    do in relational databases. A collection, like a table in relational databases,
    stores documents. Unlike relational databases, a collection can store documents
    of different schemas and structures.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, it is assumed that you have a running instance of MongoDB. To
    get ready for this recipe, you will need to install the `PyMongo` Python library
    to connect to MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install MongoDB using `conda`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To install MongoDB using `pip`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not have access to a PostgreSQL database, then the fastest way to
    get up and running is via Docker ([https://hub.docker.com/_/mongo](https://hub.docker.com/_/mongo)).
    The following is an example command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can try **MongoDB Atlas** for free here [https://www.mongodb.com/products/platform/atlas-database](https://www.mongodb.com/products/platform/atlas-database).
    **MongoDB Atlas** is a fully managed cloud database that can be deployed on your
    favorite cloud providers such as AWS, Azure, and GCP.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE ABOUT USING MONGODB ATLAS
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are connecting to MongoDB Atlas (Cloud) Free Tier or their M2/M5 shared
    tier cluster, then you will be using the `mongodb+srv` protocol. In this case,
    you can either specify this during the pip install with `python -m pip install
    "pymongo[srv]"`
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Optionally, for a GUI interface to your MongoDB you can install **MongoDB Compass**
    form here [https://www.mongodb.com/products/tools/compass](https://www.mongodb.com/products/tools/compass)
  prefs: []
  type: TYPE_NORMAL
- en: I am using MongoDB Compass to create the database, collection, and load the
    data. In Chapter 5, *Persisting Time Series Data to Databases*, you learn how
    to create your database, collection, and load data using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Using **Compass** select the option **Create Database**. For the Database Name
    you can enter `stock_data` and `microsoft` for the **Collection** Name. Click
    the **Time-Series** checkbox and specify the `date` as the **timeField**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – MongoDB Compass Create Database screen](img/file26.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – MongoDB Compass Create Database screen
  prefs: []
  type: TYPE_NORMAL
- en: Once the **Database** and **Collection** are created, click **Import Data**
    and select the MSFT stock dataset provided in the `datasets/Ch3/MSFT.csv` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – MongoDB Compass Import Data screen](img/file27.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – MongoDB Compass Import Data screen
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6- MonogDB Compass review data types screen before import](img/file28.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6- MonogDB Compass review data types screen before import
  prefs: []
  type: TYPE_NORMAL
- en: The final page confirms the data types. Finally, click **Import**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, you will connect to the MongoDB instance you have set up. If
    you are using an on-premises install (local install or Docker container), then
    your connection string will be something like `mongodb://<username>:<password>@<host>:<port>/<DatabaseName>.`
    If you are using Atlas, then your connection may look more like `mongodb+srv://<username>:<password>@<clusterName>.mongodb.net/<DatabaseName>?retryWrites=true&w=majority`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Establish a connection to MongoDB. For a self-hosted instance, such as a local
    install, this would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'This is equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: If your self-hosted MongoDB instance has a username and password, you must supply
    those.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `uri_parser` is a useful utility function that allows you to *validate*
    a MongoDB URL as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are connecting to **MongoDB** **Atlas** then your connection string
    will look more like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: In the previous recipes of this chapter, we leveraged a configuration file,
    for example a `database.cfg` file to store our connection information and hide
    our credentials. You should follow that recommendation as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your username or password contain special characters including the space
    chatacter (`: / ? # [ ] @ ! $ & '' ( ) * , ; = %`),then you will need to encode
    them. You can perform percent-encoding (percent-escape) using the `quote_plus()`
    function from the `urlib` Python library.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`username = urllib.parse.quote_plus(''user!*@'')`'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`password = urllib.parse.quote_plus(''pass/w@rd'')`'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For more information, you can read the documentation here [https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters](https://www.mongodb.com/docs/atlas/troubleshoot-connection/#std-label-special-pass-characters)
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Once connected, you can list all the databases available. In this example,
    I named the database `stock_data` and the collection `microsoft`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'You can list the collections that are available under the `stock_data` database
    using `list_collection_names`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can specify which collection to query. In this case, we are interested
    in the one called `microsoft`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, query the database into a pandas DataFrame using the `find` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to connect to the database, which we did by creating a client
    object with `MongoClient` for MongoDB instance. This will give you access to a
    set of methods, such as `list_databases_names(), list_databases()`, and additional
    attributes, such as `address` and `HOST`.
  prefs: []
  type: TYPE_NORMAL
- en: '`MongoClient()` accepts a connection string that should follow MongoDB''s URI
    format, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, the same can be accomplished by explicitly providing *host*
    (string) and *port* (numeric) positional arguments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The host string can either be the hostname or the IP address, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that to connect to your **localhost** that uses the default port (`27017`),
    you can establish a connection without providing any arguments, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, you can explicitly supply the named parameters as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explore these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`host` – this can be a hostname or IP address or a MongoDB URI. It can also
    take a Python list of hostnames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`password` – your assigned password. Read the note regarding special characters
    in the *Getting Ready* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`username` - your assigned username. Read the note regarding special characters
    in the *Getting Ready* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`document_class` – specify the class to use for the documents returned from
    your query. The default value is `dict`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tz_aware` – specify if datetime instances are time zone aware. The default
    is `False`, meaning they are naive (not time zone aware).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connect` – Whether to immediately connect to the MongoDB instance. The default
    value is `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For additional parameters, you can reference the official documentation page
    here [https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html](https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html).
  prefs: []
  type: TYPE_NORMAL
- en: Once the connection to your MongoDB instance has been established, you can specify
    which database to use, list its collections, and query any available collections.
    The overall flow in terms of navigation before you can query and retrieve the
    documents is to specify the **database**, select the **collection** you are interested
    in, and then submit the **query**.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, our database was called `stock_data`, which contained
    a collection called `microsoft`. You can have multiple collections in a database
    and multiple documents in a collection. To think of this in terms of relational
    databases, recall that a collection is like a table and that documents represent
    rows in that table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyMongo, you can specify a database using different syntax, as shown in
    the following code. Keep in mind that all these statements will produce a `pymongo.database.Database`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `get_database()` can take in additional arguments for
    the `codec_options`, `read_preference, write_concern`, and `read_concern` parameters,
    where the latter two are focused more on operations across nodes and how to determine
    if the operation was successful or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, once you have the `PyMongo` database object, you can specify a collection
    using different syntax, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The `get_collection()` method provides additional parameters, similar to `get_database()`.
  prefs: []
  type: TYPE_NORMAL
- en: The three syntax variations in the preceding example return a `pymongo.database.Collection`
    object, which comes with additional built-in methods and attributes such as `find`,
    `find_one`, `find_one_and_delete`, `find_one_and_reaplce`, `find_one_and_update`,
    `update`, `update_one`, `update_many`, `delete_one`, and `delete_many`, to name
    a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the different *retrieval* collection methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`find()` – retrieves multiple documents from a collection based on the submitted
    query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find_one()` – retrieves a single document form a collection based on the submitted
    query. If multiple documents match, then the first instance is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find_one_and_delete()` – finds the single document like `find_one`, but it
    deletes it from the collection, and returns it (the deleted document).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find_one_and_replace()` - finds a single document and replaces it with a new
    document, returning either the original or the replaced document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find_one_and_update()` - finds a single document and updates it, returning
    either the original or the updated document. This is different than the `find_one_and_replace`,
    since it updates the existing document instead of replacing the entire document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you are at the collection level, you can start querying the data. In the
    recipe, you used `find()`, which you can think of as doing something similar to
    a `SELECT` statement in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `How to do it…` section, in `step 5`, you queried the entire collection
    to retrieve all the documents using this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: The empty dictionary, `{}`, in `find()` represents our filtering criteria. When
    you pass an empty filter criterion with `{}`, you are retrieving everything. This
    resembles `SELECT *` in a SQL database. Alternatively, one could also use `collection.find()`
    to retrieve all documents.
  prefs: []
  type: TYPE_NORMAL
- en: To query documents in MongoDB you will need to be familiar with the MongoDB
    Query Language (MQL). You would normally write your query and pass it to the `find`
    method which acts like a filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'A query or a filter takes a **key-value** pair to return a select number of
    documents where the keys match the values specified. The following is an example
    of a query to find stocks with closing price great than 130:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The results objects is actually a **cursor** and does not contain the result
    set yet. You can loop through the cursor or convert into a DataFrame. Generally,
    when `collection.find()` is executed, it returns a **cursor** (more specifically,
    a `pymongo.cursor.Cursor` object). This cursor object is just a pointer to the
    result set of the query, which allows you to iterate over the results. You can
    then use a `for` loop or `next()` method (think of a Python iterator). However,
    in this recipe, instead of looping through our cursor object, we conveniently
    converted the entire result set into a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of retrieving the result set into a pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Notice from the output the `_id` column is added which was not part of the original
    `MSFT.csv` file. This was automatically added by MongoDB as a **unique identifier**
    for each document in a collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the query, which acts as a filter to only retrieve data
    where `close` values are greater than `130.`PyMongo allows you to pass a dictionary
    (key-value pair) that specifies which fields to be retrieved. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code we specified that `_id` should not be returned, and only
    `date`, `close`, and `volume` fields to be returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, in our previous example, notice the `$gt` used in the query. This represents
    greater than, and more specifically it translates to *“greater than 130”*. In
    MQL, operators start with the dollar sign `$`. Here is a sample list of commonly
    used operators in MQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$eq` - matches values that are equal to a specified value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:The query `{"close": {"$eq": 130}}` finds documents where the `close`
    field is exactly 130.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$gt` - matches values that are greater than a specified value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: The query `{"close": {"$gt": 130}}` finds documents where the close
    field is greater than 130.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$gte` - matches values that are greater than or equal to a specified value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: The query `{"close": {"$gte": 130}}` finds documents where the close
    field is greater than or equal to 130.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$lt` - matches values that are less than a specified value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: The query `{"close": {"$lt": 130}}` finds documents where the close
    field is less than 130.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$lte` - matches values that are less than or equal to a specified value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: The query `{"close": {"$lt3": 130}}` finds documents where the close
    field is less than or equal to 130.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$and` - joins query clauses with a logical **AND** operator. All conditions
    must be true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: The query `{"$and": [{"close": {"$gt": 130}}, {"volume": {"$lt": 20000000}}]}`
    finds documents where the close field is greater than 130 **AND** the volume is
    less than 20,000,000.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$or`- joins query clauses with a logical **OR** operator. At least one condition
    must be true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Example: The query {"$or": [{"close": {"$gt": 135}}, {"volume": {"$gt": 30000000}}]}`
    finds documents where the close field is greater than 135 **OR** the volume is
    greater than 30,000,000.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$in`- matches values specified in an array (list)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:The query `{"date": {"$in": [datetime.datetime(2019, 9, 4), datetime.datetime(2019,
    9, 5), datetime.datetime(2019, 9, 6)]}}` finds documents where the date field
    matches any of the specified dates: September 4, 2019; September 5, 2019; September
    6, 2019.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For a more comprehensive list of operators in MQL you can visit the official
    documentation here [https://www.mongodb.com/docs/manual/reference/operator/query/](https://www.mongodb.com/docs/manual/reference/operator/query/)
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are different ways to retrieve data from MongoDB using `PyMongo`. In
    the previous section, we used `db.collection.find()`, which always returns a cursor.
    As we discussed earlier, `find()` returns all the matching documents that are
    available in the specified collection. If you want to return the first occurrence
    of matching documents, then `db.collection.find_one()` would be the best choice
    and would return a **dictionary** object, not a cursor. Keep in mind that this
    only returns one document, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'When it comes to working with cursors, there are several ways you can traverse
    through the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting into a pandas DataFrame using `pd.DataFrame(cursor)`, as shown in
    the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting into a Python **list** or **tuple**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also convert the **Cursor** object into a Python list and then convert
    that into a pandas DataFrame, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `next()` to get move the pointer to the next item in the result set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '**Looping** through the object, for example, with a `for` loop:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code will loop through the entire result set. If for example you
    want to loop thorugh the first 5 records you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Specifying an **index**. Here, we are printing the first value:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you provided a slice, such as `cursor[0:1]`, which is a range,
    then it will return a cursor object (not a document).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information on the **PyMongo** API, please refer to the official documentation,
    which you can find here: [https://pymongo.readthedocs.io/en/stable/index.html](ch004.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from a time series databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A time series database, a type of **NoSQL** database, is optimized for time-stamped
    or time series data and provides improved performance, especially when working
    with large datasets containing IoT data or sensor data. In the past, common use
    cases for time series databases were mostly associated with financial stock data,
    but their use cases have expanded into other disciplines and domains. In this
    recipe you will explore three popular time series databases: **InfluxDB**, **TimescaleDB,**
    and **TDEngine**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**InfluxDB** is a popular open source time series database with a large community
    base. In this recipe, we will be using InfluxDB''s latest release as of this writing;
    that is, version 2.7.10\. The most recent InfluxDB releases introduced the Flux
    data scripting language, which you will use with the Python API to query our time
    series data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TimescaleDB**, on the other hand, is an extension of PostgreSQL specifically
    optimized for time series data. It leverages the power and flexibility of PostgreSQL
    while providing additional features tailored for handling time-stamped information
    efficiently. One advantage with TimescaleDB is that you can leverage SQL for querying
    the data. TimescaleDB is an open-source time series database and in this recipe,
    we will be using TimescaleDB''s latest release as of this writing, version 2.16.1'
  prefs: []
  type: TYPE_NORMAL
- en: '**TDEngine** is an open-source time series database designed for Internet of
    Things (IoT), big data, and real-time analytics. Like TimescaleDB, TDEngine utilizes
    SQL for querying the data. In this recipe, we will be working with TDEngine version
    3.3.2.0, the latest as of this writing.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This recipe assumes that you have access to a running instance of InfluxDB,
    TimeseriesDB, or TDEngine. You will install the appropriate libraries to connect
    and interact with these databases using Python. For **InfluxDB** V2, you will
    need to install `influxdb-client`; for **TimescaleDB,** you will need to install
    the PostgreSQL Python library `psycopg2` (recall in the *Reading from a relational
    database* recipe of this chapter, we used `psycopg3`); and finally, for **TDEngine,**
    you will need to install `taospy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install these libraries using `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'To install using **conda** use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not have access to these databases, then the fastest way to get up
    and running is via Docker. The following are example commands for InlfuxDB, TimescaleDB,
    and TDEngine:'
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB Docker Container
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To create a InfluxDB container you will need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: For more information you can access the official docker hub page [https://hub.docker.com/_/influxdb](https://hub.docker.com/_/influxdb)
  prefs: []
  type: TYPE_NORMAL
- en: Once the **influxdb-ch3** container is up and running you can navigate to `http://localhost:8086`
    using your favorite browser and continue the setup, such as username, password,
    initial organization name, and initial bucket name.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will use the **National Oceanic and Atmospheric Administration**
    (**NOAA**) water level sample data from August 17, 2019, to September 17, 2019,
    for Santa Monica and Coyote Creek.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Data Explorer UI, you can run the following **Flux** query to load the
    sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: In the previous snippet, the NOAA dataset was loaded into the `tscookbook` bucket
    created during the initial setup.
  prefs: []
  type: TYPE_NORMAL
- en: For instructions on how to load the sample data, or other provided *sample datasets*,
    please refer to the InfluxDB official documentation at [https://docs.influxdata.com/influxdb/v2/reference/sample-data/](https://docs.influxdata.com/influxdb/v2/reference/sample-data/)
  prefs: []
  type: TYPE_NORMAL
- en: TimescaleDB Docker Container
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To create a TimescaleDB container you will need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: For more information you can access the official docker hub page [https://hub.docker.com/r/timescale/timescaledb](https://hub.docker.com/r/timescale/timescaledb)
  prefs: []
  type: TYPE_NORMAL
- en: Once the **timescaledb-ch3** container is up and running, you can load the `MSFT.csv`
    file using the same instructions as in the `Getting Ready` section of the *Reading
    data from a relational database* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Note, the default username is `postgres` and the password is whatever password
    you setup in the docker command.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3\. – DBeaver TimescaleDB/Postgres connection settings (should look
    similar to Figure 3.1)](img/file29.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. – DBeaver TimescaleDB/Postgres connection settings (should look similar
    to Figure 3.1)
  prefs: []
  type: TYPE_NORMAL
- en: Since TimescaleDB is based on PostgreSQL, it also defaults to port 5432\. So,
    if you are already running a local instance of PostgreSQL database which defaults
    to port 5432 you may run into an issue with TimescaleDB. In such case, you may
    opt to change the docker run configuration and change the port.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: TDEngine Docker Container [TO BE DELETED SECTION]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To create a TDEngine container you will need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: For more information you can access the official docker hub page [https://hub.docker.com/r/tdengine/tdengine](https://hub.docker.com/r/tdengine/tdengine)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the **tdengine-ch3** container is up and running, you can create a demo
    dataset by running the `taosBenchmark` command from inside the container shell.
    Here are the steps to access the shell from inside the running container to run
    the needed command to install and setup the demo data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Once the demo set is created you can exit out of the terminal. You can now leverage
    DBeaver to verify the data is created. You can use the same instructions as in
    the `Getting Ready` section of the *Reading data from a relational database* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Note, the default username is `root` and the default password is `taosdata`
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3\. – DBeaver TDEngine connection settings](img/file30.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. – DBeaver TDEngine connection settings
  prefs: []
  type: TYPE_NORMAL
- en: You should now see a `test` **database** created, and a `meters` **supertable**
    with 10,000 **subtables** named `d0` to `d9999` and each table contains around
    10,000 rows and four columns (`ts`, `current`, `voltage`, and `phase`). You may
    not be able to see the `meters` supertable in the DBeaver navigator pane, but
    if you run the following SQL query “`SELECT COUNT(*) FROM test.meters;”` which
    should output 100,000,000 rows in the meters supertable (10,000 subtables multiplied
    by 10,000 rows in each).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This recipe will demonstrate how you can connect and interact with three popular
    time series database systems.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will be leveraging the `Influxdb_client` Python SDK for InfluxDB 2.x, which
    provides support for pandas DataFrames in terms of both read and write functionality.
    Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'To establish your connection using `InfluxDBClient(url="http://localhost:8086",
    token=token)`, you will need to define the `token`, `org`, and `bucket` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Think of a bucket as database in relational databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you are ready to establish your connection by passing the `url`, `token`,
    and `org` parameters to `InlfuxDBClient()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you will instantiate `query_api`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass your Flux query and request the results to be in pandas DataFrame format
    using the `query_data_frame` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding Flux script, selected the measurement `h2o_temparature` and
    where the location is `coyote_creek`. Let''s inspect the DataFrame. Pay attention
    to the data types in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to retrieve only the time and degrees columns, you can update the
    Flux query as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: The original dataset contains 15,258 observations collected every six (6) minutes
    between the two stations (locations). The moving average is calculated over 120
    data points. It is important to understand the graduality of your dataset. The
    final DataFrame contains 3885 records.
  prefs: []
  type: TYPE_NORMAL
- en: TimescaleDB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since TimescaleDB is based on PostgreSQL and we have already installed **psycopg2**
    , retrieving and querying your data should be similar to the approach used in
    the recipe *Reading data from a relational database*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a brief on how this can be done using pandas from_sql:'
  prefs: []
  type: TYPE_NORMAL
- en: Import SQLAlchemy and pandas
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Create the engine object with the proper connection string to the PostgreSQL
    backend
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, use the `read_sql` method to retrieve the result set of your query
    into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: TimescaleDB offers many advantages over PostgreSQL and you will explore some
    of these in *Chapter 5*, *Persisting Time Series Data to Databases*. Still, querying
    TimescaleDB brings a similar experience to those familiar with SQL and PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: TDEngine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this recipe let’s update our configuration file `database.cfg` from the
    *Technical Requirements* to include a [TDENGINE] section as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: You will start by establishing a connection to the TDEngine server, and then
    run a query against the demo dataset from the **taosBenchmark** described in the
    *Getting Read* section.
  prefs: []
  type: TYPE_NORMAL
- en: Start by importing the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: You will create a Python dictionary to store all the parameter values required
    to establish a connection to the database, such as `url`, `user` and `password`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Establish a connection to the server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Run the following query and execute the query using the `query` method from
    the connection object conn
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: You can verify the number of rows and column names in the result set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: The `results.data` contains the values from the result set but without column
    headers. Before we write our result set into a pandas DataFrame we need to capture
    the column names in a list from `results.fields:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both TimescaleDB and TDEngine use SQL to query the data, while InfluxDB utilizes
    their proprietary query language, Flux.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB 1.8x introduced the **Flux** query language as an alternative query
    language to **InfluxQL**, with the latter having a closer resemblance to SQL.
    InfluxDB 2.0 introduced the concept of **buckets**, which is where data is stored,
    whereas InfluxDB 1.x stored data in databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we started by creating an instance of `InfluxDbClient`, which
    later gave us access to the `query_api` method, which gives additional methods,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query()` returns the result as a **FluxTable**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_csv()` returns the result as a CSV iterator (CSV reader).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_data_frame()` returns the result as a pandas DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_data_frame_stream()` returns a stream of pandas DataFrames as a generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_raw()` returns the result as raw unprocessed data in `s` string format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_stream()` is similar to `query_data_frame_stream` but returns a stream
    of `FluxRecord` as a generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the recipe, you used `client.query_api()` to fetch the data, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: You used `query_data_frame`, which executes a synchronous Flux query and returns
    a pandas DataFrame with which you are familiar.
  prefs: []
  type: TYPE_NORMAL
- en: Notice we had to use the `pivot` function in the Flux query to transform the
    results into a tabular format suitable for pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break the preceding line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: The `pivot()` is used to reshape the data and transform it from a long to a
    wide format.
  prefs: []
  type: TYPE_NORMAL
- en: The `rowKey` parameter specifies which column to use as the unique identifier
    for each row. In our example, we specified `["_time"]` so each row will have a
    unique timestamp
  prefs: []
  type: TYPE_NORMAL
- en: The `columnKey` parameter specifies which column’s values will be used to create
    new columns in the output. In our example, we specified `["_field"]` to create
    columns from field names
  prefs: []
  type: TYPE_NORMAL
- en: The `valueColumn` parameter specifies which column contains the values, we specified
    `"_value"` to fill the new columns with corresponding values.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When working with **InfluxDB** and the `influxdb-client,` there is an additional
    argument that you can use to create the DataFrame index. In `query_data_frame(),`
    you can pass a list as an argument to the `data_frame_index` parameter, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: This returns a time series DataFrame with a `DatetimeIndex` (`_time`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are new to InfluxDB Flux query language, check out the *Get Started with
    Flux* official documentation at [https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/](https://docs.influxdata.com/influxdb/v2.0/query-data/get-started/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer to the official **InfluxDB-Client** Python library documentation
    on GitHub at [https://github.com/influxdata/influxdb-client-python](https://github.com/influxdata/influxdb-client-python).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about the **TDEngine** python library, you refer to the official
    documentation at [https://docs.tdengine.com/cloud/programming/client-libraries/python/](https://docs.tdengine.com/cloud/programming/client-libraries/python/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
