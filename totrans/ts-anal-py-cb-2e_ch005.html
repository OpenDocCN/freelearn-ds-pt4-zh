<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Time Series Analysis with Python Cookbook, 2E - Second Edition</title>
<link rel="stylesheet" type="text/css" href="../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/><link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content"><section id="persisting-time-series-data-to-files" class="level1 pkt" data-number="5">
<h1 data-number="5">4 Persisting Time Series Data to Files</h1>
<section id="join-our-book-community-on-discord-3" class="level2" data-number="5.1">
<h2 data-number="5.1">Join our book community on Discord</h2>
<p>
<img style="width:15rem" src="../media/file0.png" width="200" height="200"/>
</p>
<p><a href="https://packt.link/zmkOY">https://packt.link/zmkOY</a></p>
<p>In this chapter, you will use the <strong>pandas</strong> library to persist your <strong>time series DataFrames</strong> to different file formats, such as <strong>CSV</strong>, <strong>Excel</strong>, <strong>Parquet</strong>, and <strong>pickle</strong> files. When performing analysis or data transformations on DataFrames, you essentially leverage pandas' in-memory analytics capabilities, offering great performance. However, being in memory means the data can easily be lost since it has not yet persisted in disk storage.</p>
<p>When working with DataFrames, you will need to persist your data for future retrieval, creating backups, or sharing your data with others. The <strong>pandas</strong> library is bundled with a rich set of writer functions to persist your in-memory DataFrame (or series) to disk in various file formats. These writer functions allow you to store data on a local drive or a remote server location, such as a cloud storage filesystem, including <strong>Google Drive</strong>, <strong>AWS S3</strong>, <strong>Azure Blob Storage</strong>, and <strong>Dropbox</strong>.</p>
<p>In this chapter, you will explore writing to different file formats locally (on-premises) and cloud storage locations like on Amazon Web Services (AWS), Google Cloud, and Azure.</p>
<p>Here are the recipes that will be covered in this chapter:</p>
<ul>
<li>Time series data serialization with <code>pickle</code></li>
<li>Writing to CSV and other delimited files</li>
<li>Writing data to an Excel file</li>
<li>Storing Data to a Cloud Storage (AWS, GCP, and Azure)</li>
<li>Writing Large Datasets</li>
</ul>
<section id="technical-requirements-3" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1">Technical requirements</h3>
<p>In this chapter and beyond, we will extensively use pandas 2.2.2 (released April 10, 2023).</p>
<p>Throughout our journey, you will be installing several Python libraries to work in conjunction with pandas. These are highlighted in the <em>Getting ready</em> section for each recipe. You can also download Jupyter notebooks from the GitHub repository (<a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>) to follow along. You can download the datasets used in this chapter here <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch4">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch4</a></p>
</section>
</section>
<section id="serializing-time-series-data-with-pickle" class="level2" data-number="5.2">
<h2 data-number="5.2">Serializing time series data with pickle</h2>
<p>When working with data in Python, you may want to persist Python data structures or objects, such as a pandas DataFrame, to disk instead of keeping it in memory. One technique is to serialize your data into a byte stream to store it in a file. In Python, the <strong>pickle</strong> module is a popular approach to object serialization and de-serialization (the reverse of serialization), also known as <em>pickling</em> and <em>unpickling</em>.</p>
<section id="getting-ready-12" class="level4" data-number="5.2.0.1">
<h4 data-number="5.2.0.1">Getting ready</h4>
<p>The <code>pickle</code> module comes with Python, so no additional installation is needed.</p>
<p>In this recipe, we will explore two different methods for serializing the data, commonly referred to as <strong>pickling</strong>.</p>
<p>You will be using the COVID-19 dataset provided by the <em>COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE)</em> at <em>Johns Hopkins University</em>, which you can download from the official GitHub repository here: <a href="https://github.com/CSSEGISandData/COVID-19">https://github.com/CSSEGISandData/COVID-19</a>. Note that John Hopkins University is no longer updating the dataset as of March 10, 2023.</p>
</section>
<section id="how-to-do-it-11" class="level4" data-number="5.2.0.2">
<h4 data-number="5.2.0.2">How to do it…</h4>
<p>You will write to a <strong>pickle</strong> file using pandas' <code>DataFrame.to_pickle()</code> function and then explore an alternative option using the <code>pickle</code> library directly.</p>
<section id="writing-to-a-pickle-file-using-pandas" class="level5" data-number="5.2.0.2.1">
<h5 data-number="5.2.0.2.1">Writing to a pickle file using pandas</h5>
<p>You will start by reading the COVID-19 time series data into a DataFrame, making some transformations, and then persisting the results to a <code>pickle</code> file for future analysis. This should resemble a typical scenario for persisting data that is still a work in progress (in terms of analysis):</p>
<ol>
<li>To start, let's load the CSV data into a pandas DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
file = \
Path('../../datasets/Ch4/time_series_covid19_confirmed_global.csv')
df = pd.read_csv(file)
df.head()</code></pre>
</div>
<p>The preceding code will display the first five rows of the DataFrame:</p>
<figure>
<img src="../media/file32.png" alt="Figure 4.1: The first five rows of the COVID-19 confirmed global cases" width="1405" height="234"/><figcaption aria-hidden="true">Figure 4.1: The first five rows of the COVID-19 confirmed global cases</figcaption>
</figure>
<p>You can observe from the output that this is a wide DataFrame with 1147 columns, where each column represents the data's collection date, starting from <strong>1/22/20</strong> to <strong>3/9/23</strong>.</p>
<ol>
<li>Let's assume that part of the analysis is to focus on United States and only data collected in the summer of 2021 (June, July, August, and September). You will transform the DataFrame by applying the necessary filters, and then unpivot the data so that the dates are in rows as opposed to columns (converting from a wide to a long format):</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code># filter data where Country is United States
df_usa = df[df['Country/Region'] == 'US']
# filter columns from June to end of September
df_usa_summer = df_usa.loc[:, '6/1/21':'9/30/21']
# unpivot using pd.melt()
df_usa_summer_unpivoted = \
    pd.melt(df_usa_summer,
            value_vars=df_usa_summer.columns,
            value_name='cases',
            var_name='date').set_index('date')
df_usa_summer_unpivoted.index = \
    pd.to_datetime(df_usa_summer_unpivoted.index, format="%m/%d/%y")</code></pre>
</div>
<ol>
<li>Inspect the <code>df_usa_summer_unpivoted</code> DataFrame and print the first five records:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>df_usa_summer_unpivoted.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 122 entries, 2021-06-01 to 2021-09-30
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   cases   122 non-null    int64
dtypes: int64(1)
memory usage: 1.9 KB
df_usa_summer_unpivoted.head()
&gt;&gt;
               cases
date               
2021-06-01  33407540
2021-06-02  33424131
2021-06-03  33442100
2021-06-04  33459613
2021-06-05  33474770</code></pre>
</div>
<p>You filtered the dataset and transformed it from a wide DataFrame to a long time series DataFrame.</p>
<ol>
<li>Let’s say you are now satisfied with the dataset and ready to pickle (serialize) the dataset. You will write the DataFrame to a <code>covid_usa_summer_2020.pkl</code> file using the <code>DataFrame.to_pickle()</code> function:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>output = \
Path('../../datasets/Ch4/covid_usa_summer_2021.pkl')
df_usa_summer_unpivoted.to_pickle(output)</code></pre>
</div>
<p>Pickling preserves the schema of the DataFrame. When you ingest the pickled data again (de-serialization), you will return the DataFrame in its original construct, for example, with a <code>DatetimeIndex</code> type.</p>
<ol>
<li>Read the pickle file using the <code>pandas.read_pickle()</code> reader function and inspect the DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>unpickled_df = pd.read_pickle(output)
unpickled_df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 122 entries, 2021-06-01 to 2021-09-30
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   cases   122 non-null    int64
dtypes: int64(1)
memory usage: 1.9 KB</code></pre>
</div>
<p>From the preceding example, you were able to de-serialize the data using <code>pandas.read_pickle()</code> into a DataFrame, with all the previously committed transformations and datatypes preserved.</p>
</section>
<section id="writing-a-pickle-file-using-the-pickle-library" class="level5" data-number="5.2.0.2.2">
<h5 data-number="5.2.0.2.2">Writing a pickle file using the pickle library</h5>
<p>Python comes shipped with the <strong>pickle</strong> library, which you can import and use to serialize (pickle) objects using <code>dump</code> (to write) and <code>load</code> (to read). In the following steps, you will use <code>pickle.dump()</code> and <code>pickle.load()</code> to serialize and then de-serialize the <code>df_usa_summer_unpivoted</code> DataFrame.</p>
<ol>
<li>Import the <code>pickle</code> library:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pickle</code></pre>
</div>
<ol>
<li>You then persist the <code>df_usa_summer_unpivoted</code> DataFrame using the <code>dump()</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>file_path = \
Path('../../datasets/Ch4/covid_usa_summer_2021_v2.pkl')
with open(file_path, "wb") as file:
    pickle.dump(df_usa_summer_unpivoted, file)</code></pre>
</div>
<p>Notice the mode used is <code>“wb”</code> because we are writing in binary mode (written as raw bytes).</p>
<ol>
<li>You can read the file and inspect the DataFrame using the <code>load()</code> method. Notice in the following code that the ingested object is a pandas DataFrame, even though you used <code>pickle.load()</code> instead of <code>Pandas.read_pickle()</code>. This is because pickling preserved the schema and data structures:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>with open(file_path, "rb") as file:
    df = pickle.load(file)
type(df)
&gt;&gt;
pandas.core.frame.DataFrame</code></pre>
</div>
<p>Notice the mode used is <code>“rb”</code> because we are reading in binary mode (read as raw bytes).</p>
</section>
</section>
<section id="how-it-works-11" class="level4" data-number="5.2.0.3">
<h4 data-number="5.2.0.3">How it works…</h4>
<p>In Python, pickling is the process of serializing any Python object. More concretely, it uses a binary serialization protocol to convert objects into binary information, which is not a human-readable format. The protocol allows us to reconstruct (de-serialize) the pickled file, binary format, into its original content without losing valuable information. As in the preceding examples, we confirmed that a time series DataFrame, when reconstructed (de-serialization), returned to its exact form (schema).</p>
<p>The pandas <code>DataFrame.to_pickle()</code> function has two additional parameters that are important to know. The first is the <code>compression</code> parameter, which is also available in other writer functions such as <code>to_csv()</code>, <code>to_json()</code>, and <code>to_paraquet()</code>, to name a few.</p>
<p>In the case of the <code>DataFrame.to_pickle()</code> function, the default compression value is set to <code>infer</code>, which lets pandas determine which compression mode to use based on the file extension provided. In the previous example, we used <code>DataFrame.to_pickle(output)</code>, where <code>output</code> was defined with a <code>.pkl</code> file extension, as in <code>covid_usa_summer_2020.pkl</code>. If you change it to <code>covid_usa_summer_2020.zip</code>, the output will be a compressed binary serialized file stored in ZIP format. You can try the following example:</p>
<div class="C0-SHCodePACKT">
<pre><code>zip_output =\
Path('../../datasets/Ch4/covid_usa_summer_2021.zip')
# Write the Dataframe
df_usa_summer_unpivoted.to_pickle(zip_output)
# Read the Dataframe
pd.read_pickle(zip_output)</code></pre>
</div>
<p>Other supported compression modes include <strong>gzip</strong>, <strong>bz2</strong>, <strong>tar</strong>, and <strong>xz</strong>.</p>
<p>The second parameter is <strong>protocol</strong>. By default, the <code>DataFrame.to_pickle()</code> writer function uses the highest protocol, which, as of this writing, is set to 5. According to the Pickle documentation, there are six (6) different protocols to choose from when pickling, starting from protocol version 0 to the latest protocol version, 5.</p>
<p>Outside of pandas, you can check what is the highest protocol configuration by using the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>pickle.HIGHEST_PROTOCOL
&gt;&gt; 5</code></pre>
</div>
<p>Similarly, by default, <code>pickle.dump()</code> uses the <code>HIGHEST_PROTOCOL</code> value if no other value was provided. The construct looks like the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>with open(output, "wb") as file:
    pickle.dump(df_usa_summer_unpivoted,
                file,
                pickle.HIGHEST_PROTOCOL)
with open(output, "wb") as file:
    pickle.dump(df_usa_summer_unpivoted,
                file,
                5)</code></pre>
</div>
<p>The preceding two code snippets are equivalent.</p>
</section>
<section id="theres-more-11" class="level4" data-number="5.2.0.4">
<h4 data-number="5.2.0.4">There's more…</h4>
<p>One of the advantages of pickling a binary serialization method is that we can pretty much pickle most Python objects, whether a Python dictionary, a machine learning model, a Python function, or a more complex data structure, such as a pandas DataFrame. However, there are some limitations on certain objects, such as lambda and nested functions.</p>
<p>Let's examine how you can pickle a function and its output. You will create a <code>covid_by_country</code> function that takes three arguments: <em>the CSV file to read</em>, the <em>number of days back</em>, and the <em>country</em>. The function will return a time series DataFrame. You will then pickle both the function, the function's output, and it’s plot as well:</p>
<div class="C0-SHCodePACKT">
<pre><code>def covid_by_country(file, days, country):
    ts = pd.read_csv(file)
    ts = ts[ts['Country/Region'] == country]
    final = ts.iloc[:, -days:].sum()
    final.index = pd.to_datetime(final.index,
                                format="%m/%d/%y")
    return final
file = \
Path('../../datasets/Ch4/time_series_covid19_confirmed_global.csv')
us_past_120_days = covid_by_country(file, 200, 'US')
plot_example = \
us_past_120_days.plot(title=f'COVID confirmed case for US',
                xlabel='Date',
                ylabel='Number of Confirmed Cases');</code></pre>
</div>
<p>The function would output the following plot:</p>
<figure>
<img src="../media/file33.png" alt="Figure 4.:– Output of the covid_by_country function" width="701" height="529"/><figcaption aria-hidden="true">Figure 4.:– Output of the covid_by_country function</figcaption>
</figure>
<p>Before pickling your objects, you can further enhance the content by adding additional information to remind you what the content is all about. In the following code, you will serialize the function and the returned DataFrame with additional information (known as <strong>metadata</strong>) encapsulated using a Python dictionary:</p>
<div class="C0-SHCodePACKT">
<pre><code>from datetime import datetime
metadata = {
    'date': datetime.now(),
    'data': '''
        COVID-19 Data Repository by the
        Center for Systems Science and Engineering (CSSE)
        at Johns Hopkins University'
        ''',
    'author': 'Tarek Atwan',
    'version': 1.0,
    'function': covid_by_country,
    'example_code' : us_past_120_days,
    'example_code_plot': plot_example
}
file_path = Path('../../datasets/Ch4/covid_data.pkl')
with open(file_path, 'wb') as file:
    pickle.dump(metadata, file)</code></pre>
</div>
<p>To gain a better intuition on how this works, you can load the content and de-serialize using <code>pickle.load()</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code>with open(output, 'rb') as file:
    content = pickle.load(file)
content.keys()
&gt;&gt;
dict_keys(['date', 'data', 'author', 'version', 'function', 'example_df', 'example_plot'])</code></pre>
</div>
<p>You can retrieve and use the function, as shown in the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>file_path =\
Path('../../datasets/Ch4/time_series_covid19_confirmed_global.csv')
loaded_func = content['function']
loaded_func(file_path, 120, 'China').tail()
&gt;&gt;
2023-03-05    4903524
2023-03-06    4903524
2023-03-07    4903524
2023-03-08    4903524
2023-03-09    4903524
dtype: int64</code></pre>
</div>
<p>You can also retrieve the previous DataFrame stored for the US:</p>
<div class="C0-SHCodePACKT">
<pre><code>loaded_df = content['example_df']
loaded_df.tail()
&gt;&gt;
2023-03-05    103646975
2023-03-06    103655539
2023-03-07    103690910
2023-03-08    103755771
2023-03-09    103802702
dtype: int64</code></pre>
</div>
<p>You can also load the view of the figure you just stored. The following code will display the plot like that in <em>Figure 4.2</em>:</p>
<div class="C0-SHCodePACKT">
<pre><code>loaded_plot = content['example_plot']
loaded_plot.get_figure()</code></pre>
</div>
<p>The preceding examples demonstrate how pickling can be useful to store objects and additional <strong>metadata</strong> information. This can be helpful when storing a work-in-progress or performing multiple experiments and wanting to keep track of them and their outcomes. A similar approach can be taken when working with machine learning experiments, as you can store your model and any related information around the experiment and its outputs.</p>
</section>
<section id="see-also-12" class="level4" data-number="5.2.0.5">
<h4 data-number="5.2.0.5">See also</h4>
<ul>
<li>For more information about <code>Pandas.DataFrame.to_pickle</code>, please visit this page: <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html</a>.</li>
<li>For more information about the Python Pickle module, please visit this page: <a href="https://docs.python.org/3/library/pickle.html">https://docs.python.org/3/library/pickle.html</a>.</li>
</ul>
</section>
</section>
<section id="writing-to-csv-and-other-delimited-files" class="level2" data-number="5.3">
<h2 data-number="5.3">Writing to CSV and other delimited files</h2>
<p>In this recipe, you will export a DataFrame as a CSV file and leverage the different parameters in the <code>DataFrame.to_csv()</code> writer function.</p>
<section id="getting-ready-13" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1">Getting ready</h3>
<p>The file is provided in the GitHub repository for this book, which you can find here: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>. The file you will be working with is named <code>movieboxoffice.csv </code>which you read first to create your DataFrame.</p>
<p>To prepare for this recipe, you will read the file into a DataFrame with the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
filepath = Path('../../datasets/Ch4/movieboxoffice.csv')
movies = pd.read_csv(filepath,
                 header=0,
                 parse_dates=[0],
                 index_col=0,
                 usecols=['Date',
                          'Daily'],
                date_format="%d-%b-%y")
movies.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 128 entries, 2021-04-26 to 2021-08-31
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   Daily   128 non-null    object
dtypes: object(1)
memory usage: 2.0+ KB</code></pre>
</div>
<p>You now have a time series DataFrame with an index of type <code>DatetimeIndex</code></p>
</section>
<section id="how-to-do-it-12" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2">How to do it…</h3>
<p>Writing a DataFrame to a CSV file is pretty straightforward with pandas. The DataFrame object has access to many writer methods, such as <code>.to_csv</code>, which is what you will be using in the following steps:</p>
<ol>
<li>You will use the pandas DataFrame writer method to persist the DataFrame as a CSV file. The method has several parameters, but at a minimum, all you need is to pass a file path and filename:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>output = Path('../../datasets/Ch4/df_movies.csv')
movies.to_csv(output)</code></pre>
</div>
<p>The CSV file created is <strong>comma-delimited</strong> by default.</p>
<ol>
<li>To change the delimiter, use the <code>sep</code> parameter and pass in a different argument. In the following code, you are creating a pipe <code>(|)</code> delimited file:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>output = Path('../../datasets/Ch4/piped_df_movies.csv')
movies.to_csv(output, sep='|')</code></pre>
</div>
<ol>
<li>Read the pipe-delimited file and inspect the resulting DataFrame object:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>movies_df = pd.read_csv(output, sep='|')
movies_df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 128 entries, 0 to 127
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   Date    128 non-null    object
 1   Daily   128 non-null    object
dtypes: object(2)
memory usage: 2.1+ KB</code></pre>
</div>
<p>Notice from the preceding output that some information was lost when reading the CSV file. For example, the <code>Date</code> column in the original DataFrame “<code>movies</code>” was not a column but an index of type <code>DatetimeIndex</code>. The current DataFrame “<code>moveis_df</code>” does not have an index of type <code>DatetimeIndex</code> (the index is now a <code>RangeIndex</code> type, which is just a range for the row numbers). This means you will need to configure the <code>read_csv()</code> function and pass the necessary arguments to parse the file appropriately (this is in contrast to when reading a <strong>pickled</strong> file, as demonstrated from the preceding recipe, <em>Serializing time series data with pickle</em>).</p>
<p>Generally, CSV file formats do not preserve index type or column datatype information.</p>
</section>
<section id="how-it-works-12" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3">How it works…</h3>
<p>The default behavior for <code>DataFrame.to_csv()</code> is to write a <strong>comma-delimited</strong> CSV file based on the default <code>sep</code> parameter, which is set to <code>","</code>. You can overwrite this by passing a different delimiter, such as tab <code>("\t")</code>, <code>pipe ("|")</code>, or semicolon <code>(";")</code>.</p>
<p>The following code shows examples of different <strong>delimiters</strong> and their representations:</p>
<div class="C0-SHCodePACKT">
<pre><code># tab "\t"
Date    DOW Daily   Avg To Date Day Estimated
2019-04-26  Friday  157461641   33775   157461641   1   False
2019-04-27  Saturday    109264122   23437   266725763   2   False
2019-04-28  Sunday  90389244    19388   357115007   3   False
# comma ","
Date,DOW,Daily,Avg,To Date,Day,Estimated
2019-04-26,Friday,157461641,33775,157461641,1,False
2019-04-27,Saturday,109264122,23437,266725763,2,False
2019-04-28,Sunday,90389244,19388,357115007,3,False
# semicolon ";"
Date;DOW;Daily;Avg;To Date;Day;Estimated
2019-04-26;Friday;157461641;33775;157461641;1;False
2019-04-27;Saturday;109264122;23437;266725763;2;False
2019-04-28;Sunday;90389244;19388;357115007;3;False
# pipe "|"
Date|DOW|Daily|Avg|To Date|Day|Estimated
2019-04-26|Friday|157461641|33775|157461641|1|False
2019-04-27|Saturday|109264122|23437|266725763|2|False
2019-04-28|Sunday|90389244|19388|357115007|3|False</code></pre>
</div>
</section>
<section id="theres-more-12" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4">There's more…</h3>
<p>Notice in the preceding example that the comma-separated string values are not encapsulated within double quotes (<code>""</code>). What will happen if our string object contains commas (<code>,</code>) and we write it to a comma-separated CSV file? Let's see how pandas handles this scenario.</p>
<p>In the following code, we will create a <code>person</code> DataFrame:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
person = pd.DataFrame({
     'name': ['Bond, James', 'Smith, James', 'Bacon, Kevin'],
     'location': ['Los Angeles, CA', 'Phoenix, AZ', 'New York, NY'],
     'net_worth': [10000, 9000, 8000]
    })
print(person)
&gt;&gt;
           name         location  net_worth
0   Bond, James  Los Angeles, CA      10000
1  Smith, James      Phoenix, AZ       9000
2  Bacon, Kevin     New York, NY       8000</code></pre>
</div>
<p>Now, export the DataFrame to a CSV file. You will specify <code>index=False</code> to ignore the index (row numbers) when exporting:</p>
<div class="C0-SHCodePACKT">
<pre><code>person.to_csv('person_a.csv', index=False)</code></pre>
</div>
<p>If you inspect a <code>person_a.csv</code> file, you will see the following representation (notice the double quotes added by pandas):</p>
<div class="C0-SHCodePACKT">
<pre><code>name,location,net_worth
"Bond, James","Los Angeles, CA",10000
"Smith, James","Phoenix, AZ",9000
"Bacon, Kevin","New York, NY",8000</code></pre>
</div>
<p>The <code>to_csv()</code> function has a <code>quoting</code> parameter with a default value set to <code>csv.QUOTE_MINIMAL</code>. This comes from the Python <code>csv</code> module, which is part of the Python installation. The <code>QUOTE_MINIMAL</code> argument only quotes fields that contain special characters, such as a comma (<code>","</code>).</p>
<p>The <code>csv</code> module provides four constants we can pass as arguments to the <code>quoting</code> parameter within the <code>to_csv()</code> function. These include the following:</p>
<ul>
<li><code>csv.QUOTE_ALL</code>: Quotes all the fields, whether numeric or non-numeric</li>
<li><code>csv.QUOTE_MINIMAL</code>: The default option in the <code>to_csv()</code> function, which quotes values that contain special characters</li>
<li><code>csv.QUOTE_NONNUMERIC</code>: Quotes all non-numeric fields</li>
<li><code>csv.QUOTE_NONE</code>: To not quote any field</li>
</ul>
<p>To better understand how these values can impact the output CSV, you will test passing different quoting arguments in the following example. This is done using the <code>person</code> DataFrame:</p>
<div class="C0-SHCodePACKT">
<pre><code>import csv
person.to_csv('person_b.csv',
               index=False,
               quoting=csv.QUOTE_ALL)
person.to_csv('person_c.csv',
               index=False,
               quoting=csv.QUOTE_MINIMAL)
person.to_csv('person_d.csv',
               index=False,
               quoting= csv.QUOTE_NONNUMERIC)
person.to_csv('person_e.csv',
               index=False,
               quoting= csv.QUOTE_NONE, escapechar='\t')</code></pre>
</div>
<p>Now, if you open and inspect these files, you should see the following representations:</p>
<div class="C0-SHCodePACKT">
<pre><code>person_b.csv
"name","location","net_worth"
"Bond, James","Los Angeles, CA","10000"
"Smith, James","Phoenix, AZ","9000"
"Bacon, Kevin","New York, NY","8000"
person_c.csv
name,location,net_worth
"Bond, James","Los Angeles, CA",10000
"Smith, James","Phoenix, AZ",9000
"Bacon, Kevin","New York, NY",8000
person_d.csv
"name","location","net_worth"
"Bond, James","Los Angeles, CA",10000
"Smith, James","Phoenix, AZ",9000
"Bacon, Kevin","New York, NY",8000
person_e.csv
name,location,net_worth
Bond, James,Los Angeles , CA,10000
Smith, James,Phoenix , AZ,9000
Bacon, Kevin,New York, NY,8000</code></pre>
</div>
<p>Note that in the preceding example, when using <code>csv.QUOTE_NONE</code>, you must provide an additional argument for the <code>escapechar</code> parameter; otherwise, it will throw an error.</p>
</section>
<section id="see-also-13" class="level3" data-number="5.3.5">
<h3 data-number="5.3.5">See also</h3>
<ul>
<li>For more information on the <code>Pandas.DataFrame.to_csv()</code> function, please refer to this page: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html</a>.</li>
<li>For more information on the CSV module, please refer to this page: <a href="https://docs.python.org/3/library/csv.html">https://docs.python.org/3/library/csv.html</a>.</li>
</ul>
</section>
</section>
<section id="writing-data-to-an-excel-file" class="level2" data-number="5.4">
<h2 data-number="5.4">Writing data to an Excel file</h2>
<p>In this recipe, you will export a DataFrame as an Excel file format and leverage the different parameters available in the <code>DataFrame.to_excel()</code> writer function.</p>
<section id="getting-ready-14" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1">Getting ready</h3>
<p>In the <em>Reading data from an Excel file</em> recipe in <em>Chapter 2</em>, <em>Reading Time Series Data from Files</em>, you had to install <code>openpyxl</code> as the engine for reading Excel files with <code>pandas.read_excel()</code>. For this recipe, you will use the same <code>openpyxl</code> as the engine for writing Excel files with <code>DataFrame.to_excel()</code>.</p>
<p>To install <code>openpyxl</code> using <code>conda</code>, run the following:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; conda install openpyxl</code></pre>
</div>
<p>You can also use <code>pip</code>:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; pip install openpyxl</code></pre>
</div>
<p>The file is provided in the GitHub repository for this book, which you can find here: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>. The file is named <code>movieboxoffice.csv</code>.</p>
<p>To prepare for this recipe, you will read the file into a DataFrame with the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
filepath = Path('../../datasets/Ch4/movieboxoffice.csv')
movies = pd.read_csv(filepath,
                 header=0,
                 parse_dates=[0],
                 index_col=0,
                 usecols=['Date',
                          'Daily'],
                date_format="%d-%b-%y")</code></pre>
</div>
</section>
<section id="how-to-do-it-13" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2">How to do it…</h3>
<p>To write the DataFrame to an Excel file, you need to provide the writer function with <code>filename</code> and <code>sheet_name</code> parameters. The file name contains the file path and name. Make sure the file extension is <code>.xlsx</code> since you are using openpyxl.</p>
<p>The <code>DataFrame.to_excel()</code> method will determine which engine to use based on the file extension, for example, <code>.xlsx</code> or <code>.xls</code>. You can also explicitly specify which engine to use with the <code>engine</code> parameter, as shown in the following code:</p>
<ol>
<li>Determine the location of file output and create the file path, desired sheet name, and engine to the <code>DataFrame.to_excel()</code> writer function:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>output = \
Path('../../datasets/Ch4/daily_boxoffice.xlsx')
movies.to_excel(output,
               sheet_name='movies_data',
               engine='openpyxl', # default engine for xlsx files
               index=True)</code></pre>
</div>
<p>The preceding code will create a new Excel file in the specified location. You can open and inspect the file, as shown in the following figure:</p>
<figure>
<img src="../media/file34.png" alt="Figure 4.3: Example output from the daily_boxoffice.xlsx file" width="610" height="1048"/><figcaption aria-hidden="true">Figure 4.3: Example output from the daily_boxoffice.xlsx file</figcaption>
</figure>
<p>Note that the sheet name is <code>movies_data</code>. In the Excel file, you will notice that the <code>Date</code> is not in the format you would expect. Let's say the expectation was for the <code>Date</code> column to be in a specific format, such as <code>MM-DD-YYYY</code>.</p>
<blockquote>
<p>Reading the same file using <code>read_excel</code> will read the <code>Date</code> column properly, as expected.</p>
</blockquote>
<ol>
<li>To achieve this, you will use another pandas-provided class, the <code>pandas.ExcelWriter</code> class gives us access to two properties for date formatting: <code>datetime_format</code> and <code>date_format</code>. These two parameters work nicely when using the <code>xlsxwriter</code> engine, but as of this writing, there is an open bug with the openpyxl integration. The openpyxl has several advantages over xlsxwriter, specifically for appending existing Excel files. We will utilize openpyxl <code>number_format</code> property for the Date cell to fix this issue. The following code shows how this can be achieved:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>date_col = 'Date'
with pd.ExcelWriter(output, 
                    engine='openpyxl',
                    mode='a',
                    if_sheet_exists='replace') as writer:
    movies.to_excel(writer, sheet_name='movies_fixed_dates', index=True)
  
    worksheet = writer.sheets['movies_fixed_dates']
    for col in worksheet.iter_cols():
        header = col[0] # capture headers
        if header.value == date_col:
            for row in range(2, # skip first row
                             worksheet.max_row+1):
                    worksheet.cell(
                        row,
                        header.column
                                  ).number_format='MM-DD-YYYY'</code></pre>
</div>
<p>The following is a representation of what the new output would look like. This was accomplished by passing <code>MM-DD-YYYY</code> to the <code>datetime_format</code> property of the <code>writer</code> object:</p>
<figure>
<img src="../media/file35.png" alt="Figure 4.4: Using pd.ExcelWriter and number_format to update the Date column format to MM-DD-YYYY" width="638" height="1134"/><figcaption aria-hidden="true">Figure 4.4: Using pd.ExcelWriter and number_format to update the Date column format to MM-DD-YYYY</figcaption>
</figure>
</section>
<section id="how-it-works-13" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3">How it works…</h3>
<p>The <code>DataFrame.to_excel()</code> method by default creates a new Excel file if it doesn't exist or overwrites the file if it exists. To append to an existing Excel file or write to multiple sheets, you will need to use the <code>Pandas.ExcelWriter</code> class. The <code>ExcelWriter()</code> class has a <code>mode</code> parameter that can accept either <code>"w"</code> for write or <code>"a"</code> for append. As of this writing, xlsxwriter does not support the append mode, while the openpyxl supports both modes.</p>
<p>Keep in mind that in <code>ExcelWriter</code> the default mode is set to <code>"w"</code> (write mode) and, thus, if <code>"a"</code> (append mode) is not specified, it will result in overwriting the Excel file (any existing content will be erased).</p>
<p>Additionally, when using append mode (<code>mode="a"</code>) you will need to specify how to handle existing sheets through the <code>if_sheet_exists</code> parameter, which accepts one of three values:</p>
<ul>
<li><code>error</code>, which raises a <code>ValueError</code> exception.</li>
<li><code>replace</code>, which overwrites the existing worksheet.</li>
<li><code>new</code>, which creates a new worksheet with a new name. If you re-execute the preceding code and update <code>if_sheet_exists='new'</code>, then a new sheet will be created and named <code>movies_fixed_dates1</code>.</li>
</ul>
</section>
<section id="theres-more-13" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4">There's more…</h3>
<p>If you need to create multiple worksheets in the same Excel file, then <code>ExcelWriter</code> can be used to achieve this. For example, assume the goal is to split each month's data into its own sheet and name the sheet accordingly. In the following code, you will add a <code>Month</code> column and use that to split that DataFrame by month, using <code>groupby</code> to write each group into a new sheet.</p>
<p>First, let’s create our helper function <code>sheet_date_format</code> to format our <code>Date</code> column in each sheet to MM-DD-YYYY format:</p>
<div class="C0-SHCodePACKT">
<pre><code>def sheet_date_format(sheet_name, writer, date_col):
    worksheet = writer.sheets[sheet_name]
   
    for col in worksheet.iter_cols():
        header = col[0]
        if header.value == date_col:
            for row in range(2, worksheet.max_row+1):
                    worksheet.cell(
                        row,
                        header.column).number_format='MM-DD-YYYY'</code></pre>
</div>
<p>The next piece of code will add a Month column to the movies DataFrame, and then write each month to individual sheets and name each sheet with the corresponding month name:</p>
<div class="C0-SHCodePACKT">
<pre><code>movies['Month'] = movies.index.month_name()
output = Path('../../datasets/Ch4/boxoffice_by_month.xlsx')
with pd.ExcelWriter(output,
                    engine='openpyxl') as writer:
    for month, data in movies.groupby('Month'):
        data.to_excel(writer, sheet_name=month)
        sheet_date_format(month, writer, date_col='Date')</code></pre>
</div>
<p>The preceding code will create a new Excel file named <code>boxoffice_by_month.xlsx</code> with five sheets for each month, as shown in the following figure:</p>
<figure>
<img src="../media/file36.png" alt="Figure 4.5: Each month in the movies DataFrame was written to its own sheet in Excel" width="988" height="550"/><figcaption aria-hidden="true">Figure 4.5: Each month in the movies DataFrame was written to its own sheet in Excel</figcaption>
</figure>
</section>
<section id="see-also-14" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5">See also</h3>
<p>The pandas <code>to_excel()</code> method and <code>ExcelWriter</code> class make writing DataFrames to an Excel file very convenient. If you want a more granular control outside of pandas DataFrames, you should consider exploring the <code>openpyxl</code> library you installed as the reader/writer engine. For example, the <code>openpyxl</code> library has a dataframe module (<code>openpyxl.utils.dataframe</code>) for working with pandas DataFrames. An example is the <code>dataframe_to_rows()</code> function.</p>
<ul>
<li>To learn more about <code>Pandas.DataFrame.to_excel()</code>, please refer to <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html</a>.</li>
<li>To learn more about <code>Pandas.ExcelWriter()</code>, please refer to <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter.">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html#pandas.ExcelWriter.</a></li>
<li>To learn more about <code>openpyxl</code>, please refer to <a href="ch005.xhtml">https://openpyxl.readthedocs.io/en/stable/index.html</a>.</li>
<li>To learn more about <code>openpyxl.utils.dataframe</code>, please refer to <a href="https://openpyxl.readthedocs.io/en/stable/pandas.html#working-with-pandas-dataframes">https://openpyxl.readthedocs.io/en/stable/pandas.html#working-with-pandas-dataframes</a></li>
</ul>
</section>
</section>
<section id="storing-data-to-a-cloud-storage-aws-gcp-and-azure" class="level2" data-number="5.5">
<h2 data-number="5.5">Storing Data to a Cloud Storage (AWS, GCP, and Azure)</h2>
<p>In this recipe, you will use pandas to write to cloud storage such as Amazon S3, Google Cloud Storage, and Azure Blob Storage. Several of the pandas writer functions support writing to cloud storage through the <code>storage_options</code> parameter.</p>
<section id="getting-ready-15" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1">Getting ready</h3>
<p>In the Reading data from a URL recipe in <em>Chapter 2</em>, <em>Reading Time Series Data from Files</em>, you were instructed to install <code>boto3</code> and <code>s3fs</code> to read from AWS S3 buckets. In this recipe, you will be leveraging the same libraries in addition to the needed libraries for Google Cloud Storage (<code>gcsfs</code>) and Azure Blob Storage (<code>adlfs</code>).</p>
<p>To install using <code>pip</code>, you can use this:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; pip install boto3 s3fs
&gt;&gt;&gt; pip install google-cloud-storage gcsfs
&gt;&gt;&gt; pip install adlfs azure-storage-blob azure-identity</code></pre>
</div>
<p>To install using <code>conda</code>, you can use this:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt;&gt; conda install -c conda-forge boto3 s3fs -y
&gt;&gt;&gt; conda install -c conda-forge google-cloud-storage gcsfs -y
&gt;&gt;&gt; conda install -c conda-forge adlfs azure-storage-blob azure-identity -y</code></pre>
</div>
<p>You will work with the <code>boxoffice_by_month.xlsx</code> file we created in the previous recipe, <em>Writing data to an Excel file</em>. The file is provided in the GitHub repository for this book, which you can find here: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>.</p>
<p>To prepare for this recipe, you will read the file into a DataFrame with the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
source = "../../datasets/Ch4/boxoffice_by_month.xlsx"
movies = pd.concat(pd.read_excel(source,
             sheet_name=None,
             index_col='Date',
             parse_dates=True)).droplevel(0)
print(movies.head())
                   Daily  Month
Date                           
2021-04-26   $125,789.89   April
2021-04-27    $99,374.01   April
2021-04-28    $82,203.16   April
2021-04-29    $33,530.26   April
2021-04-30    $30,105.24   April</code></pre>
</div>
<p>Notice the movie DataFrame has two columns (Daily and Month) and a DatetimeIndex (Date).</p>
<p>Next, you will store your AWS, Google Cloud, and Azure credentials in a config <code>cloud.cfg</code> file outside your Python script. Then, use <code>configparser</code> to read and store the values in Python variables. You do not want your credentials exposed or hard coded in your code:</p>
<div class="C0-SHCodePACKT">
<pre><code># Example of configuration file "cloud.cfg file"
[AWS]
aws_access_key=&lt;your_access_key&gt;
aws_secret_key=&lt;your_secret_key&gt;
[GCP]
key_file_path=&lt;GCPKeyFileexample.json&gt;
[AZURE]
storage_account_key=&lt;your_storageaccount_key&gt;</code></pre>
</div>
<p>We can then load an <code>aws.cfg</code> file using <code>config.read()</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code>import configparser
config = configparser.ConfigParser()
config.read('cloud.cfg')
AWS_ACCESS_KEY = config['AWS']['aws_access_key']
AWS_SECRET_KEY = config['AWS']['aws_secret_key']
AZURE_ACCOUNT_KEY = config['AZURE']['storage_account_key']
GCP_KEY_FILE = config['GCP']['key_file_path']</code></pre>
</div>
</section>
<section id="how-to-do-it-14" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2">How to do it…</h3>
<p>Several of the pandas writer functions support writing directly to a remote or cloud storage filesystem using, for example, AWS's <code>s3://</code>, Google's <code>gs://</code>, and Azure’s <code>abfs://</code> and <code>az://</code> protocols. These writer functions provide the <code>storage_options</code> parameter to support working with remote file storage systems. This is in part because pandas utilized <code>fsspec</code> to handle system non HTTP(s) URLs such as those specific for each cloud storage. For each cloud, you will need to use the specific filesystem implementation, for example, <code>s3fs</code> for AWS S3, <code>gcsfs</code> for Google Cloud, and <code>adlfs</code> for Azure.</p>
<p>The <code>storage_options</code> parameter takes a Python dictionary to provide additional information such as credentials, tokens, or any information the cloud provider requires as a key-value pair.</p>
<section id="writing-to-amazon-s3-using-pandas" class="level4" data-number="5.5.2.1">
<h4 data-number="5.5.2.1">Writing to Amazon S3 using pandas</h4>
<p>In this section, you will write the <code>movies</code> DataFrame to the <code>tscookbook-private</code> S3 bucket as CSV and Excel files using pandas:</p>
<p>Several pandas writer functions such as <code>to_csv</code>, <code>to_parquet</code>, and <code>to_excel</code> allow you to pass AWS S3-specific credentials (<code>key</code> and <code>sercret</code>) as specified in s3fs through the <code>storage_accounts</code> parameter. The following code shows how you can utilize <code>to_csv</code> and <code>to_excel</code> to write your movies DataFrame to <code>tscookbook</code> S3 bucket as <code>movies_s3.csv</code> and <code>movies_s3.xlsx</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code># Writing to Amazon S3
movies.to_csv('s3://tscookbook-private/movies_s3.csv',
               storage_options={
                   'key': AWS_ACCESS_KEY,
                   'secret': AWS_SECRET_KEY
               })
movies.to_excel('s3://tscookbook-private/movies_s3.xlsx',
               storage_options={
                   'key': AWS_ACCESS_KEY,
                   'secret': AWS_SECRET_KEY
               })</code></pre>
</div>
<p>The following figure shows the content of the <code>tscookbook-private</code> bucket:</p>
<figure>
<img src="../media/file37.png" alt="Figure 4.6: movies_s3.csv and movie_s3.xlsx successfully written to AWS S3 using pandas" width="3186" height="866"/><figcaption aria-hidden="true">Figure 4.6: movies_s3.csv and movie_s3.xlsx successfully written to AWS S3 using pandas</figcaption>
</figure>
</section>
<section id="writing-to-google-cloud-storage-using-pandas" class="level4" data-number="5.5.2.2">
<h4 data-number="5.5.2.2">Writing to Google Cloud Storage using pandas</h4>
<p>In this section, you will write the <code>movies</code> DataFrame to the <code>tscookbook</code> bucket on Google Cloud Storage as CSV and Excel files using pandas:</p>
<p>When working with Google Cloud you will use a <strong>service account private key</strong> stored as a JSON file. This is generated and downloaded from Google Cloud. In <code>storage_options,</code> you will pass the file path. The following code shows how you can utilize <code>to_csv</code> and <code>to_excel</code> to write your movies DataFrame to <code>tscookbook</code> bucket as <code>movies_gs.csv</code> and <code>movies_gs.xlsx</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code># Writing to Google Cloud Storage
movies.to_csv('gs://tscookbook/movies_gs.csv',
               storage_options={'token': GCP_KEY_FILE})
movies.to_excel('gs://tscookbook/movies_gs.xlsx',
               storage_options={'token': GCP_KEY_FILE})</code></pre>
</div>
<p>The following figure shows the content of the <code>tscookbook</code> bucket:</p>
<figure>
<img src="../media/file38.png" alt="Figure 4.7: movies_gs.csv and movie_gs.xlsx successfully written to Google Cloud Storage using pandas" width="3284" height="902"/><figcaption aria-hidden="true">Figure 4.7: movies_gs.csv and movie_gs.xlsx successfully written to Google Cloud Storage using pandas</figcaption>
</figure>
</section>
<section id="writing-to-azure-blob-storage-using-pandas" class="level4" data-number="5.5.2.3">
<h4 data-number="5.5.2.3">Writing to Azure Blob Storage using pandas</h4>
<p>In this section, you will write the <code>movies</code> DataFrame to Azure Blob Storage in a container named <code>objects</code> as a CSV file using pandas:</p>
<p>When working with Azure Blob Storage, you use either the <code>abfs://</code> or <code>az://</code> protocols. In <code>storage_options</code> you will pass the account_key, this is your API Key for the Storage Account on Azure. The following code shows how you can utilize <code>to_csv</code> to write your movies DataFrame to <code>objects</code> container. The three code snippets below are equivalent and illustrate the different URI and <code>storage_options</code> you will need to pass:</p>
<div class="C1-SHCodePACKT">
<pre><code># Writing to Azure Blob Storage
movies.to_csv("abfs://objects@tscookbook.dfs.core.windows.net/movies_abfs.csv",
             storage_options={
                 'account_key': AZURE_ACCOUNT_KEY
             })
movies.to_csv("az://objects@tscookbook.dfs.core.windows.net/movies_az.csv",
             storage_options={
                 'account_key': AZURE_ACCOUNT_KEY
             })
movies.to_csv("az://objects/movies_az2.csv",
             storage_options={
                 'account_name': "tscookbook",
                 'account_key': AZURE_ACCOUNT_KEY</code></pre>
</div>
<div class="C0-SHCodePACKT">
<pre><code>             })</code></pre>
</div>
<p>The following figure shows the content of the <code>objects </code>container:</p>
<figure>
<img src="../media/file39.png" alt="Figure 4.8: movies_abfs.csv, movies_az.csv, and movie_az2.csv successfully written to Azure Blob Storage using pandas" width="3420" height="836"/><figcaption aria-hidden="true">Figure 4.8: movies_abfs.csv, movies_az.csv, and movie_az2.csv successfully written to Azure Blob Storage using pandas</figcaption>
</figure>
</section>
</section>
<section id="how-it-works-14" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3">How it works…</h3>
<p>In the preceding code section, we used the <code>DataFrame.to_csv()</code> and <code>DataFrame.to_excel()</code> methods to write to Amazon S3, Azure Blob Storage, and Google Cloud Storage. The <code>storage_options</code> parameter allows passing a key-value pair containing the information required for the storage connection; for example, AWS S3 requires passing a <code>key</code> and a <code>secret</code>, GCP requires <code>token</code>, and Azure requires an <code>account_key</code>.</p>
<p>Examples of pandas DataFrame writer functions that support <code>storage_options</code> include:</p>
<ul>
<li><code>Pandas.DataFrame.to_excel()</code></li>
<li><code>Pandas.DataFrame.to_json()</code></li>
<li><code>Pandas.DataFrame.to_parquet()</code></li>
<li><code>Pandas.DataFrame.to_pickle()</code></li>
<li><code>Pandas.DataFrame.to_markdown()</code></li>
<li><code>Pandas.DataFrame.to_pickle()</code></li>
<li><code>Pandas.DataFrame.to_stata()</code></li>
<li><code>Pandas.DataFrame.to_xml()</code></li>
</ul>
</section>
<section id="theres-more-14" class="level3" data-number="5.5.4">
<h3 data-number="5.5.4">There's more…</h3>
<p>For more granular control, you can use the specific Python SDK for AWS (<code>boto3</code>), Google Cloud (<code>google-cloud-storage</code>), or Azure (<code>azure-storage-blob</code>) to write your data.</p>
<p>First, let’s store our movie DataFrame in CSV format for uploading the data to the various cloud storage services.</p>
<div class="C0-SHCodePACKT">
<pre><code>data = movies.to_csv(encoding='utf-8', index=True)</code></pre>
</div>
<p>Note that the <code>index=True</code> because our Date column is an index, and we need to ensure it is included as a column when written as a CSV file.</p>
<section id="writing-to-amazon-s3-using-boto3-library" class="level4" data-number="5.5.4.1">
<h4 data-number="5.5.4.1">Writing to Amazon S3 using boto3 library</h4>
<p>You will explore both the <strong>Resource API</strong> and <strong>Client API.</strong> The Resource API is a higher-level abstraction that simplifies the code and interactions with AWS services. At the same time, the Client API provides a low-level abstraction, allowing for more granular control over AWS services.</p>
<p>When using the resource API with <code>boto3.resource("s3"),</code> you will first need create an Object resource using the Object method by supplying the S3 bucket name and an Object key (file name). Once defined, you will have access to several methods, including <code>copy</code>, <code>delete</code>, <code>put</code>, <code>download_file</code>, <code>load</code>, <code>get</code>, and <code>upload,</code> to name a few. The <code>put</code> method will add an object to the S3 bucket defined.</p>
<p>When using the client API with <code>boto3.client("s3"), </code>you have access to many methods at the Bucket and Object levels, including <code>create_bucket</code>, <code>delete_bucket</code>, <code>download_file</code>, <code>put_object</code>, <code>delete_object</code>, <code>get_bucket_llifecycle</code>, <code>get_bucket_location</code>, <code>list_buckets</code> and much more. The <code>put_object</code> method will add an object to the S3 bucket defined.</p>
<div class="C0-SHCodePACKT">
<pre><code>import boto3
bucket = "tscookbook-private"
# Using the Resource API
s3_resource = boto3.resource("s3",
            aws_access_key_id = AWS_ACCESS_KEY,
            aws_secret_access_key = AWS_SECRET_KEY)
s3_resource.Object(bucket, 'movies_boto3_resourceapi.csv').put(Body=data)
# Using the Client API
s3_client = boto3.client("s3",
            aws_access_key_id = AWS_ACCESS_KEY,
            aws_secret_access_key = AWS_SECRET_KEY)
s3_client.put_object(Body=data, Bucket=bucket, Key='movies_boto3_clientapi.csv')</code></pre>
</div>
</section>
<section id="writing-to-google-cloud-storage-using-google-cloud-storage-library" class="level4" data-number="5.5.4.2">
<h4 data-number="5.5.4.2">Writing to Google Cloud Storage using google-cloud-storage library</h4>
<p>You will first need to create a client object, an instance of the Client class from the storage module. You will authenticate using the service account JSON key file. This is specified using the <code>from_service_account_json</code> method. You will use the <code>bucket</code> and <code>blob</code> methods to create a reference to the blob object you want to place in the <code>tscookbook</code> bucket in Google Storage. Finally, you can upload the data using the <code>upload_from_string</code> method into the blob object specified.</p>
<div class="C0-SHCodePACKT">
<pre><code>from google.cloud import storage
# Authenticate using the service account key
storage_client = storage.Client.from_service_account_json(GCP_KEY_FILE)
bucket_name = 'tscookbook'
file_path = 'movies_gsapi.csv'
blob = storage_client.bucket(bucket_name).blob(file_path)
blob.upload_from_string(data)</code></pre>
</div>
</section>
<section id="writing-to-azure-blob-storage-using-azure-storage-blob-library" class="level4" data-number="5.5.4.3">
<h4 data-number="5.5.4.3">Writing to Azure Blob Storage using azure-storage-blob library</h4>
<p>You will start by creating a <code>BlobServiceClient</code> object and authenticate using your Azure Storage Account API key. You then create the blob object for the specified container using <code>get_blob_client</code> and upload that data into the specified object using the <code>upload_blob</code> method.</p>
<div class="C0-SHCodePACKT">
<pre><code>from azure.storage.blob import BlobServiceClient
blob_service_client = BlobServiceClient(
        account_url="https://tscookbook.blob.core.windows.net",
        credential=AZURE_ACCOUNT_KEY)
blob_client = blob_service_client.get_blob_client(
    container='objects',
    blob='movies_blobapi.csv')
blob_client.upload_blob(data)</code></pre>
</div>
</section>
</section>
<section id="see-also-15" class="level3" data-number="5.5.5">
<h3 data-number="5.5.5">See also</h3>
<p>To learn more about managing cloud storage with Python, explore the official documentation for these popular libraries</p>
<ul>
<li><strong>Amazon S3 (Boto3)</strong> <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html</a></li>
<li><strong>Azure Blob Storage</strong> <a href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python">https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python</a></li>
<li><strong>Google Cloud Storage</strong> <a href="https://cloud.google.com/python/docs/reference/storage/latest">https://cloud.google.com/python/docs/reference/storage/latest</a></li>
</ul>
</section>
</section>
<section id="writing-large-datasets" class="level2" data-number="5.6">
<h2 data-number="5.6">Writing Large Datasets</h2>
<p>In this recipe, you will explore how the choice of the different file formats can impact the overall write and read performance. You will explore Parquet, Optimized Row Columnar (ORC), and Feather and compare their performance to other popular file formats such as JSON and CSV.</p>
<p>The three file formats, ORC, Feather, and Parquet, are columnar file formats, making them efficient for analytical needs, and showing improved querying performance overall. The three file formats are also supported in Apache Arrow (PyArrow), which offers an in-memory columnar format for optimized data analysis performance. To persist this in-memory columnar and store it, you can use pandas <code>to_orc</code>, <code>to_feather</code>, and <code>to_parquet</code> writer functions to persist your data to disk.</p>
<blockquote>
<p>Arrow provides the in-memory representation of the data as a columnar format while Feather, ORC, and Parquet allows us to store this representation to disk.</p>
</blockquote>
<section id="getting-ready-16" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1">Getting Ready</h3>
<p>In this recipe, you will be working with the New York Taxi data set from (<a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a>) and we will be working with Yellow Taxi Trip Records for 2023.</p>
<p>In the following examples, we will be using one of these files, <code>yellow_tripdata_2023-01.parquet</code>, but you can select any other file to follow along. In the recipe <em>Reading data from Parquet files</em> from <em>Chapter 2</em>, you installed <strong>PyArrow</strong>. Below are the instructions for installing PyArrow using either Conda or Pip.</p>
<p>To install PyArrow using <code>conda,</code> run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge pyarrow</code></pre>
</div>
<p>To install PyArrow using <code>pip</code> run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install pyarrow</code></pre>
</div>
<p>To prepare for this recipe, you will read the file into a DataFrame with the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
file_path = Path('yellow_tripdata_2023-01.parquet')
df = pd.read_parquet(file_path, engine='pyarrow')
df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 3066766 entries, 0 to 3066765
Data columns (total 19 columns):
 #   Column                 Dtype        
---  ------                 -----        
 0   VendorID               int64        
 1   tpep_pickup_datetime   datetime64[us]
 2   tpep_dropoff_datetime  datetime64[us]
 3   passenger_count        float64      
 4   trip_distance          float64      
 5   RatecodeID             float64      
 6   store_and_fwd_flag     object       
 7   PULocationID           int64        
 8   DOLocationID           int64        
 9   payment_type           int64        
 10  fare_amount            float64      
 11  extra                  float64      
 12  mta_tax                float64      
 13  tip_amount             float64      
 14  tolls_amount           float64      
 15  improvement_surcharge  float64      
 16  total_amount           float64      
 17  congestion_surcharge   float64      
 18  airport_fee            float64      
dtypes: datetime64[us](2), float64(12), int64(4), object(1)
memory usage: 444.6+ MB</code></pre>
</div>
</section>
<section id="how-to-do-it-15" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2">How to do it</h3>
<p>You will write the DataFrame into different file formats and then compare the output in terms of compression efficiency (file size), write, and read speed.</p>
<p>To accomplish this, you will need to create a function that returns the file size:</p>
<div class="C0-SHCodePACKT">
<pre><code>import os
def size_in_mb(file):
    size_bytes = os.path.getsize(file)
    size_m = size_bytes / (1024**2)
    return round(size_m,2)</code></pre>
</div>
<p>The function will take the file you created and return the size in megabytes. The <code>os.path.getsize()</code> will return the size in bytes and the line <code>size_bytes / (1024**2)</code> will convert it into megabytes.</p>
<p>We will be writing these files into a <code>formats</code> folder so later we can read from that folder to evaluate read performance.</p>
<section id="writing-as-json-and-csv" class="level4" data-number="5.6.2.1">
<h4 data-number="5.6.2.1">Writing as JSON and CSV</h4>
<p>You will use the <code>DataFrame.to_json()</code> method to write a <code>yellow_tripdata.json</code> file:</p>
<div class="C1-SHCodePACKT">
<pre><code>%%time
df.to_json('formats/yellow_tripdata.json', orient='records')
size_in_mb('formats/yellow_tripdata.json')
&gt;&gt;
CPU times: user 4.63 s, sys: 586 ms, total: 5.22 s
Wall time: 5.24 s
1165.21</code></pre>
</div>
<p>Note the file size is around 1.16 GB and took around 5.24 seconds.</p>
<p>You will use the <code>DataFrame.to_csv()</code> method to write a <code>yellow_tripdata.csv</code> file:</p>
<div class="C1-SHCodePACKT">
<pre><code>%%time
df.to_csv('formats/yellow_tripdata.csv', index=False)
size_in_mb('formats/yellow_tripdata.csv')
&gt;&gt;
CPU times: user 16.7 s, sys: 405 ms, total: 17.1 s
Wall time: 17.1 s
307.04</code></pre>
</div>
<p>Note the file size is around 307 MB and took around 17.1 seconds,</p>
</section>
<section id="writing-as-parquet" class="level4" data-number="5.6.2.2">
<h4 data-number="5.6.2.2">Writing as Parquet</h4>
<p>The <code>to_parquet</code> writer function supports several compression algorithms, including <code>snappy</code>, <code>GZIP</code>, <code>brotli</code>, <code>LZ4</code>, <code>ZSTD</code>. You will use the <code>DataFrame.to_parquet()</code> method to write three files to compare <code>snappy</code>, <code>LZ4</code>, and <code>ZSTD</code> compression algorithms:</p>
<div class="C1-SHCodePACKT">
<pre><code>%%time
df.to_parquet('formats/yellow_tripdata_snappy.parquet',
              compression='snappy')
size_in_mb('formats/yellow_tripdata_snappy.parquet')
&gt;&gt;
CPU times: user 882 ms, sys: 24.2 ms, total: 906 ms
Wall time: 802 ms
59.89
%%time
df.to_parquet('formats/yellow_tripdata_lz4.parquet',
              compression='lz4')
size_in_mb('formats/yellow_tripdata_lz4.parquet')
&gt;&gt;
CPU times: user 898 ms, sys: 20.4 ms, total: 918 ms
Wall time: 817 ms
59.92
%%time
df.to_parquet('formats/yellow_tripdata_zstd.parquet',
              compression='zstd')
size_in_mb('formats/yellow_tripdata_zstd.parquet')
&gt;&gt;
CPU times: user 946 ms, sys: 24.2 ms, total: 970 ms
Wall time: 859 ms
48.95</code></pre>
</div>
<p>Notice that the three compression algorithms produce similar compression results (file size) and speed.</p>
</section>
<section id="writing-as-feather" class="level4" data-number="5.6.2.3">
<h4 data-number="5.6.2.3">Writing as Feather</h4>
<p>You will use the <code>DataFrame.to_feather()</code> method to write three feather files using the two supported compression algorithms <code>LZ4</code> and <code>ZSTD. </code>The last file format will be the uncompressed format for comparison:</p>
<div class="C1-SHCodePACKT">
<pre><code>%%time
df.to_feather('formats/yellow_tripdata_uncompressed.feather', compression='uncompressed')
size_in_mb('formats/yellow_tripdata_uncompressed.feather')
&gt;&gt;
CPU times: user 182 ms, sys: 75.5 ms, total: 257 ms
Wall time: 291 ms
435.84
%%time
df.to_feather('formats/yellow_tripdata_lz4.feather', compression='lz4')
size_in_mb('formats/yellow_tripdata_lz4.feather')
&gt;&gt;
CPU times: user 654 ms, sys: 42.1 ms, total: 696 ms
Wall time: 192 ms
116.44
%%time
df.to_feather('formats/yellow_tripdata_zstd.feather', compression='zstd', compression_level=3)
size_in_mb('formats/yellow_tripdata_zstd.feather')
&gt;&gt;
CPU times: user 1 s, sys: 39.2 ms, total: 1.04 s
Wall time: 243 ms
61.79</code></pre>
</div>
<ol>
<li>Notice the difference in file size between uncompressed, using LZ4 and ZSTD compression algorithms. You can further explore the <code>compression_level</code> to find the optimal output. Overall, LZ4 offers great performance on write and read (<em>compression</em> and <em>decompression</em> speed). The ZSTD algorithm may offer a higher compression ratio, resulting in much smaller files, but it may not be faster than LZ4.</li>
</ol>
</section>
<section id="writing-as-orc" class="level4" data-number="5.6.2.4">
<h4 data-number="5.6.2.4">Writing as ORC</h4>
<p>Similar to the Feather and Parquet file formats, ORC supports different compression algorithms, including uncompressed, <code>snappy</code>, <code>ZLIB</code>, <code>LZ4</code>, and <code>ZSTD</code>. You will use the <code>DataFrame.to_orc()</code> method to write three ORC files to explore ZSTD and LZ4 compression algorithms and an uncompressed file for comparison:</p>
<div class="C1-SHCodePACKT">
<pre><code>%%time
df.to_orc('formats/yellow_tripdata_uncompressed.orc',
          engine_kwargs={'compression':'uncompressed'})
size_in_mb('formats/yellow_tripdata_uncompressed.orc')
&gt;&gt;
CPU times: user 989 ms, sys: 66.3 ms, total: 1.06 s
Wall time: 1.01 s
319.94
%%time
df.to_orc(' formats /yellow_tripdata_lz4.orc',
          engine_kwargs={'compression':'lz4'})
size_in_mb('formats/yellow_tripdata_lz4.orc')
&gt;&gt;
CPU times: user 1 s, sys: 67.2 ms, total: 1.07 s
Wall time: 963 ms
319.65
%%time
df.to_orc('yellow_tripdata_zstd.orc',
          engine_kwargs={'compression':'zstd'})
size_in_mb('formats/yellow_tripdata_zstd.orc')
&gt;&gt;
CPU times: user 1.47 s, sys: 46.4 ms, total: 1.51 s
Wall time: 1.42 s
53.58</code></pre>
</div>
<p>Notice that the LZ4 algorithm did not offer better compression when compared with the uncompressed version. The ZSTD algorithm did offer better compression but took a bit longer to execute.</p>
</section>
</section>
<section id="how-it-works-15" class="level3" data-number="5.6.3">
<h3 data-number="5.6.3">How it works…</h3>
<p>Often, when working with large datasets that need to persist into disk after completing your transformations, deciding which file format to opt for can significantly impact your overall data storage strategy.</p>
<p>For example, JSON and CSV formats are human-readable choices, and pretty much any commercial or open-source data visualization or analysis tools can handle such formats. Both CSV and JSON formats do not offer compression for large file sizes and can lead to poor performance on both write and read operations. On the other hand, Parquet, Feather, and ORC are binary file formats (not human-readable) but support several compression algorithms and are columnar-based, which are optimized for analytical applications with fast read performance.</p>
<p>The pandas library supports Parquet, Feather, and ORC thanks to PyArrow, a Python wrapper to Apache Arrow.</p>
</section>
<section id="theres-more-15" class="level3" data-number="5.6.4">
<h3 data-number="5.6.4">There’s more…</h3>
<p>You have evaluated different file formats' write performance (and size). Next, you will compare read time performance and the efficiency of the various file formats and compression algorithms.</p>
<p>To do this, you will create a function (<code>measure_read_performance</code>) that reads all files in a specified folder (for example, the <code>formats</code> folder). The function will evaluate each file extension (for instance, <em>.feather</em>, <em>.orc</em>, <em>.json</em>, .<em>csv</em>, <em>.parquet</em>) to determine which pandas read function to use. The function will then capture the performance time for each file format, append the results, and return a DataFrame containing all results sorted by read time.</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
import os
import glob
import time
def measure_read_performance(folder_path):
  performance_data = []
  for file_path in glob.glob(f'{folder_path}/*'):
    _, ext = os.path.splitext(file_path)
    start_time = time.time()
     
    if ext == '.csv':
      pd.read_csv(file_path, low_memory=False)
    elif ext == '.parquet':
      pd.read_parquet(file_path)
    elif ext == '.feather':
      pd.read_feather(file_path)
    elif ext == '.orc':
      pd.read_orc(file_path)
    elif ext == '.json':
      pd.read_json(file_path)
    end_time = time.time()
    performance_data.append({'filename': file_path,
                             'read_time': end_time - start_time})
    df = pd.DataFrame(performance_data)
  return df.sort_values('read_time').reset_index(drop=True)</code></pre>
</div>
<p>You can execute the function by specifying the folder, in this case, the <code>formats</code> folder, to display the final results:</p>
<div class="C0-SHCodePACKT">
<pre><code>results =\
    measure_read_performance(folder_path='formats')
print(results)
&gt;&gt;
                                        filename  read_time
0            formats/yellow_tripdata_lz4.parquet   0.070845
1         formats/yellow_tripdata_snappy.parquet   0.072083
2           formats/yellow_tripdata_zstd.parquet   0.078382
3            formats/yellow_tripdata_lz4.feather   0.103172
4           formats/yellow_tripdata_zstd.feather   0.103918
5   formats/yellow_tripdata_uncompressed.feather   0.116974
6               formats/yellow_tripdata_zstd.orc   0.474430
7       formats/yellow_tripdata_uncompressed.orc   0.592284
8                formats/yellow_tripdata_lz4.orc   0.613846
9                    formats/yellow_tripdata.csv   4.557402
10                  formats/yellow_tripdata.json  14.590845                                     </code></pre>
</div>
<p>Overall, the results for read performance indicate that Parquet file formats perform the best, followed by Feather, then ORC. The time <code>read_time</code> is measured in seconds.</p>
</section>
<section id="see-also-16" class="level3" data-number="5.6.5">
<h3 data-number="5.6.5">See also</h3>
<p>To learn more about file formats for efficient data storage with pandas</p>
<ul>
<li><strong>Parquet</strong>
<ul>
<li>pandas documentation: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html</a></li>
</ul></li>
<li><strong>Feather</strong>
<ul>
<li>pandas documentation: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_feather.html</a></li>
<li>Additional arguments in Arrow documentation: <a href="https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html">https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html</a></li>
</ul></li>
<li><strong>ORC</strong>
<ul>
<li>pandas documentation: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_orc.html</a></li>
<li>Additional arguments in Arrow documentation: <a href="https://arrow.apache.org/docs/python/generated/pyarrow.orc.write_table.html">https://arrow.apache.org/docs/python/generated/pyarrow.orc.write_table.html</a></li>
</ul></li>
<li><strong>Apache Arrow</strong>: <a href="https://arrow.apache.org/overview/">https://arrow.apache.org/overview/</a></li>
</ul>
</section>
</section>
</section>
</div>
</div>
</body>
</html>