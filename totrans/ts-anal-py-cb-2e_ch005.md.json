["```py\nimport pandas as pd\nfrom pathlib import Path\nfile = \\\nPath('../../datasets/Ch4/time_series_covid19_confirmed_global.csv')\ndf = pd.read_csv(file)\ndf.head()\n```", "```py\n# filter data where Country is United States\ndf_usa = df[df['Country/Region'] == 'US']\n# filter columns from June to end of September\ndf_usa_summer = df_usa.loc[:, '6/1/21':'9/30/21']\n# unpivot using pd.melt()\ndf_usa_summer_unpivoted = \\\n    pd.melt(df_usa_summer,\n            value_vars=df_usa_summer.columns,\n            value_name='cases',\n            var_name='date').set_index('date')\ndf_usa_summer_unpivoted.index = \\\n    pd.to_datetime(df_usa_summer_unpivoted.index, format=\"%m/%d/%y\")\n```", "```py\ndf_usa_summer_unpivoted.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 122 entries, 2021-06-01 to 2021-09-30\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   cases   122 non-null    int64\ndtypes: int64(1)\nmemory usage: 1.9 KB\ndf_usa_summer_unpivoted.head()\n>>\n               cases\ndate               \n2021-06-01  33407540\n2021-06-02  33424131\n2021-06-03  33442100\n2021-06-04  33459613\n2021-06-05  33474770\n```", "```py\noutput = \\\nPath('../../datasets/Ch4/covid_usa_summer_2021.pkl')\ndf_usa_summer_unpivoted.to_pickle(output)\n```", "```py\nunpickled_df = pd.read_pickle(output)\nunpickled_df.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 122 entries, 2021-06-01 to 2021-09-30\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   cases   122 non-null    int64\ndtypes: int64(1)\nmemory usage: 1.9 KB\n```", "```py\nimport pickle\n```", "```py\nfile_path = \\\nPath('../../datasets/Ch4/covid_usa_summer_2021_v2.pkl')\nwith open(file_path, \"wb\") as file:\n    pickle.dump(df_usa_summer_unpivoted, file)\n```", "```py\nwith open(file_path, \"rb\") as file:\n    df = pickle.load(file)\ntype(df)\n>>\npandas.core.frame.DataFrame\n```", "```py\nzip_output =\\\nPath('../../datasets/Ch4/covid_usa_summer_2021.zip')\n# Write the Dataframe\ndf_usa_summer_unpivoted.to_pickle(zip_output)\n# Read the Dataframe\npd.read_pickle(zip_output)\n```", "```py\npickle.HIGHEST_PROTOCOL\n>> 5\n```", "```py\nwith open(output, \"wb\") as file:\n    pickle.dump(df_usa_summer_unpivoted,\n                file,\n                pickle.HIGHEST_PROTOCOL)\nwith open(output, \"wb\") as file:\n    pickle.dump(df_usa_summer_unpivoted,\n                file,\n                5)\n```", "```py\ndef covid_by_country(file, days, country):\n    ts = pd.read_csv(file)\n    ts = ts[ts['Country/Region'] == country]\n    final = ts.iloc[:, -days:].sum()\n    final.index = pd.to_datetime(final.index,\n                                format=\"%m/%d/%y\")\n    return final\nfile = \\\nPath('../../datasets/Ch4/time_series_covid19_confirmed_global.csv')\nus_past_120_days = covid_by_country(file, 200, 'US')\nplot_example = \\\nus_past_120_days.plot(title=f'COVID confirmed case for US',\n                xlabel='Date',\n                ylabel='Number of Confirmed Cases');\n```", "```py\nfrom datetime import datetime\nmetadata = {\n    'date': datetime.now(),\n    'data': '''\n        COVID-19 Data Repository by the\n        Center for Systems Science and Engineering (CSSE)\n        at Johns Hopkins University'\n        ''',\n    'author': 'Tarek Atwan',\n    'version': 1.0,\n    'function': covid_by_country,\n    'example_code' : us_past_120_days,\n    'example_code_plot': plot_example\n}\nfile_path = Path('../../datasets/Ch4/covid_data.pkl')\nwith open(file_path, 'wb') as file:\n    pickle.dump(metadata, file)\n```", "```py\nwith open(output, 'rb') as file:\n    content = pickle.load(file)\ncontent.keys()\n>>\ndict_keys(['date', 'data', 'author', 'version', 'function', 'example_df', 'example_plot'])\n```", "```py\nfile_path =\\\nPath('../../datasets/Ch4/time_series_covid19_confirmed_global.csv')\nloaded_func = content['function']\nloaded_func(file_path, 120, 'China').tail()\n>>\n2023-03-05    4903524\n2023-03-06    4903524\n2023-03-07    4903524\n2023-03-08    4903524\n2023-03-09    4903524\ndtype: int64\n```", "```py\nloaded_df = content['example_df']\nloaded_df.tail()\n>>\n2023-03-05    103646975\n2023-03-06    103655539\n2023-03-07    103690910\n2023-03-08    103755771\n2023-03-09    103802702\ndtype: int64\n```", "```py\nloaded_plot = content['example_plot']\nloaded_plot.get_figure()\n```", "```py\nimport pandas as pd\nfrom pathlib import Path\nfilepath = Path('../../datasets/Ch4/movieboxoffice.csv')\nmovies = pd.read_csv(filepath,\n                 header=0,\n                 parse_dates=[0],\n                 index_col=0,\n                 usecols=['Date',\n                          'Daily'],\n                date_format=\"%d-%b-%y\")\nmovies.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 128 entries, 2021-04-26 to 2021-08-31\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   Daily   128 non-null    object\ndtypes: object(1)\nmemory usage: 2.0+ KB\n```", "```py\noutput = Path('../../datasets/Ch4/df_movies.csv')\nmovies.to_csv(output)\n```", "```py\noutput = Path('../../datasets/Ch4/piped_df_movies.csv')\nmovies.to_csv(output, sep='|')\n```", "```py\nmovies_df = pd.read_csv(output, sep='|')\nmovies_df.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 128 entries, 0 to 127\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   Date    128 non-null    object\n 1   Daily   128 non-null    object\ndtypes: object(2)\nmemory usage: 2.1+ KB\n```", "```py\n# tab \"\\t\"\nDate    DOW Daily   Avg To Date Day Estimated\n2019-04-26  Friday  157461641   33775   157461641   1   False\n2019-04-27  Saturday    109264122   23437   266725763   2   False\n2019-04-28  Sunday  90389244    19388   357115007   3   False\n# comma \",\"\nDate,DOW,Daily,Avg,To Date,Day,Estimated\n2019-04-26,Friday,157461641,33775,157461641,1,False\n2019-04-27,Saturday,109264122,23437,266725763,2,False\n2019-04-28,Sunday,90389244,19388,357115007,3,False\n# semicolon \";\"\nDate;DOW;Daily;Avg;To Date;Day;Estimated\n2019-04-26;Friday;157461641;33775;157461641;1;False\n2019-04-27;Saturday;109264122;23437;266725763;2;False\n2019-04-28;Sunday;90389244;19388;357115007;3;False\n# pipe \"|\"\nDate|DOW|Daily|Avg|To Date|Day|Estimated\n2019-04-26|Friday|157461641|33775|157461641|1|False\n2019-04-27|Saturday|109264122|23437|266725763|2|False\n2019-04-28|Sunday|90389244|19388|357115007|3|False\n```", "```py\nimport pandas as pd\nperson = pd.DataFrame({\n     'name': ['Bond, James', 'Smith, James', 'Bacon, Kevin'],\n     'location': ['Los Angeles, CA', 'Phoenix, AZ', 'New York, NY'],\n     'net_worth': [10000, 9000, 8000]\n    })\nprint(person)\n>>\n           name         location  net_worth\n0   Bond, James  Los Angeles, CA      10000\n1  Smith, James      Phoenix, AZ       9000\n2  Bacon, Kevin     New York, NY       8000\n```", "```py\nperson.to_csv('person_a.csv', index=False)\n```", "```py\nname,location,net_worth\n\"Bond, James\",\"Los Angeles, CA\",10000\n\"Smith, James\",\"Phoenix, AZ\",9000\n\"Bacon, Kevin\",\"New York, NY\",8000\n```", "```py\nimport csv\nperson.to_csv('person_b.csv',\n               index=False,\n               quoting=csv.QUOTE_ALL)\nperson.to_csv('person_c.csv',\n               index=False,\n               quoting=csv.QUOTE_MINIMAL)\nperson.to_csv('person_d.csv',\n               index=False,\n               quoting= csv.QUOTE_NONNUMERIC)\nperson.to_csv('person_e.csv',\n               index=False,\n               quoting= csv.QUOTE_NONE, escapechar='\\t')\n```", "```py\nperson_b.csv\n\"name\",\"location\",\"net_worth\"\n\"Bond, James\",\"Los Angeles, CA\",\"10000\"\n\"Smith, James\",\"Phoenix, AZ\",\"9000\"\n\"Bacon, Kevin\",\"New York, NY\",\"8000\"\nperson_c.csv\nname,location,net_worth\n\"Bond, James\",\"Los Angeles, CA\",10000\n\"Smith, James\",\"Phoenix, AZ\",9000\n\"Bacon, Kevin\",\"New York, NY\",8000\nperson_d.csv\n\"name\",\"location\",\"net_worth\"\n\"Bond, James\",\"Los Angeles, CA\",10000\n\"Smith, James\",\"Phoenix, AZ\",9000\n\"Bacon, Kevin\",\"New York, NY\",8000\nperson_e.csv\nname,location,net_worth\nBond, James,Los Angeles , CA,10000\nSmith, James,Phoenix , AZ,9000\nBacon, Kevin,New York, NY,8000\n```", "```py\n>>> conda install openpyxl\n```", "```py\n>>> pip install openpyxl\n```", "```py\nimport pandas as pd\nfrom pathlib import Path\nfilepath = Path('../../datasets/Ch4/movieboxoffice.csv')\nmovies = pd.read_csv(filepath,\n                 header=0,\n                 parse_dates=[0],\n                 index_col=0,\n                 usecols=['Date',\n                          'Daily'],\n                date_format=\"%d-%b-%y\")\n```", "```py\noutput = \\\nPath('../../datasets/Ch4/daily_boxoffice.xlsx')\nmovies.to_excel(output,\n               sheet_name='movies_data',\n               engine='openpyxl', # default engine for xlsx files\n               index=True)\n```", "```py\ndate_col = 'Date'\nwith pd.ExcelWriter(output, \n                    engine='openpyxl',\n                    mode='a',\n                    if_sheet_exists='replace') as writer:\n    movies.to_excel(writer, sheet_name='movies_fixed_dates', index=True)\n\n    worksheet = writer.sheets['movies_fixed_dates']\n    for col in worksheet.iter_cols():\n        header = col[0] # capture headers\n        if header.value == date_col:\n            for row in range(2, # skip first row\n                             worksheet.max_row+1):\n                    worksheet.cell(\n                        row,\n                        header.column\n                                  ).number_format='MM-DD-YYYY'\n```", "```py\ndef sheet_date_format(sheet_name, writer, date_col):\n    worksheet = writer.sheets[sheet_name]\n\n    for col in worksheet.iter_cols():\n        header = col[0]\n        if header.value == date_col:\n            for row in range(2, worksheet.max_row+1):\n                    worksheet.cell(\n                        row,\n                        header.column).number_format='MM-DD-YYYY'\n```", "```py\nmovies['Month'] = movies.index.month_name()\noutput = Path('../../datasets/Ch4/boxoffice_by_month.xlsx')\nwith pd.ExcelWriter(output,\n                    engine='openpyxl') as writer:\n    for month, data in movies.groupby('Month'):\n        data.to_excel(writer, sheet_name=month)\n        sheet_date_format(month, writer, date_col='Date')\n```", "```py\n>>> pip install boto3 s3fs\n>>> pip install google-cloud-storage gcsfs\n>>> pip install adlfs azure-storage-blob azure-identity\n```", "```py\n>>> conda install -c conda-forge boto3 s3fs -y\n>>> conda install -c conda-forge google-cloud-storage gcsfs -y\n>>> conda install -c conda-forge adlfs azure-storage-blob azure-identity -y\n```", "```py\nimport pandas as pd\nfrom pathlib import Path\nsource = \"../../datasets/Ch4/boxoffice_by_month.xlsx\"\nmovies = pd.concat(pd.read_excel(source,\n             sheet_name=None,\n             index_col='Date',\n             parse_dates=True)).droplevel(0)\nprint(movies.head())\n                   Daily  Month\nDate                           \n2021-04-26   $125,789.89   April\n2021-04-27    $99,374.01   April\n2021-04-28    $82,203.16   April\n2021-04-29    $33,530.26   April\n2021-04-30    $30,105.24   April\n```", "```py\n# Example of configuration file \"cloud.cfg file\"\n[AWS]\naws_access_key=<your_access_key>\naws_secret_key=<your_secret_key>\n[GCP]\nkey_file_path=<GCPKeyFileexample.json>\n[AZURE]\nstorage_account_key=<your_storageaccount_key>\n```", "```py\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read('cloud.cfg')\nAWS_ACCESS_KEY = config['AWS']['aws_access_key']\nAWS_SECRET_KEY = config['AWS']['aws_secret_key']\nAZURE_ACCOUNT_KEY = config['AZURE']['storage_account_key']\nGCP_KEY_FILE = config['GCP']['key_file_path']\n```", "```py\n# Writing to Amazon S3\nmovies.to_csv('s3://tscookbook-private/movies_s3.csv',\n               storage_options={\n                   'key': AWS_ACCESS_KEY,\n                   'secret': AWS_SECRET_KEY\n               })\nmovies.to_excel('s3://tscookbook-private/movies_s3.xlsx',\n               storage_options={\n                   'key': AWS_ACCESS_KEY,\n                   'secret': AWS_SECRET_KEY\n               })\n```", "```py\n# Writing to Google Cloud Storage\nmovies.to_csv('gs://tscookbook/movies_gs.csv',\n               storage_options={'token': GCP_KEY_FILE})\nmovies.to_excel('gs://tscookbook/movies_gs.xlsx',\n               storage_options={'token': GCP_KEY_FILE})\n```", "```py\n# Writing to Azure Blob Storage\nmovies.to_csv(\"abfs://objects@tscookbook.dfs.core.windows.net/movies_abfs.csv\",\n             storage_options={\n                 'account_key': AZURE_ACCOUNT_KEY\n             })\nmovies.to_csv(\"az://objects@tscookbook.dfs.core.windows.net/movies_az.csv\",\n             storage_options={\n                 'account_key': AZURE_ACCOUNT_KEY\n             })\nmovies.to_csv(\"az://objects/movies_az2.csv\",\n             storage_options={\n                 'account_name': \"tscookbook\",\n                 'account_key': AZURE_ACCOUNT_KEY\n```", "```py\n })\n```", "```py\ndata = movies.to_csv(encoding='utf-8', index=True)\n```", "```py\nimport boto3\nbucket = \"tscookbook-private\"\n# Using the Resource API\ns3_resource = boto3.resource(\"s3\",\n            aws_access_key_id = AWS_ACCESS_KEY,\n            aws_secret_access_key = AWS_SECRET_KEY)\ns3_resource.Object(bucket, 'movies_boto3_resourceapi.csv').put(Body=data)\n# Using the Client API\ns3_client = boto3.client(\"s3\",\n            aws_access_key_id = AWS_ACCESS_KEY,\n            aws_secret_access_key = AWS_SECRET_KEY)\ns3_client.put_object(Body=data, Bucket=bucket, Key='movies_boto3_clientapi.csv')\n```", "```py\nfrom google.cloud import storage\n# Authenticate using the service account key\nstorage_client = storage.Client.from_service_account_json(GCP_KEY_FILE)\nbucket_name = 'tscookbook'\nfile_path = 'movies_gsapi.csv'\nblob = storage_client.bucket(bucket_name).blob(file_path)\nblob.upload_from_string(data)\n```", "```py\nfrom azure.storage.blob import BlobServiceClient\nblob_service_client = BlobServiceClient(\n        account_url=\"https://tscookbook.blob.core.windows.net\",\n        credential=AZURE_ACCOUNT_KEY)\nblob_client = blob_service_client.get_blob_client(\n    container='objects',\n    blob='movies_blobapi.csv')\nblob_client.upload_blob(data)\n```", "```py\nconda install -c conda-forge pyarrow\n```", "```py\npip install pyarrow\n```", "```py\nimport pandas as pd\nfrom pathlib import Path\nfile_path = Path('yellow_tripdata_2023-01.parquet')\ndf = pd.read_parquet(file_path, engine='pyarrow')\ndf.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3066766 entries, 0 to 3066765\nData columns (total 19 columns):\n #   Column                 Dtype        \n---  ------                 -----        \n 0   VendorID               int64        \n 1   tpep_pickup_datetime   datetime64[us]\n 2   tpep_dropoff_datetime  datetime64[us]\n 3   passenger_count        float64      \n 4   trip_distance          float64      \n 5   RatecodeID             float64      \n 6   store_and_fwd_flag     object       \n 7   PULocationID           int64        \n 8   DOLocationID           int64        \n 9   payment_type           int64        \n 10  fare_amount            float64      \n 11  extra                  float64      \n 12  mta_tax                float64      \n 13  tip_amount             float64      \n 14  tolls_amount           float64      \n 15  improvement_surcharge  float64      \n 16  total_amount           float64      \n 17  congestion_surcharge   float64      \n 18  airport_fee            float64      \ndtypes: datetime64[us](2), float64(12), int64(4), object(1)\nmemory usage: 444.6+ MB\n```", "```py\nimport os\ndef size_in_mb(file):\n    size_bytes = os.path.getsize(file)\n    size_m = size_bytes / (1024**2)\n    return round(size_m,2)\n```", "```py\n%%time\ndf.to_json('formats/yellow_tripdata.json', orient='records')\nsize_in_mb('formats/yellow_tripdata.json')\n>>\nCPU times: user 4.63 s, sys: 586 ms, total: 5.22 s\nWall time: 5.24 s\n1165.21\n```", "```py\n%%time\ndf.to_csv('formats/yellow_tripdata.csv', index=False)\nsize_in_mb('formats/yellow_tripdata.csv')\n>>\nCPU times: user 16.7 s, sys: 405 ms, total: 17.1 s\nWall time: 17.1 s\n307.04\n```", "```py\n%%time\ndf.to_parquet('formats/yellow_tripdata_snappy.parquet',\n              compression='snappy')\nsize_in_mb('formats/yellow_tripdata_snappy.parquet')\n>>\nCPU times: user 882 ms, sys: 24.2 ms, total: 906 ms\nWall time: 802 ms\n59.89\n%%time\ndf.to_parquet('formats/yellow_tripdata_lz4.parquet',\n              compression='lz4')\nsize_in_mb('formats/yellow_tripdata_lz4.parquet')\n>>\nCPU times: user 898 ms, sys: 20.4 ms, total: 918 ms\nWall time: 817 ms\n59.92\n%%time\ndf.to_parquet('formats/yellow_tripdata_zstd.parquet',\n              compression='zstd')\nsize_in_mb('formats/yellow_tripdata_zstd.parquet')\n>>\nCPU times: user 946 ms, sys: 24.2 ms, total: 970 ms\nWall time: 859 ms\n48.95\n```", "```py\n%%time\ndf.to_feather('formats/yellow_tripdata_uncompressed.feather', compression='uncompressed')\nsize_in_mb('formats/yellow_tripdata_uncompressed.feather')\n>>\nCPU times: user 182 ms, sys: 75.5 ms, total: 257 ms\nWall time: 291 ms\n435.84\n%%time\ndf.to_feather('formats/yellow_tripdata_lz4.feather', compression='lz4')\nsize_in_mb('formats/yellow_tripdata_lz4.feather')\n>>\nCPU times: user 654 ms, sys: 42.1 ms, total: 696 ms\nWall time: 192 ms\n116.44\n%%time\ndf.to_feather('formats/yellow_tripdata_zstd.feather', compression='zstd', compression_level=3)\nsize_in_mb('formats/yellow_tripdata_zstd.feather')\n>>\nCPU times: user 1 s, sys: 39.2 ms, total: 1.04 s\nWall time: 243 ms\n61.79\n```", "```py\n%%time\ndf.to_orc('formats/yellow_tripdata_uncompressed.orc',\n          engine_kwargs={'compression':'uncompressed'})\nsize_in_mb('formats/yellow_tripdata_uncompressed.orc')\n>>\nCPU times: user 989 ms, sys: 66.3 ms, total: 1.06 s\nWall time: 1.01 s\n319.94\n%%time\ndf.to_orc(' formats /yellow_tripdata_lz4.orc',\n          engine_kwargs={'compression':'lz4'})\nsize_in_mb('formats/yellow_tripdata_lz4.orc')\n>>\nCPU times: user 1 s, sys: 67.2 ms, total: 1.07 s\nWall time: 963 ms\n319.65\n%%time\ndf.to_orc('yellow_tripdata_zstd.orc',\n          engine_kwargs={'compression':'zstd'})\nsize_in_mb('formats/yellow_tripdata_zstd.orc')\n>>\nCPU times: user 1.47 s, sys: 46.4 ms, total: 1.51 s\nWall time: 1.42 s\n53.58\n```", "```py\nimport pandas as pd\nimport os\nimport glob\nimport time\ndef measure_read_performance(folder_path):\n  performance_data = []\n  for file_path in glob.glob(f'{folder_path}/*'):\n    _, ext = os.path.splitext(file_path)\n    start_time = time.time()\n\n    if ext == '.csv':\n      pd.read_csv(file_path, low_memory=False)\n    elif ext == '.parquet':\n      pd.read_parquet(file_path)\n    elif ext == '.feather':\n      pd.read_feather(file_path)\n    elif ext == '.orc':\n      pd.read_orc(file_path)\n    elif ext == '.json':\n      pd.read_json(file_path)\n    end_time = time.time()\n    performance_data.append({'filename': file_path,\n                             'read_time': end_time - start_time})\n    df = pd.DataFrame(performance_data)\n  return df.sort_values('read_time').reset_index(drop=True)\n```", "```py\nresults =\\\n    measure_read_performance(folder_path='formats')\nprint(results)\n>>\n                                        filename  read_time\n0            formats/yellow_tripdata_lz4.parquet   0.070845\n1         formats/yellow_tripdata_snappy.parquet   0.072083\n2           formats/yellow_tripdata_zstd.parquet   0.078382\n3            formats/yellow_tripdata_lz4.feather   0.103172\n4           formats/yellow_tripdata_zstd.feather   0.103918\n5   formats/yellow_tripdata_uncompressed.feather   0.116974\n6               formats/yellow_tripdata_zstd.orc   0.474430\n7       formats/yellow_tripdata_uncompressed.orc   0.592284\n8                formats/yellow_tripdata_lz4.orc   0.613846\n9                    formats/yellow_tripdata.csv   4.557402\n10                  formats/yellow_tripdata.json  14.590845 \n```"]