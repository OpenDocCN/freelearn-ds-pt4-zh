<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Time Series Analysis with Python Cookbook, 2E - Second Edition</title>
<link rel="stylesheet" type="text/css" href="../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/><link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content"><section id="persisting-time-series-data-to-databases" class="level1 pkt" data-number="6">
<h1 data-number="6">5 Persisting Time Series Data to Databases</h1>
<section id="join-our-book-community-on-discord-4" class="level2" data-number="6.1">
<h2 data-number="6.1">Join our book community on Discord</h2>
<p>
<img style="width:15rem" src="../media/file0.png" width="200" height="200"/>
</p>
<p><a href="https://packt.link/zmkOY">https://packt.link/zmkOY</a></p>
<p>It is common that, after completing a <strong>data analysis</strong> task, in which data is extracted from a source system, processed, transformed, and possibly modeled, the output is stored in a database for persistence. You can always store the data in a flat file or export it to a CSV, but when dealing with a large amount of corporate data (including proprietary data), you will need a more robust and secure way to store it. <strong>Databases</strong> offer several advantages: security (encryption at rest), concurrency (allowing many users to query the database without impacting performance), fault tolerance, <strong>ACID</strong> compliance, optimized read-write mechanisms, distributed computing, and distributed storage.</p>
<p>In a corporate context, once data is stored in a database, it can be shared across different departments; for example, finance, marketing, sales, and product development can now access the data stored for their own needs. Furthermore, the data can now be democratized and applied to numerous use cases by different organizational roles, such as business analysts, data scientists, data engineers, marketing analysts, and business intelligence developers.</p>
<p>In this chapter, you will write your time series data to a database system for persistence. You will explore different types of databases (relational and non-relational) and use <strong>Python</strong> to push your data.</p>
<p>More specifically, you will be using the <strong>pandas</strong> library since you will be doing much of your analysis using pandas <strong>DataFrames</strong>. You will learn how to use the pandas library to persist your time series DataFrame to a database storage system. Many databases offer Python APIs and connectors, and recently, many of them support pandas DataFrames (for reading and writing) given their popularity and mainstream adoption. In this chapter, you will work with a relational database, a document database, a cloud data warehouse, and a specialized time series database.</p>
<p>This chapter aims to give you first-hand experience working with different methods to connect to these database systems to persist your time series DataFrame.</p>
<p>Here is the list of the recipes that we will cover in this chapter:</p>
<ul>
<li>Writing time series data to a relational database</li>
<li>Writing time series data to MongoDB</li>
<li>Writing time series data to InfluxDB</li>
<li>Writing time series data to Snowflake</li>
</ul>
<blockquote>
<p>WRITING TO A DATABASE AND PERMISSIONS</p>
<blockquote>
<p>Remember that when you install your database instance or use your cloud service, writing your data is straightforward since you are in the owner/admin role.</p>
</blockquote>
<blockquote>
<p>This will not be the case in any corporation when it's their database system. You must align and work with the database owners, maintainers, and possibly IT, database admins, or cloud admins. In most cases, they can permit you to write your data in a sandbox or a development environment. Then, once you are done, possibly the same or another team (such as a DevOps team) may want to inspect the code and evaluate performance before they migrate the code to a Quality Assurance (QA) / User Acceptance Testing (UAT) environment. Once there, the business may get involved to test and validate the data for approval. Finally, it may be promoted to the production environment so that everyone can start using the data.</p>
</blockquote>
</blockquote>
</section>
<section id="technical-requirements-4" class="level2" data-number="6.2">
<h2 data-number="6.2">Technical requirements</h2>
<p>In this chapter, we will extensively use pandas 2.2.2 (released April 10, 2024).</p>
<p>Throughout our journey, you will install several Python libraries to work with pandas. These are highlighted in the Getting ready section for each recipe. You can also download the Jupyter notebooks from the GitHub repository at <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a> to follow along.</p>
<blockquote>
<p>You should refer to the <em>Technical Requirements</em> section in <em>Chapter 3</em>, <em>Reading Time Series Data from Databases</em>. This includes creating a <strong>configuration file</strong> such as the <code>database.cfg</code>.</p>
</blockquote>
<p>You will be using the same dataset throughout the recipes in this chapter. The dataset is based on Amazon's stock data from January 2019 to December 2023 pulled using the <code>yfnance</code> library and written as a pandas DataFrame.</p>
<p>Start by installing the <code>yfinance</code> library, which you can install using <strong>conda</strong> with:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge yfinance</code></pre>
</div>
<p>You can also install using <strong>pip</strong> with:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install yfinance</code></pre>
</div>
<p>To understand how the library works, you will start by pulling Amazon stock data using <code>yfinance</code></p>
<div class="C0-SHCodePACKT">
<pre><code>import yfinance as yf
amzn = yf.Ticker("AMZN")
amzn_hist = amzn.history(start="2019-01-01", end="2023-12-31")
amzn_hist.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 1258 entries, 2019-01-02 00:00:00-05:00 to 2023-12-29 00:00:00-05:00
Data columns (total 7 columns):
 #   Column        Non-Null Count  Dtype 
---  ------        --------------  ----- 
 0   Open          1258 non-null   float64
 1   High          1258 non-null   float64
 2   Low           1258 non-null   float64
 3   Close         1258 non-null   float64
 4   Volume        1258 non-null   int64 
 5   Dividends     1258 non-null   float64
 6   Stock Splits  1258 non-null   float64
dtypes: float64(6), int64(1)
memory usage: 78.6 KB</code></pre>
</div>
<p>The resulting DataFrame has seven (7) columns and <strong>1258</strong> rows. There is a <code>DatetimeIndex</code> with the following format <code>2019-01-02 00:00:00-05:00</code>. Let’s focus on a handful of columns (Open, High, Low, Close, and Volume), and change the <code>DatetimeIndex</code> datetime format to be <code>YYYY-MM-DD</code> as well:</p>
<div class="C0-SHCodePACKT">
<pre><code>amzn_hist.index = amzn_hist.index.strftime('%Y-%m-%d')
amzn_hist = amzn_hist[['Open', 'High', 'Low', 'Close', 'Volume']]
print(amzn_hist.head())
&gt;&gt;
                 Open       High        Low      Close     Volume
Date                                                            
2019-01-02  73.260002  77.667999  73.046501  76.956497  159662000
2019-01-03  76.000504  76.900002  74.855499  75.014000  139512000
2019-01-04  76.500000  79.699997  75.915497  78.769501  183652000
2019-01-07  80.115501  81.727997  79.459503  81.475502  159864000
2019-01-08  83.234497  83.830498  80.830498  82.829002  177628000</code></pre>
</div>
<p>Based on the preceding example, we can generalize the approach by creating a function that we can call throughout this chapter:</p>
<div class="C0-SHCodePACKT">
<pre><code>import yfinance as yf
def get_stock_data(ticker, start, end):
    stock_data = yf.Ticker(ticker)
    stock_data = stock_data.history(start=start, end=end)
    stock_data.index = stock_data.index.strftime('%Y-%m-%d')
    stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]
    return stock_data</code></pre>
</div>
<p>The <code>get_stock_data</code> function will return a pandas DataFrame with select columns and formatted DatetimeIndex. It requires three inputs: a <code>ticker</code> symbol, a <code>start</code> date, and an <code>end</code> date. If you can want to get stock data from January 1, 2024, up to today, just pass <code>None</code> to the <code>end</code> parameter. Here is an example:</p>
<div class="C0-SHCodePACKT">
<pre><code>msft = get_stock_data('MSFT', '2024-01-01', None)</code></pre>
</div>
<p>This will give you stock data from January 1, 2024, up to the latest available data as of when the request is made.</p>
</section>
<section id="writing-time-series-data-to-a-relational-database-postgresql-and-mysql" class="level2" data-number="6.3">
<h2 data-number="6.3">Writing time series data to a relational database (PostgreSQL and MySQL)</h2>
<p>In this recipe, you will write your DataFrame to a relational database such as <strong>PostgreSQL</strong>. The approach is the same for any relational database system supported by the <code>SQLAlchemy</code> Python library. You will experience how SQLAlchemy makes switching the backend database (called <code>dialect</code>) simple without altering the code. The abstraction layer provided by the SQLAlchemy library makes it feasible to switch to any supported database, such as from PostgreSQL to Amazon Redshift, using the same code.</p>
<p>The sample list of supported relational databases (dialects) in SQLAlchemy includes the following:</p>
<ul>
<li>Microsoft SQL Server</li>
<li>MySQL/MariaDB</li>
<li>PostgreSQL</li>
<li>Oracle</li>
<li>SQLite</li>
</ul>
<p>Additionally, external dialects are available to install and use with SQLAlchemy to support other databases (dialects), such as <code>Snowflake</code>, <code>Microsoft SQL Server</code>, and <code>Google BigQuery</code>. Please visit the official page of SQLAlchemy for a list of available dialects: <a href="https://docs.sqlalchemy.org/en/14/dialects/">https://docs.sqlalchemy.org/en/14/dialects/</a>.</p>
<section id="getting-ready-17" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1">Getting ready</h3>
<blockquote>
<p>You should refer to the recipe “<em>Reading data from a relational database</em>” in <em>Chapter 3</em>, <em>Reading Time Series Data from Databases,</em> as a refresher on the different ways to connect to PostgreSQL.</p>
</blockquote>
<p>In this recipe, you will use the <code>yfinance</code> Python library to pull stock data.</p>
<p>To install the libraries using <strong>conda</strong>, run the following:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt; conda install sqlalchemy psycopg</code></pre>
</div>
<p>To install the libraries using <code>pip</code>, run the following:</p>
<div class="C0-SHConPACKT">
<pre><code>&gt;&gt; pip install sqlalchemy
&gt;&gt; pip install pyscopg</code></pre>
</div>
<p>The file is provided in the GitHub repository for this book, which you can find here: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>.</p>
</section>
<section id="how-to-do-it-16" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2">How to do it…</h3>
<p>In this recipe, you will be pulling Amazon's stock data from January, 2019 to December, 2023 using the <code>yfnance</code> library into a pandas DataFrame, and then writing the DataFrame to a table in a PostgreSQL database:</p>
<ol>
<li>Start by calling the <code>get_stock_data</code> function created in the <em>Technical Requirements</em> section.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_hist = get_stock_data('AMZN', '2019-01-01', '2023-12-31')</code></pre>
</div>
<ol>
<li>You will need to create a SQLAlchemy <strong>engine</strong> object. The engine informs SQLAlchemy and pandas which dialect (backend database) we plan to interact with and connection details for the running database instance. Utilize <code>URL.create()</code> method to create a properly formatted URL object by providing the necessary parameters (<code>drivername</code>, <code>username</code>, <code>password</code>, <code>host</code>, <code>port</code>, and <code>database</code>). These parameters are stored in a <code>database.cfg</code> file.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from sqlalchemy import create_engine, URL
from configparser import ConfigParser
config = ConfigParser()
config.read('database.cfg')
config.sections()
params = dict(config['POSTGRESQL'])
url = URL.create('postgresql+psycopg', **params)
print(url)
&gt;&gt;
postgresql+psycopg://postgres:***@127.0.0.1/postgres</code></pre>
</div>
<p>You can now pass the <code>url</code> object to <code>create_engine</code>:</p>
<div class="C1-SHCodePACKT">
<pre><code>engine = create_engine(url)
print(engine)
&gt;&gt;
Engine(postgresql+psycopg://postgres:***@127.0.0.1/postgres)</code></pre>
</div>
<ol>
<li>Let's write the <code>amz_hist</code> DataFrame in a new <code>amzn</code> table in our PostgreSQL database instance. This is achieved using <code>the DataFrame.to_sql()</code> writer function, which leverages SQLAlchemy's capabilities to convert the DataFrame into the appropriate table schema and translate the data into the appropriate SQL statements such as <code>CREATE TABLE</code> and <code>INSERT INTO</code> specific to the dialect (backend database). If the table does not exist, a new table is created before loading the data, otherwise, if the table exists, then you will need to provide instructions on how it should be handled. This is done through the <code>if_exists</code> parameter, which accepts one of these options: <code>'fail'</code>, <code>'replace'</code>, or <code>'append'</code>.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_hist.to_sql('amzn',
            engine,
            if_exists='replace')</code></pre>
</div>
<p>An alternative method to accomplish the same tasks as the preceding code is by utilizing the <code>with</code> clause, this way you do not have to manage the connection. This would be a preferred approach in general.</p>
<div class="C1-SHCodePACKT">
<pre><code>with engine.connect() as connection:
    hist.to_sql('amzn',
                    connection,
                    if_exists='replace')</code></pre>
</div>
<p>Once the preceding code is executed, a new <code>amzn</code> table is created under the public schema in the default <code>postgres</code> database (default).</p>
<p>You can validate this by running the following against the database:</p>
<div class="C1-SHCodePACKT">
<pre><code>from sqlalchemy import text
query = """
SELECT EXISTS (
   SELECT FROM information_schema.tables
   WHERE  table_schema = 'public'
   AND    table_name   = 'amzn'
   );"""
with engine.connect() as conn:
    result = conn.execute(text(query))
print(result.fetchone())
&gt;&gt;
(True,)</code></pre>
</div>
<p>Notice the use of <code>text()</code> function that wraps our query. The <code>text()</code> constructs a new <code>TextClause</code> to represent a textual SQL string.</p>
<ol>
<li>Confirm the data was written to the database by querying the <code>amzn</code> table and counting the number of records:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = "select count(*) from amzn;"
with engine.connect() as conn:
    result = conn.execute(text(query))
result.fetchone()
&gt;&gt;
(1258,)</code></pre>
</div>
<ol>
<li>Next, request additional Amazon stock prices using <code>get_stock_data</code>, this time the 2024 data (for example, January 1, 2024, to September 23, 2024), and append it to the existing <code>amzn</code> table. Here, you will take advantage of the <code>if_exists</code> parameter in the <code>to_sql()</code> writer function.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_hist_2024 = get_stock_data('AMZN', '2024-01-01', None)
print(amzn_hist_2024.shape)
&gt;&gt;
(182, 5)</code></pre>
</div>
<p>Make sure to pass <code>append</code> to the <code>if_exists</code> parameter, as shown in the following code:</p>
<div class="C1-SHCodePACKT">
<pre><code>with engine.connect() as connection:
    amzn_hist_2024.to_sql('amzn',
                    connection,
                    if_exists='append')</code></pre>
</div>
<ol>
<li>Count the total number of records to ensure we have appended all 182 to the original 1258 records. You will run the same query that was executed earlier, as shown in the following code:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = "select count(*) from amzn;"
with engine.connect() as conn:
    result = conn.execute(text(query))
print(result.fetchone())
&gt;&gt;
(1440,)</code></pre>
</div>
<p>Indeed, you can observe that all of the 1440 records were written to the <code>amzn</code> table.</p>
</section>
<section id="how-it-works-16" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3">How it works…</h3>
<p>Using the <code>DataFrame.to_sql()</code> writer function, SQLAlchemy handles many details under the hood, such as creating the table schema, inserting our records, and committing to the database.</p>
<p>Working with pandas and SQLAlchemy to write and read to a relational database is very similar. We discussed using SQLAlchemy for reading data in the <em>Reading data from a relational database</em> recipe in <em>Chapter 3</em>, <em>Reading Time Series Data from Databases</em>. Many of the concepts discussed apply here as well.</p>
<p>We always start with <code>create_engine</code> and specify the <strong>dialect</strong> (backend database). The <code>to_sql()</code> function will map the DataFrame data types to the appropriate PostgreSQL data types. The advantage of using an <strong>Object Relational Mapper (ORM)</strong> such as SQLAlchemy is that it gives you an abstraction layer, so you do not have to worry about <em>converting</em> the DataFrame schema into a specific database schema.</p>
<p>In the preceding example, you used the <code>if_exists</code> parameter in the <code>DataFrame.to_sql()</code> function with two different arguments:</p>
<ol>
<li>Initially, you set the value to <code>replace</code>, which would overwrite the table if it existed. If we translate this overwrite operation into SQL commands, it will execute a <code>DROP TABLE</code> followed by <code>CREATE TABLE</code>. This can be dangerous if you already have a table with data and you intend to append to it. Because of this concern, the default value is set to <code>fail</code> if you do not pass any argument. This default behavior would throw an error if the table existed.</li>
<li>In the second portion of the recipe, the plan was to insert additional records into the existing table, and you updated the argument from <code>replace</code> to <code>append</code>.</li>
</ol>
<p>When you pulled the stock data using <code>yfinance</code>, it automatically assigned the <code>Date</code> field as <code>DatetimeIndex</code>. In other words, the <code>Date</code> was not a column but an index. The default behavior in <code>to_sql()</code> is to write the DataFrame index as a column in the database, which is controlled by the <code>index</code> parameter. This is a Boolean parameter, and the default is set to <code>True</code>, which writes the DataFrame index as a column.</p>
<p>Another interesting parameter that can be extremely useful is <code>chunksize</code>. The default value is <code>None</code>, which writes all the rows in the DataFrame at once. If your dataset is extremely large, you can use the <code>chunksize</code> parameter to write to the database in batches; for example, a <code>chunksize</code> of 500 would write to the database in batches of 500 rows at a time.</p>
<div class="C0-SHCodePACKT">
<pre><code>with engine.connect() as connection:
    amzn_hist.to_sql('amzn',
                    connection,
                    chunksize=500,
                    if_exists='append')</code></pre>
</div>
</section>
<section id="theres-more-16" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4">There's more…</h3>
<p>When using the pandas <code>read_sql</code>, <code>read_sql_table</code>, <code>read_sql_query</code>, and <code>to_sql</code> I/O functions, they expect a SQLAlchemy connection object (SQLAlchemy engine). To use SQLAlchemy to connect to a database of choice, you need to install the appropriate Python DBAPI (driver) for that specific database (for example, Amazon Redshift, Google BigQuery, MySQL, MariaDB, PostgreSQL, or Oracle). This gives you the advantage of writing your script once and still having it work with other dialects (backend databases) supported by SQLAlchemy. To demonstrate this, let's extend the last example.</p>
<p><strong>Amazon Redshift</strong>, a popular cloud data warehouse database, is based on PostgreSQL at its core, and it has several enhancements, including columnar storage for fast analytical queries. You will explore the simplicity of SQLAlchmey, as well as other options for writing a pandas DataFrame to Amazon Redshift.</p>
<section id="writing-to-amazon-redshift-with-sqlaclhemy" class="level4" data-number="6.3.4.1">
<h4 data-number="6.3.4.1">Writing to Amazon Redshift with SQLAclhemy</h4>
<p>You will use the same code but write to an Amazon Redshift database this time. The only requirement, aside from a running MySQL instance, is installing a Python DBAPI (driver) for<br/>
Amazon Redshift. Note, that <code>sqlalchemy-redshift</code> requires <code>psycopgy2</code>.</p>
<p>To install using <strong>conda</strong>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge psycopg2 sqlalchemy-redshift</code></pre>
</div>
<p>To install using <strong>pip</strong>, run the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install pip install psycopg2 sqlalchemy-redshift</code></pre>
</div>
<p>You will use the same code for PostgreSQL; the only difference is the SQLAlchemy engine, which uses Amazon Redshift DBAPI. Start by loading the connection parameters from your configuration file. In this example, the configurations are stored in a database.cfg file</p>
<div class="C0-SHCodePACKT">
<pre><code>[AWS]
host=yourendpoint
port=5439
database=dev
username=username
password=password</code></pre>
</div>
<p>Using <code>ConfigParser</code> and <code>URL</code> to extract the parameters and construct the URL:</p>
<div class="C0-SHCodePACKT">
<pre><code>from configparser import ConfigParser
config = ConfigParser()
config.read('database.cfg')
config.sections()
params = dict(config['AWS'])
from sqlalchemy import URL, create_engine
url = URL.create('redshift+psycopg2', **params)
print(url)
&gt;&gt;
redshift+psycopg2://awsuser:***@redshift-cluster-1.cltc17lacqp7.us-east-1.redshift.amazonaws.com:5439/dev</code></pre>
</div>
<p>You can now create the engine using:</p>
<div class="C0-SHCodePACKT">
<pre><code>aws_engine = create_engine(url)</code></pre>
</div>
<p>Using the <code>yfinance</code> library create a new <code>amzn_hist</code> DataFrame based on the past <strong>5-years</strong> of stock data:</p>
<div class="C0-SHCodePACKT">
<pre><code>amzn = yf.Ticker("AMZN")
amzn_hist = amzn.history(period="5y")
amzn_hist = amzn_hist[['Open', 'High', 'Low', 'Close', 'Volume']]</code></pre>
</div>
<p>Before writing our DataFrame, we need to reset the index. This will give us back our Date column. We do this because Amazon Redshift does not support traditional indexes since it is a columnar database (instead, you can define a <strong>sort key</strong>).</p>
<div class="C0-SHCodePACKT">
<pre><code>amzn_hist = amzn_hist.reset_index()
with aws_engine.connect() as conn:
    amzn_hist.to_sql('amzn',
                    conn,
                    if_exists='replace', index=False)</code></pre>
</div>
<p>Notice the index=False in the preceding code. This is done because <code>to_sql</code> will write the index object in a DataFrame since the default is <code>index=True</code>. When you reset the DataFrame index, it moves the DatetimeIndex to a Date column and replaces the index with a <code>RangeIndex</code> (ranging from 0 to 1257). Using <code>index=False</code> ensures we do not attempt to write the <code>RangeIndex</code> in Amazon Redshift.</p>
<p>Finally, you can validate the total number of records written:</p>
<div class="C0-SHCodePACKT">
<pre><code>from sqlalchemy import text
query = "select count(*) from amzn;"
with aws_engine.connect() as conn:
    result = conn.execute(text(query))
result.fetchone()
&gt;&gt;
(1258,)</code></pre>
</div>
</section>
<section id="writing-to-amazon-redshift-using-the-redshift_connector" class="level4" data-number="6.3.4.2">
<h4 data-number="6.3.4.2">Writing to Amazon Redshift using the redshift_connector</h4>
<p>In this example, you will utilize a different library, the <code>redshift_connector</code>. You will first need to install the library.</p>
<p>You can install it with <strong>conda</strong> using:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge redshift_connector</code></pre>
</div>
<p>You can also install it with <strong>pip</strong> using:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install redshift_connector</code></pre>
</div>
<p>Note, the <code>redshift_connector</code> expets a <code>user</code> parameter, unlike SQLAclhemy which expects a <code>username</code> parameter. For this, you can create a new section in your configuration file. An example is shown below:</p>
<div class="C0-SHCodePACKT">
<pre><code>[AWS2]
host=yourendpoint
port=5439
database=dev
user=username
password=password</code></pre>
</div>
<p>The following code reads the parameters from a <code>database.cfg</code> file, and passes these parameters to <code>redshift_connector.connect()</code> to create a connection object.</p>
<div class="C0-SHCodePACKT">
<pre><code>import redshift_connector
from configparser import ConfigParser
config = ConfigParser()
config.read('database.cfg')
config.sections()
params = dict(config['AWS2'])
conn = redshift_connector.connect(**params)</code></pre>
</div>
<p>You will create a cursor object which gives access to the <code>write_dataframe</code> method.</p>
<div class="C0-SHCodePACKT">
<pre><code>cursor = conn.cursor()
cursor.write_dataframe(amzn_hist, 'amzn')</code></pre>
</div>
<p>Finally, you will commit the transaction.</p>
<div class="C0-SHCodePACKT">
<pre><code>conn.commit()</code></pre>
</div>
<p>Do note that the <code>write_dataframe</code> does not provide arguments for specifying append, replace/overwrite, or fail behaviors, as you have seen with SQLAclehmy. The <code>write_dataframe</code> method expects an existing table in Amazon Redshift to append to.</p>
</section>
<section id="writing-to-amazon-redshift-with-aws-sdk-for-pandas" class="level4" data-number="6.3.4.3">
<h4 data-number="6.3.4.3">Writing to Amazon Redshift with AWS SDK for pandas</h4>
<p>The <strong>awswrangler</strong> library or AWS SDK for pandas make integrating with several AWS services such as Athena, Glue, Redshift, Neptune, DynamoDB, EMR, S3, and others easy.</p>
<p>You can install the library using <strong>conda</strong>:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge awswrangler</code></pre>
</div>
<p>You can also install it using <strong>pip</strong>:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install 'awswrangler[redshift]'</code></pre>
</div>
<p>You can utilize the <code>conn</code> object created in the previous section, <em>Writing to Amazon Redshift using the redshift_connector</em></p>
<div class="C0-SHCodePACKT">
<pre><code>import awswrangler as wr
wr.redshift.to_sql(
    df=amzn_hist,
    table='amzn',
    schema='public',
    con=conn,
    mode='overwrite'
)</code></pre>
</div>
<p>Note the mode parameter supports three (3) different options: <code>overwrite</code>, <code>append</code>, or <code>upsert</code>.</p>
</section>
</section>
<section id="see-also-17" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5">See also</h3>
<p>Here are some additional resources:</p>
<ul>
<li>To learn more about the <code>DataFrame.to_sql()</code> function, you can visit <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html</a>.</li>
<li>To learn more about <strong>SQLAlchemy</strong> features, you can start by reading their features page: <a href="https://www.sqlalchemy.org/features.html">https://www.sqlalchemy.org/features.html</a>.</li>
<li>To learn about <code>awswrangler</code> you can visit their GitHub repo here <a href="https://github.com/aws/aws-sdk-pandas">https://github.com/aws/aws-sdk-pandas</a></li>
</ul>
</section>
</section>
<section id="writing-time-series-data-to-mongodb" class="level2" data-number="6.4">
<h2 data-number="6.4">Writing time series data to MongoDB</h2>
<p><strong>MongoDB</strong> is a document database system that stores data in <strong>BSON</strong> format. When you query data from MongoDB, the data will be represented in JSON format. BSON is similar to JSON; it is the binary encoding of JSON. Unlike JSON, though, it is not in a human-readable format. JSON is great for transmitting data and is system-agnostic. BSON is designed to store data and is associated with MongoDB.</p>
<p>In this recipe, you will explore writing a pandas DataFrame to MongoDB.</p>
<section id="getting-ready-18" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1">Getting ready</h3>
<blockquote>
<p>You should refer to the recipe “<em>Reading data from a document database</em>” in <em>Chapter 3</em>, <em>Reading Time Series Data from Databases</em> as a refresher on the different ways to connect to MongoDB.</p>
</blockquote>
<p>In the <em>Reading data from a document database recipe</em> in <em>Chapter 3</em>, <em>Reading Time Series Data from Databases</em>, we installed <code>pymongo</code>. For this recipe, you will be using that same library again.</p>
<p>To install using <strong>conda</strong>, run the following:</p>
<div class="C0-SHConPACKT">
<pre><code>$ conda install -c anaconda pymongo -y</code></pre>
</div>
<p>To install using <strong>pip</strong>, run the following:</p>
<div class="C0-SHConPACKT">
<pre><code>$ python -m pip install pymongo</code></pre>
</div>
<p>The file is provided in the GitHub repository for this book, which you can find here: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook</a>.</p>
</section>
<section id="how-to-do-it-17" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2">How to do it…</h3>
<p>To store data in MongoDB, you will create a <strong>database</strong> and a <strong>collection</strong>. A database contains one or more collections, which are like tables in relational databases. Once a collection is created, you will write your data as documents. A collection contains documents, which are equivalent to rows in relational databases.</p>
<ol>
<li>Start by importing the necessary libraries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from pymongo import MongoClient</code></pre>
</div>
<ol>
<li>Create a <code>MongoClient</code> instance to establish a connection to the database:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>client = MongoClient('mongodb://localhost:27017')</code></pre>
</div>
<ol>
<li>Create a new database named <code>stock_data</code> and a <strong>time series collection</strong> named <code>daily_stock</code>.</li>
</ol>
<p>First, we create a <strong>regular collection</strong> in MongoDB:</p>
<div class="C1-SHCodePACKT">
<pre><code>db = client['stock_data']
collection = db.create_collection('amazon')</code></pre>
</div>
<p>This will create a new database called <code>stock_data</code> and a collection named amazon. If <code>stock_data</code> already exists, it will add the <code>amazon</code> collection to the existing database.</p>
<p>However, since we are working with time series data, we can use a more efficient way to store and query our data by creating a <strong>time series collection.</strong> Starting with MongoDB version 5.0, time series collections are optimized for time-stamped data. We can modify the previous code to create a <code>daily_stock</code> time series collection.</p>
<div class="C1-SHCodePACKT">
<pre><code>db = client['stock_data']
ts = db.create_collection(
    name="daily_stock",
    timeseries={
        "timeField": "Date",
        "metaField": "symbol",
        "granularity": "hours"
    }
)</code></pre>
</div>
<p>With this update, we are now using a time series collection, which improved storage efficiency and query performance for time-based data like our stock prices. Moving forward, we’ll use the <code>ts</code> reference to interact with the time series collection.</p>
<ol>
<li>You will utilize the <code>get_stock_data</code> function created in the Technical Requirements section to pull Amazon stock data from January 1, 2019, to August 31, 2024:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_hist = get_stock_data('AMZN', '2019-01-01', '2024-8-31')</code></pre>
</div>
<ol>
<li>In pandas, we work with data in a tabular format, where each column represents a variable, and each row represents a data point. MongoDB, however, stores data as documents in a JSON-like format (BSON), where each document is an independent record that can include timestamp, metadata, and other key-value pairs.</li>
</ol>
<p>Before inserting data into MongoDB, you need to transform the DataFrame into a list of dictionaries where each dictionary (or document) represents a stock data point. Each dictionary will include a timestamp (<code>Date</code>), stock information (e.g. <code>High</code>, <code>Low</code>), and metadata (e.g.<code> "ticker": "AMZN"</code>)</p>
<p>You will explore two options: the fist option utilizing the <code>to_dict()</code> method, and a second option were you iterate over the DataFrame.</p>
<p>Let’s explore the first option:</p>
<div class="C1-SHCodePACKT">
<pre><code>metadata = {"ticker": "AMZN"}
amzn_hist['metadata'] = [metadata] * len(amzn_hist)
amzn_hist = amzn_hist.reset_index()
amzn_hist['Date'] = pd.to_datetime(amzn_hist['Date'])
amzn_records = amzn_hist.to_dict(orient='records')
amzn_records[0:2]
&gt;&gt;
[{'Date': Timestamp('2019-01-02 00:00:00'),
  'Open': 73.26000213623047,
  'High': 77.66799926757812,
  'Low': 73.04650115966797,
  'Close': 76.95649719238281,
  'Volume': 159662000,
  'metadata': {'ticker': 'AMZN'}},
 {'Date': Timestamp('2019-01-03 00:00:00'),
  'Open': 76.00050354003906,
  'High': 76.9000015258789,
  'Low': 74.85549926757812,
  'Close': 75.01399993896484,
  'Volume': 139512000,
  'metadata': {'ticker': 'AMZN'}}]</code></pre>
</div>
<p>Here, we assumed that all the data have the same metadata information (e.g. <code>"ticker": "AMZN</code>).</p>
<blockquote>
<p>The default value for the <code>orient</code> parameter in the <code>to_dict()</code> method is <code>dict</code> which produces a <strong>dictionary</strong> that follows the <code>{column -&gt; {index -&gt; value}}</code> pattern.</p>
<blockquote>
<p>On the other hand, using <code>records</code> as the value, produces a <strong>list</strong> that follows the <code>[{column -&gt; value}, … , {column -&gt; value}]</code> pattern.</p>
</blockquote>
</blockquote>
<p>Now, let’s explore the second option which provides more flexibility for adding specific fields or applying transformation to individual records, for example, a different ticker value based:</p>
<div class="C1-SHCodePACKT">
<pre><code>amzn_hist = amzn_hist.reset_index()
amzn_records = []
for idx, row in amzn_hist.iterrows():
    doc = {
        "Date": pd.to_datetime(row['Date']),
        "metadata": {"ticker": "AMZN"},
        "High": row['High'],
        "Low": row['Low'],
        "Close": row['Close'],
        "Open": row['Open'],
        "Volume": row['Volume']
    }
    amzn_records.append(doc)
amzn_records[0:2]
&gt;&gt;
[{'Date': Timestamp('2019-01-02 00:00:00'),
  'metadata': {'ticker': 'AMZN'},
  'High': 77.66799926757812,
  'Low': 73.04650115966797,
  'Close': 76.95649719238281,
  'Open': 73.26000213623047,
  'Volume': 159662000},
 {'Date': Timestamp('2019-01-03 00:00:00'),
  'metadata': {'ticker': 'AMZN'},
  'High': 76.9000015258789,
  'Low': 74.85549926757812,
  'Close': 75.01399993896484,
  'Open': 76.00050354003906,
  'Volume': 139512000}]</code></pre>
</div>
<p>You now have a Python list of length <code>1426</code> (a dictionary for each record):</p>
<div class="C1-SHCodePACKT">
<pre><code>len(amzn_records)
&gt;&gt;
1426</code></pre>
</div>
<ol>
<li>Now, you are ready to write to the time series <code>daily_stock </code>collection using the <code>insert_many()</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>result = ts.insert_many(amzn_records)</code></pre>
</div>
<ol>
<li>You can validate that the database and collection are created with the following code:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>client.list_database_names()
&gt;&gt;
['admin', 'config', 'local', 'stock_data']
db.list_collection_names()
&gt;&gt;
['daily_stock', 'system.buckets.daily_stock', 'system.views']</code></pre>
</div>
<ol>
<li>Next, pull Microsoft stock data (MSFT) and add it to the same <code>daily_stock</code> time series collection. Later, you will explore how the metadata can be used to distinguish between different stock symbols (AMZN vs MSFT) when querying the data.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>msft_hist = get_stock_data('MSFT', '2019-01-01', '2024-8-31')
metadata = {"ticker": "MSFT"}
msft_hist['metadata'] = [metadata] * len(msft_hist)
msft_hist = msft_hist.reset_index()
msft_hist['Date'] = pd.to_datetime(msft_hist['Date'])
msft_records = msft_hist.to_dict(orient='records')
result = ts.insert_many(msft_records)</code></pre>
</div>
<p>You can check the total number of documents written by querying the database, as shown in the following code:</p>
<div class="C1-SHCodePACKT">
<pre><code>ts.count_documents({})
&gt;&gt;
2852</code></pre>
</div>
<ol>
<li>Now, the collection contains data for two stock symbols. You can use metadata to filter for each symbol in your queries. You will start by querying the <code>daily_stock</code> collection to retrieve only the Microsoft (MSFT) stock data. This is where the <code>metadata</code> field becomes useful, allowing you to filter by the stock symbol. Let's first define a date range and then query for MSFT data only.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from datetime import datetime
# Define date range
start_date = datetime(2019, 1, 1)
end_date = datetime(2019, 1, 31)
# Query for MSFT stock data within the date range
results = ts.find({
    "metadata.ticker": "MSFT",
    "Date": {"$gte": start_date, "$lte": end_date}
})
# Convert the query results to a DataFrame
msft_df = (pd.DataFrame(results)
             .set_index('Date')
             .drop(columns=['_id', 'metadata']))
print(msft_df.head())
&gt;&gt;
                Close       High       Open    Volume        Low
Date                                                           
2019-01-02  95.501335  96.096327  94.018571  35329300  93.442465
2019-01-03  91.988037  94.623014  94.538011  42579100  91.799146
2019-01-04  96.266327  96.814101  94.179125  44060600  93.433020
2019-01-07  96.389107  97.531873  95.992445  35656100  95.369122
2019-01-08  97.087975  98.192962  97.314637  31514400  96.058536</code></pre>
</div>
<ol>
<li>You can also perform an aggregation to calculate the average <code>Close</code> price per ticker (symbol):</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>msft_avg_close = ts.aggregate([
    {"$group":
         {"_id": "$metadata.ticker",
          "avgClose":
                  {"$avg": "$Close"}}
    }
])
for doc in msft_avg_close:
    print(doc)
&gt;&gt;
{'_id': 'AMZN', 'avgClose': 133.4635473361022}
{'_id': 'MSFT', 'avgClose': 252.4193055419066}</code></pre>
</div>
</section>
<section id="how-it-works-17" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3">How it works…</h3>
<p><code>PyMongo</code> provides two insert functions to write our records as documents into a collection. These functions are as follows:</p>
<ul>
<li><code>insert_one()</code> inserts one document into a collection.</li>
<li><code>insert_many()</code> inserts multiple documents into a collection.</li>
</ul>
<p>In the preceding example, you used <code>insert_many()</code> and passed the data to be written as documents simultaneously. However, before doing so, it was essential to convert the DataFrame to a list of dictionaries format that follows the <code>[{column -&gt; value}, … , {column -&gt; value}]</code> pattern. This was accomplished with <code>orient='records'</code> in the <code>to_dict()</code> DataFrame method.</p>
<p>When documents are inserted into the database, they are assigned a unique <code>_id </code>value. MongoDB will automatically generate one during the insert operation if the document does not already have an <code>_id</code>. You can capture the generated <code>_id</code> because the insert functions return a result object—either an <code>InsertOneResult</code> for single inserts or an <code>InsertManyResultfor</code> bulk inserts. The following code demonstrates how this works using <code>insert_one</code> and the <code>InsertOneResult</code> class:</p>
<div class="C0-SHCodePACKT">
<pre><code>one_record = amzn_records[0]
one_record
&gt;&gt;
{'Date': Timestamp('2019-01-02 00:00:00'),
 'metadata': {'ticker': 'AMZN'},
 'High': 77.66799926757812,
 'Low': 73.04650115966797,
 'Close': 76.95649719238281,
 'Open': 73.26000213623047,
 'Volume': 159662000}
result_id = ts.insert_one(one_record)
result_id
&gt;&gt;
InsertOneResult(ObjectId('66f2ed5efad8cbd88968d02e'), acknowledged=True)</code></pre>
</div>
<p>The returned object is an instance of <code>InsertOneResult</code>; to see the actual value, you can use the <code>insert_id</code> property:</p>
<div class="C0-SHCodePACKT">
<pre><code>result_id.inserted_id
&gt;&gt;
ObjectId('66f2ed5efad8cbd88968d02e')</code></pre>
</div>
<p>If you have ticker data in minutes, you can take advantage of the <code>granularity</code> attribute, which can be <code>seconds</code>, <code>minutes</code>, or <code>hours</code>.</p>
</section>
<section id="theres-more-17" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4">There's more…</h3>
<p>In the previous example, if you run the following code to query the database to list the collections available, you will see three collections:</p>
<div class="C0-SHCodePACKT">
<pre><code>db = client['stock_data']
db.list_collection_names()
&gt;&gt;
['daily_stock', 'system.buckets.daily_stock', 'system.views']</code></pre>
</div>
<p>You created the <code>daily_stock</code> collection, so what are the other two collections? Let’s first explore the <strong>Bucket Pattern</strong> in MongoDB.</p>
<p>The bucket patterns is a data modeling technique to optimize how data is stored in the database. By default, when you convert your DataFrame into a list of dictionaries, you are essentially inserting each DataFrame record (data point) as a separate MongoDB document. This creates a 1-on-1 mapping between records and documents.</p>
<p>However, the <strong>bucket strategy</strong> allows you to group related data points into a single document. For example, if you have hourly data, you can group it into a bucket, such as 24-hour period, and store all the data for that range in one document. Similarly, if we have sensor data from multiple devices, you can use the bucket pattern to group the data (for example, by device ID and by a time range) and insert them as a single document. This will reduce the number of documents in the database, improve overall performance, and simplify querying.</p>
<p>When you create a time series collection, MongoDB automatically applies the bucket pattern to store the data in an efficient format. Let’s break it down:</p>
<ul>
<li><code>daily_stock</code>: This is the main time series collection you created. It acts as a view that allows you to interact with your time series data using standard MongoDB operations.</li>
<li><code>system.buckets.daily_stock</code>: This is an internal collection where MongoDB stores the actual time series data using the bucket pattern. MongoDB automatically implements this strategy for time series collections to improve storage and query performance. Here's how it works:
<ul>
<li>Documents are grouped into "buckets" based on timestamps and metadata fields (e.g., the stock symbol).</li>
<li>Each bucket contains data points close together in time and share the same metadata values.</li>
<li>This bucketing strategy significantly reduces the number of documents stored, improving query efficiency and reducing disk usage.</li>
</ul></li>
<li><code>system.views</code>: This is a system collection that MongoDB uses to store information about all views in the database, including the view for your time series collection.</li>
</ul>
<p>To better understand how the bucket pattern is applied, let’s explore the technique by creating a new collection (a regular collection, not a time series collection) and bucket the daily stock data by year and month:</p>
<div class="C0-SHCodePACKT">
<pre><code>db = client['stock_data']
bucket = db.create_collection(name='stock_bucket')</code></pre>
</div>
<p>Next, let’s create a new DataFrame and add two additional columns: <strong>month</strong> and <strong>year</strong>:</p>
<div class="C0-SHCodePACKT">
<pre><code>amzn = yf.Ticker("AMZN")
amzn_hist = amzn.history(period="5y")
amzn_hist = amzn_hist[['Open',
                       'High',
                       'Low',
                       'Close',
                       'Volume']].reset_index()
amzn_hist['Date'] = pd.to_datetime(amzn_hist['Date'])
amzn_hist['month'] = amzn_hist['Date'].dt.month
amzn_hist['year'] = amzn_hist['Date'].dt.year</code></pre>
</div>
<p>In the preceding code, you added a <code>month</code> and <code>year</code> columns to the DataFrame and initiated a new collection as <code>stocks_bucket</code>. In the next code segment, you will loop through the data and write your groups (by year and month) as a single document:</p>
<div class="C0-SHCodePACKT">
<pre><code>for year in amzn_hist['year'].unique():
    for month in amzn_hist['month'].unique():
        record = {}
        record['month'] = int(month)
        record['year'] = int(year)
        record['symbol'] = 'AMZN'
        try:
            prices = amzn_hist[(amzn_hist['month'] == month) &amp; (amzn_hist['year'] == year)]['Close'].values
            record['price'] = [float(price) for price in prices]
        except Exception as e:
            print(f"Error processing data for {month}/{year}: {str(e)}")
            continue
        else:
            bucket.insert_one(record)</code></pre>
</div>
<p>In the code, you looped through the unique year and month combinations, then for each combination you create a record dictionary containing the month, year, symbol, and a list of Close prices. The record is then inserted into the <code>stock_bucket</code> collection, effectively bucketing the data by month and year.</p>
<p>To illustrate the difference in number of documents, run the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>print('without bucketing: ',
      db.daily_stock.count_documents({}))
print('with bucketing: ',
      db.stock_bucket.count_documents({}))
&gt;&gt;
without bucketing:  2853
with bucketing:  72</code></pre>
</div>
<p>Notice the <code>stock_bucket</code> collection contains 72 documents, representing the data grouped by year and month.</p>
<p>To query the database for the year 2024 and the month of June and see how the document is represented, use the following code example:</p>
<div class="C0-SHCodePACKT">
<pre><code>results = pd.DataFrame(bucket.find({'year':2024, 'month': 6}))
results['price'].to_dict()[0]
&gt;&gt;
[178.33999633789062,
 179.33999633789062,
 181.27999877929688,
 185.0,
 184.3000030517578,
 187.05999755859375,
 187.22999572753906,
 186.88999938964844,
 183.8300018310547,
 183.66000366210938,
 184.05999755859375,
 182.80999755859375,
 186.10000610351562,
 189.0800018310547,
 185.57000732421875,
 186.33999633789062,
 193.61000061035156,
 197.85000610351562,
 193.25]</code></pre>
</div>
<p>You can also run the same query using MongoDB Compass, and you should get similar results as shown in the figure:</p>
<figure>
<img src="../media/file41.png" alt="Figure – Using MongoDB Compass to query the stock_bucket collection" width="1216" height="1300"/><figcaption aria-hidden="true">Figure – Using MongoDB Compass to query the stock_bucket collection</figcaption>
</figure>
</section>
<section id="see-also-18" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5">See also</h3>
<ul>
<li>For more information on storing time series data and bucketing in MongoDB, you can refer to this MongoDB blog post: <a href="https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices">https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices</a></li>
<li>Check out the MongoDB manual to learn more about <strong>time series collections</strong> <a href="https://www.mongodb.com/docs/manual/core/timeseries-collections/">https://www.mongodb.com/docs/manual/core/timeseries-collections/</a></li>
</ul>
</section>
</section>
<section id="writing-time-series-data-to-influxdb" class="level2" data-number="6.5">
<h2 data-number="6.5">Writing time series data to InfluxDB</h2>
<p>When working with large time series data, such as a sensor or <strong>Internet of Things</strong> (<strong>IoT</strong>) data, you will need a more efficient way to store and query such data for further analytics. This is where <strong>time series databases</strong> shine, as they are built exclusively to work with complex and very large time series datasets.</p>
<p>In this recipe, we will work with <strong>InfluxDB</strong> as an example of how to write to a time series database.</p>
<section id="getting-ready-19" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1">Getting ready</h3>
<blockquote>
<p>You should refer to the recipe “<em>Reading data from a time series database</em>” in Chapter 3 “<em>Reading Time Series Data from Databases</em>” as a refresher on the different ways to connect to InfluxDB.</p>
</blockquote>
<p>You will be using the <code>ExtraSensory</code> dataset, a mobile sensory dataset made available by the University of California, San Diego: <em>Vaizman, Y., Ellis, K., and Lanckriet, G. "Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches". IEEE Pervasive Computing, vol. 16, no. 4, October-December 2017, pp. 62-74. doi:10.1109/MPRV.2017.3971131</em></p>
<p>You can download the dataset here: <a href="http://extrasensory.ucsd.edu/#download">http://extrasensory.ucsd.edu/#download</a></p>
<p>The dataset consists of 60 files, each representing a participant, each identified by a unique identifier (UUID). Each file contains a total of 278 columns: 225 (features), 51 (labels), and 2 (timestamp and label_source).</p>
<p>This recipe aims to demonstrate how to write a time series DataFrame to InfluxDB. In this recipe, two columns are selected: the timestamp (date ranges from <code>2015-07-23</code> to <code>2016-06-02</code>, covering 152 days) and the watch accelerometer reading (measured in milli G-forces or milli-G).</p>
<p>Before you can interact with InfluxDB in Python, you will need to install the InfluxDB Python library.</p>
<p>You can install the library with <strong>pip</strong> by running the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>$ pip install 'influxdb-client[ciso]'</code></pre>
</div>
<p>To install using <strong>conda</strong> use the following:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge influxdb-client</code></pre>
</div>
</section>
<section id="how-to-do-it-18" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2">How to do it…</h3>
<p>You will start this recipe by reading a file from the <strong>ExtraSensory</strong> dataset (for a specific UUID) focusing on one feature column - the watch accelerometer. You will be performing some data transformations to prepare the data before writing the time series DataFrame to InfluxDB:</p>
<ol>
<li>Start by loading the required libraries:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from influxdb_client import InfluxDBClient, WriteOptions
from influxdb_client.client.write_api import SYNCHRONOUS
import pandas as pd
from  pathlib import Path</code></pre>
</div>
<ol>
<li>The data consists of 60 compressed CSV files <code>(csv.gz</code>), which you can read using <code>pandas.read_csv()</code>. The default <code>compression</code> parameter in <code>read_csv</code> is set to <code>infer</code>. This means that pandas will infer based on the file extension which compression or decompression protocol to use. The files have a (<code>gz</code>) extension, which will be used to infer which decompression protocol to use. Alternatively, you can indicate which compression protocol to use with <code>compression='gzip'</code>.</li>
</ol>
<p>In the following code, you will read one of these files, select both <code>timestamp</code> and <code>watch_acceleration:magnitude_stats:mean</code> columns, rename the columns, and, finally, perform a <strong>backfill</strong> operation for all <strong>na</strong> (missing) values:</p>
<div class="C1-SHCodePACKT">
<pre><code>path = Path('../../datasets/Ch5/ExtraSensory/')
file = '0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv.gz'
columns = ['timestamp',
           'watch_acceleration:magnitude_stats:mean']
df = pd.read_csv(path.joinpath(file),
                usecols=columns,
                compression='gzip')
df = df.bfill()
df.columns = ['timestamp','wacc']
df.shape
&gt;&gt;
(3960, 2)</code></pre>
</div>
<p>From the preceding output, you have <code>3960</code> sensor readings from that one file.</p>
<ol>
<li>To write the data to InfluxDB, you need at least one <code>measurement</code> column and a <code>timestamp</code> column. Currently, the timestamp is a Unix timestamp (<strong>epoch</strong>) captured in seconds, which is an acceptable format for writing out data to InfluxDB. For example<code>, 2015-12-08 7:06:37 PM</code> is stored as <code>1449601597</code> in the dataset.</li>
</ol>
<p><strong>InfluxDB</strong> stores timestamps in epoch nanoseconds on disk, but when querying data, InfluxDB will display the data in <strong>RFC3339 UTC format</strong> to make it more human-readable. So, <code>1449601597</code> in <strong>RFC3339</strong> would be represented as <code>2015-12-08T19:06:37+00:00.000Z</code>. Note the precision in InfluxDB is in <em>nanoseconds</em>.</p>
<p>In the following step, you will convert the Unix timestamp to a format that is more human readable for your analysis in <strong>pandas</strong>, which is also an acceptable format with InfluxDB:</p>
<div class="C1-SHCodePACKT">
<pre><code>df['timestamp'] = pd.to_datetime(df['timestamp'],
                                  origin='unix',
                                  unit='s',
                                  utc=True)
df.set_index('timestamp', inplace=True)
print(df.head())
&gt;&gt;
                                  wacc
timestamp                            
2015-12-08 19:06:37+00:00   995.369977
2015-12-08 19:07:37+00:00   995.369977
2015-12-08 19:08:37+00:00   995.369977
2015-12-08 19:09:37+00:00   996.406005
2015-12-08 19:10:55+00:00  1034.180063</code></pre>
</div>
<p>In the preceding code, the <code>unit</code> parameter is set to <code>'s'</code> for <strong>seconds</strong>. This instructs pandas to calculate the number of seconds based on the origin. The <code>origin</code> parameter is set to <code>unix</code> by default, so the conversion will calculate the number of seconds to the Unix epoch start provided. The <code>utc</code> parameter is set to <code>True</code>, which will return a <strong>UTC</strong> <code>DatetimeIndex</code> type. The <code>dtype</code> of our DataFrame index is now <code>datetime64[ns, UTC]</code>.</p>
<blockquote>
<p>You can learn more about Unix epoch timestamps in the recipe, <em>Working with Unix epoch timestamps</em>, from <em>Chapter 6</em>, <em>Working with Date and Time in Python</em></p>
</blockquote>
<ol>
<li>Next, you will need to establish a connection with the InfluxDB database instance running. All you need is to pass your API read/write token. When writing to the database, you will need to specify the bucket and organization name as well:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>bucket = "sensor"
org = "&lt;yourorg&gt;"
token = "&lt;yourtoken&gt;"
client = InfluxDBClient(url="http://localhost:8086",
                        token=token,
                        org=org)</code></pre>
</div>
<ol>
<li>Initialize the <code>write_api</code> and configure <code>WriterOptions</code>. This includes specifying <code>writer_type</code> as <code>SYNCHRONOUS</code>, <code>batch_size</code>, and <code>max_retries</code> before it fails:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>writer = client.write_api(WriteOptions(SYNCHRONOUS,
                     batch_size=500,
                     max_retries=5))
writer.write(bucket=bucket,
                record=df,
                write_precision='ns',
                data_frame_measurement_name='wacc',
                data_frame_tag_columns=[])</code></pre>
</div>
<ol>
<li>To verify that the data is written properly you can query the database using the <code>query_data_frame</code> method as shown in the following code:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>query = '''
         from(bucket: "sensor")
         |&gt; range(start: 2015-12-08)
         |&gt; pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
         '''
result = client.query_api()
influx_df = result.query_data_frame(
                             org=org,
                             query=query,
                             data_frame_index='_time')</code></pre>
</div>
<p>Inspect the returned DataFrame:</p>
<div class="C1-SHCodePACKT">
<pre><code>Influx_df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
DatetimeIndex: 3960 entries, 2015-12-08 19:06:37+00:00 to 2015-12-11 18:48:27+00:00
Data columns (total 6 columns):
 #   Column        Non-Null Count  Dtype             
---  ------        --------------  -----             
 0   result        3960 non-null   object            
 1   table         3960 non-null   int64             
 2   _start        3960 non-null   datetime64[ns, UTC]
 3   _stop         3960 non-null   datetime64[ns, UTC]
 4   _measurement  3960 non-null   object            
 5   wacc          3960 non-null   float64           
dtypes: datetime64[ns, UTC](2), float64(1), int64(1), object(2)
memory usage: 216.6+ KB</code></pre>
</div>
<p>Notice that the DataFrame has two <code>datetime64[ns, UTC]</code> type columns.</p>
<ol>
<li>Now that you are done, you can close your writer object and shut down the client as shown:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>writer.close()
client.close()</code></pre>
</div>
</section>
<section id="how-it-works-18" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3">How it works…</h3>
<p>Before writing a pandas DataFrame to InfluxDB using the <code>write_api</code> you will need to define few things required in InfluxDB. This includes the following:</p>
<ul>
<li><strong>Measurement</strong>: These are the values you are tracking. InfluxDB accepts one measurement per data point.</li>
<li><strong>Field</strong>: We do not need to specify fields per se, since any columns not in the tag definition will be marked as fields. Fields are metadata objects stored as key-value pairs. Fields are not indexed, unlike tags.</li>
<li><strong>Tag</strong> (optional): A metadata object in which you specify the columns that would get indexed for improved query performance. This is stored as a key-value pair as well.</li>
</ul>
<p>The <strong>WriteAPI</strong> supports <em>synchronous</em> and <em>asynchronous</em> writes. Additionally, the <strong>WriteAPI</strong> also provides several options when writing to InfluxDB (such as line protocol strings, line protocol bytes, data point structure, dictionary style, as well as support for pandas DataFrames). In the <em>Reading data from time series database</em> recipe in Chapter 3, <em>Reading Time Series Data from Databases</em>, you used the <code>query_data_frame()</code> method to specify that the results of the query should be returned as a <strong>pandas DataFrame</strong>.</p>
<p>Similarly, <code>write_api</code> provides additional parameters when writing a pandas DataFrames to InfluxDB:</p>
<ul>
<li><code>data_frame_measurement_name</code>: The name of the measurement for writing pandas DataFrames</li>
<li><code>data_frame_tag_columns</code>: The list of DataFrame columns that are tags; the rest of the columns will be fields</li>
</ul>
</section>
<section id="theres-more-18" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4">There's more…</h3>
<p>In the previous example, we had to manually flush the data using <code>writer.close()</code> and terminate the connection using <code>client.close()</code>. For better resource management (for example, automatically closing the connection) and exception handling, you can benefit from using the <code>with</code> statement. The following example shows how you can rewrite the same code in a cleaner and more efficient format:</p>
<div class="C0-SHCodePACKT">
<pre><code>with InfluxDBClient(url="http://localhost:8086", token=token) as client:
    with client.write_api(WriteOptions(SYNCHRONOUS,
                     batch_size=500,
                     max_retries=5_000)) as writer:
       
        writer.write(bucket=bucket,
                        org=org,
                        record=df,
                        write_precision='ns',
                        data_frame_measurement_name='wacc',
                        data_frame_tag_columns=[])</code></pre>
</div>
</section>
<section id="see-also-19" class="level3" data-number="6.5.5">
<h3 data-number="6.5.5">See also</h3>
<ul>
<li>To learn more about the InfluxDB line protocol, please refer to their documentation here: <a href="https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/">https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/</a>.</li>
<li>To learn more about the Python API for InfluxDB 2.x, please refer to the official documentation here: <a href="https://docs.influxdata.com/influxdb/cloud/tools/client-libraries/python/">https://docs.influxdata.com/influxdb/cloud/tools/client-libraries/python/</a>.</li>
</ul>
</section>
</section>
<section id="writing-time-series-data-to-snowflake" class="level2" data-number="6.6">
<h2 data-number="6.6">Writing time series data to Snowflake</h2>
<p><strong>Snowflake</strong> has become a very popular cloud database option for building big data analytics, due to its scalability, performance, and being SQL-oriented (a columnar-stored relational database).</p>
<p>Snowflake's connector for Python simplifies the interaction with the database whether it's for reading or writing data, or, more specifically, the built-in support for <code>pandas </code>DataFrames. In this recipe, you will use the sensor IoT dataset prepared in the <em>Writing time series data to InfluxDB</em> recipe. The technique applies to any <code>pandas</code> DataFrame that you plan to write to Snowflake.</p>
<section id="getting-ready-20" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1">Getting ready</h3>
<blockquote>
<p>You should refer to the recipe <em>Reading data from a Snowflake</em> in <em>Chapter 3</em>, <em>Reading Time Series Data from Databases</em> as a refresher on the different ways to connect to Snowflake.</p>
</blockquote>
<p>The recommended approach for the <code>snowflake-connector-python</code> library is to install it using <strong>pip</strong> allowing you to install <em>extras</em> such as <code>pandas</code> as shown:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install snowflake-sqlalchemy snowflake-snowpark-python
pip install "snowflake-connector-python[pandas]"</code></pre>
</div>
<p>You can also install with <strong>conda</strong>, but if you want to use <code>snowflake-connector-python</code> with pandas you will need to use the pip install.</p>
<div class="C0-SHConPACKT">
<pre><code>conda install -c conda-forge snowflake-sqlalchemy snowflake-snowpark-python
conda install -c conda-froge snowflake-connector-python</code></pre>
</div>
<p>Create a configuration, for example <code>database.cfg</code> to store your Snowflake connection information as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>[SNOWFLAKE]
ACCOUNT=&lt;YOURACCOUNT&gt;
USER=&lt;YOURUSERNAME&gt;
PASSWORD= &lt;YOURPASSWORD&gt;
WAREHOUSE=COMPUTE_WH
DATABASE=TSCOOKBOOK
SCHEMA=CHAPTER5
ROLE=&lt;YOURROLE&gt;</code></pre>
</div>
<p>Using <code>ConfigParser</code>, extract the content under the <code>[SNOWFLAKE]</code> section to avoid exposing or hardcoding your credentials. Read parameters under the <code>[SNOWFLAKE]</code> section and convert to a Python dictionary as shown:</p>
<div class="C0-SHCodePACKT">
<pre><code>config = ConfigParser()
config.read('database.cfg)
config.sections()
params = dict(config['SNOWFLAKE'])</code></pre>
</div>
<p>You will utilize the <code>get_stock_data</code> function created in the <em>Technical Requirements</em> section to pull <strong>Amazon</strong> stock data from January 1, 2019 to August 31, 2024:</p>
<div class="C0-SHCodePACKT">
<pre><code>amzn_hist = get_stock_data('AMZN', '2019-01-01', '2024-8-31')</code></pre>
</div>
<p>The <code>amzn_hist</code> DataFrame does not have a <strong>Date</strong> column, instead it has a <code>DatetimeIndex</code> . You will need to convert the index into a column since the API do not support writing index objects.</p>
<div class="C0-SHCodePACKT">
<pre><code>amzn_hist = amzn_hist.reset_index()
amzn_hist.shape
&gt;&gt;
(1426, 6)</code></pre>
</div>
<p>You will be referencing the <code>amzn_hist</code> DataFrame and the object <code>params</code> throughout this recipe,</p>
</section>
<section id="how-to-do-it-19" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2">How to do it…</h3>
<p>We will explore three (3) methods and libraries to connect to the Snowflake database. You will start by using the <strong>Snowflake Python connector</strong>, then explore the <strong>Snowflake</strong> <strong>SQLAlchemy</strong>, and finally <strong></strong> you will explore the <strong>Snowpark Python API</strong>. Let’s get started.</p>
<section id="using-snowflake-connector-python-write_pandas" class="level4" data-number="6.6.2.1">
<h4 data-number="6.6.2.1">Using snowflake-connector-python (write_pandas)</h4>
<p>The recipe in this section will utilize the snowflake-connector-python library for connecting and writing data to a Snowflake database.</p>
<ol>
<li>Import the required libraries for this recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from snowflake import connector
from snowflake.connector.pandas_tools import pd_writer, write_pandas</code></pre>
</div>
<p>The <code>pands_tools</code> module provides several functions for working with pandas DataFrames, this includes two writer methods (<code>write_pandas</code> and <code>pd_writer</code>).</p>
<p>The <code>write_pandas</code> is a method for writing pandas DataFrame to a Snowflake database. Behind the scenes, the function will store the data to <strong>Parquet</strong> files, uploads the files to a <strong>temporary stage</strong>, and finally inserts the data from the files to the specified table via <code>COPY INTO</code> command.</p>
<p>On the other hand, the <code>pd_writer</code> method, is an insertion method for inserting data into a Snowflake database with the <code>DataFrame.to_sql()</code> method and passing a SQLAlchemy engine. You will explore <code>pd_writer</code> in the <em>Using SQLAlchemy</em> section following this recipe.</p>
<ol>
<li>Establish a connection to your Snowflake database instance and create a cursor object:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>con = connector.connect(**params)
cursor = con.cursor()</code></pre>
</div>
<p>The <code>cursor</code> object will be used to execute a SQL query to verify the dataset has been properly written to the Snowflake database.</p>
<ol>
<li>Write the amzn_hist DataFrame to Snowflake using the writer_pandas method. The method takes the connection object con, the DataFrame, destination table name, and other optional arguments such as auto_create_table, and table_type to name a few.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>success, nchunks, nrows, copy_into = write_pandas(
                                            con,
                                            amzn_hist,
                                            auto_create_table=True,
                                            table_name='AMAZON',
                                            table_type='temporary')</code></pre>
</div>
<p>When using write_pandas it returns a tuple. In the previous code we unpacked the tuple into: success, nchunks, nrows, and copy_into. Let’s inspect the values inside these objects:</p>
<div class="C1-SHCodePACKT">
<pre><code>print('success: ', success)
print('number of chunks: ', nchunks)
print('number of rows: ', nrows)
print('COPY INTO output', copy_into)
&gt;&gt;
success:  True
number of chunks:  1
number of rows:  1426
COPY INTO output [('ntporcytgv/file0.txt', 'LOADED', 1426, 1426, 1, 0, None, None, None, None)]</code></pre>
</div>
<p>The <code>success</code> objects is a boolean (True or False) to indicate whether the function successfully write the data to the specified table. The <code>nchunks</code> represents the number of chunks during the write process, which in this case, the entire data was written as one chunk. The <code>nrows</code> represents the number of rows inserted by the function. Lastly, the <code>copy_into</code> object contains the output of the <code>COPY INTO</code> command.</p>
<p>Notice the use of <code>auto_create_table=True</code>, if this is not set to True and the table AMAZON did not already exist in Snowflake, <code>write_pandas</code> will through an error. When it is set to <code>True</code> we are explicitly asking <code>write_pandas</code> to create the table. Additionally, if the table exists, you can specify if you want to overwrite the existing table with the parameter <code>overwrite=True</code>.</p>
<p>The <code>table_type</code> supports <strong>permanent</strong>, <strong>temporary</strong>, and <strong>transient</strong> table types in Snowflake. The parameter can take on of these values: <code>'temp'</code>, <code>'temporary'</code>, and <code>'transient'</code>. If an empty string is passed <code>table_type='',</code> then it will create a <strong>permanent</strong> table (the default behavior).</p>
<ol>
<li>You can further validate that all 1426 are written in the temporary table:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>cursor.execute('SELECT count(*) FROM AMAZON;')
count = cursor.fetchone()[0]
print(count)
&gt;&gt;
1426</code></pre>
</div>
<p>Indeed, you have all 1426 records written into the <code>AMAZON</code> table in Snowflake.</p>
</section>
<section id="using-sqlalchmey-1" class="level4" data-number="6.6.2.2">
<h4 data-number="6.6.2.2">Using SQLAlchmey</h4>
<p>The recipe in this section will utilize snowflake-sqlalchemy and the snowflake-connector-python libraries for connecting and writing data to a Snowflake database.</p>
<ol>
<li>Import the required libraries for this recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>import pandas as pd
from snowflake.connector.pandas_tools import pd_writer
from snowflake.sqlalchemy import URL
from sqlalchemy import create_engine</code></pre>
</div>
<ol>
<li>You will use the <code>URL</code> function from the Snowflake SQLAlchemy library to construct the connection string and create the SQLAlchemy <strong>engine</strong> to establish a connection to your Snowflake instance:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>url = URL(**params)
engine = create_engine(url)</code></pre>
</div>
<ol>
<li>Write the DataFrame to the Snowflake database using the <code>to_sql()</code> writer function. You will need to pass an <strong>insertion method</strong>; in this case, you will pass <code>pd_writer</code> :</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>try:
    amzn_hist.to_sql(
    'amazon_alchemy',
    engine,
    index=False,
    if_exists='replace'
)
except:
    print('failed to write')</code></pre>
</div>
<p>The preceding code uses standard <code>SQL INSERT</code> clauses one per row. The Snowflake connector API provides an insertion method, <code>pd_writer</code>, that you can pass to the method parameter in the <code>to_sql</code> method as shown:</p>
<div class="C1-SHCodePACKT">
<pre><code>try:
    amzn_hist.to_sql(
    'amazon_alchemy',
    engine,
    index=False,
    if_exists='replace',
    method=pd_writer
)
except:
    print('failed to write')</code></pre>
</div>
<p>Behind the scenes, the <code>pd_writer</code> function will use the <code>write_pandas</code> function to write the DataFrame to the Snowflake database instead.</p>
<ol>
<li>To read and verify that the data was written, you can use <code>pandas.read_sql()</code> to query the table:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>pd.read_sql_table('amazon_alchemy',
                  con=engine).info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1426 entries, 0 to 1425
Data columns (total 6 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   Date    1426 non-null   object
 1   Open    1426 non-null   float64
 2   High    1426 non-null   float64
 3   Low     1426 non-null   float64
 4   Close   1426 non-null   float64
 5   Volume  1426 non-null   float64
dtypes: float64(5), object(1)
memory usage: 67.0+ KB</code></pre>
</div>
<p>The new DataFrame contains all 1426 records and exact number of columns expected.</p>
</section>
<section id="using-snowflake-snowpark-python-1" class="level4" data-number="6.6.2.3">
<h4 data-number="6.6.2.3">Using snowflake-snowpark-python</h4>
<p>The recipe in this section will utilize the Snowpark API for writing a pandas DataFrame.</p>
<ol>
<li>Import the required libraries for this recipe:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from snowflake.snowpark import Session
import pandas as pd</code></pre>
</div>
<ol>
<li>Create a session by establishing a connection with the Snowflake database</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>session = Session.builder.configs(params).create()</code></pre>
</div>
<ol>
<li>Before writing the DataFrame, you must convert the pandas DataFrame to a Snowpark DataFrame.</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_snowpark_df = session.create_dataframe(amzn_hist)</code></pre>
</div>
<p>Snowpark DataFrames uses lazy evaluation and provides many advantages over Panda DataFrames.</p>
<ol>
<li>To write the Snowpark DataFrame, you can use the <code>write</code> and <code>save_as_table</code> methods:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_snowpark_df.write.mode("overwrite").save_as_table("amazon_snowpark")</code></pre>
</div>
<ol>
<li>To read and verify that the data was written, you can use <code>session.table</code> to query the table:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>amzn_df = session.table("amazon_snowpark")</code></pre>
</div>
<p>If you are more comfortable with pandas, then you can convert the Snowpark DataFrame to a pandas DataFrame using the <code>to_pandas</code> method as shown:</p>
<div class="C1-SHCodePACKT">
<pre><code>df = amzn_df.to_pandas()
df.shape
&gt;&gt;
(1426, 6)</code></pre>
</div>
<p>The DataFrame contains all 1426 records and all six (6) columns as expected.</p>
</section>
</section>
<section id="how-it-works...-1" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3">How it works...</h3>
<p>The Snowflake Python API provides two mechanisms for writing pandas DataFrames to Snowflake, which are provided to you in the <code>pandas_tools</code> module:</p>
<div class="C0-SHCodePACKT">
<pre><code>from snowflake.connector.pandas_tools import pd_writer, write_pandas</code></pre>
</div>
<p>In the recipe, you used <code>pd_writer</code> and passed it as an <strong>insertion method</strong> to the <code>DataFrame.to_sql()</code> writer function. When using <code>pd_writer</code> within <code>to_sql()</code>, you can change the insertion behavior through the <code>if_exists</code> parameter, which takes three arguments:</p>
<ul>
<li><code>fail</code>, which raises <code>ValueError</code> if the table exists</li>
<li><code>replace</code>, which drops the table before inserting new values</li>
<li><code>append</code>, which inserts the data into the existing table</li>
</ul>
<p>If the table doesn't exist, SQLAlchemy takes care of creating the table for you and maps the data types from pandas DataFrames to the appropriate data types in the Snowflake database. This is also true when reading the data from Snowflake using the SQLAlchemy engine through <code>pandas.read_sql()</code>.</p>
<p>Note that <code>pd_writer</code> uses the <code>write_pandas</code> function behind the scenes. They both work by dumping the DataFrame into Parquet files, uploading them to a temporary stage, and, finally, copying the data into the table via <code>COPY INTO</code>.</p>
<p>You used the <code>write.mode()</code> method when using the Snowpark API to write the DataFrame. The <code>mode()</code> method accepts different write mode options:</p>
<ul>
<li><code>append</code>: Append data of the DataFrame to the existing table. If the table does not exist, it will be created.</li>
<li><code>overwrite</code>: Overwrite the existing table by <strong>dropping</strong> the old table.</li>
<li><code>truncate</code>: Overwrite the existing table by <strong>truncating</strong> the old table.</li>
<li><code>errorifexists</code>: Throw an exception <strong>error</strong> if the table already exists.</li>
<li><code>ignore</code>: Ignore the operation if the table already exists.</li>
</ul>
<p>Keep in mind the default value is <code>errorifexists</code>.</p>
</section>
<section id="theres-more...-1" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4">There's more...</h3>
<p>There is a useful method in Snowpark to write a pandas DataFrame directly without the need to convert it into a Snowpark DataFrame:</p>
<div class="C0-SHCodePACKT">
<pre><code>snowpark_df = session.write_pandas(amzn_hist,
                               table_name="amazon_temp",
                               auto_create_table=True,
                               table_type="temp")</code></pre>
</div>
<p>The <code>write_pandas</code> function writes the pandas DataFrame to Snowflake and returns a Snowpark DataFrame object.</p>
</section>
<section id="see-also-20" class="level3" data-number="6.6.5">
<h3 data-number="6.6.5">See also</h3>
<ul>
<li>Visit the Snowflake documentation to learn more about <code>write_pandas</code> and <code>pd_write</code> methods: <a href="https://docs.snowflake.com/en/user-guide/python-connector-api.html#write_pandas">https://docs.snowflake.com/en/user-guide/python-connector-api.html#write_pandas</a>.</li>
<li>You can learn more about the <code>pandas DataFrame.to_sql()</code> function here: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html</a>.</li>
<li>To learn more about the write_pandas method from the Snowpark API refer to the official documentation here <a href="ch006.xhtml">https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.22.1/snowpark/api/snowflake.snowpark.Session.write_pandas</a></li>
</ul>
</section>
</section>
</section>
</div>
</div>
</body>
</html>