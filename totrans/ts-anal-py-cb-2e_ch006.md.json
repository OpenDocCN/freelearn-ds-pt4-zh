["```py\nconda install -c conda-forge yfinance\n```", "```py\npip install yfinance\n```", "```py\nimport yfinance as yf\namzn = yf.Ticker(\"AMZN\")\namzn_hist = amzn.history(start=\"2019-01-01\", end=\"2023-12-31\")\namzn_hist.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1258 entries, 2019-01-02 00:00:00-05:00 to 2023-12-29 00:00:00-05:00\nData columns (total 7 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Open          1258 non-null   float64\n 1   High          1258 non-null   float64\n 2   Low           1258 non-null   float64\n 3   Close         1258 non-null   float64\n 4   Volume        1258 non-null   int64 \n 5   Dividends     1258 non-null   float64\n 6   Stock Splits  1258 non-null   float64\ndtypes: float64(6), int64(1)\nmemory usage: 78.6 KB\n```", "```py\namzn_hist.index = amzn_hist.index.strftime('%Y-%m-%d')\namzn_hist = amzn_hist[['Open', 'High', 'Low', 'Close', 'Volume']]\nprint(amzn_hist.head())\n>>\n                 Open       High        Low      Close     Volume\nDate                                                            \n2019-01-02  73.260002  77.667999  73.046501  76.956497  159662000\n2019-01-03  76.000504  76.900002  74.855499  75.014000  139512000\n2019-01-04  76.500000  79.699997  75.915497  78.769501  183652000\n2019-01-07  80.115501  81.727997  79.459503  81.475502  159864000\n2019-01-08  83.234497  83.830498  80.830498  82.829002  177628000\n```", "```py\nimport yfinance as yf\ndef get_stock_data(ticker, start, end):\n    stock_data = yf.Ticker(ticker)\n    stock_data = stock_data.history(start=start, end=end)\n    stock_data.index = stock_data.index.strftime('%Y-%m-%d')\n    stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n    return stock_data\n```", "```py\nmsft = get_stock_data('MSFT', '2024-01-01', None)\n```", "```py\n>> conda install sqlalchemy psycopg\n```", "```py\n>> pip install sqlalchemy\n>> pip install pyscopg\n```", "```py\namzn_hist = get_stock_data('AMZN', '2019-01-01', '2023-12-31')\n```", "```py\nfrom sqlalchemy import create_engine, URL\nfrom configparser import ConfigParser\nconfig = ConfigParser()\nconfig.read('database.cfg')\nconfig.sections()\nparams = dict(config['POSTGRESQL'])\nurl = URL.create('postgresql+psycopg', **params)\nprint(url)\n>>\npostgresql+psycopg://postgres:***@127.0.0.1/postgres\n```", "```py\nengine = create_engine(url)\nprint(engine)\n>>\nEngine(postgresql+psycopg://postgres:***@127.0.0.1/postgres)\n```", "```py\namzn_hist.to_sql('amzn',\n            engine,\n            if_exists='replace')\n```", "```py\nwith engine.connect() as connection:\n    hist.to_sql('amzn',\n                    connection,\n                    if_exists='replace')\n```", "```py\nfrom sqlalchemy import text\nquery = \"\"\"\nSELECT EXISTS (\n   SELECT FROM information_schema.tables\n   WHERE  table_schema = 'public'\n   AND    table_name   = 'amzn'\n   );\"\"\"\nwith engine.connect() as conn:\n    result = conn.execute(text(query))\nprint(result.fetchone())\n>>\n(True,)\n```", "```py\nquery = \"select count(*) from amzn;\"\nwith engine.connect() as conn:\n    result = conn.execute(text(query))\nresult.fetchone()\n>>\n(1258,)\n```", "```py\namzn_hist_2024 = get_stock_data('AMZN', '2024-01-01', None)\nprint(amzn_hist_2024.shape)\n>>\n(182, 5)\n```", "```py\nwith engine.connect() as connection:\n    amzn_hist_2024.to_sql('amzn',\n                    connection,\n                    if_exists='append')\n```", "```py\nquery = \"select count(*) from amzn;\"\nwith engine.connect() as conn:\n    result = conn.execute(text(query))\nprint(result.fetchone())\n>>\n(1440,)\n```", "```py\nwith engine.connect() as connection:\n    amzn_hist.to_sql('amzn',\n                    connection,\n                    chunksize=500,\n                    if_exists='append')\n```", "```py\nconda install -c conda-forge psycopg2 sqlalchemy-redshift\n```", "```py\npip install pip install psycopg2 sqlalchemy-redshift\n```", "```py\n[AWS]\nhost=yourendpoint\nport=5439\ndatabase=dev\nusername=username\npassword=password\n```", "```py\nfrom configparser import ConfigParser\nconfig = ConfigParser()\nconfig.read('database.cfg')\nconfig.sections()\nparams = dict(config['AWS'])\nfrom sqlalchemy import URL, create_engine\nurl = URL.create('redshift+psycopg2', **params)\nprint(url)\n>>\nredshift+psycopg2://awsuser:***@redshift-cluster-1.cltc17lacqp7.us-east-1.redshift.amazonaws.com:5439/dev\n```", "```py\naws_engine = create_engine(url)\n```", "```py\namzn = yf.Ticker(\"AMZN\")\namzn_hist = amzn.history(period=\"5y\")\namzn_hist = amzn_hist[['Open', 'High', 'Low', 'Close', 'Volume']]\n```", "```py\namzn_hist = amzn_hist.reset_index()\nwith aws_engine.connect() as conn:\n    amzn_hist.to_sql('amzn',\n                    conn,\n                    if_exists='replace', index=False)\n```", "```py\nfrom sqlalchemy import text\nquery = \"select count(*) from amzn;\"\nwith aws_engine.connect() as conn:\n    result = conn.execute(text(query))\nresult.fetchone()\n>>\n(1258,)\n```", "```py\nconda install -c conda-forge redshift_connector\n```", "```py\npip install redshift_connector\n```", "```py\n[AWS2]\nhost=yourendpoint\nport=5439\ndatabase=dev\nuser=username\npassword=password\n```", "```py\nimport redshift_connector\nfrom configparser import ConfigParser\nconfig = ConfigParser()\nconfig.read('database.cfg')\nconfig.sections()\nparams = dict(config['AWS2'])\nconn = redshift_connector.connect(**params)\n```", "```py\ncursor = conn.cursor()\ncursor.write_dataframe(amzn_hist, 'amzn')\n```", "```py\nconn.commit()\n```", "```py\nconda install -c conda-forge awswrangler\n```", "```py\npip install 'awswrangler[redshift]'\n```", "```py\nimport awswrangler as wr\nwr.redshift.to_sql(\n    df=amzn_hist,\n    table='amzn',\n    schema='public',\n    con=conn,\n    mode='overwrite'\n)\n```", "```py\n$ conda install -c anaconda pymongo -y\n```", "```py\n$ python -m pip install pymongo\n```", "```py\nimport pandas as pd\nfrom pymongo import MongoClient\n```", "```py\nclient = MongoClient('mongodb://localhost:27017')\n```", "```py\ndb = client['stock_data']\ncollection = db.create_collection('amazon')\n```", "```py\ndb = client['stock_data']\nts = db.create_collection(\n    name=\"daily_stock\",\n    timeseries={\n        \"timeField\": \"Date\",\n        \"metaField\": \"symbol\",\n        \"granularity\": \"hours\"\n    }\n)\n```", "```py\namzn_hist = get_stock_data('AMZN', '2019-01-01', '2024-8-31')\n```", "```py\nmetadata = {\"ticker\": \"AMZN\"}\namzn_hist['metadata'] = [metadata] * len(amzn_hist)\namzn_hist = amzn_hist.reset_index()\namzn_hist['Date'] = pd.to_datetime(amzn_hist['Date'])\namzn_records = amzn_hist.to_dict(orient='records')\namzn_records[0:2]\n>>\n[{'Date': Timestamp('2019-01-02 00:00:00'),\n  'Open': 73.26000213623047,\n  'High': 77.66799926757812,\n  'Low': 73.04650115966797,\n  'Close': 76.95649719238281,\n  'Volume': 159662000,\n  'metadata': {'ticker': 'AMZN'}},\n {'Date': Timestamp('2019-01-03 00:00:00'),\n  'Open': 76.00050354003906,\n  'High': 76.9000015258789,\n  'Low': 74.85549926757812,\n  'Close': 75.01399993896484,\n  'Volume': 139512000,\n  'metadata': {'ticker': 'AMZN'}}]\n```", "```py\namzn_hist = amzn_hist.reset_index()\namzn_records = []\nfor idx, row in amzn_hist.iterrows():\n    doc = {\n        \"Date\": pd.to_datetime(row['Date']),\n        \"metadata\": {\"ticker\": \"AMZN\"},\n        \"High\": row['High'],\n        \"Low\": row['Low'],\n        \"Close\": row['Close'],\n        \"Open\": row['Open'],\n        \"Volume\": row['Volume']\n    }\n    amzn_records.append(doc)\namzn_records[0:2]\n>>\n[{'Date': Timestamp('2019-01-02 00:00:00'),\n  'metadata': {'ticker': 'AMZN'},\n  'High': 77.66799926757812,\n  'Low': 73.04650115966797,\n  'Close': 76.95649719238281,\n  'Open': 73.26000213623047,\n  'Volume': 159662000},\n {'Date': Timestamp('2019-01-03 00:00:00'),\n  'metadata': {'ticker': 'AMZN'},\n  'High': 76.9000015258789,\n  'Low': 74.85549926757812,\n  'Close': 75.01399993896484,\n  'Open': 76.00050354003906,\n  'Volume': 139512000}]\n```", "```py\nlen(amzn_records)\n>>\n1426\n```", "```py\nresult = ts.insert_many(amzn_records)\n```", "```py\nclient.list_database_names()\n>>\n['admin', 'config', 'local', 'stock_data']\ndb.list_collection_names()\n>>\n['daily_stock', 'system.buckets.daily_stock', 'system.views']\n```", "```py\nmsft_hist = get_stock_data('MSFT', '2019-01-01', '2024-8-31')\nmetadata = {\"ticker\": \"MSFT\"}\nmsft_hist['metadata'] = [metadata] * len(msft_hist)\nmsft_hist = msft_hist.reset_index()\nmsft_hist['Date'] = pd.to_datetime(msft_hist['Date'])\nmsft_records = msft_hist.to_dict(orient='records')\nresult = ts.insert_many(msft_records)\n```", "```py\nts.count_documents({})\n>>\n2852\n```", "```py\nfrom datetime import datetime\n# Define date range\nstart_date = datetime(2019, 1, 1)\nend_date = datetime(2019, 1, 31)\n# Query for MSFT stock data within the date range\nresults = ts.find({\n    \"metadata.ticker\": \"MSFT\",\n    \"Date\": {\"$gte\": start_date, \"$lte\": end_date}\n})\n# Convert the query results to a DataFrame\nmsft_df = (pd.DataFrame(results)\n             .set_index('Date')\n             .drop(columns=['_id', 'metadata']))\nprint(msft_df.head())\n>>\n                Close       High       Open    Volume        Low\nDate                                                           \n2019-01-02  95.501335  96.096327  94.018571  35329300  93.442465\n2019-01-03  91.988037  94.623014  94.538011  42579100  91.799146\n2019-01-04  96.266327  96.814101  94.179125  44060600  93.433020\n2019-01-07  96.389107  97.531873  95.992445  35656100  95.369122\n2019-01-08  97.087975  98.192962  97.314637  31514400  96.058536\n```", "```py\nmsft_avg_close = ts.aggregate([\n    {\"$group\":\n         {\"_id\": \"$metadata.ticker\",\n          \"avgClose\":\n                  {\"$avg\": \"$Close\"}}\n    }\n])\nfor doc in msft_avg_close:\n    print(doc)\n>>\n{'_id': 'AMZN', 'avgClose': 133.4635473361022}\n{'_id': 'MSFT', 'avgClose': 252.4193055419066}\n```", "```py\none_record = amzn_records[0]\none_record\n>>\n{'Date': Timestamp('2019-01-02 00:00:00'),\n 'metadata': {'ticker': 'AMZN'},\n 'High': 77.66799926757812,\n 'Low': 73.04650115966797,\n 'Close': 76.95649719238281,\n 'Open': 73.26000213623047,\n 'Volume': 159662000}\nresult_id = ts.insert_one(one_record)\nresult_id\n>>\nInsertOneResult(ObjectId('66f2ed5efad8cbd88968d02e'), acknowledged=True)\n```", "```py\nresult_id.inserted_id\n>>\nObjectId('66f2ed5efad8cbd88968d02e')\n```", "```py\ndb = client['stock_data']\ndb.list_collection_names()\n>>\n['daily_stock', 'system.buckets.daily_stock', 'system.views']\n```", "```py\ndb = client['stock_data']\nbucket = db.create_collection(name='stock_bucket')\n```", "```py\namzn = yf.Ticker(\"AMZN\")\namzn_hist = amzn.history(period=\"5y\")\namzn_hist = amzn_hist[['Open',\n                       'High',\n                       'Low',\n                       'Close',\n                       'Volume']].reset_index()\namzn_hist['Date'] = pd.to_datetime(amzn_hist['Date'])\namzn_hist['month'] = amzn_hist['Date'].dt.month\namzn_hist['year'] = amzn_hist['Date'].dt.year\n```", "```py\nfor year in amzn_hist['year'].unique():\n    for month in amzn_hist['month'].unique():\n        record = {}\n        record['month'] = int(month)\n        record['year'] = int(year)\n        record['symbol'] = 'AMZN'\n        try:\n            prices = amzn_hist[(amzn_hist['month'] == month) & (amzn_hist['year'] == year)]['Close'].values\n            record['price'] = [float(price) for price in prices]\n        except Exception as e:\n            print(f\"Error processing data for {month}/{year}: {str(e)}\")\n            continue\n        else:\n            bucket.insert_one(record)\n```", "```py\nprint('without bucketing: ',\n      db.daily_stock.count_documents({}))\nprint('with bucketing: ',\n      db.stock_bucket.count_documents({}))\n>>\nwithout bucketing:  2853\nwith bucketing:  72\n```", "```py\nresults = pd.DataFrame(bucket.find({'year':2024, 'month': 6}))\nresults['price'].to_dict()[0]\n>>\n[178.33999633789062,\n 179.33999633789062,\n 181.27999877929688,\n 185.0,\n 184.3000030517578,\n 187.05999755859375,\n 187.22999572753906,\n 186.88999938964844,\n 183.8300018310547,\n 183.66000366210938,\n 184.05999755859375,\n 182.80999755859375,\n 186.10000610351562,\n 189.0800018310547,\n 185.57000732421875,\n 186.33999633789062,\n 193.61000061035156,\n 197.85000610351562,\n 193.25]\n```", "```py\n$ pip install 'influxdb-client[ciso]'\n```", "```py\nconda install -c conda-forge influxdb-client\n```", "```py\nfrom influxdb_client import InfluxDBClient, WriteOptions\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nimport pandas as pd\nfrom  pathlib import Path\n```", "```py\npath = Path('../../datasets/Ch5/ExtraSensory/')\nfile = '0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv.gz'\ncolumns = ['timestamp',\n           'watch_acceleration:magnitude_stats:mean']\ndf = pd.read_csv(path.joinpath(file),\n                usecols=columns,\n                compression='gzip')\ndf = df.bfill()\ndf.columns = ['timestamp','wacc']\ndf.shape\n>>\n(3960, 2)\n```", "```py\ndf['timestamp'] = pd.to_datetime(df['timestamp'],\n                                  origin='unix',\n                                  unit='s',\n                                  utc=True)\ndf.set_index('timestamp', inplace=True)\nprint(df.head())\n>>\n                                  wacc\ntimestamp                            \n2015-12-08 19:06:37+00:00   995.369977\n2015-12-08 19:07:37+00:00   995.369977\n2015-12-08 19:08:37+00:00   995.369977\n2015-12-08 19:09:37+00:00   996.406005\n2015-12-08 19:10:55+00:00  1034.180063\n```", "```py\nbucket = \"sensor\"\norg = \"<yourorg>\"\ntoken = \"<yourtoken>\"\nclient = InfluxDBClient(url=\"http://localhost:8086\",\n                        token=token,\n                        org=org)\n```", "```py\nwriter = client.write_api(WriteOptions(SYNCHRONOUS,\n                     batch_size=500,\n                     max_retries=5))\nwriter.write(bucket=bucket,\n                record=df,\n                write_precision='ns',\n                data_frame_measurement_name='wacc',\n                data_frame_tag_columns=[])\n```", "```py\nquery = '''\n         from(bucket: \"sensor\")\n         |> range(start: 2015-12-08)\n         |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n         '''\nresult = client.query_api()\ninflux_df = result.query_data_frame(\n                             org=org,\n                             query=query,\n                             data_frame_index='_time')\n```", "```py\nInflux_df.info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3960 entries, 2015-12-08 19:06:37+00:00 to 2015-12-11 18:48:27+00:00\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype             \n---  ------        --------------  -----             \n 0   result        3960 non-null   object            \n 1   table         3960 non-null   int64             \n 2   _start        3960 non-null   datetime64[ns, UTC]\n 3   _stop         3960 non-null   datetime64[ns, UTC]\n 4   _measurement  3960 non-null   object            \n 5   wacc          3960 non-null   float64           \ndtypes: datetime64[ns, UTC](2), float64(1), int64(1), object(2)\nmemory usage: 216.6+ KB\n```", "```py\nwriter.close()\nclient.close()\n```", "```py\nwith InfluxDBClient(url=\"http://localhost:8086\", token=token) as client:\n    with client.write_api(WriteOptions(SYNCHRONOUS,\n                     batch_size=500,\n                     max_retries=5_000)) as writer:\n\n        writer.write(bucket=bucket,\n                        org=org,\n                        record=df,\n                        write_precision='ns',\n                        data_frame_measurement_name='wacc',\n                        data_frame_tag_columns=[])\n```", "```py\npip install snowflake-sqlalchemy snowflake-snowpark-python\npip install \"snowflake-connector-python[pandas]\"\n```", "```py\nconda install -c conda-forge snowflake-sqlalchemy snowflake-snowpark-python\nconda install -c conda-froge snowflake-connector-python\n```", "```py\n[SNOWFLAKE]\nACCOUNT=<YOURACCOUNT>\nUSER=<YOURUSERNAME>\nPASSWORD= <YOURPASSWORD>\nWAREHOUSE=COMPUTE_WH\nDATABASE=TSCOOKBOOK\nSCHEMA=CHAPTER5\nROLE=<YOURROLE>\n```", "```py\nconfig = ConfigParser()\nconfig.read('database.cfg)\nconfig.sections()\nparams = dict(config['SNOWFLAKE'])\n```", "```py\namzn_hist = get_stock_data('AMZN', '2019-01-01', '2024-8-31')\n```", "```py\namzn_hist = amzn_hist.reset_index()\namzn_hist.shape\n>>\n(1426, 6)\n```", "```py\nimport pandas as pd\nfrom snowflake import connector\nfrom snowflake.connector.pandas_tools import pd_writer, write_pandas\n```", "```py\ncon = connector.connect(**params)\ncursor = con.cursor()\n```", "```py\nsuccess, nchunks, nrows, copy_into = write_pandas(\n                                            con,\n                                            amzn_hist,\n                                            auto_create_table=True,\n                                            table_name='AMAZON',\n                                            table_type='temporary')\n```", "```py\nprint('success: ', success)\nprint('number of chunks: ', nchunks)\nprint('number of rows: ', nrows)\nprint('COPY INTO output', copy_into)\n>>\nsuccess:  True\nnumber of chunks:  1\nnumber of rows:  1426\nCOPY INTO output [('ntporcytgv/file0.txt', 'LOADED', 1426, 1426, 1, 0, None, None, None, None)]\n```", "```py\ncursor.execute('SELECT count(*) FROM AMAZON;')\ncount = cursor.fetchone()[0]\nprint(count)\n>>\n1426\n```", "```py\nimport pandas as pd\nfrom snowflake.connector.pandas_tools import pd_writer\nfrom snowflake.sqlalchemy import URL\nfrom sqlalchemy import create_engine\n```", "```py\nurl = URL(**params)\nengine = create_engine(url)\n```", "```py\ntry:\n    amzn_hist.to_sql(\n    'amazon_alchemy',\n    engine,\n    index=False,\n    if_exists='replace'\n)\nexcept:\n    print('failed to write')\n```", "```py\ntry:\n    amzn_hist.to_sql(\n    'amazon_alchemy',\n    engine,\n    index=False,\n    if_exists='replace',\n    method=pd_writer\n)\nexcept:\n    print('failed to write')\n```", "```py\npd.read_sql_table('amazon_alchemy',\n                  con=engine).info()\n>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1426 entries, 0 to 1425\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Date    1426 non-null   object\n 1   Open    1426 non-null   float64\n 2   High    1426 non-null   float64\n 3   Low     1426 non-null   float64\n 4   Close   1426 non-null   float64\n 5   Volume  1426 non-null   float64\ndtypes: float64(5), object(1)\nmemory usage: 67.0+ KB\n```", "```py\nfrom snowflake.snowpark import Session\nimport pandas as pd\n```", "```py\nsession = Session.builder.configs(params).create()\n```", "```py\namzn_snowpark_df = session.create_dataframe(amzn_hist)\n```", "```py\namzn_snowpark_df.write.mode(\"overwrite\").save_as_table(\"amazon_snowpark\")\n```", "```py\namzn_df = session.table(\"amazon_snowpark\")\n```", "```py\ndf = amzn_df.to_pandas()\ndf.shape\n>>\n(1426, 6)\n```", "```py\nfrom snowflake.connector.pandas_tools import pd_writer, write_pandas\n```", "```py\nsnowpark_df = session.write_pandas(amzn_hist,\n                               table_name=\"amazon_temp\",\n                               auto_create_table=True,\n                               table_type=\"temp\")\n```"]