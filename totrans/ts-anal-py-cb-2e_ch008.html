<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Time Series Analysis with Python Cookbook, 2E - Second Edition</title>
<link rel="stylesheet" type="text/css" href="../override_v1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/><link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content"><section id="handling-missing-data" class="level1 pkt" data-number="8">
<h1 data-number="8">7 Handling Missing Data</h1>
<section id="join-our-book-community-on-discord-6" class="level2" data-number="8.1">
<h2 data-number="8.1">Join our book community on Discord</h2>
<p>
<img style="width:15rem" src="../media/file0.png" width="200" height="200"/>
</p>
<p><a href="https://packt.link/zmkOY">https://packt.link/zmkOY</a></p>
<p>As a data scientist, data analyst, or business analyst, you have probably discovered that hoping to obtain a <em>perfect</em> clean dataset is too optimistic. What is more common, though, is that the data you are working with suffers from flaws such as missing values, erroneous data, duplicate records, insufficient data, or the presence of outliers in the data.</p>
<p>Time series data is no different, and before plugging the data into any analysis or modeling workflow, you must investigate the data first. It is vital to understand the <em>business context around the time series data</em> to detect and identify these problems successfully. For example, if you work with stock data, the context is very different from COVID data or sensor data.</p>
<p>Having that intuition or domain knowledge will allow you to anticipate what to expect and what is considered acceptable when analyzing the data. Always try to understand the business context around the data. For example, why is the data collected in the first place? How was the data collected? What business rules, logic, or transformations have been applied to the data? Were these modifications applied during the data acquisition process or built into the systems that generate the data?</p>
<p>During the discovery phase, such prior knowledge will help you determine the best approach to clean and prepare your dataset for analysis or modeling. Missing data and outliers are two common problems that need to be dealt with during data cleaning and preparation. You will dive into outlier detection in <em>Chapter 8</em>, <em>Outlier Detection Using Statistical Methods</em>, and <em>Chapter 14</em>, <em>Outlier Detection Using Unsupervised Machine Learning</em>. In this chapter, you will explore techniques to handle missing data through <strong>imputation</strong> and <strong>interpolation</strong>.</p>
<p>Here is the list of recipes that we will cover in this chapter:</p>
<ul>
<li>Performing data quality checks</li>
<li>Handling missing data with univariate imputation using pandas</li>
<li>Handling missing data with univariate imputation using scikit-learn</li>
<li>Handling missing data with multivariate imputation</li>
<li>Handling missing data with interpolation</li>
</ul>
</section>
<section id="technical-requirements-6" class="level2" data-number="8.2">
<h2 data-number="8.2">Technical requirements</h2>
<p>You can download the Jupyter notebooks and the requisite datasets from the GitHub repository to follow along:</p>
<ul>
<li>Jupyter notebooks: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch7/Chapter%207.ipynb">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/code/Ch7/Chapter%207.ipynb</a></li>
<li>Datasets: <a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch7">https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./tree/main/datasets/Ch7</a></li>
</ul>
<p>In this chapter and beyond, you will extensively use pandas 2.1.3 (released November 10, 2023). There will be four additional libraries that you will be using:</p>
<ul>
<li>numpy (1.26.0)</li>
<li>matplotlob (3.8.1)</li>
<li>statsmodels (0.14.0)</li>
<li>scikit-learn (1.3.2)</li>
<li>SciPy (1.11.3)</li>
</ul>
<p>If you are using <code>pip</code>, then you can install these packages from your terminal with the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>pip install matplotlib numpy statsmodels scikit-learn scipy</code></pre>
</div>
<p>If you are using <code>conda</code>, then you can install these packages with the following command:</p>
<div class="C0-SHConPACKT">
<pre><code>conda install matplotlib numpy statsmodels scikit-learn scipy</code></pre>
</div>
<p>In this chapter, two datasets will be used extensively for the imputation and interpolation recipes: the <em>CO2 Emissions</em> dataset, and the <em>e-Shop Clickstream</em> dataset. The source for the Clickstream dataset comes from <em>clickstream data for online shopping</em> from the <em>UCI machine learning repository</em>, which you can find here:</p>
<p><a href="https://archive.ics.uci.edu/ml/datasets/clickstream+data+for">https://archive.ics.uci.edu/ml/datasets/clickstream+data+for +online+shopping</a></p>
<p>The source for the CO2 emissions dataset comes from the Annual <em>CO2 emissions</em> report from <em>Our World in Data</em>, which you can find here: <a href="https://ourworldindata.org/co2-emissions">https://ourworldindata.org/co2-emissions</a>.</p>
<p>For demonstration purposes, the two datasets have been modified by removing observations (missing data). The original versions are provided, in addition to the modified versions, to be used for evaluating the different techniques discussed in this chapter.</p>
<p>Throughout this chapter, you will follow similar steps for handling missing data: ingest the data into a DataFrame, identify missing data, impute missing data, evaluate it against the original data, and finally, visualize and compare the different imputation techniques.</p>
<p>These steps can be translated into functions for reusability. You can create functions for these steps in the process: a function to read the data into a DataFrame, a function to evaluate using the RMSE score, and a function to plot the results.</p>
<p>Start by loading the standard libraries that you will be using throughout this chapter:</p>
<div class="C0-SHCodePACKT">
<pre><code>import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np</code></pre>
</div>
<section id="function-1-read_datasets" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1">Function 1 – read_datasets</h3>
<p>The <code>read_datasets</code> function takes a path to the folder, CSV filename, and the column name that contains the date variable.</p>
<p>The <code>read_datasets</code> function is defined as follows:</p>
<div class="C0-SHCodePACKT">
<pre><code>def read_dataset(folder, file, date_col=None, format=None, index=False):
    '''
    folder: is a Path object
    file: the CSV filename in that Path object.
    date_col: specify a column which has datetime
    index_col: True if date_col should be the index
   
    returns: a pandas DataFrame with a DatetimeIndex
    '''
    index_col = date_col if index is True else None
   
    df = pd.read_csv(folder / file,
                     index_col=index_col,
                     parse_dates=[date_col],
                     date_format=format)
    return df</code></pre>
</div>
</section>
<section id="function-2-plot_dfs" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2">Function 2 – plot_dfs</h3>
<p>The <code>plot_dfs()</code> function takes two DataFrames: the original DataFrame (<code>df1</code>) with no missing data (as the baseline), and the imputed DataFrame (<code>df2</code>) to compare against. The function creates multiple time series subplots using the specified response column (<code>col</code>). Note that the imputed DataFrame will contain additional columns (a column for the output of each imputation technique), and the plotting function accommodates this fact. This is done by looping through the columns. The function will plot each imputation technique for visual comparison and will be utilized throughout this chapter.</p>
<p>This <code>plot_dfs</code> function is defined as follows:</p>
<div class="C0-SHCodePACKT">
<pre><code>def plot_dfs(df1, df2, col, title=None, xlabel=None, ylabel=None):
    '''   
    df1: original dataframe without missing data
    df2: dataframe with missing data
    col: column name that contains missing data
    '''   
    df_missing = df2.rename(columns={col: 'missing'})
   
    columns = df_missing.loc[:, 'missing':].columns.tolist()
    subplots_size = len(columns)   
    fig, ax = plt.subplots(subplots_size+1, 1, sharex=True)
    plt.subplots_adjust(hspace=0.25)
    fig.suptitle = title
   
    df1[col].plot(ax=ax[0], figsize=(10, 12))
    ax[0].set_title('Original Dataset')
    ax[0].set_xlabel(xlabel)
    ax[0].set_ylabel(ylabel)   
   
    for i, colname in enumerate(columns):
        df_missing[colname].plot(ax=ax[i+1])
        ax[i+1].set_title(colname.upper())
    plt.show()</code></pre>
</div>
</section>
<section id="function-3-rmse_score" class="level3" data-number="8.2.3">
<h3 data-number="8.2.3">Function 3 – rmse_score</h3>
<p>In addition to a visual comparison between imputation techniques using the <code>plot_dfs</code> function, you will need a method to compare the different imputation techniques numerically (using a statistical measure).</p>
<p>This is where the <code>rmse_score</code> function will come in handy. It takes two DataFrames: the original DataFrame (<code>df1</code>) as the baseline and the imputed DataFrame (<code>df2</code>) to compare against. The function allows you to specify which column contains the response column (<code>col</code>) used as the basis for the calculation.</p>
<p>The <code>rmse_score</code> function is defined as follows:</p>
<div class="C0-SHCodePACKT">
<pre><code>def rmse_score(df1, df2, col=None):
    '''
    df1: original dataframe without missing data
    df2: dataframe with missing data
    col: column name that contains missing data
    returns: a list of scores
    '''
    df_missing = df2.rename(columns={col: 'missing'})
    columns = df_missing.loc[:, 'missing':].columns.tolist()
    scores = []
    for comp_col in columns[1:]:
        rmse = np.sqrt(np.mean((df1[col] - df_missing[comp_col])**2))
        scores.append(rmse)
        print(f'RMSE for {comp_col}: {rmse}')
    return scores</code></pre>
</div>
</section>
</section>
<section id="understanding-missing-data" class="level2" data-number="8.3">
<h2 data-number="8.3">Understanding missing data</h2>
<p>Data can be missing for a variety of reasons, such as unexpected power outages, a device that got accidentally unplugged, a sensor that just became defective, a survey respondent declined to answer a question, or the data was intentionally removed for privacy and compliance reasons. In other words, missing data is inevitable.</p>
<p>Generally, missing data is very common, yet sometimes it is not given the proper level of attention in terms of formulating a strategy on how to handle the situation. One approach for handling rows with missing data is to drop those observations (delete the rows). However, this may not be a good strategy if you have limited data in the first place, for example, if collecting the data is a complex and expensive process. Additionally, the drawback of deleting records, if done prematurely, is that you will not know if the missing data was due to censoring (an observation is only partially collected) or due to bias (for example, high-income participants declining to share their total household income in a survey).</p>
<p>A second approach may involve tagging the rows with missing data by adding a column describing or labeling the missing data. For example, suppose you know that there was a power outage on a particular day. In that case, you can add Power Outage to label the missing data and differentiate it from other missing data labeled with Missing Data if the cause is unknown.</p>
<p>A third approach, which this chapter is about, is estimating the missing data values. The methods can range from simple and naive to more complex techniques leveraging machine learning and complex statistical models. But how can you measure the accuracy of the estimated values for data missing in the first place?</p>
<p>There are different options and measures to consider, and the answer is not as simple. Therefore, you should explore different approaches, emphasizing a thorough evaluation and validation process to ensure the selected method is ideal for your situation. In this chapter, you will use <strong>Root Mean Squared Error</strong> (<strong>RMSE</strong>) to evaluate the different imputation techniques.</p>
<p>The process to calculate the RMSE can be broken down into a few simple steps: first, computing the error, which is the difference between the actual values and the predicted or estimated values. This is done for each observation. Since the errors may be either negative or positive, and to avoid having a zero summation, the errors (differences) are squared. Finally, all the errors are summed and divided by the total number of observations to compute the mean. This gives you the <strong>Mean Squared Error (MSE)</strong>. RMSE is just the square root of the MSE.</p>
<p>The RMSE equation can be written as:</p>
<figure>
<img style="width:15rem" src="../media/file61.jpg" width="533" height="176"/>
</figure>
In our estimate of the missing observations,
<figure> <img style="width:2rem" src="../media/file62.png" width="40" height="46"/>
</figure>
 is the imputed value,
<figure>
<img style="width:2rem" src="../media/file63.png" width="39" height="41"/>
</figure>
is the actual (original) value, and <em>N</em> is the number of observations.
<blockquote>
<p>RMSE FOR EVALUATING MULTIPLE IMPUTATION METHODS</p>
<blockquote>
<p>I want to point out that RMSE is commonly used to measure the performance of <em>predictive</em> models (for example, comparing regression models). Generally, a <em>lower</em> RMSE is desirable; it tells us that the model can fit the dataset. Simply stated, it tells us the average distance (error) between the predicted value and the actual value. You want this distance minimized.</p>
</blockquote>
<blockquote>
<p>When comparing different imputation methods, we want our imputed values to resemble (as close as possible) the actual data, which contains random effects (uncertainty). This means we are not seeking a perfect prediction, and thus a lower RMSE score does not necessarily indicate a better imputation method. Ideally, you would want to find a balance, hence, in this chapter, the use of RMSE is combined with visualization to help illustrate how the different techniques compare and work.</p>
</blockquote>
</blockquote>
<p>As a reminder, we have intentionally removed some values (synthetically causing missing data) but retained the original data to compare against for when using RMSE.</p>
</section>
<section id="performing-data-quality-checks" class="level2" data-number="8.4">
<h2 data-number="8.4">Performing data quality checks</h2>
<p><strong>Missing data</strong> are values not captured or not observed in the dataset. Values can be missing for a <em>particular feature</em> (column), or an <em>entire observation</em> (row). When ingesting the data using pandas, missing values will show up as either <code>NaN</code>, <code>NaT</code>, or <code>NA</code>.</p>
<p>Sometimes, in a given data set, missing observations are replaced with other values from the source system; for example, this can be a numeric filler such as <code>99999</code> or <code>0</code>, or a string such as <code>missing</code> or <code>N/A</code>. When missing values are represented by <code>0</code>, you need to be cautious and investigate further to determine whether those zero values are legitimate or if they are indicative of missing data.</p>
<p>In this recipe, you will explore how to identify the presence of missing data.</p>
<section id="getting-ready-21" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1">Getting ready</h3>
<p>You can download the Jupyter notebooks and requisite datasets from the GitHub repository. Please refer to the <em>Technical requirements</em> section of this chapter.</p>
<p>You will be using two datasets from the <code>Ch7</code> folder: <code>clicks_missing_multiple.csv</code> and <code>co2_missing.csv</code>.</p>
</section>
<section id="how-to-do-it-27" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2">How to do it…</h3>
<p>The <code>pandas</code> library provides convenient methods for discovering missing data and for summarizing data in a DataFrame:</p>
<ol>
<li>Start by reading the two CSV files (<code>co2_missing.csv</code> and <code>clicks_missing.csv</code>) using the <code>read_dataset()</code> function :</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>folder = Path('../../datasets/Ch7/')
co2_file = Path('co2_missing.csv')
ecom_file = Path('clicks_missing_multiple.csv')
co2_df = read_dataset(folder,
                      co2_file,
                      index=True,
                      date_col='year')
ecom_df = read_dataset(folder,
                       ecom_file,
                       index=True,
                       date_col='date')
ecom_df.head()</code></pre>
</div>
<p>This should display the first five rows from the <code>ecom_df</code> DataFrame:</p>
<figure>
<img src="../media/file64.png" alt="Figure 7.1: First five rows from the ecom_df DataFrame showing NaN and NaT" width="622" height="420"/><figcaption aria-hidden="true">Figure 7.1: First five rows from the ecom_df DataFrame showing NaN and NaT</figcaption>
</figure>
<p>The output from the preceding code shows that there are five missing values from the source dataset. <code>NaN</code> is how pandas represents empty <em>numeric</em> values (<code>NaN</code> is short for <strong>Not a Number</strong>). <code>NaT</code> is how pandas represents missing <code>Datetime</code> values (<code>NaT</code> is short for <strong>Not a Time</strong>).</p>
<ol>
<li>To count the number of missing values in both DataFrames, you can use the <code>DataFrame.isnull()</code> or <code>DataFrame.isna()</code> methods. This will return <code>True</code> (if missing) or <code>False</code> (if not missing) for each value. For example, to get the total count of missing values for each column, you can use <code>DataFrame.isnull().sum() </code>or <code>DataFrame.isna().sum()</code></li>
</ol>
<p>In Python, Booleans (<code>True</code> or <code>False</code>) are a subtype of integers. <code>True</code> is equivalent to <code>1</code>, and <code>False</code> is equivalent to <code>0</code>. To validate this concept, try the following:</p>
<div class="C1-SHCodePACKT">
<pre><code>isinstance(True, int)
&gt;&gt; True
int(True)
&gt;&gt; 1</code></pre>
</div>
<p>Now, let's get the total number of missing values for each DataFrame:</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.isnull().sum()
&gt;&gt;
co2     25
dtype: int64
ecom_df.isnull().sum()
&gt;&gt;
price        1
location     1
clicks      14
dtype: int64</code></pre>
</div>
<p>Notice in the preceding code that both <code>.isnull()</code> and <code>.isna()</code> were used. They both can be used interchangeably since <code>.isnull()</code> is an alias of <code>.isna()</code>.</p>
<ol>
<li>In the previous step, the <code>year</code> column from the <code>co2_df</code> and the <code>date</code> column from the <code>ecom_df</code> were not included in the counting result set. This is because <code>isnull()</code> or <code>isna()</code> focuses on the DataFrame’s columns and does not include the index. Our <code>read_datasets()</code> function from the <em>Technical Requirements</em> section was setting them as <strong>index</strong> columns. One simple approach is to reset the index to become a column as follows:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.reset_index(inplace=True)
ecom_df.reset_index(inplace=True)</code></pre>
</div>
<p>Now if you execute the <code>isnull().sum()</code> you should see the year column from the <code>co2_df</code> and date column from the <code>ecom_df</code> included in the counts:</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.isnull().sum()
&gt;&gt;
year     0
co2     25
dtype: int64
ecom_df.isnull().sum()
&gt;&gt;
date         4
price        1
location     1
clicks      14
dtype: int64</code></pre>
</div>
<p>From the results, <code>co2_df</code> has <code>25</code> missing values from the <code>co2</code> column, while <code>ecom_df</code> has <code>20</code> missing values in total (<code>4</code> from the <code>date</code> column, <code>1</code> from the <code>price</code> column, <code>1</code> from the <code>location</code> column, and <code>14</code> from the <code>clicks</code> column).</p>
<ol>
<li>To get the grand total for the entire <code>ecom_df</code> DataFrame, simply just chain another <code>.sum()</code> function to the end of the statement:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>ecom_df.isnull().sum().sum()
&gt;&gt; 20</code></pre>
</div>
<p>Similarly, for <code>co2_df </code>you can chain another .sum()</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.isnull().sum().sum()
&gt;&gt; 25</code></pre>
</div>
<ol>
<li>If you inspect the <code>co2_missing.csv</code> file using a text/code editor (such as Excel, or Jupyter Lab) and scroll down to <em>rows 192-194</em>, you will notice that there are string placeholder values in there: <code>NA</code>, <code>N/A</code>, and <code>null</code>:</li>
</ol>
<figure>
<img src="../media/file65.jpg" alt="Figure 7.2: co2_missing.csv shows string values that were converted to NaN (missing) by pandas" width="498" height="461"/><figcaption aria-hidden="true">Figure 7.2: co2_missing.csv shows string values that were converted to NaN (missing) by pandas</figcaption>
</figure>
<p><em>Figure 7.2</em> shows the three string values. Interestingly, <code>pandas.read_csv()</code> interpreted the three string values as <code>NaN</code>. This is the default behavior in <code>read_csv()</code>, which can be modified through the <code>na_values</code> parameter. To see how pandas represents these values, you can run the following command:</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_df[190:195]</code></pre>
</div>
<p>This should produce the following output:</p>
<figure>
<img src="../media/file66.png" alt="Figure 7.3: pandas.read_csv() interpreted the NA, N/A, and null strings as a NaN type" width="191" height="212"/><figcaption aria-hidden="true">Figure 7.3: pandas.read_csv() interpreted the NA, N/A, and null strings as a NaN type</figcaption>
</figure>
<ol>
<li>If all you need is to check whether the DataFrame contains any missing values, use <code>isnull().values.any()</code>. This will output <code>True</code> if there are any missing values in the DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>ecom_df.isnull().values.any()
&gt;&gt; True
co2_df.isnull().values.any()
&gt;&gt; True</code></pre>
</div>
<ol>
<li>So far, <code>isnull()</code> helped identify all the missing values in the DataFrames. But what if the missing values were masked or replaced by other placeholder values such as <code>?</code> or <code>99999</code>. The presence of these values will be skipped and considered missing (NaN) in pandas. Technically, they are not empty cells (missing) and hold values. On the other hand, domain or prior knowledge will tell us that the CO2 emission dataset is measured annually and should have values greater than 0.</li>
</ol>
<p>Similarly, we expect the number of clicks to be numeric for the Clickstream data. If the column is not numeric, it should trigger an investigation as to why pandas could not parse the column as numeric. For example, this could be due to the presence of string values.</p>
<p>To gain a better insight into the DataFrame schema and data types, you can use <code>DataFrame.info()</code> to display the schema, total records, column names, column dtypes, count of non-missing values per column, index dtype, and the DataFrame's total memory usage:</p>
<div class="C1-SHCodePACKT">
<pre><code>ecom_df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 135 entries, 0 to 134
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype        
---  ------    --------------  -----        
 0   date      132 non-null    datetime64[ns]
 1   price     134 non-null    float64      
 2   location  134 non-null    float64      
 3   clicks    121 non-null    object       
dtypes: datetime64[ns](1), float64(2), object(1)
memory usage: 4.3+ KB
co2_df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 226 entries, 0 to 225
Data columns (total 2 columns):
 #   Column   Non-Null Count  Dtype 
---  ------   --------------  ----- 
 0   year     226 non-null    datetime64[ns]
 1   co2      201 non-null    float64
dtypes: float64(1), int64(1)
memory usage: 3.7 KB</code></pre>
</div>
<p>The <code>co2_df</code> summary output looks reasonable, confirming that we have <code>25</code> missing values (226 total records less the 221 non-null give us 25) for the <code>co2</code> column.</p>
<p>On the other hand, the summary for <code>ecom_df</code> indicates that the <code>clicks</code> column is of the <code>object</code> dtype (indicating mixed types), and not the expected <code>float64</code>. Let's investigate further using basic summary statistics.</p>
<ol>
<li>To get the summary statistics for a DataFrame, use the <code>DataFrame.describe()</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.describe(include='all')</code></pre>
</div>
<p>The output is as follows:</p>
<figure>
<img src="../media/file67.png" alt="Figure 7.4: co2_df summary statistics indicating zero values present in the data" width="350" height="263"/><figcaption aria-hidden="true">Figure 7.4: co2_df summary statistics indicating zero values present in the data</figcaption>
</figure>
<p>Note the use of <code>include='all'</code> to replace the default value <code>include=None</code> . The default behavior is to show summary statistics for only numeric columns. By changing the value to <code>'all'</code>, the results will include all column types.</p>
<p>The summary statistics for the <code>co2_df</code> DataFrame confirms that we have zero values under the <code>co2</code> column (min = 0.00). As pointed out earlier, prior knowledge tells us that <code>0</code> represents a null (or missing) value. Therefore, the zeros will need to be replaced with <code>NaN</code> to include such values in the imputation process. Now, review the summary statistics for <code>ecom_df</code>:</p>
<div class="C1-SHCodePACKT">
<pre><code>ecom_df.describe(include='all')</code></pre>
</div>
<p>The output is as follows:</p>
<figure>
<img src="../media/file68.jpg" alt="Figure 7.5: ecom_df summary statistics indicating the ? value in the clicks column" width="666" height="488"/><figcaption aria-hidden="true">Figure 7.5: ecom_df summary statistics indicating the ? value in the clicks column</figcaption>
</figure>
<p>As you can see, the summary statistics for the <code>ecom_df</code> DataFrame indicate that we have a <code>? </code>value under the <code>clicks</code> column. This explains why pandas did not parse the column as numeric (due to mixed types). Similarly, the <code>? </code>values will need to be replaced with <code>NaN</code> to be treated as missing values for imputation.</p>
<ol>
<li>Convert the instances of <code>0</code> and <code>?</code> values to <code>NaN</code> types. This can be accomplished using the <code>DataFrame.replace()</code> method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.replace(0, np.NaN, inplace=True)
ecom_df.replace('?', np.NaN, inplace=True)
ecom_df['clicks'] = ecom_df['clicks'].astype('float')</code></pre>
</div>
<p>To validate, run <code>DataFrame.isnull().sum()</code> and you should notice that the missing value counts have increased:</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_df.isnull().sum()
&gt;&gt;
year        0
missing    35
dtype: int64
ecom_df.isnull().sum()
&gt;&gt;
date         4
price        1
location     1
clicks      16
dtype: int64</code></pre>
</div>
<p>The new numbers do a better job of reflecting the number of actual missing values in both DataFrames.</p>
</section>
<section id="how-it-works-26" class="level3" data-number="8.4.3">
<h3 data-number="8.4.3">How it works…</h3>
<p>When reading the CSV files using <code>pandas.read_csv()</code>, the default behavior is to recognize and parse certain string values, such as <code>NA</code>, <code>N/A</code>, and <code>null</code>, to the <code>NaN</code> type (missing). Thus, once these values became a <code>NaN</code>, the CSV reader could parse the <code>co2</code> column as <code>float64</code> (numeric) based on the remaining non-null values.</p>
<p>This is possible due to two parameters: <code>na_values</code> and <code>keep_default_na</code>. The <code>na_values</code> parameter, by default, contains a list of strings that are interpreted as <code>NaN</code>. The list includes <code>#N/A</code>, <code>#N/A N/A</code>, <code>#NA</code>, <code>-1.#IND</code>, <code>-1.#QNAN</code>, <code>-NaN</code>, <code>-nan</code>, <code>1.#IND</code>, <code>1.#QNAN</code>, <code>&lt;NA&gt;</code>, <code>N/A</code>, <code>NA</code>, <code>NULL</code>, <code>NaN</code>, <code>n/a</code>, <code>nan</code>, and <code>null</code>.</p>
<p>You can append to this list by providing additional values to the <code>na_values</code> parameter. Additionally, <code>keep_default_na</code> is set to <code>True</code> by default, thus using (appending) <code>na_values</code> with the default list for parsing.</p>
<p>If you change <code>keep_default_na</code> to <code>False</code> without providing new values to <code>na_values</code>, then none of the strings (<code>NA</code>, <code>N/A</code>, and <code>null</code>) would be parsed to <code>NaN</code> unless you provide a custom list. For example, if <code>keep_default_na</code> was set to <code>False</code> and no values provided to <code>na_values</code>, then the entire <code>co2</code> column would be parsed as a <code>string</code> (object), and any missing values will show up as strings; in other words, they will be coming in as <code>''</code>, which is an empty string.</p>
<p>Here is an example:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_df = pd.read_csv(folder/co2_file,
                     keep_default_na=False)
co2_df.isna().sum()
&gt;&gt;
year    0
co2     0
dtype: int64
co2_df.shape
&gt;&gt; (226, 2)</code></pre>
</div>
<p>Notice that we did not lose any data (<code>226</code> records) but showed no <code>NaN</code> (or missing) values. Let's inspect the DataFrame structure:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_df.info()
&gt;&gt;
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 226 entries, 0 to 225
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype        
---  ------  --------------  -----        
 0   year    226 non-null    datetime64[ns]
 1   co2     226 non-null    object       
dtypes: datetime64[ns](1), object(1)
memory usage: 3.7+ KB</code></pre>
</div>
<p>Notice the change in <em>dtype</em> for the <code>co2</code> columns. Let's check the data from index <code>190</code> to <code>195</code> again:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_df[190:195]</code></pre>
</div>
<p>The output is as follows:</p>
<figure>
<img src="../media/file69.jpg" alt="Figure 7.6: Output from the co2_df DataFrame without NaN parsing" width="213" height="255"/><figcaption aria-hidden="true">Figure 7.6: Output from the co2_df DataFrame without NaN parsing</figcaption>
</figure>
<p>Finally, you can check how the missing values were handled:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_df.iloc[132:139]</code></pre>
</div>
<p>You will notice all seven rows have blank values (empty string).</p>
<p>In this recipe you explored the <code>.isna()</code> method. Once the data is read into a DataFrame or series, you get access to the <code>.isna()</code> and <code>.isnull()</code> interchangeable methods, which return <code>True</code> if data is missing and <code>False</code> otherwise. To get the counts for each column, we just chain a <code>.sum()</code> function, and to get the grand total, we chain another <code>.sum()</code> function following that:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_df.isnull().sum()
co2_df.isnull().sum().sum()</code></pre>
</div>
</section>
<section id="theres-more-26" class="level3" data-number="8.4.4">
<h3 data-number="8.4.4">There's more…</h3>
<p>If you know that the data will always contain <code>?</code>, which should be converted to <code>NaN</code> (or any other value), then you can utilize the <code>pd.read_csv()</code> function and update the <code>na_values</code> parameter. This will reduce the number of steps needed to clean the data after creating the DataFrame:</p>
<div class="C0-SHCodePACKT">
<pre><code>ecom_df = pd.read_csv(folder/ecom_file,
                      parse_dates=['date'],
                      na_values={'?'})</code></pre>
</div>
<p>This will replace all instances of <code>? </code>with <code>NaN</code>.</p>
</section>
<section id="see-also-28" class="level3" data-number="8.4.5">
<h3 data-number="8.4.5">See also</h3>
<ul>
<li>To learn more about the <code>na_values</code> and <code>keep_default_na</code> parameters from <code>pandas.read_csv()</code>, please visit the official documentation here: <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html">https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html</a></li>
<li>To learn more about the <code>DataFrame.isna()</code> function, please visit the official documentation here: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html</a></li>
</ul>
</section>
</section>
<section id="handling-missing-data-with-univariate-imputation-using-pandas" class="level2" data-number="8.5">
<h2 data-number="8.5">Handling missing data with univariate imputation using pandas</h2>
<p>Generally, there are two approaches to imputing missing data: <code>univariate imputation</code> and <code>multivariate imputation</code>. This recipe will explore univariate imputation techniques available in pandas.</p>
<p>In univariate imputation, you use non-missing values in a single variable (think a column or feature) to impute the missing values for that variable. For example, if you have a sales column in the dataset with some missing values, you can use a univariate imputation method to impute missing sales observations using average sales. Here, a single column (<code>sales</code>) was used to calculate the mean (from non-missing values) for imputation.</p>
<p>Some basic univariate imputation techniques include the following:</p>
<ul>
<li>Imputing using the <strong>mean</strong>.</li>
<li>Imputing using the last observation forward (<strong>forward fill</strong>). This can be referred to as <strong>Last Observation Carried Forward</strong> (<strong>LOCF</strong>).</li>
<li>Imputing using the next observation backward (<strong>backward fill</strong>). This can be referred to as <strong>Next Observation Carried Backward</strong> (<strong>NOCB</strong>).</li>
</ul>
<p>You will use two datasets to impute missing data using different techniques and then compare the results.</p>
<section id="getting-ready-22" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1">Getting ready</h3>
<p>You can download the Jupyter notebooks and requisite datasets from the GitHub repository. Please refer to the <em>Technical requirements</em> section of this chapter.</p>
<p>You will be using four datasets from the <code>Ch7</code> folder: <code>clicks_original.csv</code>, <code>clicks_missing.csv</code>, <code>clicks_original.csv</code>, and <code>co2_missing_only.csv</code>. The datasets are available from the GitHub repository.</p>
</section>
<section id="how-to-do-it-28" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2">How to do it…</h3>
<p>You will start by importing the libraries and then read all four CSV files. You will use the original versions of datasets to compare the results of the imputations to gain a better intuition of how they perform. For the comparison measure, you will use <em>RMSE</em> to evaluate each technique and then visualize the outputs to compare the imputation results visually:</p>
<ol>
<li>Use the <code>read_dataset()</code> function to read the four datasets:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>co2_original = read_dataset(folder, 'co2_original.csv', 'year', index=True)
co2_missing = read_dataset(folder, 'co2_missing_only.csv', 'year', index=True)
clicks_original = read_dataset(folder, 'clicks_original.csv', 'date', index=True)
clicks_missing = read_dataset(folder, 'clicks_missing.csv', 'date', index=True)</code></pre>
</div>
<ol>
<li>Visualize the CO2 DataFrames (original and missing) and specify the column with missing values (<code>co2</code>):</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(co2_original,
         co2_missing,
         'co2',
         title="Annual CO2 Emission per Capita",
         xlabel="Years",
         ylabel="x100 million tons")</code></pre>
</div>
<p>The <code>plot_dfs</code> function will produce two plots: the original CO2 dataset without missing values, and the altered dataset with missing values.</p>
<figure>
<img src="../media/file70.jpg" alt="Figure 7.7: CO2 dataset showing a comparison between the missing values and the original" width="1146" height="1264"/><figcaption aria-hidden="true">Figure 7.7: CO2 dataset showing a comparison between the missing values and the original</figcaption>
</figure>
<p>From <em>Figure 7.7</em>, you can see a noticeable upward trend in CO2 levels over time. There is missing data in three different spots. Now, visualize the Clickstream DataFrames:</p>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(clicks_original,
         clicks_missing,
         'clicks',
         title="Page Clicks per Day",
         xlabel="date",
         ylabel="# of clicks")</code></pre>
</div>
<p>The <code>plot_dfs</code> function will produce two plots: the original Clickstream dataset without missing values, and the altered dataset with missing values.</p>
<figure>
<img src="../media/file71.jpg" alt="Figure 7.8: Clickstream dataset showing a comparison between the missing values and the original" width="1057" height="1210"/><figcaption aria-hidden="true">Figure 7.8: Clickstream dataset showing a comparison between the missing values and the original</figcaption>
</figure>
<p>Notice, the output shows missing data from May 15 to May 30. You can confirm this by running the following code:</p>
<div class="C1-SHCodePACKT">
<pre><code>clicks_missing[clicks_missing['clicks'].isna()]</code></pre>
</div>
<ol>
<li>Now you are ready to perform your first imputation. You will use the <code>.fillna()</code> method which has a <code>value</code> parameter that takes either a numeric or a string value to substitute for all the <code>NaN</code> instances. Additionally you will use <code>.ffill() </code>for forward fill, and <code>.bfill()</code> for backward fill.</li>
</ol>
<p>Let's impute the missing values utilizing the <code>method</code> parameter and append the results as new columns in the DataFrame. Start with the CO2 DataFrame:</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_missing['ffill'] = co2_missing['co2'].ffill()
co2_missing['bfill'] = co2_missing['co2'].bfill()
co2_missing['mean'] = co2_missing['co2'].fillna(co2_missing['co2'].mean())</code></pre>
</div>
<p>Use the <code>rmse_score</code> function to get the scores:</p>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(co2_original,
                    co2_missing,
                    'co2')
&gt;&gt;
RMSE for ffil: 0.05873012599267133
RMSE for bfill: 0.05550012995280968
RMSE for mean: 0.7156383637041684</code></pre>
</div>
<p>Now, visualize the results using the <code>plot_dfs</code> function:</p>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(co2_original, co2_missing, 'co2')</code></pre>
</div>
<p>The preceding code produces the results as follows:</p>
<figure>
<img src="../media/file72.jpg" alt="Figure 7.9: Comparison between the three imputation methods for the CO2 DataFrame" width="1081" height="761"/><figcaption aria-hidden="true">Figure 7.9: Comparison between the three imputation methods for the CO2 DataFrame</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.9</em> with the original data in <em>Figure 7.7</em>. Notice that both <code>ffill</code> and <code>bfill</code> produce better results than when using the <code>mean</code>. Both techniques have favorable RMSE scores and visual representation.</p>
<ol>
<li>Now, perform the same imputation methods on the Clickstream DataFrame:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>clicks_missing['ffil'] = clicks_missing['clicks'].ffill()
clicks_missing['bfill'] = clicks_missing['clicks'].bfill()
clicks_missing['mean'] = clicks_missing['clicks'].fillna(clicks_missing['clicks'].mean())</code></pre>
</div>
<p>Now, calculate the RMSE scores:</p>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(clicks_original,
                    clicks_missing,
                    'clicks')
&gt;&gt;
RMSE for ffil: 1034.1210689204554
RMSE for bfill: 2116.6840489225033
RMSE for mean: 997.7600138929953</code></pre>
</div>
<p>Interestingly, for the Clickstream dataset, the mean imputation had the lowest RMSE score, in contrast to the results from the CO2 dataset. Let's visualize the results to get another perspective on performance:</p>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(clicks_original, clicks_missing, 'clicks')</code></pre>
</div>
<p>You get the plots as follows:</p>
<figure>
<img src="../media/file73.jpg" alt="Figure 7.10: Comparison between the three imputation methods for the Clickstream DataFrame" width="1129" height="829"/><figcaption aria-hidden="true">Figure 7.10: Comparison between the three imputation methods for the Clickstream DataFrame</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.10</em> with the original data in <em>Figure 7.8</em>. Notice that from imputing two different datasets (CO2 and Clickstream), there is no <em>one-size-fits-all strategy</em> when it comes to handling missing data. Instead, each dataset requires a different strategy. Therefore, you should always inspect your results and align the outputs with the expectations based on the nature of your data.</p>
</section>
<section id="how-it-works-27" class="level3" data-number="8.5.3">
<h3 data-number="8.5.3">How it works…</h3>
<p>Using <code>DataFrame.fillna()</code> is the simplest imputation method. In the previous section you used the <code>value</code> parameter within <code>.fillna()</code> where you passed the mean (a scalar numeric value) to use to fill for all missing values.</p>
<p>Other options used were <strong>backward filling</strong> with<code>.bfill()</code>, which uses the next observation, after the missing spot(s) and fills the gaps backward. You also used <strong>forward filling</strong> with <code>.ffill()</code>, which uses the last value, before the missing spot(s) and fills the gaps forward.</p>
</section>
<section id="theres-more-27" class="level3" data-number="8.5.4">
<h3 data-number="8.5.4">There's more…</h3>
<p>The <code>value</code> parameter in <code>.fillna()</code> can also take a Python <strong>dictionary</strong>, a pandas <strong>Series</strong>, or a pandas <strong>DataFrame</strong> and not just a <strong>scalar</strong>.</p>
<section id="using-a-python-dictionary" class="level4" data-number="8.5.4.1">
<h4 data-number="8.5.4.1">Using a Python Dictionary</h4>
<p>Let’s demonstrate this with another example how we can use a Python dictionary to impute missing values for multiple columns. Start by reading the <code>clicks_missing_more.csv</code> file:</p>
<div class="C0-SHCodePACKT">
<pre><code>clicks_missing = read_dataset(folder, 'clicks_missing_more.csv', 'date', index=True)
clicks_missing.isna().sum()</code></pre>
</div>
<p>This should produce the following:</p>
<div class="C0-SHCodePACKT">
<pre><code>price        5
location     6
clicks      16
dtype: int64</code></pre>
</div>
<p>Here we have three columns with missing values. We can use a dictionary to define a mapping in which each <strong>key-value</strong> pair corresponds to a column in the <code>clicks_missing</code> DataFrame. We can define different statistical measures (<em>median</em>, <em>mean</em>, and <em>mode</em>) for the imputation strategy for different columns. This is illustrated in the following code:</p>
<div class="C0-SHCodePACKT">
<pre><code>values = {'clicks': clicks_missing['clicks'].median(),
         'price': clicks_missing['clicks'].mean(),
         'location': clicks_missing['location'].mode()}
clicks_missing.fillna(value=values, inplace=True)
clicks_missing.isna().sum()</code></pre>
</div>
<p>The preceding code should produce the following results indicating all three columns have their missing values filled.</p>
<div class="C0-SHCodePACKT">
<pre><code>price       0
location    0
clicks      0
dtype: int64</code></pre>
</div>
<p>The <code>inplace=True</code> argument modifies the <code>clicks_missing</code> DataFrame in place, meaning the changes are applied directly to the DataFrame.</p>
</section>
<section id="using-another-dataframe" class="level4" data-number="8.5.4.2">
<h4 data-number="8.5.4.2">Using another DataFrame</h4>
<p>You can also use another pandas DataFrame (or Series) to impute missing values, the column names need to match in order to map the columns appropriately. In the following example you will read the <code>clicks_missing_more.csv</code> file and the <code>clicks_original.csv</code> file for demonstration.</p>
<div class="C0-SHCodePACKT">
<pre><code>clicks_missing = read_dataset(folder, 'clicks_missing_more.csv', 'date', index=True)
clicks_original = read_dataset(folder, 'clicks_original.csv', 'date', index=True)</code></pre>
</div>
<p>You will use the <code>clicks_original</code> DataFrame to impute missing values in the <code>clicks_missing</code> DataFrame.</p>
<div class="C0-SHCodePACKT">
<pre><code>clicks_missing.fillna(value=clicks_original, inplace=True)
clicks_missing.isna().sum()</code></pre>
</div>
<p>The preceding code should produce the following results indicating all three columns have their missing values filled.</p>
<div class="C0-SHCodePACKT">
<pre><code>price       0
location    0
clicks      0
dtype: int64</code></pre>
</div>
</section>
</section>
<section id="see-also-29" class="level3" data-number="8.5.5">
<h3 data-number="8.5.5">See also</h3>
<p>To learn more about <code>DataFrame.fillna()</code>, please visit the official documentation page here: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html</a>.</p>
<p>In the following recipe, you will perform similar univariate imputation, but this time using the <code>Scikit-Learn</code> library.</p>
</section>
</section>
<section id="handling-missing-data-with-univariate-imputation-using-scikit-learn" class="level2" data-number="8.6">
<h2 data-number="8.6">Handling missing data with univariate imputation using scikit-learn</h2>
<p><code>Scikit-Learn</code> is a very popular machine learning library in Python. The <code>scikit-learn</code> library offers a plethora of options for everyday machine learning tasks and algorithms such as classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.</p>
<p>Additionally, the library offers multiple options for univariate and multivariate data imputation.</p>
<section id="getting-ready-23" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1">Getting ready</h3>
<p>You can download the Jupyter notebooks and requisite datasets from the GitHub repository. Please refer to the <em>Technical requirements</em> section of this chapter.</p>
<p>This recipe will utilize the three functions prepared earlier (<code>read_dataset</code>, <code>rmse_score</code>, and <code>plot_dfs</code>).</p>
<p>You will be using four datasets from the <code>Ch7</code> folder: <code>clicks_original.csv</code>, <code>clicks_missing.csv</code>, <code>co2_original.csv</code>, and <code>co2_missing_only.csv</code>. The datasets are available from the GitHub repository.</p>
</section>
<section id="how-to-do-it-29" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2">How to do it…</h3>
<p>You will start by importing the libraries and then read all four CSV files:</p>
<ol>
<li>You will be using the <code>SimpleImputer</code> class from the scikit-learn library to perform univariate imputation:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from sklearn.impute import SimpleImputer
folder = Path('../../datasets/Ch7/')
co2_original = read_dataset(folder,
                            'co2_original.csv', 'year', index=True)
co2_missing = read_dataset(folder,
                           'co2_missing_only.csv', 'year', index=True)
clicks_original = read_dataset(folder,
                               'clicks_original.csv', 'date', index=True)
clicks_missing = read_dataset(folder,
                              'clicks_missing.csv', 'date', index=True)</code></pre>
</div>
<ol>
<li><code>SimpleImputer</code> accepts different values for the <code>strategy</code> parameter, including <code>mean</code>, <code>median</code>, and <code>most_frequent</code>. Let's explore all three strategies and see how they compare. Create a list of tuples for each method:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>strategy = [
    ('Mean Strategy', 'mean'),
    ('Median Strategy', 'median'),
    ('Most Frequent Strategy', 'most_frequent')]</code></pre>
</div>
<p>You can loop through the <code>Strategy</code> list to apply the different imputation strategies. <code>SimpleImptuer</code> has a <code>fit_transform</code> method. It combines two steps into one: fitting to the data (<code>.fit</code>), and then transforming the data (<code>.transform</code>).</p>
<p>Keep in mind that <code>SimpleImputer</code> accepts a NumPy array, so you will need to use the <code>Series.values</code> property followed by the <code>.reshape(-1, 1)</code> method to create a 2D NumPy array. Simply, what this is doing is transforming the 1D array from <code>.values</code> of shape <code>(226, )</code> to a 2D array of shape <code>(226, 1)</code>, which is a column vector:</p>
<div class="C1-SHCodePACKT">
<pre><code>co2_vals = co2_missing['co2'].values.reshape(-1,1)
clicks_vals = clicks_missing['clicks'].values.reshape(-1,1)
for s_name, s in strategy:
    co2_missing[s_name] = (
        SimpleImputer(strategy=s).fit_transform(co2_vals))
    clicks_missing[s_name] = (
        SimpleImputer(strategy=s).fit_transform(clicks_vals))</code></pre>
</div>
<p>Now, both the <code>clicks_missing</code> and <code>co2_missing</code> DataFrames have three additional columns, one for each of the imputation strategies implemented.</p>
<ol>
<li>Using the <code>rmse_score</code> function, you can now evaluate each strategy. Start with the CO2 data. You should get an output like the following:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(co2_original, co2_missing, 'co2')
&gt;&gt;
RMSE for Mean Strategy: 0.7156383637041684
RMSE for Median Strategy: 0.8029421606859859
RMSE for Most Frequent Strategy: 1.1245663822743381</code></pre>
</div>
<p>For the Clickstream data, you should get an output like the following:</p>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(clicks_original, clicks_missing, 'clicks')
&gt;&gt;
RMSE for Mean Strategy: 997.7600138929953
RMSE for Median Strategy: 959.3580492530756
RMSE for Most Frequent Strategy: 1097.6425985146868</code></pre>
</div>
<p>Notice how the RMSE strategy rankings vary between the two datasets. For example, the <code>Mean</code> strategy performed best on the CO2 data, while the <code>Median</code> strategy did best on the Clickstream data.</p>
<ol>
<li>Finally, use the <code>plot_dfs</code> function to plot the results. Start with the CO2 dataset:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(co2_original, co2_missing, 'co2')</code></pre>
</div>
<p>It produces the following plots:</p>
<figure>
<img src="../media/file74.jpg" alt="Figure 7.11: Comparing three SimpleImputer strategies for the CO2 dataset" width="1081" height="760"/><figcaption aria-hidden="true">Figure 7.11: Comparing three SimpleImputer strategies for the CO2 dataset</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.11</em> with the original data in <em>Figure 7.7</em>. For the Clickstream dataset, you should use the following:</p>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(clicks_original, clicks_missing, 'clicks')</code></pre>
</div>
<p>This should plot all three strategies:</p>
<figure>
<img src="../media/file75.jpg" alt="Figure 7.12: Comparing three SimpleImputer strategies for the Clickstream dataset" width="1129" height="827"/><figcaption aria-hidden="true">Figure 7.12: Comparing three SimpleImputer strategies for the Clickstream dataset</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.12</em> with the original data in <em>Figure 7.8</em>.</p>
<p><code>SimpleImputer</code> provides basic strategies that may be suitable with some data but not others. The advantage of these simple imputation strategies (including the ones from the previous <em>Handling missing data with univariate imputation using pandas</em> recipe) is that they are fast and straightforward to implement.</p>
</section>
<section id="how-it-works-28" class="level3" data-number="8.6.3">
<h3 data-number="8.6.3">How it works…</h3>
<p>You used the <code>SimpleImputer</code> class to implement three simple strategies to impute missing values: mean, median, and most frequent (mode).</p>
<p>This is a univariate imputation technique, meaning only one feature or column was used to compute the mean, median, and most frequent value.</p>
<p>The <code>SimpleImptuer</code> class has three parameters that you need to know:</p>
<ul>
<li><code>missing_values</code>, which, by default, is set to <code>nan</code>, and, more specifically, <code>np.nan</code>. NumPy <code>nan</code> and pandas <code>NaN</code> are similar, as you can see from the following example:</li>
</ul>
<div class="C1-SHCodePACKT">
<pre><code>test = pd.Series([np.nan, np.nan, np.nan])
test
&gt;&gt;
0   NaN
1   NaN
2   NaN
dtype: float64</code></pre>
</div>
<p><code>SimpleImputer</code> will impute all occurrences of the <code>missing_values</code>, which you can update with <code>pandas.NA</code>, an integer, float, or a string value.</p>
<ul>
<li><code>strategy</code>, which defaults to <code>mean</code>, and takes string values.</li>
<li><code>fill_value</code> can be used to replace all instances from <code>missing_values</code> with a specific value. This can either be a string or a numeric value. If the <code>strategy</code> was set to <code>constant</code>, then you will need to provide your custom <code>fill_value</code>.</li>
</ul>
<p>In Scikit-Learn, a common workflow for preprocessing data, such as imputation, involves two main steps:</p>
<ol>
<li><strong>Fitting the Imputer</strong>: First, you <strong>fit</strong> the imputer to the data using the <code>.fit()</code> method. This step involves "<strong>training</strong>" the imputer, which in the context of imputation means calculating the necessary statistics (like mean, median, etc.) from the provided data. The fitting process usually is done on the training dataset.</li>
<li><strong>Applying the Transform</strong>: After fitting, you apply the imputer to the data with the <code>.transform()</code> method. This step actually performs the imputation, replacing missing values with the computed statistics.</li>
</ol>
<p>In our example, these two steps were combined into one using the <code>.fit_transform()</code> method. This method first fits the imputer on the data (i.e., computes the necessary statistics) and then immediately applies the transformation (i.e., replaces missing values). Using <code>.fit_transform() </code>is a convenient approach, especially during the initial data preprocessing phase.</p>
<p>Additionally, the pandas DataFrame, <code>.fillna()</code>, can provide the same functionality as <code>SimpleImputer</code>. For example, the <code>mean</code> strategy can be accomplished by using the pandas <code>DataFrame.mean()</code> method and passing it to <code>.fillna()</code>.</p>
<p>The following example illustrates this and compares the two outcomes from Scikit-Learn and pandas:</p>
<div class="C0-SHCodePACKT">
<pre><code>avg = co2_missing['co2'].mean()
co2_missing['pands_fillna'] = co2_missing['co2'].fillna(avg)
cols = ['co2', 'Mean Strategy', 'pands_fillna']
_ = rmse_score(co2_original, co2_missing[cols], 'co2')
&gt;&gt;
RMSE for Mean Strategy: 0.7156383637041684
RMSE for pands_fillna: 0.7156383637041684</code></pre>
</div>
<p>Notice how you were able to accomplish the same results as the <code>SimpleImputer</code> class from scikit-learn. The <code>.fillna()</code> method makes it easier to scale the imputation across the entire DataFrame (column by column). For example, if you have a <code>sales_report_data</code> DataFrame with multiple columns containing missing data, you can perform a mean imputation with a single line, <code>sales_report_data.fillna(sales_report_data.mean()).</code></p>
</section>
<section id="theres-more-28" class="level3" data-number="8.6.4">
<h3 data-number="8.6.4">There’s more</h3>
<p>The add_indicator option in scikit-learn’s SimpleImputer is a useful feature for enhancing the imputation process. What it does, is add a MissingIndicator transform into the output (adds an additional binary column either indicating if original data was missing with 1 or observed with 0. This can be useful for encoding missing information as a feature which can provide additional insights.</p>
<p>The following is an example on how to enable this feature with <code>add_indicator=True</code>. You will use the <code>.fit() </code>followed by <code>.transform()</code> in this example:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_missing = read_dataset(folder,
                           'co2_missing_only.csv', 'year', index=True)
co2_vals = co2_missing['co2'].values.reshape(-1,1)
imputer = SimpleImputer(strategy='mean', add_indicator=True)
imputer.fit(co2_vals)
imputer.get_params()
&gt;&gt;
{'add_indicator': True,
 'copy': True,
 'fill_value': None,
 'keep_empty_features': False,
 'missing_values': nan,
 'strategy': 'mean'}</code></pre>
</div>
<p>You can then use <code>.transform()</code> and add the two columns to the original <code>co2_missing</code> DataFrame:</p>
<div class="C0-SHCodePACKT">
<pre><code>co2_missing[['imputed', 'indicator']] = (imputer.transform(co2_vals))
co2_missing.head(5)</code></pre>
</div>
<p>The preceding code should produce the following output</p>
<figure>
<img src="../media/file76.png" alt="Figure 7.13: Updating the co2_missing DataFrame with two columns" width="616" height="426"/><figcaption aria-hidden="true">Figure 7.13: Updating the co2_missing DataFrame with two columns</figcaption>
</figure>
<p>Notice how the indicator column is added in which 0 indicates original observed value and 1 indicates missing value.</p>
</section>
<section id="see-also-30" class="level3" data-number="8.6.5">
<h3 data-number="8.6.5">See also</h3>
<p>To learn more about scikit-learn's <code>SimpleImputer</code> class, please visit the official documentation page here: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer">https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer</a>.</p>
<p>So far, you have been dealing with univariate imputation. A more powerful approach is multivariate imputation, which you will learn in the following recipe.</p>
</section>
</section>
<section id="handling-missing-data-with-multivariate-imputation" class="level2" data-number="8.7">
<h2 data-number="8.7">Handling missing data with multivariate imputation</h2>
<p>Earlier, we discussed the fact that there are two approaches to imputing missing data: <strong>univariate</strong> <strong>imputation</strong> and <strong>multivariate</strong> <strong>imputation</strong>.</p>
<p>As you have seen in the previous recipes, univariate imputation involves using one variable (column) to substitute for the missing data, disregarding other variables in the dataset. Univariate imputation techniques are usually faster and simpler to implement, but a multivariate approach may produce better results in most situations.</p>
<p>Instead of using a single variable (column), in a multivariate imputation, the method uses multiple variables within the dataset to impute missing values. The idea is simple: Have more variables within the dataset chime in to improve the predictability of missing values.</p>
<p>In other words, univariate imputation methods handle missing values for a particular variable in isolation of the entire dataset and just focus on that variable to derive the estimates. In multivariate imputation, the assumption is that there is some synergy within the variables in the dataset, and collectively, they can provide better estimates to fill in for the missing values.</p>
<p>In this recipe, you will be working with the <code>Clickstream</code> dataset since it has additional variables (<code>clicks</code>, <code>price</code>, and <code>location</code> columns) to perform multivariate imputation for <em>clicks</em>.</p>
<section id="getting-ready-24" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1">Getting ready</h3>
<p>You can download the Jupyter notebooks and requisite datasets from the GitHub repository. Please refer to the <em>Technical requirements</em> section of this chapter.</p>
<p>In addition, you will leverage the three functions defined earlier in the chapter (<code>read_dataset</code>, <code>rmse_score</code>, and <code>plot_dfs</code>).</p>
</section>
<section id="how-to-do-it-30" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2">How to do it…</h3>
<p>In this recipe, you will use scikit-learn for the multivariate imputation. The library provides the <code>IterativeImputer</code> class, which allows you to pass a regressor to predict the missing values from other variables (columns) within the dataset:</p>
<ol>
<li>Start by importing the necessary libraries, methods, and classes:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import ExtraTreesRegressor, BaggingRegressor
from sklearn.linear_model import ElasticNet, LinearRegression
from sklearn.neighbors import KneighborsRegressor</code></pre>
</div>
<p>Load the two Clickstream datasets into DataFrames:</p>
<div class="C1-SHCodePACKT">
<pre><code>folder = Path('../../datasets/Ch7/')
clicks_original = read_dataset(folder,
                            'clicks_original.csv', 'date')
clicks_missing = read_dataset(folder,
                            'clicks_missing.csv', 'date')</code></pre>
</div>
<ol>
<li>With <code>IterativeImputer</code>, you can test different estimators. So, let's try different regressors and compare the results. Create a list of the regressors (estimators) to be used in <code>IterativeImputer</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>estimators = [
    ('bayesianRidge', BayesianRidge()),
    ('extra_trees', ExtraTreesRegressor(n_estimators=50)),
    ('bagging', BaggingRegressor(n_estimators=50)),
    ('elastic_net', ElasticNet()),
    ('linear_regression', LinearRegression()),
    ('knn', KNeighborsRegressor(n_neighbors=3))
]</code></pre>
</div>
<ol>
<li>Loop through the estimators and train on the dataset using <code>.fit()</code>, thereby building different models, and finally apply the imputation using <code>.transform()</code> on the variable with missing data. The results of each estimator will be appended as a new column to the <code>clicks_missing</code> DataFrame so that it can be used for scoring and compare the results visually:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>clicks_vals = clicks_missing.iloc[:,0:3].values
for e_name, e in estimators:
    est = IterativeImputer(
                random_state=15,
                estimator=e).fit(clicks_vals)
    clicks_missing[e_name] = est.transform(clicks_vals)[: , 2]</code></pre>
</div>
<ol>
<li>Using the <code>rmse_score</code> function, evaluate each estimator:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(clicks_original, clicks_missing, 'clicks')</code></pre>
</div>
<p>This should print the following scores:</p>
<div class="C1-SHCodePACKT">
<pre><code>RMSE for bayesianRidge: 949.4393973455851
RMSE for extra_trees: 1577.3003394830464
RMSE for bagging: 1237.4433923801062
RMSE for elastic_net: 945.40752093431
RMSE for linear_regression: 938.9419831427184
RMSE for knn: 1336.8798392251822</code></pre>
</div>
<p>Observe that Bayesian Ridge, ElasticNet, and Linear Regression produce similar results.</p>
<ol>
<li>Finally, plot the results for a visual comparison between the different estimators:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>plot_dfs(clicks_original, clicks_missing, 'clicks')</code></pre>
</div>
<p>The output is as follows:</p>
<figure>
<img src="../media/file77.jpg" alt="Figure 7.14: Comparing different estimators using IterativeImputation" width="835" height="981"/><figcaption aria-hidden="true">Figure 7.14: Comparing different estimators using IterativeImputation</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.14</em> with the original data in <em>Figure 7.8</em>.</p>
<p>At the beginning of the chapter, we discussed that using RMSE (<em>Root Mean Square Error)</em> for evaluating imputation methods can be somewhat misleading. This is because our objective with imputation is not necessarily to achieve the 'best' score (i.e., the smallest RMSE value), as we would aim for in predictive modeling. Instead, our goal with imputation is to fill missing data in a way that closely resembles the true nature and distribution of the original dataset.</p>
<p>While RMSE does have limitations, it can still provide valuable insights when comparing different imputation methods. It helps us understand which method estimates the missing values more closely to their actual values, based on the available data.</p>
<p>However, it's crucial to recognize that a lower RMSE doesn't always mean a more 'accurate' imputation in the context of real-world data. This is because real datasets often contain noise and randomness, which some imputation methods might fail to capture, especially those that produce the lowest RMSE scores. Methods like <code>BayesianRidge</code>, <code>ElasticNet</code>, and <code>Linear Regression</code> might yield lower RMSE values but could <strong>oversmooth</strong> the data (see <em>Figure 7.14</em> for those three estimators), failing to reflect the inherent randomness and variability present in real datasets.</p>
<p>Later, when using the imputed data for building predictive models (like forecasting models), we need to acknowledge that some level of imperfection in the imputed values is acceptable. This is because we often don't know the true nature of the missing data, and our aim is to create a dataset that provides a 'good enough' representation for model training and analysis. In essence, the objective is to achieve a balance – an imputation that provides a reasonable estimate of missing values while preserving the overall characteristics of the data, including its randomness and variability.</p>
</section>
<section id="how-it-works-29" class="level3" data-number="8.7.3">
<h3 data-number="8.7.3">How it works…</h3>
<p>The R MICE package inspired the <code>IterativeImputer</code> class from the scikit-learn library to implement <code>Multivariate Imputation by Chained Equation</code> (<a href="https://www.jstatsoft.org/article/view/v045i03">https://www.jstatsoft.org/article/view/v045i03</a>). <code>IterativeImputer</code> does differ from the original implementation, which you can read more about here: <a href="https://scikit-learn.org/stable/modules/impute.html#id2">https://scikit-learn.org/stable/modules/impute.html#id2</a>.</p>
<p>Keep in mind that <code>IterativeImputer</code> is still in experimental mode. In the next section, you will use another implementation of <code>MICE</code> from the <code>statsmodels</code> library.</p>
</section>
<section id="theres-more-29" class="level3" data-number="8.7.4">
<h3 data-number="8.7.4">There's more…</h3>
<p>The <code>statsmodels</code> library has an implementation of <code>MICE</code> that you can use to compare with <code>IterariveImputer</code>. This implementation is closer to the <code>MICE</code> implementation in R.</p>
<p>You will use the same DataFrames (<code>clicks_original</code> and <code>clicks_missing</code>) and append the <code>statsmodels</code> MICE imputation output to the <code>clicks_missing</code> DataFrame as an additional column.</p>
<p>Start by loading the required libraries:</p>
<div class="C0-SHCodePACKT">
<pre><code>from statsmodels.imputation.mice import MICE, MICEData, MICEResults
import statsmodels.api as sm</code></pre>
</div>
<p>Since your goal is to impute missing data, you can use the <code>MICEData</code> class to wrap the <code>clicks_missing</code> DataFrame. Start by creating an instance of <code>MICEData</code> and store it in a <code>mice_data</code> variable:</p>
<div class="C0-SHCodePACKT">
<pre><code># create a MICEData object
fltr = ['price', 'location','clicks']
mice_data = MICEData(clicks_missing[fltr],
                     perturbation_method='gaussian')
# 20 iterations
mice_data.update_all(n_iter=20)
mice_data.set_imputer('clicks', formula='~ price + location', model_class=sm.OLS)</code></pre>
</div>
<p>The <code>MICEData</code> prepares the dataset for MICE imputation and the <code>update_all()</code> method is called in a loop (20 times) to perform multiple iterations of imputations, each time refining the imputed values based on the other variables in the dataset. The <code>perturbation_method='gaussian'</code> specifies the method used for perturbing the missing data during the imputation process. The <code>'gaussian'</code> method adds noise drawn from a normal (gaussian) distribution.</p>
<p>Store the results in a new column and call it <code>MICE</code>. This way, you can compare the scores with results from <code>IterativeImputer</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code>clicks_missing['MICE']  = mice_data.data['clicks'].values.tolist()
_ = rmse_score(clicks_original, clicks_missing, 'clicks')
&gt;&gt;
RMSE for bayesianRidge: 949.4393973455851
RMSE for extra_trees: 1577.3003394830464
RMSE for bagging: 1237.4433923801062
RMSE for elastic_net: 945.40752093431
RMSE for linear_regression: 938.9419831427184
RMSE for knn: 1336.8798392251822
RMSE for MICE: 1367.190103013395</code></pre>
</div>
<p>Finally, visualize the results for a final comparison. This will include some of the imputations from <code>IterativeImputer</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code>cols = ['clicks','bayesianRidge', 'bagging', 'knn', 'MICE']
plot_dfs(clicks_original, clicks_missing[cols], 'clicks')</code></pre>
</div>
<p>The output is as follows:</p>
<figure>
<img src="../media/file78.jpg" alt="Figure 7.15 Comparing the statsmodels MICE implementation with the scikit-learn IterativeImputer" width="835" height="876"/><figcaption aria-hidden="true">Figure 7.15 Comparing the statsmodels MICE implementation with the scikit-learn IterativeImputer</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.15</em> with the original data in <em>Figure 7.8</em>.</p>
<p>Overall, multivariate imputation techniques generally produce better results than univariate methods. This is true when working with more complex time-series datasets in terms of the number of features (columns) and records. Though univariate imputers are more efficient in terms of speed and simplicity to interpret, there is a need to balance complexity, quality, and analytical requirements.</p>
</section>
<section id="see-also-31" class="level3" data-number="8.7.5">
<h3 data-number="8.7.5">See also</h3>
<ul>
<li>To learn more about <code>IterativeImputer</code>, please visit the official documentation page here: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer">https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer</a>.</li>
<li>To learn more about <code>statsmodels</code> MICE implementation, please visit the official documentation page here: <a href="https://www.statsmodels.org/dev/imputation.html">https://www.statsmodels.org/dev/imputation.html</a>.</li>
<li>An interesting library, <code>FancyImpute</code>, that originally inspired scikit-learn's <code>IterativeImputer</code> offers a variety of imputation algorithms that you can check out here: <a href="https://github.com/iskandr/fancyimpute">https://github.com/iskandr/fancyimpute</a>.</li>
</ul>
</section>
</section>
<section id="handling-missing-data-with-interpolation" class="level2" data-number="8.8">
<h2 data-number="8.8">Handling missing data with interpolation</h2>
<p>Another commonly used technique for imputing missing values is <em>interpolation</em>. The pandas library provides the <code>DataFrame.interpolate()</code> method for more complex univariate imputation strategies.</p>
<p>For example, one of the interpolation methods available is linear interpolation. <strong>Linear interpolation</strong> can be used to impute missing data by drawing a straight line between the two points surrounding the missing value (in time series, this means for a missing data point, it looks at a prior past value and the next future value to draw a line between them). A polynomial interpolation, on the other hand, will attempt to draw a curved line between the two points. Hence, each method will have a different mathematical operation to determine how to fill in for the missing data.</p>
<p>The interpolation capabilities in pandas can be extended further through the <strong>SciPy</strong> library, which offers additional univariate and multivariate interpolations.</p>
<p>In this recipe, you will use the pandas <code>DataFrame.interpolate()</code> function to examine different interpolation methods, including linear, polynomial, quadratic, nearest, and spline.</p>
<section id="getting-ready-25" class="level3" data-number="8.8.1">
<h3 data-number="8.8.1">Getting ready</h3>
<p>You can download the Jupyter notebooks and requisite datasets from the GitHub repository. Please refer to the <em>Technical requirements</em> section of this chapter.</p>
<p>You will utilize the three functions prepared earlier (<code>read_dataset</code>, <code>rmse_score</code>, and <code>plot_dfs</code>).</p>
<p>You will be using four datasets from the <code>Ch7</code> folder: <code>clicks_original.csv</code>, <code>clicks_missing.csv</code>, <code>co2_original.csv</code>, and <code>co2_missing_only.csv</code>. The datasets are available from the GitHub repository.</p>
</section>
<section id="how-to-do-it-31" class="level3" data-number="8.8.2">
<h3 data-number="8.8.2">How to do it…</h3>
<p>You will perform multiple interpolations on two different datasets and then compare the results using RMSE and visualization:</p>
<ol>
<li>Start by importing the libraries and reading the data into DataFrames:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>folder = Path('../../datasets/Ch7/')
co2_original = read_dataset(folder,
                            'co2_original.csv', 'year', index=True)
co2_missing = read_dataset(folder,
                           'co2_missing_only.csv', 'year', index=True)
clicks_original = read_dataset(folder,
                               'clicks_original.csv', 'date', index=True)
clicks_missing = read_dataset(folder,
                              'clicks_missing.csv', 'date', index=True)</code></pre>
</div>
<ol>
<li>Create a list of the interpolation methods to be tested: <code>linear</code>, <code>quadratic</code>, <code>nearest</code>, and <code>cubic</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>interpolations = [
    'linear',
    'quadratic',
    'nearest',
    'cubic'
]</code></pre>
</div>
<ol>
<li>You will loop through the list to run different interpolations using <code>.interpolate()</code>. Append a new column for each interpolation output to be used for comparison:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>for intp in interpolations:
    co2_missing[intp] = co2_missing['co2'].interpolate(method=intp)
    clicks_missing[intp] = clicks_missing['clicks'].interpolate(method=intp)</code></pre>
</div>
<ol>
<li>There are two additional methods that it would be interesting to test: <em>spline</em> and <em>polynomial</em>. To use these methods, you will need to provide an integer value for the order parameter. You can try <code>order = 2</code> for the spline method, and <code>order = 5</code> for the polynomial method. For the spline method, for example, it would look like this: <code>.interpolate(method="spline", order = 2)</code>:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>co2_missing['spline'] = \
        co2_missing['co2'].interpolate(method='spline', order=2)
clicks_missing['spline'] = \
        clicks_missing['clicks'].interpolate(method='spline',order=2)
co2_missing['polynomial'] = \
        co2_missing['co2'].interpolate(method='polynomial',order=5)
clicks_missing['polynomial'] = \
        clicks_missing['clicks'].interpolate(method='polynomial',order=5)</code></pre>
</div>
<ol>
<li>Use the <code>rmse_score</code> function to compare the results from the different interpolation strategies. Start with CO2 data:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(co2_original, co2_missing, 'co2')
&gt;&gt;
RMSE for linear: 0.05507291327761665
RMSE for quadratic: 0.08367561505614347
RMSE for nearest: 0.05385422309469095
RMSE for cubic: 0.08373627305833133
RMSE for spline: 0.1878602347541416
RMSE for polynomial: 0.06728323553134927</code></pre>
</div>
<p>Now, let's check the Clickstream data:</p>
<div class="C1-SHCodePACKT">
<pre><code>_ = rmse_score(clicks_original, clicks_missing, 'clicks')
&gt;&gt;
RMSE for linear: 1329.1448378562811
RMSE for quadratic: 5224.641260626975
RMSE for nearest: 1706.1853705030173
RMSE for cubic: 6199.304875782831
RMSE for spline: 5222.922993448641
RMSE for polynomial: 56757.29323647127</code></pre>
</div>
<ol>
<li>Lastly, visualize the results to gain a better idea of how each interpolation worked. Start with the CO2 dataset:</li>
</ol>
<div class="C1-SHCodePACKT">
<pre><code>cols = ['co2', 'linear', 'nearest', 'polynomial']
plot_dfs(co2_original, co2_missing[cols], 'co2')</code></pre>
</div>
<p>This should plot the selected columns:</p>
<figure>
<img src="../media/file79.jpg" alt="Figure 7.16: Comparing the different interpolation strategies on the CO2 dataset" width="801" height="565"/><figcaption aria-hidden="true">Figure 7.16: Comparing the different interpolation strategies on the CO2 dataset</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.16</em> with the original data in <em>Figure 7.7</em>.</p>
<p>Both the <code>linear</code> and <code>nearest</code> methods seem to have a similar effect regarding how the missing values were imputed. This can be seen from the RMSE scores and plot.</p>
<p>Now, create the plots for the Clickstream dataset:</p>
<div class="C1-SHCodePACKT">
<pre><code>cols = ['clicks', 'linear', 'nearest', 'polynomial', 'spline']
plot_dfs(clicks_original, clicks_missing[cols], 'clicks')</code></pre>
</div>
<p>This should plot the selected columns:</p>
<figure>
<img src="../media/file80.jpg" alt="Figure 7.17: Comparing the different interpolation strategies on the Clickstream dataset" width="844" height="679"/><figcaption aria-hidden="true">Figure 7.17: Comparing the different interpolation strategies on the Clickstream dataset</figcaption>
</figure>
<p>Compare the results in <em>Figure 7.17</em> with the original data in <em>Figure 7.8</em>.</p>
<p>From the output, you can see how the <code>polynomial</code> method exaggerated the curve when using <code>5</code> as the polynomial order. On the other hand, the <code>Linear</code> method attempts to draw a straight line.</p>
<p>One thing to note is that between the strategies implemented, only linear interpolation ignores the index, while the rest use numerical values for the index.</p>
</section>
<section id="how-it-works-30" class="level3" data-number="8.8.3">
<h3 data-number="8.8.3">How it works…</h3>
<p>Overall, the interpolation technique detects patterns in neighboring data points (to the missing points) to predict what the missing values should be. The simplest form is linear interpolation, which assumes a straight line between two neighboring data points. On the other hand, a polynomial defines a curve between the two adjacent data points. Each interpolation method uses a different function and mechanism to predict the missing data.</p>
<p>In pandas, you will use the <code>DataFrame.interpolate</code> function. The default interpolation method is the linear interpolation (<code>method = "linear"</code>). There are additional parameters to provide more control over how the imputation with interpolation is done.</p>
<p>The <code>limit</code> parameter allows you to set the maximum number of consecutive <code>NaN</code> to fill. Recall in the previous recipe, <em>Performing data quality checks</em>, that the Clickstream dataset had <code>16</code> consecutive missing points. You can limit the number of consecutive <code>NaN</code>, for example, to <code>5</code>:</p>
<div class="C0-SHCodePACKT">
<pre><code>clicks_missing['clicks'].isna().sum()
&gt;&gt; 16
example = clicks_missing['clicks'].interpolate(limit = 5)
example.isna().sum()
&gt;&gt; 11</code></pre>
</div>
<p>Only 5 data points were imputed; the remaining 11 were not.</p>
</section>
<section id="theres-more-30" class="level3" data-number="8.8.4">
<h3 data-number="8.8.4">There's more…</h3>
<p>Other libraries also offer interpolation, including the following:</p>
<ul>
<li>SciPy provides a more extensive selection covering univariate and multivariate techniques: <a href="https://docs.scipy.org/doc/scipy/reference/interpolate.html">https://docs.scipy.org/doc/scipy/reference/interpolate.html</a>.</li>
<li>NumPy offers a couple of interpolation options; the most widely used is the <code>numpy.interp()</code> function: <a href="https://numpy.org/doc/stable/reference/generated/numpy.interp.html?highlight=interp#numpy.interp.">https://numpy.org/doc/stable/reference/generated/numpy.interp.html?highlight=interp#numpy.interp.</a></li>
</ul>
</section>
<section id="see-also-32" class="level3" data-number="8.8.5">
<h3 data-number="8.8.5">See also</h3>
<p>To learn more about <code>DataFrame.interpolate</code>, please visit the official documentation page here: <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html</a>.</p>
</section>
</section>
</section>
</div>
</div>
</body>
</html>