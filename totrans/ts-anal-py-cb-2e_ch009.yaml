- en: 8 Outlier Detection Using Statistical Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/file0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://packt.link/zmkOY](https://packt.link/zmkOY)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to missing data, as discussed in *Chapter 7*, *Handling Missing
    Data*, a common data issue you may face is the presence of **outliers**. Outliers
    can be point outliers, collective outliers, or contextual outliers. For example,
    a **point outlier** occurs when a data point deviates from the rest of the population—sometimes
    referred to as a **global outlier**. **Collective outliers**, which are groups
    of observations, differ from the population and don't follow the expected pattern.
    Lastly, **contextual outliers** occur when an observation is considered an outlier
    based on a particular condition or context, such as deviation from neighboring
    data points. Note that with contextual outliers, the same observation may not
    be considered an outlier if the context changes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to a handful of practical statistical
    techniques that cover parametric and non-parametric methods. In *Chapter 14*,
    *Outlier Detection Using Unsupervised Machine Learning*, you will dive into more
    advanced machine learning and deep learning-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the literature, you will find another popular term, **anomaly detection**,
    which can be synonymous with **outlier detection**. The methods and techniques
    to identify outlier or anomaly observations are similar; the difference lies in
    the context and the actions that follow once these points have been identified.
    For example, an outlier transaction in financial transactions may be referred
    to as an anomaly and trigger a fraud investigation to stop them from re-occurring.
    Under a different context, survey data with outlier data points may simply be
    removed by the researchers once they examine the overall impact of keeping versus
    removing such points. Sometimes you may decide to keep these outlier points if
    they are part of the natural process. In other words, they are legitimate and
    opt to use robust statistical methods that are not influenced by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Another concept, known as **change point detection** (**CPD**), relates to outlier
    detection. In CPD, the goal is to anticipate abrupt and impactful fluctuations
    (increasing or decreasing) in the time series data. CPD covers specific techniques,
    for example, **cumulative sum** (**CUSUM**) and **Bayesian online change point
    detection** (**BOCPD**). Detecting change is vital in many situations. For example,
    a machine may break if the internal temperature reaches a certain point or if
    you are trying to understand whether the discounted price did increase sales or
    not. This distinction between outlier detection and CPD is vital since sometimes
    you may want the latter. Where the two disciplines converge, depending on the
    context, sudden changes may indicate the potential presence of outliers (anomalies).
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipes that you will encounter in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Resampling time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers using visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers using the Tukey method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers using a z-score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers using a modified z-score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers using other non-parametric methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download the Jupyter Notebooks and needed datasets from the GitHub
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jupyter Notebook: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook-Second-Edition/blob/main/code/Ch8/Chapter%208.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook-Second-Edition/blob/main/code/Ch8/Chapter%208.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datasets: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook-Second-Edition/tree/main/datasets/Ch8](https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook-Second-Edition/tree/main/datasets/Ch8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughout the chapter, you will be using a dataset from the **Numenta Anomaly
    Benchmark** (**NAB**), which provides outlier detection benchmark datasets. For
    more information about NAB, please visit their GitHub repository here: [https://github.com/numenta/NAB](https://github.com/numenta/NAB).'
  prefs: []
  type: TYPE_NORMAL
- en: The *New York Taxi dataset* captures the number of NYC taxi passengers at a
    specific timestamp. The data contains known anomalies that are provided to evaluate
    the performance of our outlier detectors. The dataset contains *10,320* records
    between *July 1, 2014*, to *May 31, 2015*. The observations are captured in a
    30-minute interval, which translates to `freq = ‘30T’`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prepare for the outlier detection recipes, start by loading the libraries
    that you will be using throughout the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `nyc_taxi.csv` data into a pandas DataFrame as it will be used throughout
    the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can store the known dates containing outliers, also known as **ground truth
    labels**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you investigate these dates to gain more insight into their significance,
    you will find similar information to the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Saturday, November 1, 2014*, was before the New York Marathon, and the official
    marathon event was on Sunday, November 2, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Thursday, November 27, 2014*, was Thanksgiving Day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Thursday, December 25, 2014*, was Christmas Day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Thursday, January 1, 2015*, was New Year’s Day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tuesday, January 27, 2015*, was the North American Blizzard where all vehicles
    were ordered off the street from January 26 to January 27, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can plot the time series data to gain an intuition on the data you will
    be working with for outlier detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a time series with a 30-minute frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: Plot of the New York City taxi time series data](img/file82.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Plot of the New York City taxi time series data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, create the `plot_outliers` function that you will use throughout the
    recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we proceed with the outlier detection recipes, the goal is to see how the
    different techniques capture outliers and compare them to the ground truth labels.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The presence of outliers requires special handling and further investigation
    before hastily jumping to decisions on how to handle them. First, you will need
    to detect and spot their existence, which this chapter is all about. Domain knowledge
    can be instrumental in determining whether these identified points are outliers,
    their impact on your analysis, and how you should deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers can indicate bad data due to a random variation in the process, known
    as noise, or due to data entry error, faulty sensors, bad experiment, or natural
    variation. Outliers are usually undesirable if they seem synthetic, for example,
    bad data. On the other hand, if outliers are a natural part of the process, you
    may need to rethink removing them and opt to keep these data points. In such circumstances,
    you can rely on **non-parametric** statistical methods that do not make assumptions
    on the underlying distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, outliers can cause side effects when building a model based on strong
    assumptions on the data distribution; for example, the data is from a Gaussian
    (normal) distribution. Statistical methods and tests based on assumptions of the
    underlying distribution are referred to as **parametric methods**.
  prefs: []
  type: TYPE_NORMAL
- en: There is no fixed protocol for dealing with outliers, and the magnitude of their
    impact will vary. For example, sometimes you may need to test your model with
    outliers and again without outliers to understand the overall impact on your analysis.
    In other words, not all outliers are created, nor should they be treated equally.
    However, as stated earlier, having domain knowledge is essential when dealing
    with these outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before using a dataset to build a model, you will need to test for the
    presence of such outliers so you can further investigate their significance. Spotting
    outliers is usually part of the data cleansing and preparation process before
    going deep into your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A common approach to handling outliers is to delete these data points and not
    have them be part of the analysis or model development. Alternatively, you may
    wish to replace the outliers using similar techniques highlighted in *Chapter
    7, Handling Missing Data*, such as imputation and interpolation. Other methods,
    such as smoothing the data, could minimize the impact of outliers. Smoothing,
    such as exponential smoothing, is discussed in *Chapter 10, Building Univariate
    Time Series Models Using Statistical Methods* . You may also opt to keep the outliers
    and use more resilient algorithms to their effect.
  prefs: []
  type: TYPE_NORMAL
- en: There are many well-known methods for outlier detection. The area of research
    is evolving, ranging from basic statistical techniques to more advanced approaches
    leveraging neural networks and deep learning. In statistical methods, you have
    different tools that you can leverage, such as the use of visualizations (boxplots,
    QQ-plots, histograms, and scatter plots), z-score, **interquartile range** (**IQR**)
    and Tukey fences, and statistical tests such as Grubbs test, the Tietjen-Moore
    test, or the generalized **Extreme Studentized Deviate** (**ESD**) test. These
    are basic, easy to interpret, and effective methods.
  prefs: []
  type: TYPE_NORMAL
- en: In your first recipe, you will be introduced to a crucial time series transformation
    technique known as resampling before diving into outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A typical transformation that is done on time series data is **resampling**.
    The process implies changing the frequency or level of granularity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, you will have limited control over how the time series is generated
    in terms of frequency. For example, the data can be generated and stored in small
    intervals, such as milliseconds, minutes, or hours. In some cases, the data can
    be in larger intervals, such as daily, weekly, or monthly.
  prefs: []
  type: TYPE_NORMAL
- en: The need for resampling time series can be driven by the nature of your analysis
    and at what granular level you need your data to be. For instance, you can have
    daily data, but your analysis requires the data to be weekly, and thus you will
    need to resample. This process is known as **downsampling**. When you are downsampling,
    you will need to provide some level of aggregation, such as mean, sum, min, or
    max, to name a few. On the other hand, some situations require you to resample
    your data from daily to hourly. This process is known as **upsampling**. When
    upsampling, you may introduce null rows, which you can fill using imputation or
    interpolation techniques. See *Chapter 7*, *Handling Missing Data*, where both
    imputation and interpolation methods were discussed in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will explore how resampling is done using the `pandas` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, you will work with the `nyc_taxis` DataFrame created earlier
    in the *Technical requirements* section. The data captures the number of passengers
    in *30-minute* intervals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Downsample** the data to a daily frequency. Currently, you have `10,320`
    records, and when you resample the data to daily, you will need to aggregate the
    data. In this example, you will use the `.mean()` function. This will reduce the
    number of samples to 215 records. The term *downsampling* indicates that the number
    of samples went down, and more concretely, we are decreasing the frequency of
    the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect the first five rows of the original DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Resampling is done using the `DataFrame.resample()` function. For daily, you
    will use `''D''` as the date offset rule, followed by `.mean()` as the method
    of aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how `DatetimeIndex` is now at a daily frequency, and the number of passengers
    now reflects the daily average. Inspect the first `DatetimeIndex` to check its
    frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check frequency directly using the `.freq` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the number of records now after downsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, now you have `215` records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Resample the `nyc_taxi` data again, but this time as a 3-day frequency. You
    can do this by using the offset string `''3D''`. This time, use the `.sum()` method
    instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the frequency of `DatetimeIndex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you use `df_downsampled.shape` you will notice the number of records got
    reduced to 72 records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, change the frequency to three (3) business days instead. The default in
    `pandas` is Monday to Friday. In the *Working with custom business days recipe*
    in *Chapter 6*, *Working with Date and Time in Python*, You learned how to create
    custom business days. For now, you will use the default definition of business
    days. If you observe the output from the previous step, `2014-07-13` falls on
    a Sunday. Using `''3B''` as the `DateOffset` will push it to the following Monday,
    `2014-07-14`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Interesting output in how it skips 5 days from 2014-07-04 to 2014-07-09, and
    then again from 2014-07-09 to 2014-07-14\. The reason is the *Business Day rule*
    which specifies we have two (2) days of the week as weekends. Since the function
    is calendar-aware, it knows a weekend is coming after the first 3-day increment,
    so it adds a 2-day weekend to skip them, and thus, it makes a 5-day jump. Starting
    from 2014-07-04, thus moving to 2014-07-09, and from 2017-07-09 moving to 2017-07-14\.
    And similarly, from 2014-07-17 to 2014-07-22, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let''s **upsample** the data from a 30-minute interval (frequency)
    to a 15-minutes frequency. This will create an empty entry (`NaN`) between every
    other entry. You will use offset string `''T''` for minutes, since `''M''` is
    used for monthly aggregation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that upsampling creates `NaN` rows. Unlike *downsampling*, when *upsampling*,
    you need to give instructions on how to fill the `NaN` rows. You might be wondering
    why we used `.mean()` here? The simple answer is because it would not matter whether
    you used `.sum()`, `.max()`, or `.min()`, for example. You will need to augment
    the missing rows using imputation or interpolation techniques when you *upsample*.
    For example, you can specify an imputation value using `.fillna()` or by using
    `.ffill()` or `.bfill() methods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first five records show the use of forward filling. For more information
    on using `.fillna()`, `.bfill()`, or `.ffill()` or imputation in general, refer
    to *Chapter 7, Handling Missing Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, resampling in pandas is very convenient and straightforward. This can
    be a handy tool when you want to change the frequency of your time series.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `DataFrame.resample()` method allows you to group rows in a specified time
    frame, for example, by day, week, month, year, or any DateTime attribute. The
    way `.resample()` works is by grouping the data using the `DatetimeIndex` and
    the frequency provided, hence, this method is specific to time series DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: The `.resample()` function works in a very similar manner to the `.groupby()`
    function; the difference is that `.resample()` is specific to time series data
    and groups at the `DatetimeIndex`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can supply more than one aggregation at once when downsampling using the
    `.agg()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, using `''M''` for monthly, you can supply the `.agg()` function
    with a list of aggregations you want to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a DataFrame with five columns, one for each aggregation
    method specified. The index column, a `timestamp` column, will be grouped at monthly
    intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: Multiple aggregations using the .agg() method](img/file83.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Multiple aggregations using the .agg() method'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the default behavior for `'M'` or monthly frequency is at the month's
    end (example *2014-07-31*). You can change to month's start instead by using `'MS'`.
    For example, this will produce *2014-07-01* instead (the beginning of each month).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To learn more about pandas `DataFrame.resample()`, please visit the official
    documentation here: [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting outliers using visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two general approaches for using statistical techniques to detect
    outliers: **parametric** and **non-parametric** methods. **Parametric** methods
    assume you know the underlying distribution of the data. For example, if your
    data follows a normal distribution. On the other hand, in **non-parametric** methods,
    you make no such assumptions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using histograms and box plots are basic non-parametric techniques that can
    provide insight into the distribution of the data and the presence of outliers.
    More specifically, box plots, also known as **box and whisker** plots, provide
    a five-number summary: the *minimum*, *first quartile* (25th percentile), *median*
    (50th percentile), *third quartile* (75th percentile), and the *maximum*. There
    are different implementations for how far the whiskers extend, for example, the
    whiskers can extend to the *minimum* and *maximum* values. In most statistical
    software, including Python''s **matplotlib** and **seaborn** libraries, the whiskers
    extend to what is called **Tukey''s Lower and Upper Fences**. Any data point outside
    these boundaries is considered a potential outlier. You will dive into the actual
    calculation and implementation in the *Detecting outliers using the Tukey method*
    recipe. For now, let''s focus on the visualization aspect of the analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will use **seaborn** as another Python visualization library
    that is based on **matplotlib**`.`
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can download the Jupyter Notebooks and required datasets from the GitHub
    repository. Please refer to the *Technical requirements* section of this chapter.
    You will be using the `nyc_taxi` DataFrame that you loaded earlier in the *Technical
    requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: You will be using `seaborn` version *0.11.2*, which is the latest version as
    of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `seaborn` using `pip`, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To install `seaborn` using `conda`, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, you will explore different plots available from `seaborn` including
    `histplot()`, `displot()`, `boxplot()`, `boxenplot()`, and `violinplot()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that these plots tell a similar story but visually, each plot
    represents the information differently. Eventually, you will develop a preference
    toward some of these plots for your own use when investigating your data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the `seaborn` library to begin exploring how these plots
    work to help you detect outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Recall from *Figure 8.1*, the `nyc_taxi` DataFrame contains passenger counts
    recorded every 30 minutes. Keep in mind that every analysis or investigation is
    unique and so should be your approach to align with the problem you are solving.
    This also means that you will need to consider your data preparation approach,
    for example, determine what transformations you need to apply to your data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this recipe, the goal is to find which days have outlier observations, not
    at which interval within the day, so you will *resample* the data to a daily frequency.
    You will start by **downsampling** the data using the `mean` aggregation. Even
    though such a transformation will smooth out the data, you will not lose too much
    of the detail as it pertains to finding outliers since the `mean` is sensitive
    to outliers. In other words, if there was an extreme outlier on a specific interval
    (there are 48 intervals in a day), the `mean` will still carry that information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step you will Downsample the data to a daily frequency using the `.resample()`
    method and using `‘D’` as the offset string. This will reduce the number of observations
    from `10,320` to `215` (which is `10320/48 = 215`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the new `tx` DataFrame with the ground truth labels to use as a reference.
    You will call the `plot_outliers` function that you created from the *Technical
    requirements* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This should produce a time series plot with `X` markers for known outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Plotting the NYC Taxi data after downsampling with ground truth
    labels (outliers)](img/file84.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Plotting the NYC Taxi data after downsampling with ground truth
    labels (outliers)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start with your first plot for inspecting your time series data
    using the `histplot()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Histogram showing extreme daily mean passenger rides](img/file85.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Histogram showing extreme daily mean passenger rides'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 8.4*, the observations labeled as `1`, `2`, `3`, `4`, and `5` seem
    to represent extreme passenger values. Recall, these numbers represent the average
    daily passengers after resampling. The question you should ask is whether these
    observations are outliers. The center of the histogram is close to 15,000 daily
    average passengers. This should make you question whether the extreme value close
    to 20,000 (*label* `5`) is that extreme. Similarly, the observations labeled `3`
    and `4` (since they are close to the tail of the distribution), are they actually
    extreme values? How about labels `1` and `2` with average passenger rides at 3,000
    and 8,000 daily average passengers respectively? These do seem more extreme compared
    to the rest and may potentially be actual outliers. Again, determining what is
    an outlier and what is not requires domain knowledge and further analysis. There
    is no specific rule, and you will see throughout this chapter that some of the
    generally accepted rules are arbitrary and subjective. You should not jump to
    conclusions immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can achieve a similar plot using `displot()`, which has a `kind` parameter.
    The `kind` parameter can take one of three values: `hist` for the histogram plot,
    `kde` for the kernel density estimate plot, and `ecdf` for the empirical cumulative
    distribution function plot.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will use `displot(kind=''hist'')` to plot a similar histogram as the one
    in *Figure 8.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A box plot provides more information than a histogram and can be a better choice
    for spotting outliers. In a box plot, observations that are outside the whiskers
    or boundaries are considered potential outliers. The whiskers represent the visual
    boundary for the upper and lower fences as proposed by mathematician **John Tukey**
    in 1977.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code shows how to create a box plot using `seaborn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the potential outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: A box plot showing potential outliers that are outside the boundaries
    (whiskers)](img/file86.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: A box plot showing potential outliers that are outside the boundaries
    (whiskers)'
  prefs: []
  type: TYPE_NORMAL
- en: The width of the box (**Q1** to **Q3**) is called the **interquartile range**
    (**IQR**) calculated as the difference between the 75th and 25th percentiles (**Q3**
    – **Q1**). The lower fence is calculated as `Q1 - (1.5 x IQR)`, and the upper
    fence as `Q3 + (1.5 x IQR)`. Any observation less than the lower boundary or greater
    than the upper boundary is considered a potential outlier. More on that in the
    *Detecting outliers using the Tukey method* recipe. The `whis` parameter in the
    `boxplot()` function is set to `1.5` by default (1.5 times IQR), which controls
    the width or distance between the upper and lower fences. Larger values mean fewer
    observations will be deemed as outliers, and smaller values will make non-outlier
    points seem outside of boundaries (more outliers).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two more variations for box plots in **seaborn** (`boxenplot()` and
    `violinplot()`). They provide similar insight as to the boxplot but are presented
    differently. The *boxen plot*, which in literature is referred to as a *letter-value*
    plot, can be considered as an enhancement to regular box plots to address some
    of their shortcomings, as described in the paper *Heike Hofmann, Hadley Wickham
    & Karen Kafadar (2017) Letter-Value Plots: Boxplots for Large Data, Journal of
    Computational and Graphical Statistics, 26:3, 469-477*. More specifically, boxen
    (letter-value) plots are better suited when working with larger datasets (higher
    number of observations for displaying data distribution and more suitable for
    differentiating outlier points for larger datasets). The `seaborn` implementation
    of `boxenplot` is based on that paper.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code shows how to create a boxen (letter-value) plot using `seaborn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This should produce a plot that looks like the box plot in *Figure 8.5*, but
    with boxes extending beyond the quartiles (**Q1**, **Q2**, and **Q3**). The 25th
    percentile is at the 14,205 daily average passengers mark, and the 75th percentile
    is at the 16,209 daily average passengers mark.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: A boxen (letter-value) plot for the average daily taxi passengers
    in NYC](img/file87.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: A boxen (letter-value) plot for the average daily taxi passengers
    in NYC'
  prefs: []
  type: TYPE_NORMAL
- en: PERCENTILE VALUES
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are you wondering how I was able to determine the exact value for the 25th,
    50th, and 75th percentiles?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can obtain these values for a DataFrame or series with the `describe` method.
    For example, if you run `tx.describe()`, you should see a table of descriptive
    statistics that includes count, mean, standard deviation, minimum, maximum, 25th,
    50th, and 75th percentile values for the dataset.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: In *Figure 8.6*, you are getting additional insight into the distribution of
    passengers beyond the quantiles. In other words, it extends the box plot to show
    additional distributions to give more insight into the tail of the data. The boxes
    in theory could keep going to accommodate all the data points, but in order to
    show outliers, there needs to be a stopping point, referred to as **depth**. In
    `seaborn`, this parameter is called `k_depth`, which can take a numeric value,
    or you can specify different methods such as `tukey`, `proportion`, `trustworthy`,
    or `full`. For example, a `k_depth=1` numeric value will show a similar box to
    the boxplot in *Figure 8.5* (one box). As a reference, *Figure 8.6* shows four
    boxes determined using the `Tukey` method, which is the default value (`k_depth="tukey"`).
    Using `k_depth=4` would produce the same plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'These methods are explained in the referenced paper by *Heike Hofmann, Hadley
    Wickham & Karen Kafadar (2017)*. To explore the different methods, you can try
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce four plots; notice the different numbers of boxes that
    were determined by each method. Recall, you can also specify `k_depth` numerically
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: The different k_depth methods available in seaborn for the boxenplot
    function](img/file88.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: The different k_depth methods available in seaborn for the boxenplot
    function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the final variation is the violin plot, which you can display using the
    `violinplot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This should produce a plot that is a hybrid between a box plot and a **kernel
    density estimation** (**KDE**). A kernel is a function that estimates the probability
    density function, the larger peaks (wider area), for example, show where the majority
    of the points are concentrated. This means that there is a higher probability
    that a data point will be in that region as opposed to the much thinner regions
    showing much lower probability.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: A violin plot for the average daily taxi passengers in NYC](img/file89.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: A violin plot for the average daily taxi passengers in NYC'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that *Figure 8.8* shows the distribution for the entire dataset. Another
    observation is the number of peaks; in this case, we have one peak, which makes
    it a unimodal distribution. If there is more than one peak, we call it a multimodal
    distribution, which should trigger a further investigation into the data. A KDE
    plot will provide similar insight as a histogram but with a more smoothed curve.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the recipe, we were introduced to several plots that help visualize the distribution
    of the data and show outliers. Generally, histograms are great for showing distribution,
    but a box plot (and its variants) are much better for outlier detection. We also
    explored the boxen (letter-value) plot, which is more suited for larger datasets
    and is more appropriate than regular box plots.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In **seaboarn** both `histlpot()` and `displot()` support adding the kernel
    density estimate (KDE) plot with the parameter `kde`. The following code snippets
    should produce a similar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a plot that combines a histogram and a KDE plot as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9: A histogram with a KDE plot](img/file90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: A histogram with a KDE plot'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, another useful visualization for spotting outliers is the **lag
    plot**. A lag plot is essentially a scatter plot, but instead of plotting two
    variables to observe correlation, as an example, we plot the same variable against
    its lagged version. This means, it is a scatter plot using the same variable,
    but the *y* axis represents passenger count at the current time (*t*) and the
    *x* axis will show passenger count at a prior period (*t-1*), which we call **lag**.
    The lag parameter determines how many periods to go back; for example, a lag of
    `1` means one period back, while a lag of `2` means two periods back. In our resampled
    data (downsampled to daily), a lag of `1` represents the prior day.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pandas library provides the `lag_plot` function, which you can use as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10: A lag plot of average daily taxi passengers in NYC](img/file91.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: A lag plot of average daily taxi passengers in NYC'
  prefs: []
  type: TYPE_NORMAL
- en: The circled data points highlight interesting points that can be potential outliers.
    Some seem more extreme than others. Further, you can see some linear relationship
    between the passenger counts and its lagged version (prior day) indicating the
    existence of an autocorrelation. Recall from basic statistics that correlation
    shows the relationship between two independent variables, so you can think of
    autocorrelation as a correlation of a variable at a time (*t*) and its prior version
    at a time (*t-1*). More on this in *Chapter 9, Exploratory Data Analysis and Diagnosis*,
    and *Chapter 10, Building Univariate Time Series Models Using Statistical Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The labels for the *x*-axis and the *y*-axis in *Figure 8.10* can be a bit
    confusing, with the *y-axis* being labeled as *y(t+1)*. Essentially it is saying
    the same thing we described earlier: the *x*-axis represents prior values (the
    predictor) to its future self at *t+1*, which is what the *y*-axis represents.
    To make it clearer, you can recreate the exact visualization produced by `lag_plot`
    using `seaborn` manually, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This should produce a similar plot to that in *Figure 8.10*.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in the code that the `y` values start from *t+1* (we skipped the value
    at index `0`) up to the last observation, and the `x` values start from index
    `0` up to index `-1` (we skip the last observation). This makes the values in
    the *y* axis ahead by one period.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, we will dive further into **IQR** and **Tukey fences** that
    we briefly discussed when talking about box plots.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can learn more about the plots we used and the different options available
    from the `seaborn` documentation. To learn more about the following, visit the
    associated URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: For box plots (`boxplot`), you can visit [https://seaborn.pydata.org/generated/seaborn.boxplot.html](https://seaborn.pydata.org/generated/seaborn.boxplot.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For boxen plots (`boxenplot`), you can visit [https://seaborn.pydata.org/generated/seaborn.boxenplot.html](https://seaborn.pydata.org/generated/seaborn.boxenplot.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For violin plots (`violinplot`), you can visit [https://seaborn.pydata.org/generated/seaborn.violinplot.html#seaborn.violinplot](https://seaborn.pydata.org/generated/seaborn.violinplot.html#seaborn.violinplot).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For histograms (`histpolot`), you can visit [https://seaborn.pydata.org/generated/seaborn.histplot.html](https://seaborn.pydata.org/generated/seaborn.histplot.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For distribution plots (`displot`), you can visit [https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot](https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers using the Tukey method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will extend on the previous recipe, *Detecting outliers using visualizations*.
    In *Figure 8.5*, the box plot showed the quartiles with whiskers extending to
    the upper and lower fences. These boundaries or fences were calculated using the
    Tukey method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s expand on *Figure 8.5* with additional information of other components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11: Box plot for the daily average taxi passengers data](img/file92.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Box plot for the daily average taxi passengers data'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizations are great to give you a high-level perspective on the data you
    are dealing with, such as the overall distribution and potential outliers. Ultimately
    you want to identify these outliers programmatically so you can isolate these
    data points for further investigation and analysis. This recipe will teach you
    how to calculate IQR and define points that fall outside the lower and upper Tukey
    fences.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most statistical methods allow you to spot extreme values beyond a certain threshold.
    For example, this could be the mean, standard deviation, the 10th or 90th percentile,
    or some other value that you want to compare against. You will start the recipe
    by learning how to obtain basic descriptive statistics and more specifically,
    the quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both DataFrame and Series have the `describe` method that outputs summary descriptive
    statistics. By default, it shows the quartiles: the first quartile, which is the
    25th percentile, the second quartile (median), which is the 50th percentile, and
    the third quartile, which is the 75th percentile. You can customize the percentiles
    by providing a list of values to the `percentiles` parameter. The following code
    shows how you can get values for additional percentiles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12: Descriptive statistics with custom percentiles for the daily
    taxi passenger data](img/file93.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Descriptive statistics with custom percentiles for the daily taxi
    passenger data'
  prefs: []
  type: TYPE_NORMAL
- en: QUANTILES VERSUS QUARTILES VERSUS PERCENTILES
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The terms can be confusing, but essentially both percentiles and quartiles are
    quantiles. Sometimes you will see people use percentiles more loosely and interchangeably
    with quantiles.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quartiles divide your distribution into four segments (hence the name) marked
    as *Q1* (25th percentile), *Q2* (50th percentile or Median), and *Q3* (75th percentile).
    Percentiles, on the other hand, can take any range from 0 to 100 (in pandas from
    0 to 1, while in NumPy from 0 to 100), but most commonly refer to when the distribution
    is partitioned into 100 segments. These segments are called quantiles.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The names pretty much indicate the type of partitioning (number of quantiles)
    applied on the distribution; for example, with four quantiles we call it quartiles,
    with two quantiles we call it median, with 10 quantiles we call it deciles, and
    with 100 quantiles we call it percentiles.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The NumPy library also offers the `percentile` function, which would return
    the value(s) for the specified percentiles. The following code explains how this
    can be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 8.11*, notice that most extreme values, potential outliers, fall
    below the lower fence calculated as `Q1 – (1.5 x IQR)` or above the upper fence
    calculated as `Q3 + (1.5 x IQR)`. IQR is calculated as the difference between
    *Q3* and *Q1* (`IQR = Q3 – Q1`), which determines the width of the box in the
    box plot. These upper and lower fences are known as **Tukey's fences**, and more
    specifically, they are referred to as **inner boundaries**. The **outer boundaries**
    also have lower `Q1 - (3.0 x IQR)` and upper `Q3 + (3.0 x IQR)` fences. We will
    focus on the inner boundaries and describe anything outside of those as potential
    outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will create a function, `iqr_outliers`, which calculates the IQR, upper
    (inner) fence, lower (inner) fence, and then filters the data to return the outliers.
    These outliers are any data points that are below the lower fence or above the
    upper fence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the function by passing the `tx` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: These dates (points) are the same ones identified in *Figure 8.5* and *Figure
    8.11* as outliers based on Tukey's fences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `plot_outliers` function defined earlier in the *Technical requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a plot similar to that in *Figure 8.3*, except the `x`
    markers are based on the outliers identified using the Tukey method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13: Daily average taxi passengers and outliers identified using
    the Tukey method](img/file94.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Daily average taxi passengers and outliers identified using the
    Tukey method'
  prefs: []
  type: TYPE_NORMAL
- en: Compare *Figures 8.13* and *Figure 8.3* and you will see that this simple method
    did a great job at identifying four of the five known outliers. In addition, Tukey's
    method identified two additional outliers on *2014-12-26* and *2015-01-26*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using IQR and Tukey's fences is a simple non-parametric statistical method.
    Most box plot implementations use `1.5x(IQR)` to define the upper and lower fences.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of `1.5x(IQR)` is common when it comes to defining outliers; the choice
    is still arbitrary, even though there is a lot of discussion about its reasoning.
    You can change the value for more experimentation. For example, in `seaborn,`
    you can change the default `1.5` value by updating the `whis` parameter in the
    `boxplot` function. The choice of `1.5` makes the most sense when the data follows
    a Gaussian distribution (normal), but this is not always the case. Generally,
    the larger the value, the fewer outliers you will capture as you expand your boundaries
    (fences). Similarly, the smaller the value, the more non-outliers will be defined
    as outliers, as you are shrinking the boundaries (fences).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s update the `iqr_outliers` function to accept a `p` parameter so you
    can experiment with different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the function on different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The best value will depend on your data and how sensitive you need the outlier
    detection to be.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To learn more about Tukey''s fences for outlier detection, you can refer to
    this Wikipedia page: [https://en.wikipedia.org/wiki/Outlier#Tukey''s_fences](https://en.wikipedia.org/wiki/Outlier#Tukey''s_fences).'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore another statistical method based on a z-score in the following
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting outliers using a z-score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **z-score** is a common transformation for standardizing data. This is
    common when you want to compare different datasets. For example, it is easier
    to compare two data points from two different datasets relative to their distributions.
    This can be done because the z-score standardizes the data to be centered around
    a zero mean and the units represent standard deviations away from the mean. For
    example, in our dataset, the unit is measured in daily taxi passengers (in thousands).
    Once you apply the z-score transformation, you are no longer dealing with the
    number of passengers, but rather, the units represent standard deviation, which
    tells us how far an observation is from the mean. Here is the formula for the
    z-score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file95.jpg)Where![](img/file96.png)is a data point (an observation),
    mu (![](img/file97.png)) is the mean of the dataset, and sigma (![](img/file98.png))
    is the standard deviation for the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the z-score is a lossless transformation, which means you
    will not lose information such as its distribution (shape of the data) or the
    relationship between the observations. All that is changing is the units of measurement
    as they are being scaled (standardized).
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is transformed using the z-score, you can pick a threshold. So,
    any data point above or below that threshold (in standard deviation) is considered
    an outlier. For example, your threshold can be `+3` and `-3` standard deviations
    away from the mean. Any point lower than `-3` or higher than `+3` standard deviation
    can be considered an outlier. In other words, the further a point is from the
    mean, the higher the probability of it being an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: The z-score has one major shortcoming due to it being a parametric statistical
    method based on assumptions. It assumes a Gaussian (normal) distribution. So,
    suppose the data is not normal. In that case, you will need to use a modified
    version of the z-score, which is discussed in the following recipe, *Detecting
    outliers using a modified z-score*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will start by creating the `zscore` function that takes in a dataset and
    a threshold value that we will call `degree`. The function will return the standardized
    data and the identified outliers. These outliers are any points above the positive
    threshold or below the negative threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `zscore()` function to standardize the data and filter out the extreme
    values based on a threshold. Recall, the threshold is based on the standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the `zscore` function and store the returned objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the effect of the z-score transformation, you can plot a histogram.
    The transformed DataFrame contains two columns, the original data labeled `value`
    and the standardized data labeled `zscore`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce two histograms for the two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14: Histogram to compare the distribution of the original and standardized
    data](img/file99.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: Histogram to compare the distribution of the original and standardized
    data'
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the shape of the data did not change, hence why the z-score is called
    a *lossless transformation*. The only difference between the two is the scale
    (units).
  prefs: []
  type: TYPE_NORMAL
- en: 'You ran the `zscore` function using a threshold of `2.5`, meaning any data
    point that is 2.5 standard deviations away from the mean in either direction.
    For example, any data point that is above the `+2.5` standard deviations or below
    the `-2.5` standard deviations will be considered an outlier. Print out the results
    captured in the `outliers` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This simple method managed to capture three out of the five known outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `plot_outliers` function defined earlier in the *Technical requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a plot similar to that in *Figure 8.3*, except the `x`
    markers are based on the outliers identified using the z-score method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15: Daily average taxi passengers and outliers identified using
    the z-score method](img/file100.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15: Daily average taxi passengers and outliers identified using the
    z-score method'
  prefs: []
  type: TYPE_NORMAL
- en: You will need to play around to determine the best threshold value. The larger
    the threshold, the fewer outliers you will capture, and the smaller the threshold,
    the more non-outliers will be labeled as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s create a `plot_zscore` function that takes the standardized
    data to plot the data with the threshold lines. This way you can visually see
    how the threshold is isolating extreme values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the function using a threshold of `2.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a scatter plot with two horizontal lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16: Plot of the standardized data and outliers based on the threshold
    lines](img/file101.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: Plot of the standardized data and outliers based on the threshold
    lines'
  prefs: []
  type: TYPE_NORMAL
- en: The four circled data points represent the outliers that were returned by the
    `zscore` function. Run the function using different threshold values to gain a
    deeper understanding of this simple technique.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The z-score method is a very simple and interpretable method. The z-scores are
    interpreted as standard deviation units away from the mean, which is the center
    of the distribution. Since we are subtracting the mean from all observations,
    we are essentially mean-centering the data. We also divide by the standard deviation
    to standardize the data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.15* pretty much explains the understanding of this method. Once the
    data is standardized, it became easy to just use the standard deviation threshold.
    If the data was not standardized, it may have been challenging to determine the
    threshold based on daily passengers.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The z-score is a parametric method and assumes the data comes from a Gaussian
    (normal) distribution. There are several tests available in the `statsmodels`
    library to test if the data is normally distributed. One of these tests is the
    *Kolmogorov-Smirnov* test. The null hypothesis is that the data comes from a normal
    distribution. The test returns the test statistics and a `p`-value; if the `p`-value
    is less than `0.05`, you can reject the null hypothesis (data is not normally
    distributed). Otherwise, you would fail to reject the null hypothesis (data is
    normally distributed).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will use the `kstest_normal` function from the `statsmodels` library. To
    make the results easier to interpret, create the `test_normal` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the test using the `test_normal` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the dataset is not normally distributed. In Chapter 9, Exploratory
    Data Analysis and Diagnosis, you will learn about additional normality tests under
    the Applying power transformations recipe. But do be cautious; these tests will
    usually fail in the presence of outliers. If your data fails a normality test,
    then use some of the plotting methods discussed in the Detecting outliers using
    visualizations recipe to examine any outliers that may be causing the test to
    fail.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To read more about z-scores and standardization, you can refer to this Wikipedia
    page: [https://en.wikipedia.org/wiki/Standard_score](https://en.wikipedia.org/wiki/Standard_score).'
  prefs: []
  type: TYPE_NORMAL
- en: In the following recipe, you will explore a very similar method to the z-score
    that is more robust to outliers and is more suitable with non-normal data.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting outliers using a modified z-score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *Detecting outliers using a z-score* recipe, you experienced how simple
    and intuitive the method is. But it has one major drawback: it assumes your data
    is normally distributed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But, what if your data is not normally distributed? Luckily, there is a modified
    version of the z-score to work with non-normal data. The main difference between
    the regular z-score and the modified z-score is that we replace the mean with
    the median:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file102.jpg)Where![](img/file103.png)(*tilde x*) is the median of the
    dataset, and MAD is the median absolute deviation of the dataset:![](img/file104.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `0.6745` value is the standard deviation unit that corresponds to the 75th
    percentile (*Q3*) in a Gaussian distribution and is used as a normalization factor.
    In other words, it is used to approximate the standard deviation. This way, the
    units you obtain from this method are measured in standard deviation, similar
    to how you would interpret the regular z-score.
  prefs: []
  type: TYPE_NORMAL
- en: You can obtain this value using SciPy's **percent point function** (**PPF**),
    also known as the inverse of the **cumulative distribution function** (**CDF**).
    Simply give the PPF function a percentile, for example, 75%, and it will return
    the quantile corresponding to the lower tail probability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This is the normalization factor used in the formula.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the modified z-score is sometimes referred to as the **robust z-score**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overall, the approach will work exactly as the steps used when using the standard
    z-score method. You will start by creating the `modified_zscore` function that
    takes in a dataset, and a threshold value we will call `degree`, and the function
    will return the standardized data as well as the identified outliers. These outliers
    are any points above the positive threshold or below the negative threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `modified_zscore` `()` function to standardize the data and filter
    out the extreme values based on a threshold. Recall, the threshold is based on
    the standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the `modified_zscore` function and store the returned objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: To see the effect of the modified z-score transformation, let's plot a histogram.
    The transformed DataFrame contains two columns, the original data labeled `value`
    and the standardized data labeled `zscore`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce two histograms for the two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17: Histogram to compare the distribution of the original and modified
    z-score standardized data](img/file105.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: Histogram to compare the distribution of the original and modified
    z-score standardized data'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the results from *Figure 8.16* with *Figure 8.13*. Both approaches,
    the z-score and modified z-score approaches, do not change the shape of the data.
    The difference is in the scaling factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `modified_zscore` function using a threshold of `3`, meaning any data
    point three standard deviations away from the median in either direction. For
    example, any data point above `+3` standard deviations or below `-3` standard
    deviations will be considered an outlier. Print out the results captured in the
    `outliers` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the modified z-score did a much better job capturing four out
    of the five known outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `plot_outliers` function defined earlier in the *Technical requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a plot similar to that in *Figure 8.3,* except the **x**
    markers are based on the outliers identified using the modified z-score method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18: Daily average taxi passengers and outliers identified using
    the modified z-score method](img/file106.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Daily average taxi passengers and outliers identified using the
    modified z-score method'
  prefs: []
  type: TYPE_NORMAL
- en: You will need to play around to determine the best threshold value. The larger
    the threshold, the fewer outliers you will capture, and the smaller the threshold,
    the more non-outliers will be labeled as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s create a `plot_m_zscore` function that takes the standardized
    data to plot the data with the threshold lines. This way you can visually see
    how the threshold is isolating extreme values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the function using a threshold of `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a scatter plot with two horizontal lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19: Plot of the standardized data and outliers based on the threshold
    lines](img/file107.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.19: Plot of the standardized data and outliers based on the threshold
    lines'
  prefs: []
  type: TYPE_NORMAL
- en: The six circled data points represent the outliers that were returned by the
    `modified_score` function. Run the function using different threshold values to
    gain more profound intuition into this simple technique.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in *Figure 8.19* how we have a data point that is right at the threshold
    line. Would you consider this an outlier? Generally, when it comes to outlier
    detection you will still need to apply due diligence to inspect the results.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The modified z-score (robust z-score) method is very similar to the z-score
    approach, as it depends on defining a standard deviation threshold. What makes
    this method more robust to outliers is the use of the median instead of the mean.
    We also use the **median absolute deviation** (**MAD**) instead of the standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous recipe, *Detecting outliers using a z-score*, we used `kstest_normal`
    from `statsmodels` to test normality.
  prefs: []
  type: TYPE_NORMAL
- en: Another helpful plot that is specifically designed to test for normality and
    sometimes can help detect outliers is the **Quantile-Quantile plot** (**QQ-plot**).
  prefs: []
  type: TYPE_NORMAL
- en: You can plot a QQ-plot using SciPy or `statsmodels`. Both will produce the same
    plot. The following code will show you can plot using either.
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows how you can plot using SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows how you can plot using `statsmodels`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Both SciPy and `statsmodels` will produce the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20: QQ-plot comparing the taxi passenger data against a hypothetical
    normal distribution](img/file108.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.20: QQ-plot comparing the taxi passenger data against a hypothetical
    normal distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The solid line represents a reference line for what normally distributed data
    would look like. If the data you are comparing is normally distributed, all the
    points will lie on that straight line. In *Figure 8.19*, we can see that the distribution
    is almost normal (not perfect), and we see issues toward the distribution's tails.
    This also aligns with what we have seen in *Figure 8.16* and *Figure 8.13,* showing
    the majority of the outliers are at the bottom tail end (less than `-2` standard
    deviation).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To learn more about MAD, you can refer to the Wikipedia page here: [https://en.wikipedia.org/wiki/Median_absolute_deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation).'
  prefs: []
  type: TYPE_NORMAL
