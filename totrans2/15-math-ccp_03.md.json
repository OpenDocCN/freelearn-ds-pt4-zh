["```py\n import numpy as np\n# Set parameters of the Binomial distribution\n# We will use 20 trials and a success probability of 0.6\nN = 20\np = 0.6\n# Calculate the cumulative probability distribution\n# of the Binomial\ncumulative_probs = cumulative_binomial_probability(N=N,p=p)\n# Set how many random numbers we want to generate\n# and initialize an empty array to hold them\nn_random = 1000\nrandom_binomial = np.empty(n_random, dtype=np.int8)\n# Loop to generate the random Binomial numbers\nfor i in range(n_random):\n     # Draw a random number from U(0,1)\n     r_tmp = np.random.rand(1)\n     # See where the cumulative probability distribution first exceeds\n     # the uniform random number we just created\n     random_binomial[i] = np.where(\n        cumulative_probs - r_tmp > 0.0)[0][0]\n```", "```py\n # Generate Binomial random numbers directly using numpy\nimport numpy as np\nN = 20\np = 0.6\nn_random = 1000\nrandom_binomial2 = np.random.binomial(n=N, p=p, size=n_random)\n```", "```py\n # Using numpy to sample from a Poisson distribution\n# We set the mean number of counts to 5.2\n# This is specified by the 'lam' argument of the\n# numpy function. import numpy as np\nn_random = 10000\nrandom_poisson = np.random.poisson(lam=5.2, size=n_random)\n```", "```py\n # Using numpy to sample from a Gaussian distribution\n# We set the population mean=2.0 and the\n# standard deviation=1.5. import numpy as np\nmean = 2.0\nsd = 1.5\n# We first sample from the standard Gaussian\n# distribution (zero mean and unit variance)\n# and apply a simple linear transformation\nn_random = 10000\nrandom_gaussian1 = mean + (sd*np.random.randn(n_random))\n# Alternatively, we can use the normal function of\n# numpy.random without having to apply the linear\n# transformation\nrandom_gaussian2 = np.random.normal(\n    loc=mean, scale=sd, size=n_random)\n```", "```py\n import numpy as np\nimport matplotlib.pyplot as plt\n## Central Limit Theorem Uniform Example\nn_random_var = 20 # The number of random variables we add together\nn_simulations = 100000 # The number or experiments we will run\n# Initialize an array to hold the results of the experiments\ntotal_of_random_vars = np.zeros(n_simulations)\n# Loop over the experiments\nfor i in range(n_simulations):\n     # generate n_random_var Uniform(0, 1) values and sum them\n     total_of_random_vars[i] = np.sum(np.random.rand(n_random_var))\n# Plot the frequencies of the experiment totals as a histogram\nplot = plt.hist(total_of_random_vars, bins=100, label='Sum')\n# Construct expected frequencies according to Central Limit Theorem\n# The mean of the sum of the random variables:\nmean_CLT = n_random_var * 0.5 \n# The variance of the sum of the random variables:\nvar_CLT = n_random_var /12.0 \n# The standard deviation of the sum of the random variables:\nstd_CLT = np.sqrt(var_CLT) \n# Store the x values at which we which to calculate the \n# CLT approximation. # Since we want to plot the CLT approximation on top of the \n# histogram we\n# will use the positions of the histogram bins as our x values\nx_values = plot[1]\n# Initialize an array to hold the results of the CLT approximation\nclt_expected_frequency = np.zeros(len(x_values)-1)\n# Loop over the x values\nfor i in range(len(x_values)-1):\n     # Since we are comparing to the frequencies in the histogram we \n     # need to calculate\n     # the expect number of experiments whose sum is in between the \n     # current and next\n     # x value. This expected frequency is equal to the total number \n     # of experiments\n     # multiplied by the probability of an experiment having its sum \n     # between the current\n     # and next x value. This probability is the difference in \n     # cumulative probability\n     # between those two x values. Since we are using the CLT, we are \n     # approximating the\n     # probability density function of an experiment sum by a Normal \n     # distribution with\n     # mean = number of random variables * mean of each random \n     # variable, and\n     # variance = number of random variables * variance of each random \n     # variable. # To calculate the cumulative probability of a Normal \n     # distribution we use the\n     # scipy.norm.cdf function. cumulative_prob1 = norm.cdf((x_values[i+1] - mean_CLT)/std_CLT)\n     cumulative_prob2 = norm.cdf((x_values[i] - mean_CLT)/std_CLT)\n     cumulative_prob_diff = cumulative_prob1 - cumulative_prob2\n     clt_expected_frequency[i] = n_simulations * cumulative_prob_diff\n# Plot the CLT expected frequencies on top of the histogram\nplt.plot(x_values[:-1], clt_expected_frequency, 'red', \n         label='CLT Approx.')\n# Add graph annotation and display\nplt.legend(loc='upper right')\nplt.title('CLT demonstration adding 20 Uniform(0, 1) \\\n          random variables')\nplt.xlabel('Sum of random variables')\nplt.ylabel('Frequency')\nplt.show()\n```", "```py\n ## Kernel density estimation example\nimport numpy as np\nfrom scipy.stats import norm, gamma, iqr\nfrom sklearn.neighbors import KernelDensity\nimport matplotlib.pyplot as plt\n# Set the seed for the random number generator\nnp.random.seed(280)\n# First we sample our x values\nshape = 5.0\nscale = 1.0\nn_sample = 30 # The number of data points in our sample\nx_sample = np.random.gamma(\n    shape=shape, scale=scale, size=(n_sample,1))\n# Calculate summary statistics of the sample\nsample_std = np.std(x_sample)\nsample_iqr = iqr(x_sample)\n# Construct the kernel density function using a \n# Parzen (Gaussian window)\n# and using the default bandwidth (=1.0)\nparzen_kde = KernelDensity(kernel='gaussian').fit(x_sample)\n# Construct x values at which we want to calculate the\n# kernel density estimate\nx = np.linspace(0, 2.0*np.max(x_sample),200)\n# Calculate the log of the kernel density estimate for the \n# regular spaced points\nlog_density_parzen = parzen_kde.score_samples(x.reshape(200,1))\nplt.plot(x, np.exp(log_density_parzen), label='KDE:Parzen')\n# Repeat kernel density estimation using a different kernel function\n# We use a exponential kernel. We will set the bandwidth using\n# Silverman's rule of thumb\nbandwidth = 0.9*np.min(\n    [sample_std, sample_iqr/1.35]) * np.power(float(n_sample), -0.2)\nexponential_kde = KernelDensity(\n    kernel='exponential', bandwidth=bandwidth).fit(x_sample)\nlog_density_exponential = exponential_kde.score_samples(\n    x.reshape(200,1))\nplt.plot(x, np.exp(log_density_exponential), 'red',\n    label='KDE:Exponential')\n# Add the true density function\nplt.plot(x, gamma.pdf(x, shape)/scale, 'green', label='True Density')\n# Add graph annotation and display\nplt.legend(loc='upper right')\nplt.title('Kernel density estimates from gamma random values')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.show()\n```"]